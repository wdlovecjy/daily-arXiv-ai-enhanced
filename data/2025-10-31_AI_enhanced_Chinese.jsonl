{"id": "2510.25961", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.25961", "abs": "https://arxiv.org/abs/2510.25961", "authors": ["Amanda Glazer"], "title": "Tractable Algorithms for Changepoint Detection in Player Performance Metrics", "comment": null, "summary": "We present tractable methods for detecting changes in player performance\nmetrics and apply these methods to Major League Baseball (MLB) batting and\npitching data from the 2023 and 2024 seasons. First, we derive principled\nbenchmarks for when performance metrics can be considered statistically\nreliable, assuming no underlying change, using distributional assumptions and\nstandard concentration inequalities. We then propose a changepoint detection\nalgorithm that combines a likelihood-based approach with split-sample inference\nto control false positives, using either nonparametric tests or tests\nappropriate to the underlying data distribution. These tests incorporate a\nshift parameter, allowing users to specify the minimum magnitude of change to\ndetect. We demonstrate the utility of this approach across several baseball\napplications: detecting changes in batter plate discipline metrics (e.g., chase\nand whiff rate), identifying velocity changes in pitcher fastballs, and\nvalidating velocity changepoints against a curated ground-truth dataset of\npitchers who transitioned from relief to starting roles. Our method flags\nmeaningful changes in 91% of these `ground-truth' cases and reveals that, for\nsome metrics, more than 60% of detected changes occur in-season. While\ndeveloped for baseball, the proposed framework is broadly applicable to any\nsetting involving monitoring of individual performance over time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u68c0\u6d4b\u68d2\u7403\u8fd0\u52a8\u5458\u8868\u73b0\u53d8\u5316\u7684\u53ef\u5904\u7406\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e2023-2024\u8d5b\u5b63MLB\u6570\u636e\uff0c\u5305\u62ec\u57fa\u4e8e\u7edf\u8ba1\u539f\u7406\u7684\u6027\u80fd\u6307\u6807\u53ef\u9760\u6027\u57fa\u51c6\u548c\u7ed3\u5408\u4f3c\u7136\u65b9\u6cd5\u4e0e\u5206\u5272\u6837\u672c\u63a8\u65ad\u7684\u53d8\u70b9\u68c0\u6d4b\u7b97\u6cd5\u3002", "motivation": "\u5f00\u53d1\u7cfb\u7edf\u65b9\u6cd5\u6765\u68c0\u6d4b\u68d2\u7403\u8fd0\u52a8\u5458\u8868\u73b0\u6307\u6807\u7684\u663e\u8457\u53d8\u5316\uff0c\u4e3a\u7403\u5458\u8868\u73b0\u8bc4\u4f30\u63d0\u4f9b\u7edf\u8ba1\u4e0a\u53ef\u9760\u7684\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u9996\u5148\u5efa\u7acb\u6027\u80fd\u6307\u6807\u7684\u7edf\u8ba1\u53ef\u9760\u6027\u57fa\u51c6\uff0c\u7136\u540e\u63d0\u51fa\u7ed3\u5408\u4f3c\u7136\u65b9\u6cd5\u548c\u5206\u5272\u6837\u672c\u63a8\u65ad\u7684\u53d8\u70b9\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4f7f\u7528\u975e\u53c2\u6570\u68c0\u9a8c\u6216\u9002\u5408\u6570\u636e\u5206\u5e03\u7684\u68c0\u9a8c\uff0c\u5e76\u5305\u542b\u504f\u79fb\u53c2\u6570\u4ee5\u6307\u5b9a\u6700\u5c0f\u68c0\u6d4b\u53d8\u5316\u5e45\u5ea6\u3002", "result": "\u65b9\u6cd5\u572891%\u7684\u771f\u5b9e\u6848\u4f8b\u4e2d\u68c0\u6d4b\u5230\u6709\u610f\u4e49\u7684\u53d8\u5316\uff0c\u67d0\u4e9b\u6307\u6807\u8d85\u8fc760%\u7684\u53d8\u5316\u53d1\u751f\u5728\u8d5b\u5b63\u4e2d\uff0c\u5e76\u5728\u51fb\u7403\u5458\u677f\u7eaa\u5f8b\u6307\u6807\u548c\u6295\u624b\u7403\u901f\u53d8\u5316\u68c0\u6d4b\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u68d2\u7403\u6570\u636e\u5206\u6790\uff0c\u8fd8\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4efb\u4f55\u6d89\u53ca\u4e2a\u4f53\u8868\u73b0\u968f\u65f6\u95f4\u76d1\u6d4b\u7684\u573a\u666f\u3002"}}
{"id": "2510.26496", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.26496", "abs": "https://arxiv.org/abs/2510.26496", "authors": ["Dimas Abreu Archanjo Dutra"], "title": "Variational System Identification of Aircraft", "comment": "AIAA Paper number AIAA 2025-1253. Presented at the AIAA SciTech 2025\n  Forum", "summary": "Variational system identification is a new formulation of maximum likelihood\nfor estimation of parameters of dynamical systems subject to process and\nmeasurement noise, such as aircraft flying in turbulence. This formulation is\nan alternative to the filter-error method that circumvents the solution of a\nRiccati equation and does not have problems with unstable predictors. In this\npaper, variational system identification is demonstrated for estimating\naircraft parameters from real flight-test data. The results show that, in real\napplications of practical interest, it has better convergence properties than\nthe filter-error method, reaching the optimum even when null initial guesses\nare used for all parameters and decision variables. This paper also presents\nthe theory behind the method and practical recommendations for its use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u7cfb\u7edf\u8fa8\u8bc6\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u4f20\u7edf\u6ee4\u6ce2\u8bef\u5dee\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u4f30\u8ba1\u53d7\u8fc7\u7a0b\u548c\u6d4b\u91cf\u566a\u58f0\u5f71\u54cd\u7684\u52a8\u6001\u7cfb\u7edf\u53c2\u6570\uff0c\u5e76\u5728\u771f\u5b9e\u98de\u884c\u6d4b\u8bd5\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6ee4\u6ce2\u8bef\u5dee\u65b9\u6cd5\u5728\u52a8\u6001\u7cfb\u7edf\u53c2\u6570\u4f30\u8ba1\u4e2d\u5b58\u5728Riccati\u65b9\u7a0b\u6c42\u89e3\u56f0\u96be\u548c\u9884\u6d4b\u5668\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u8fa8\u8bc6\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53d8\u5206\u7cfb\u7edf\u8fa8\u8bc6\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u65b0\u516c\u5f0f\uff0c\u907f\u514d\u4e86Riccati\u65b9\u7a0b\u7684\u6c42\u89e3\uff0c\u5e76\u80fd\u5904\u7406\u4e0d\u7a33\u5b9a\u9884\u6d4b\u5668\u7684\u60c5\u51b5\u3002", "result": "\u5728\u771f\u5b9e\u98de\u884c\u6d4b\u8bd5\u6570\u636e\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u6bd4\u6ee4\u6ce2\u8bef\u5dee\u65b9\u6cd5\u66f4\u597d\u7684\u6536\u655b\u7279\u6027\uff0c\u5373\u4f7f\u6240\u6709\u53c2\u6570\u548c\u51b3\u7b56\u53d8\u91cf\u4f7f\u7528\u96f6\u521d\u59cb\u731c\u6d4b\u4e5f\u80fd\u8fbe\u5230\u6700\u4f18\u89e3\u3002", "conclusion": "\u53d8\u5206\u7cfb\u7edf\u8fa8\u8bc6\u65b9\u6cd5\u5728\u5b9e\u8df5\u5e94\u7528\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u672c\u6587\u8fd8\u63d0\u4f9b\u4e86\u8be5\u65b9\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u4f7f\u7528\u5efa\u8bae\u3002"}}
{"id": "2510.25811", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.25811", "abs": "https://arxiv.org/abs/2510.25811", "authors": ["William R\u00e9veillard", "Richard Combes"], "title": "Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "comment": "31 pages; NeurIPS 2025", "summary": "We consider a stochastic multi-armed bandit problem with i.i.d. rewards where\nthe expected reward function is multimodal with at most m modes. We propose the\nfirst known computationally tractable algorithm for computing the solution to\nthe Graves-Lai optimization problem, which in turn enables the implementation\nof asymptotically optimal algorithms for this bandit problem. The code for the\nproposed algorithms is publicly available at\nhttps://github.com/wilrev/MultimodalBandits", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97Graves-Lai\u4f18\u5316\u95ee\u9898\u89e3\u7684\u53ef\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u6700\u591am\u4e2a\u6a21\u6001\u7684\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6e10\u8fd1\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5177\u6709\u591a\u6a21\u6001\u671f\u671b\u5956\u52b1\u51fd\u6570\u7684\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u53ef\u884c\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u7b2c\u4e00\u4e2a\u5df2\u77e5\u7684\u8ba1\u7b97\u53ef\u884c\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97Graves-Lai\u4f18\u5316\u95ee\u9898\u7684\u89e3\uff0c\u4ece\u800c\u80fd\u591f\u5b9e\u73b0\u8be5\u8001\u864e\u673a\u95ee\u9898\u7684\u6e10\u8fd1\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u8ba1\u7b97Graves-Lai\u4f18\u5316\u95ee\u9898\u89e3\u7684\u7b97\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u6e10\u8fd1\u6700\u4f18\u7684\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u63d0\u4f9b\u4e86\u9996\u4e2a\u8ba1\u7b97\u53ef\u884c\u7684\u6e10\u8fd1\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2510.25781", "categories": ["cs.LG", "cs.AI", "cs.NA", "cs.NE", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.25781", "abs": "https://arxiv.org/abs/2510.25781", "authors": ["Amir Noorizadegan", "Sifan Wang", "Leevan Ling"], "title": "A Practitioner's Guide to Kolmogorov-Arnold Networks", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional Multilayer Perceptrons (MLPs), inspired by the\nKolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed\nactivation functions on nodes, KANs employ learnable univariate basis functions\non edges, offering enhanced expressivity and interpretability. This review\nprovides a systematic and comprehensive overview of the rapidly expanding KAN\nlandscape, moving beyond simple performance comparisons to offer a structured\nsynthesis of theoretical foundations, architectural variants, and practical\nimplementation strategies. By collecting and categorizing a vast array of\nopen-source implementations, we map the vibrant ecosystem supporting KAN\ndevelopment. We begin by bridging the conceptual gap between KANs and MLPs,\nestablishing their formal equivalence and highlighting the superior parameter\nefficiency of the KAN formulation. A central theme of our review is the\ncritical role of the basis function; we survey a wide array of choices,\nincluding B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,\nGaussian RBFs, and Fourier series, and analyze their respective trade-offs in\nterms of smoothness, locality, and computational cost. We then categorize\nrecent advancements into a clear roadmap, covering techniques for improving\naccuracy, efficiency, and regularization. Key topics include physics-informed\nloss design, adaptive sampling, domain decomposition, hybrid architectures, and\nspecialized methods for handling discontinuities. Finally, we provide a\npractical \"Choose-Your-KAN\" guide to help practitioners select appropriate\narchitectures, and we conclude by identifying current research gaps. The\nassociated GitHub repository https://github.com/AmirNoori68/kan-review\ncomplements this paper and serves as a structured reference for ongoing KAN\nresearch.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86Kolmogorov-Arnold Networks (KANs)\u7684\u7406\u8bba\u57fa\u7840\u3001\u67b6\u6784\u53d8\u4f53\u548c\u5b9e\u8df5\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86KANs\u4e0eMLPs\u7684\u5bf9\u6bd4\u5206\u6790\u3001\u57fa\u51fd\u6570\u9009\u62e9\u6307\u5357\u4ee5\u53ca\u6539\u8fdb\u6280\u672f\u8def\u7ebf\u56fe\u3002", "motivation": "KANs\u4f5c\u4e3a\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u5668(MLPs)\u7684\u6709\u524d\u666f\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u66f4\u597d\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u5feb\u901f\u53d1\u5c55\u7684KAN\u7814\u7a76\u9886\u57df\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u7efc\u8ff0\u3002", "method": "\u6536\u96c6\u548c\u5206\u7c7b\u5927\u91cf\u5f00\u6e90\u5b9e\u73b0\uff0c\u5efa\u7acbKANs\u4e0eMLPs\u7684\u6982\u5ff5\u6865\u6881\uff0c\u5206\u6790\u4e0d\u540c\u57fa\u51fd\u6570\u9009\u62e9\uff08B\u6837\u6761\u3001\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u3001ReLU\u7ec4\u5408\u7b49\uff09\u7684\u6743\u8861\uff0c\u5e76\u5206\u7c7b\u6574\u7406\u6539\u8fdb\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86KANs\u4e0eMLPs\u7684\u5f62\u5f0f\u7b49\u4ef7\u6027\uff0c\u7a81\u51fa\u4e86KAN\u516c\u5f0f\u7684\u4f18\u8d8a\u53c2\u6570\u6548\u7387\uff0c\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6539\u8fdb\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u63d0\u9ad8\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u6b63\u5219\u5316\u7684\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\"\u9009\u62e9\u4f60\u7684KAN\"\u6307\u5357\uff0c\u5e2e\u52a9\u5b9e\u8df5\u8005\u9009\u62e9\u5408\u9002\u7684\u67b6\u6784\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7a7a\u767d\u3002\u76f8\u5173GitHub\u4ed3\u5e93\u4f5c\u4e3aKAN\u7814\u7a76\u7684\u7ed3\u6784\u5316\u53c2\u8003\u3002"}}
{"id": "2510.25775", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25775", "abs": "https://arxiv.org/abs/2510.25775", "authors": ["Francesco Spinnato"], "title": "Towards Piece-by-Piece Explanations for Chess Positions with SHAP", "comment": null, "summary": "Contemporary chess engines offer precise yet opaque evaluations, typically\nexpressed as centipawn scores. While effective for decision-making, these\noutputs obscure the underlying contributions of individual pieces or patterns.\nIn this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the\ndomain of chess analysis, aiming to attribute a chess engines evaluation to\nspecific pieces on the board. By treating pieces as features and systematically\nablating them, we compute additive, per-piece contributions that explain the\nengines output in a locally faithful and human-interpretable manner. This\nmethod draws inspiration from classical chess pedagogy, where players assess\npositions by mentally removing pieces, and grounds it in modern explainable AI\ntechniques. Our approach opens new possibilities for visualization, human\ntraining, and engine comparison. We release accompanying code and data to\nfoster future research in interpretable chess AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u5c06SHAP\u89e3\u91ca\u6027AI\u6280\u672f\u5e94\u7528\u4e8e\u56fd\u9645\u8c61\u68cb\u5206\u6790\uff0c\u901a\u8fc7\u5c06\u68cb\u5b50\u89c6\u4e3a\u7279\u5f81\u5e76\u8fdb\u884c\u7cfb\u7edf\u6027\u6d88\u878d\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u68cb\u5b50\u7684\u8d21\u732e\u5ea6\u6765\u89e3\u91ca\u5f15\u64ce\u8bc4\u4f30\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u63d0\u4f9b\u7cbe\u786e\u4f46\u4e0d\u900f\u660e\u7684\u8bc4\u4f30\uff08\u901a\u5e38\u4ee5\u767e\u5206\u5175\u5206\u6570\u8868\u793a\uff09\uff0c\u8fd9\u4e9b\u8f93\u51fa\u63a9\u76d6\u4e86\u5355\u4e2a\u68cb\u5b50\u6216\u6a21\u5f0f\u7684\u6f5c\u5728\u8d21\u732e\u3002", "method": "\u5c06\u68cb\u5b50\u4f5c\u4e3a\u7279\u5f81\uff0c\u91c7\u7528SHAP\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u6027\u6d88\u878d\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u68cb\u5b50\u7684\u52a0\u6027\u8d21\u732e\uff0c\u4ee5\u5c40\u90e8\u5fe0\u5b9e\u4e14\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u89e3\u91ca\u5f15\u64ce\u8f93\u51fa\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u5c06\u5f15\u64ce\u8bc4\u4f30\u5f52\u56e0\u4e8e\u68cb\u76d8\u4e0a\u7279\u5b9a\u68cb\u5b50\u7684\u65b9\u6cd5\uff0c\u4e3a\u53ef\u89c6\u5316\u3001\u4eba\u7c7b\u8bad\u7ec3\u548c\u5f15\u64ce\u6bd4\u8f83\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u7ecf\u5178\u56fd\u9645\u8c61\u68cb\u6559\u5b66\u7406\u5ff5\uff08\u901a\u8fc7\u5fc3\u7406\u79fb\u9664\u68cb\u5b50\u8bc4\u4f30\u5c40\u9762\uff09\u4e0e\u73b0\u4ee3\u53ef\u89e3\u91caAI\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u5e76\u53d1\u5e03\u4e86\u914d\u5957\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u53ef\u89e3\u91ca\u56fd\u9645\u8c61\u68cbAI\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.26026", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26026", "abs": "https://arxiv.org/abs/2510.26026", "authors": ["Feichen Gan", "Youcun Lu", "Yingying Zhang", "Yukun Liu"], "title": "Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation", "comment": null, "summary": "Reliable uncertainty quantification is crucial for reinforcement learning\n(RL) in high-stakes settings. We propose a unified conformal prediction\nframework for infinite-horizon policy evaluation that constructs\ndistribution-free prediction intervals {for returns} in both on-policy and\noff-policy settings. Our method integrates distributional RL with conformal\ncalibration, addressing challenges such as unobserved returns, temporal\ndependencies, and distributional shifts. We propose a modular pseudo-return\nconstruction based on truncated rollouts and a time-aware calibration strategy\nusing experience replay and weighted subsampling. These innovations mitigate\nmodel bias and restore approximate exchangeability, enabling uncertainty\nquantification even under policy shifts. Our theoretical analysis provides\ncoverage guarantees that account for model misspecification and importance\nweight estimation. Empirical results, including experiments in synthetic and\nbenchmark environments like Mountain Car, show that our method significantly\nimproves coverage and reliability over standard distributional RL baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5171\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u9650\u65f6\u57df\u7b56\u7565\u8bc4\u4f30\uff0c\u5728\u5728\u7ebf\u548c\u79bb\u7ebf\u7b56\u7565\u8bbe\u7f6e\u4e0b\u6784\u5efa\u65e0\u5206\u5e03\u7684\u56de\u62a5\u9884\u6d4b\u533a\u95f4\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u8bbe\u7f6e\u4e2d\uff0c\u53ef\u9760\u7684\u5f3a\u5316\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4e0e\u5171\u5f62\u6821\u51c6\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u57fa\u4e8e\u622a\u65ad\u6eda\u52a8\u7684\u6a21\u5757\u5316\u4f2a\u56de\u62a5\u6784\u5efa\u548c\u65f6\u95f4\u611f\u77e5\u6821\u51c6\u7b56\u7565\uff0c\u4f7f\u7528\u7ecf\u9a8c\u56de\u653e\u548c\u52a0\u6743\u5b50\u91c7\u6837\u3002", "result": "\u5728\u5408\u6210\u73af\u5883\u548c\u57fa\u51c6\u73af\u5883\uff08\u5982Mountain Car\uff09\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u672a\u89c2\u6d4b\u56de\u62a5\u3001\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5206\u5e03\u504f\u79fb\u7b49\u6311\u6218\uff0c\u5373\u4f7f\u5728\u7b56\u7565\u504f\u79fb\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2510.26166", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26166", "abs": "https://arxiv.org/abs/2510.26166", "authors": ["Juncong Zhou", "Chao Hu", "Guanlin Wu", "Zixiang Ren", "Han Hu", "Juyong Zhang", "Rui Zhang", "Jie Xu"], "title": "6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting", "comment": null, "summary": "This paper investigates the construction of channel knowledge map (CKM) from\nsparse channel measurements. Dif ferent from conventional\ntwo-/three-dimensional (2D/3D) CKM approaches assuming fixed base station\nconfigurations, we present a six-dimensional (6D) CKM framework named\nbidirectional wireless Gaussian splatting (BiWGS), which is capable of mod\neling wireless channels across dynamic transmitter (Tx) and receiver (Rx)\npositions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual\nscatterer clusters and environmental obstacles in the wireless environment. By\nproperly learning the bidirectional scattering patterns and complex attenuation\nprofiles based on channel measurements, these ellipsoids inherently cap ture\nthe electromagnetic transmission characteristics of wireless environments,\nthereby accurately modeling signal transmission under varying transceiver\nconfigurations. Experiment results show that BiWGS significantly outperforms\nclassic multi-layer perception (MLP) for the construction of 6D channel power\ngain map with varying Tx-Rx positions, and achieves spatial spectrum prediction\naccuracy comparable to the state-of-the art wireless radiation field Gaussian\nsplatting (WRF-GS) for 3D CKM construction. This validates the capability of\nthe proposed BiWGS in accomplishing dimensional expansion of 6D CKM\nconstruction, without compromising fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u5411\u65e0\u7ebf\u9ad8\u65af\u6563\u5c04\uff08BiWGS\uff09\u7684\u516d\u7ef4\u4fe1\u9053\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u80fd\u591f\u5efa\u6a21\u52a8\u6001\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\u57283D\u7a7a\u95f4\u4e2d\u7684\u65e0\u7ebf\u4fe1\u9053\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u57286D\u4fe1\u9053\u529f\u7387\u589e\u76ca\u56fe\u6784\u5efa\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf2D/3D\u4fe1\u9053\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u5047\u8bbe\u57fa\u7ad9\u914d\u7f6e\u56fa\u5b9a\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\u4f4d\u7f6e\u53d8\u5316\u7684\u60c5\u51b5\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5efa\u6a21\u516d\u7ef4\u7a7a\u95f4\u4fe1\u9053\u7279\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u692d\u7403\u4f53\u8868\u793a\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u865a\u62df\u6563\u5c04\u7c07\u548c\u73af\u5883\u969c\u788d\u7269\uff0c\u901a\u8fc7\u5b66\u4e60\u53cc\u5411\u6563\u5c04\u6a21\u5f0f\u548c\u590d\u6742\u8870\u51cf\u7279\u6027\u6765\u6355\u83b7\u65e0\u7ebf\u73af\u5883\u7684\u7535\u78c1\u4f20\u8f93\u7279\u6027\uff0c\u4ece\u800c\u51c6\u786e\u5efa\u6a21\u4e0d\u540c\u6536\u53d1\u5668\u914d\u7f6e\u4e0b\u7684\u4fe1\u53f7\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBiWGS\u5728\u6784\u5efa6D\u4fe1\u9053\u529f\u7387\u589e\u76ca\u56fe\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\uff0c\u57283D\u4fe1\u9053\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u65e0\u7ebf\u8f90\u5c04\u573a\u9ad8\u65af\u6563\u5c04\uff08WRF-GS\uff09\u76f8\u5f53\u7684\u7a7a\u95f4\u9891\u8c31\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "BiWGS\u80fd\u591f\u5728\u4e0d\u727a\u7272\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u5b9e\u73b06D\u4fe1\u9053\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u7ef4\u5ea6\u6269\u5c55\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u6536\u53d1\u5668\u914d\u7f6e\u4e0b\u5efa\u6a21\u65e0\u7ebf\u4fe1\u9053\u7684\u80fd\u529b\u3002"}}
{"id": "2510.26121", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26121", "abs": "https://arxiv.org/abs/2510.26121", "authors": ["Mara Daniels", "Liam Hodgkinson", "Michael Mahoney"], "title": "Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning", "comment": null, "summary": "Physics-informed machine learning (PIML) integrates prior physical\ninformation, often in the form of differential equation constraints, into the\nprocess of fitting machine learning models to physical data. Popular PIML\napproaches, including neural operators, physics-informed neural networks,\nneural ordinary differential equations, and neural discrete equilibria, are\ntypically fit to objectives that simultaneously include both data and physical\nconstraints. However, the multi-objective nature of this approach creates\nambiguity in the measurement of model quality. This is related to a poor\nunderstanding of epistemic uncertainty, and it can lead to surprising failure\nmodes, even when existing statistical metrics suggest strong fits. Working\nwithin a Gaussian process regression framework, we introduce the\nPhysics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test\nlosses, the PILE score is a single, uncertainty-aware metric that provides a\nselection principle for hyperparameters of a PIML model. We show that PILE\nminimization yields excellent choices for a wide variety of model parameters,\nincluding kernel bandwidth, least squares regularization weights, and even\nkernel function selection. We also show that, even prior to data acquisition, a\nspecial 'data-free' case of the PILE score identifies a priori kernel choices\nthat are 'well-adapted' to a given PDE. Beyond the kernel setting, we\nanticipate that the PILE score can be extended to PIML at large, and we outline\napproaches to do so.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7269\u7406\u4fe1\u606f\u5bf9\u6570\u8bc1\u636e\uff08PILE\uff09\u8bc4\u5206\uff0c\u4f5c\u4e3a\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\uff08PIML\uff09\u6a21\u578b\u7684\u5355\u4e00\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u76ee\u6807\u65b9\u6cd5\u5728\u6a21\u578b\u8d28\u91cf\u8bc4\u4f30\u4e0a\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPIML\u65b9\u6cd5\u540c\u65f6\u5305\u542b\u6570\u636e\u548c\u7269\u7406\u7ea6\u675f\u7684\u591a\u76ee\u6807\u7279\u6027\u5bfc\u81f4\u6a21\u578b\u8d28\u91cf\u8bc4\u4f30\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u8fd9\u4e0e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u7406\u89e3\u4e0d\u8db3\u76f8\u5173\uff0c\u5373\u4f7f\u7edf\u8ba1\u6307\u6807\u663e\u793a\u826f\u597d\u62df\u5408\u4e5f\u53ef\u80fd\u51fa\u73b0\u610f\u5916\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u5728\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6846\u67b6\u5185\u5f15\u5165PILE\u8bc4\u5206\uff0c\u7ed5\u8fc7\u6d4b\u8bd5\u635f\u5931\u7684\u6a21\u7cca\u6027\uff0c\u4e3aPIML\u6a21\u578b\u7684\u8d85\u53c2\u6570\u63d0\u4f9b\u9009\u62e9\u539f\u5219\u3002\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u6838\u5e26\u5bbd\u3001\u6700\u5c0f\u4e8c\u4e58\u6b63\u5219\u5316\u6743\u91cd\u751a\u81f3\u6838\u51fd\u6570\u9009\u62e9\u3002", "result": "PILE\u6700\u5c0f\u5316\u80fd\u4e3a\u5404\u79cd\u6a21\u578b\u53c2\u6570\u63d0\u4f9b\u4f18\u79c0\u9009\u62e9\uff0c\u5305\u62ec\u6838\u5e26\u5bbd\u3001\u6b63\u5219\u5316\u6743\u91cd\u548c\u6838\u51fd\u6570\u9009\u62e9\u3002\u5728\u6570\u636e\u83b7\u53d6\u524d\uff0cPILE\u7684'\u65e0\u6570\u636e'\u7279\u6b8a\u60c5\u51b5\u8fd8\u80fd\u8bc6\u522b\u4e0e\u7ed9\u5b9aPDE'\u826f\u597d\u9002\u5e94'\u7684\u5148\u9a8c\u6838\u9009\u62e9\u3002", "conclusion": "PILE\u8bc4\u5206\u53ef\u4f5c\u4e3aPIML\u7684\u7edf\u4e00\u8bc4\u4f30\u6307\u6807\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684PIML\u5e94\u7528\u4e2d\uff0c\u4e3a\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6a21\u578b\u9009\u62e9\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.25796", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.25796", "abs": "https://arxiv.org/abs/2510.25796", "authors": ["Farnoosh Namdarpour", "Joseph Y. J. Chow"], "title": "Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning", "comment": null, "summary": "Ride-pooling, also known as ride-sharing, shared ride-hailing, or\nmicrotransit, is a service wherein passengers share rides. This service can\nreduce costs for both passengers and operators and reduce congestion and\nenvironmental impacts. A key limitation, however, is its myopic\ndecision-making, which overlooks long-term effects of dispatch decisions. To\naddress this, we propose a simulation-informed reinforcement learning (RL)\napproach. While RL has been widely studied in the context of ride-hailing\nsystems, its application in ride-pooling systems has been less explored. In\nthis study, we extend the learning and planning framework of Xu et al. (2018)\nfrom ride-hailing to ride-pooling by embedding a ride-pooling simulation within\nthe learning mechanism to enable non-myopic decision-making. In addition, we\npropose a complementary policy for rebalancing idle vehicles. By employing\nn-step temporal difference learning on simulated experiences, we derive\nspatiotemporal state values and subsequently evaluate the effectiveness of the\nnon-myopic policy using NYC taxi request data. Results demonstrate that the\nnon-myopic policy for matching can increase the service rate by up to 8.4%\nversus a myopic policy while reducing both in-vehicle and wait times for\npassengers. Furthermore, the proposed non-myopic policy can decrease fleet size\nby over 25% compared to a myopic policy, while maintaining the same level of\nperformance, thereby offering significant cost savings for operators.\nIncorporating rebalancing operations into the proposed framework cuts wait time\nby up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%\ncompared to using the framework for matching decisions alone at the cost of\nincreased vehicle minutes traveled per passenger.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26262", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26262", "abs": "https://arxiv.org/abs/2510.26262", "authors": ["Francesco Verde", "Donatella Darsena", "Marco Di Renzo", "Vincenzo Galdi"], "title": "Optimal transmit field distribution for partially obstructed continuous radiating surfaces in near-field communication systems", "comment": "5 pages, 5 figures, conference", "summary": "This paper deals with the optimal synthesis of aperture fields for\n(radiating) near-field communications in obstructed environments. A physically\nconsistent model based on knife-edge diffraction is used to formulate the\nproblem as a maximization in Hilbert space. The optimal solution is obtained as\na matched filter that ``matches\" the shape of a diffraction-induced kernel,\nthus linking wave propagation with signal processing methods. The framework\nsupports hardware implementation using continuous apertures such as\nmetasurfaces or lens antennas. This approach bridges physically grounded\nmodeling, signal processing, and hardware design for efficient energy focusing\nin near-field obstructed channels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u906e\u6321\u73af\u5883\u4e2d\u8fdb\u884c\u8fd1\u573a\u901a\u4fe1\u7684\u5b54\u5f84\u573a\u6700\u4f18\u5408\u6210\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5200\u8fb9\u884d\u5c04\u7269\u7406\u6a21\u578b\uff0c\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u6700\u5927\u5316\u95ee\u9898\uff0c\u5f97\u5230\u5339\u914d\u6ee4\u6ce2\u5668\u5f62\u5f0f\u7684\u4f18\u5316\u89e3\u3002", "motivation": "\u89e3\u51b3\u5728\u906e\u6321\u73af\u5883\u4e2d\u8fd1\u573a\u901a\u4fe1\u7684\u80fd\u91cf\u805a\u7126\u95ee\u9898\uff0c\u5efa\u7acb\u7269\u7406\u6a21\u578b\u4e0e\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u7684\u8054\u7cfb\uff0c\u652f\u6301\u786c\u4ef6\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5200\u8fb9\u884d\u5c04\u7684\u7269\u7406\u4e00\u81f4\u6a21\u578b\uff0c\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u6700\u5927\u5316\u95ee\u9898\uff0c\u5f97\u5230\u5339\u914d\u884d\u5c04\u6838\u7684\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u6700\u4f18\u89e3\u3002", "result": "\u83b7\u5f97\u4e86\u5339\u914d\u6ee4\u6ce2\u5668\u5f62\u5f0f\u7684\u6700\u4f18\u89e3\uff0c\u80fd\u591f\u6709\u6548\u805a\u7126\u80fd\u91cf\uff0c\u652f\u6301\u8fde\u7eed\u5b54\u5f84\u786c\u4ef6\u5b9e\u73b0\u5982\u8d85\u8868\u9762\u6216\u900f\u955c\u5929\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u7269\u7406\u5efa\u6a21\u3001\u4fe1\u53f7\u5904\u7406\u548c\u786c\u4ef6\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u4e3a\u906e\u6321\u8fd1\u573a\u4fe1\u9053\u4e2d\u7684\u9ad8\u6548\u80fd\u91cf\u805a\u7126\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.25798", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25798", "abs": "https://arxiv.org/abs/2510.25798", "authors": ["Jin Seong", "Jiyun Park", "Wencke Liermann", "Hongseok Choi", "Yoonji Nam", "Hyun Kim", "Soojong Lim", "Namhoon Lee"], "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "comment": "NeurIPS 2025, 38 pages, 8 figures", "summary": "The dynamic nature of information necessitates continuously updating large\nvision-language models (LVLMs). While recent knowledge editing techniques hint\nat promising directions, they often focus on editing a single modality (vision\nor language) in isolation. This prevalent practice neglects the inherent\nmultimodality of LVLMs and the continuous nature of knowledge updates,\npotentially leading to suboptimal editing outcomes when considering the\ninterplay between modalities and the need for ongoing knowledge refinement. To\naddress these limitations, we propose MemEIC, a novel method for Continual and\nCompositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional\nediting of both visual and textual knowledge sequentially. Our approach employs\na hybrid external-internal editor featuring a dual external memory for\ncross-modal evidence retrieval and dual LoRA adapters that facilitate\ndisentangled parameter updates for each modality. A key component is a\nbrain-inspired knowledge connector, activated selectively for compositional\nreasoning, that integrates information across different modalities. Experiments\ndemonstrate that MemEIC significantly improves performance on complex\nmultimodal questions and effectively preserves prior edits, setting a new\nbenchmark for CCKE in LVLMs.", "AI": {"tldr": "MemEIC\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u6301\u7eed\u7ec4\u5408\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u5916\u90e8-\u5185\u90e8\u7f16\u8f91\u5668\u548c\u53ccLoRA\u9002\u914d\u5668\u5b9e\u73b0\u89c6\u89c9\u548c\u6587\u672c\u77e5\u8bc6\u7684\u7ec4\u5408\u7f16\u8f91\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u95ee\u9898\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u6a21\u6001\uff08\u89c6\u89c9\u6216\u8bed\u8a00\uff09\uff0c\u5ffd\u89c6\u4e86LVLMs\u56fa\u6709\u7684\u591a\u6a21\u6001\u7279\u6027\u548c\u77e5\u8bc6\u66f4\u65b0\u7684\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\u7f16\u8f91\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5916\u90e8-\u5185\u90e8\u7f16\u8f91\u5668\uff0c\u5305\u542b\u7528\u4e8e\u8de8\u6a21\u6001\u8bc1\u636e\u68c0\u7d22\u7684\u53cc\u5916\u90e8\u8bb0\u5fc6\u548c\u7528\u4e8e\u89e3\u8026\u53c2\u6570\u66f4\u65b0\u7684\u53ccLoRA\u9002\u914d\u5668\uff0c\u4ee5\u53ca\u53d7\u5927\u8111\u542f\u53d1\u7684\u77e5\u8bc6\u8fde\u63a5\u5668\u8fdb\u884c\u7ec4\u5408\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMemEIC\u5728\u590d\u6742\u591a\u6a21\u6001\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u4fdd\u7559\u5148\u524d\u7684\u7f16\u8f91\u7ed3\u679c\u3002", "conclusion": "MemEIC\u4e3aLVLMs\u4e2d\u7684\u6301\u7eed\u7ec4\u5408\u77e5\u8bc6\u7f16\u8f91\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u7f16\u8f91\u7684\u6311\u6218\u3002"}}
{"id": "2510.25933", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.25933", "abs": "https://arxiv.org/abs/2510.25933", "authors": ["Nissan Yaron", "Dan Bystritsky", "Ben-Etzion Yaron"], "title": "Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning", "comment": null, "summary": "We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS\nGrounding public subset within a $\\pm 5$ pp equivalence margin.\n  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI\n69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference\nis 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's\n$d = 0.023$). TOST establishes equivalence at $\\pm 5$ pp (not at $\\pm 3$ pp).\nWhen purchased as managed APIs, Humans-Junior's base model\n(Phi-3.5-mini-instruct) is $\\approx 19\\times$ less expensive than GPT-4o on\nMicrosoft AI Foundry pricing; self-hosted or edge deployments can drive\nincremental inference cost toward zero. Measured vs estimated pricing sources\nare tabulated in Appendix E.\n  Method. Our approach combines minimal directed \"Exoskeleton Reasoning\"\nscaffolds with behavioral fine-tuning that teaches protocol compliance\n(epistemic discipline) rather than domain answers. Fine-tuning alone adds\nlittle; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance\n($\\approx 25\\%$). In prompt-only settings on frontier models (Q1--Q100;\nnon-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and\nGemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.\n  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within\n$\\pm 5$ pp on Q1--Q500). Cloud pricing shows $\\approx 19\\times$ lower cost\nversus GPT-4o, and self-hosted/edge deployments can approach zero marginal\ncost. Pricing sources are listed in Appendix E. Frontier prompt-only gains\n(Q1--Q100; non-comparable) and optimized-prompt exploratory results under\nearlier judges are summarized in Appendix F.\n  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,\nFine-Tuning, Model Alignment, Cost-Efficient AI", "AI": {"tldr": "3.8B\u53c2\u6570\u7684Humans-Junior\u6a21\u578b\u5728FACTS Grounding\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eGPT-4o\u8868\u73b0\u76f8\u5f53\uff08\u5728\u00b15%\u7b49\u6548\u8303\u56f4\u5185\uff09\uff0c\u540c\u65f6\u4e91\u670d\u52a1\u6210\u672c\u964d\u4f4e\u7ea619\u500d\uff0c\u81ea\u6258\u7ba1/\u8fb9\u7f18\u90e8\u7f72\u53ef\u63a5\u8fd1\u96f6\u8fb9\u9645\u6210\u672c\u3002", "motivation": "\u5f00\u53d1\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "method": "\u7ed3\u5408\u6700\u5c0f\u5316\u7684\"\u5916\u9aa8\u9abc\u63a8\u7406\"\u652f\u67b6\u548c\u884c\u4e3a\u5fae\u8c03\uff0c\u6559\u5bfc\u534f\u8bae\u5408\u89c4\u6027\u800c\u975e\u9886\u57df\u77e5\u8bc6\uff0c\u4e24\u8005\u534f\u540c\u4f5c\u7528\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728Q1-Q500\u6d4b\u8bd5\u4e2d\uff0cGPT-4o\u5f97\u520673.5%\uff0cHumans-Junior\u5f97\u520672.7%\uff0c\u914d\u5bf9\u5dee\u5f020.8\u4e2a\u767e\u5206\u70b9\uff0c\u7edf\u8ba1\u7b49\u6548\u6027\u5728\u00b15%\u8303\u56f4\u5185\u6210\u7acb\u3002\u524d\u6cbf\u6a21\u578b\u4ec5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u53ef\u83b7\u5f97\u989d\u5916\u63d0\u5347\u3002", "conclusion": "\u5c0f\u578b\u6a21\u578b\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a8\u7406\u652f\u67b6\u548c\u884c\u4e3a\u5fae\u8c03\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2510.26723", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.26723", "abs": "https://arxiv.org/abs/2510.26723", "authors": ["Masahiro Kato"], "title": "Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning", "comment": null, "summary": "The goal of policy learning is to train a policy function that recommends a\ntreatment given covariates to maximize population welfare. There are two major\napproaches in policy learning: the empirical welfare maximization (EWM)\napproach and the plug-in approach. The EWM approach is analogous to a\nclassification problem, where one first builds an estimator of the population\nwelfare, which is a functional of policy functions, and then trains a policy by\nmaximizing the estimated welfare. In contrast, the plug-in approach is based on\nregression, where one first estimates the conditional average treatment effect\n(CATE) and then recommends the treatment with the highest estimated outcome.\nThis study bridges the gap between the two approaches by showing that both are\nbased on essentially the same optimization problem. In particular, we prove an\nexact equivalence between EWM and least squares over a reparameterization of\nthe policy class. As a consequence, the two approaches are interchangeable in\nseveral respects and share the same theoretical guarantees under common\nconditions. Leveraging this equivalence, we propose a novel regularization\nmethod for policy learning. Our findings yield a convex and computationally\nefficient training procedure that avoids the NP-hard combinatorial step\ntypically required in EWM.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u7ecf\u9a8c\u798f\u5229\u6700\u5927\u5316(EWM)\u548c\u63d2\u4ef6\u65b9\u6cd5\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u7b49\u4ef7\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u907f\u514dNP-hard\u7ec4\u5408\u4f18\u5316\u6b65\u9aa4\u7684\u51f8\u5316\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u7b56\u7565\u5b66\u4e60\u4e2d\u7684EWM\u65b9\u6cd5\u548c\u63d2\u4ef6\u65b9\u6cd5\u770b\u4f3c\u4e0d\u540c\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u8bc1\u660eEWM\u4e0e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5728\u7b56\u7565\u7c7b\u91cd\u65b0\u53c2\u6570\u5316\u4e0b\u7684\u7cbe\u786e\u7b49\u4ef7\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u6b64\u7b49\u4ef7\u6027\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u5171\u540c\u6761\u4ef6\u4e0b\u5177\u6709\u76f8\u540c\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51f8\u5316\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "EWM\u548c\u63d2\u4ef6\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u76f8\u540c\u7684\u4f18\u5316\u95ee\u9898\uff0c\u53ef\u4ee5\u76f8\u4e92\u8f6c\u6362\uff0c\u8fd9\u4e3a\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8ba1\u7b97\u6846\u67b6\u3002"}}
{"id": "2510.25951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25951", "abs": "https://arxiv.org/abs/2510.25951", "authors": ["Sounak Banerjee", "Daphne Cornelisse", "Deepak Gopinath", "Emily Sumner", "Jonathan DeCastro", "Guy Rosman", "Eugene Vinitsky", "Mark K. Ho"], "title": "Estimating cognitive biases with attention-aware inverse planning", "comment": null, "summary": "People's goal-directed behaviors are influenced by their cognitive biases,\nand autonomous systems that interact with people should be aware of this. For\nexample, people's attention to objects in their environment will be biased in a\nway that systematically affects how they perform everyday tasks such as driving\nto work. Here, building on recent work in computational cognitive science, we\nformally articulate the attention-aware inverse planning problem, in which the\ngoal is to estimate a person's attentional biases from their actions. We\ndemonstrate how attention-aware inverse planning systematically differs from\nstandard inverse reinforcement learning and how cognitive biases can be\ninferred from behavior. Finally, we present an approach to attention-aware\ninverse planning that combines deep reinforcement learning with computational\ncognitive modeling. We use this approach to infer the attentional strategies of\nRL agents in real-life driving scenarios selected from the Waymo Open Dataset,\ndemonstrating the scalability of estimating cognitive biases with\nattention-aware inverse planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u611f\u77e5\u9006\u89c4\u5212\u95ee\u9898\uff0c\u65e8\u5728\u4ece\u4eba\u7c7b\u884c\u4e3a\u4e2d\u63a8\u65ad\u8ba4\u77e5\u504f\u5dee\uff0c\u7279\u522b\u662f\u6ce8\u610f\u529b\u504f\u5dee\u3002\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8ba1\u7b97\u8ba4\u77e5\u5efa\u6a21\uff0c\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4eba\u7c7b\u7684\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u53d7\u5230\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\uff0c\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u610f\u8bc6\u5230\u8fd9\u4e00\u70b9\u3002\u7279\u522b\u662f\u6ce8\u610f\u529b\u504f\u5dee\u4f1a\u7cfb\u7edf\u6027\u5730\u5f71\u54cd\u65e5\u5e38\u4efb\u52a1\u6267\u884c\uff0c\u5982\u9a7e\u9a76\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u8ba1\u7b97\u8ba4\u77e5\u5efa\u6a21\uff0c\u6784\u5efa\u6ce8\u610f\u529b\u611f\u77e5\u9006\u89c4\u5212\u65b9\u6cd5\uff0c\u4ece\u884c\u4e3a\u6570\u636e\u4e2d\u63a8\u65ad\u6ce8\u610f\u529b\u7b56\u7565\u3002", "result": "\u5728Waymo\u5f00\u653e\u6570\u636e\u96c6\u7684\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u6210\u529f\u63a8\u65adRL\u667a\u80fd\u4f53\u7684\u6ce8\u610f\u529b\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u6ce8\u610f\u529b\u611f\u77e5\u9006\u89c4\u5212\u5728\u4f30\u8ba1\u8ba4\u77e5\u504f\u5dee\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6ce8\u610f\u529b\u611f\u77e5\u9006\u89c4\u5212\u4e0e\u6807\u51c6\u9006\u5f3a\u5316\u5b66\u4e60\u6709\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u80fd\u591f\u6709\u6548\u63a8\u65ad\u8ba4\u77e5\u504f\u5dee\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.26783", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.26783", "abs": "https://arxiv.org/abs/2510.26783", "authors": ["Masahiro Kato"], "title": "A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression", "comment": null, "summary": "This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u56e0\u679c\u63a8\u65ad\u7406\u8bba\uff0c\u6574\u5408\u4e86Riesz\u56de\u5f52\u3001\u534f\u53d8\u91cf\u5e73\u8861\u3001\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u3001\u76ee\u6807\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u5339\u914d\u4f30\u8ba1\u5668\u5728\u5e73\u5747\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u52a8\u673a\u662f\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u591a\u79cd\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u5185\u5728\u5173\u7cfb\u548c\u7b49\u4ef7\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u5c55\u793aRiesz\u56de\u5f52\u3001\u534f\u53d8\u91cf\u5e73\u8861\u3001\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u3001\u76ee\u6807\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u5339\u914d\u4f30\u8ba1\u5668\u5728\u5e73\u5747\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u4e2d\u7684\u7b49\u4ef7\u6027\u548c\u5bf9\u5076\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1aRiesz\u56de\u5f52\u5728ATE\u80cc\u666f\u4e0b\u4e0e\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u672c\u8d28\u7b49\u4ef7\uff1b\u5339\u914d\u4f30\u8ba1\u5668\u662f\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u7684\u7279\u4f8b\uff1b\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e0e\u534f\u53d8\u91cf\u5e73\u8861\u5b58\u5728\u5bf9\u5076\u5173\u7cfb\uff1b\u76ee\u6807\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7528\u4e8e\u6784\u9020\u4f7f\u4e3b\u8981\u504f\u5dee\u9879\u4e3a\u96f6\u7684\u56de\u5f52\u51fd\u6570\u4f30\u8ba1\u5668\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u4e9b\u770b\u4f3c\u4e0d\u540c\u7684\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u662f\u7edf\u4e00\u7684\uff0c\u5e73\u8861\u6743\u91cd\uff08Riesz\u8868\u793a\u5668\u3001\u504f\u5dee\u6821\u6b63\u9879\u3001\u806a\u660e\u534f\u53d8\u91cf\uff09\u548c\u7ed3\u679c\u56de\u5f52\u51fd\u6570\u5728ATE\u4f30\u8ba1\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.25892", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25892", "abs": "https://arxiv.org/abs/2510.25892", "authors": ["Harris Hardiman-Mostow", "Jack Mauro", "Adrien Weihs", "Andrea L. Bertozzi"], "title": "Topology-Aware Active Learning on Graphs", "comment": null, "summary": "We propose a graph-topological approach to active learning that directly\ntargets the core challenge of exploration versus exploitation under scarce\nlabel budgets. To guide exploration, we introduce a coreset construction\nalgorithm based on Balanced Forman Curvature (BFC), which selects\nrepresentative initial labels that reflect the graph's cluster structure. This\nmethod includes a data-driven stopping criterion that signals when the graph\nhas been sufficiently explored. We further use BFC to dynamically trigger the\nshift from exploration to exploitation within active learning routines,\nreplacing hand-tuned heuristics. To improve exploitation, we introduce a\nlocalized graph rewiring strategy that efficiently incorporates multiscale\ninformation around labeled nodes, enhancing label propagation while preserving\nsparsity. Experiments on benchmark classification tasks show that our methods\nconsistently outperform existing graph-based semi-supervised baselines at low\nlabel rates.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u62d3\u6251\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861Forman\u66f2\u7387\u9009\u62e9\u4ee3\u8868\u6027\u521d\u59cb\u6807\u7b7e\u6765\u6307\u5bfc\u63a2\u7d22\uff0c\u5e76\u52a8\u6001\u89e6\u53d1\u4ece\u63a2\u7d22\u5230\u5229\u7528\u7684\u8f6c\u6362\uff0c\u540c\u65f6\u5f15\u5165\u5c40\u90e8\u56fe\u91cd\u8fde\u7b56\u7565\u63d0\u5347\u6807\u7b7e\u4f20\u64ad\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u4e3b\u52a8\u5b66\u4e60\u4e2d\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6838\u5fc3\u6311\u6218\uff0c\u66ff\u4ee3\u624b\u52a8\u8c03\u6574\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4f4e\u6807\u7b7e\u7387\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5e73\u8861Forman\u66f2\u7387\u8fdb\u884c\u6838\u5fc3\u96c6\u6784\u5efa\u6765\u9009\u62e9\u521d\u59cb\u6807\u7b7e\uff0c\u5305\u542b\u6570\u636e\u9a71\u52a8\u7684\u505c\u6b62\u51c6\u5219\uff1b\u5229\u7528BFC\u52a8\u6001\u89e6\u53d1\u63a2\u7d22\u5230\u5229\u7528\u7684\u8f6c\u6362\uff1b\u5f15\u5165\u5c40\u90e8\u56fe\u91cd\u8fde\u7b56\u7565\u6574\u5408\u591a\u5c3a\u5ea6\u4fe1\u606f\u3002", "result": "\u5728\u57fa\u51c6\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6807\u7b7e\u7387\u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u62d3\u6251\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u95ee\u9898\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2510.26094", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26094", "abs": "https://arxiv.org/abs/2510.26094", "authors": ["Yuxin Li", "Minghao Liu", "Ruida Wang", "Wenzhao Ji", "Zhitao He", "Rui Pan", "Junming Huang", "Tong Zhang", "Yi R. Fung"], "title": "Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4", "comment": null, "summary": "We present **Lean4PHYS**, a comprehensive reasoning framework for\ncollege-level physics problems in Lean4. **Lean4PHYS** includes\n*LeanPhysBench*, a college-level benchmark for formal physics reasoning in\nLean4, which contains 200 hand-crafted and peer-reviewed statements derived\nfrom university textbooks and physics competition problems. To establish a\nsolid foundation for formal reasoning in physics, we also introduce *PhysLib*,\na community-driven repository containing fundamental unit systems and theorems\nessential for formal physics reasoning. Based on the benchmark and Lean4\nrepository we composed in **Lean4PHYS**, we report baseline results using major\nexpert Math Lean4 provers and state-of-the-art closed-source models, with the\nbest performance of DeepSeek-Prover-V2-7B achieving only 16% and\nClaude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that\nour *PhysLib* can achieve an average improvement of 11.75% in model\nperformance. This demonstrates the challenging nature of our *LeanPhysBench*\nand the effectiveness of *PhysLib*. To the best of our knowledge, this is the\nfirst study to provide a physics benchmark in Lean4.", "AI": {"tldr": "Lean4PHYS\u662f\u4e00\u4e2a\u57fa\u4e8eLean4\u7684\u5927\u5b66\u7269\u7406\u95ee\u9898\u63a8\u7406\u6846\u67b6\uff0c\u5305\u542bLeanPhysBench\u57fa\u51c6\u6d4b\u8bd5\u548cPhysLib\u7269\u7406\u5e93\uff0c\u5728\u73b0\u6709\u6a21\u578b\u4e0a\u8868\u73b0\u4e0d\u4f73\u4f46PhysLib\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u4e3a\u5927\u5b66\u7269\u7406\u95ee\u9898\u63d0\u4f9b\u6b63\u5f0f\u7684\u63a8\u7406\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865Lean4\u5728\u7269\u7406\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b200\u4e2a\u624b\u5de5\u5236\u4f5c\u548c\u540c\u884c\u8bc4\u5ba1\u7684\u7269\u7406\u95ee\u9898\u7684LeanPhysBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d1\u793e\u533a\u9a71\u52a8\u7684PhysLib\u7269\u7406\u5e93\u5305\u542b\u57fa\u672c\u5355\u4f4d\u7cfb\u7edf\u548c\u5b9a\u7406", "result": "DeepSeek-Prover-V2-7B\u4ec5\u8fbe\u523016%\u51c6\u786e\u7387\uff0cClaude-Sonnet-4\u8fbe\u523035%\uff0cPhysLib\u5e73\u5747\u63d0\u5347\u6a21\u578b\u6027\u80fd11.75%", "conclusion": "LeanPhysBench\u5177\u6709\u6311\u6218\u6027\uff0cPhysLib\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8fd9\u662f\u9996\u4e2a\u5728Lean4\u4e2d\u63d0\u4f9b\u7684\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u7814\u7a76"}}
{"id": "2510.25926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25926", "abs": "https://arxiv.org/abs/2510.25926", "authors": ["Kianoosh Ashouritaklimi", "Tom Rainforth"], "title": "Active Learning with Task-Driven Representations for Messy Pools", "comment": null, "summary": "Active learning has the potential to be especially useful for messy,\nuncurated pools where datapoints vary in relevance to the target task. However,\nstate-of-the-art approaches to this problem currently rely on using fixed,\nunsupervised representations of the pool, focusing on modifying the acquisition\nfunction instead. We show that this model setup can undermine their\neffectiveness at dealing with messy pools, as such representations can fail to\ncapture important information relevant to the task. To address this, we propose\nusing task-driven representations that are periodically updated during the\nactive learning process using the previously collected labels. We introduce two\nspecific strategies for learning these representations, one based on directly\nlearning semi-supervised representations and the other based on supervised\nfine-tuning of an initial unsupervised representation. We find that both\nsignificantly improve empirical performance over using unsupervised or\npretrained representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u4e3b\u52a8\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4efb\u52a1\u9a71\u52a8\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u671f\u66f4\u65b0\u8868\u793a\u6765\u6539\u5584\u5bf9\u6df7\u4e71\u6570\u636e\u6c60\u7684\u5904\u7406\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6df7\u4e71\u3001\u672a\u6574\u7406\u7684\u6570\u636e\u6c60\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u56fa\u5b9a\u7684\u65e0\u76d1\u7763\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u4e0e\u76ee\u6807\u4efb\u52a1\u76f8\u5173\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5b66\u4e60\u4efb\u52a1\u9a71\u52a8\u8868\u793a\u7684\u7b56\u7565\uff1a\u76f4\u63a5\u5b66\u4e60\u534a\u76d1\u7763\u8868\u793a\uff0c\u4ee5\u53ca\u5bf9\u521d\u59cb\u65e0\u76d1\u7763\u8868\u793a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002\u8fd9\u4e9b\u8868\u793a\u5728\u4e3b\u52a8\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5b9a\u671f\u66f4\u65b0\u3002", "result": "\u4e24\u79cd\u7b56\u7565\u90fd\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u65e0\u76d1\u7763\u6216\u9884\u8bad\u7ec3\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u5728\u5b9e\u8bc1\u6027\u80fd\u4e0a\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u5728\u4e3b\u52a8\u5b66\u4e60\u4e2d\u4f7f\u7528\u4efb\u52a1\u9a71\u52a8\u7684\u52a8\u6001\u8868\u793a\u66f4\u65b0\u7b56\u7565\u80fd\u591f\u6709\u6548\u5904\u7406\u6df7\u4e71\u6570\u636e\u6c60\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2510.26136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26136", "abs": "https://arxiv.org/abs/2510.26136", "authors": ["Boqin Zhuang", "Jiacheng Qiao", "Mingqian Liu", "Mingxing Yu", "Ping Hong", "Rui Li", "Xiaoxia Song", "Xiangjun Xu", "Xu Chen", "Yaoyao Ma", "Yujie Gao"], "title": "Beyond Benchmarks: The Economics of AI Inference", "comment": null, "summary": "The inference cost of Large Language Models (LLMs) has become a critical\nfactor in determining their commercial viability and widespread adoption. This\npaper introduces a quantitative ``economics of inference'' framework, treating\nthe LLM inference process as a compute-driven intelligent production activity.\nWe analyze its marginal cost, economies of scale, and quality of output under\nvarious performance configurations. Based on empirical data from WiNEval-3.0,\nwe construct the first ``LLM Inference Production Frontier,'' revealing three\nprinciples: diminishing marginal cost, diminishing returns to scale, and an\noptimal cost-effectiveness zone. This paper not only provides an economic basis\nfor model deployment decisions but also lays an empirical foundation for the\nfuture market-based pricing and optimization of AI inference resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7ecf\u6d4e\u5b66\u6846\u67b6\uff0c\u5c06LLM\u63a8\u7406\u89c6\u4e3a\u8ba1\u7b97\u9a71\u52a8\u7684\u667a\u80fd\u751f\u4ea7\u6d3b\u52a8\uff0c\u5206\u6790\u4e86\u8fb9\u9645\u6210\u672c\u3001\u89c4\u6a21\u7ecf\u6d4e\u6548\u5e94\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u5e76\u57fa\u4e8eWiNEval-3.0\u6570\u636e\u6784\u5efa\u4e86\u9996\u4e2aLLM\u63a8\u7406\u751f\u4ea7\u524d\u6cbf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u5df2\u6210\u4e3a\u51b3\u5b9a\u5176\u5546\u4e1a\u53ef\u884c\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8981\u5efa\u7acb\u7ecf\u6d4e\u5206\u6790\u6846\u67b6\u6765\u6307\u5bfc\u6a21\u578b\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u91cf\u5316\u7ecf\u6d4e\u5b66\u6846\u67b6\uff0c\u5c06LLM\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u8ba1\u7b97\u9a71\u52a8\u7684\u667a\u80fd\u751f\u4ea7\u6d3b\u52a8\uff0c\u57fa\u4e8eWiNEval-3.0\u5b9e\u8bc1\u6570\u636e\u6784\u5efaLLM\u63a8\u7406\u751f\u4ea7\u524d\u6cbf\u3002", "result": "\u63ed\u793a\u4e86\u4e09\u4e2a\u539f\u5219\uff1a\u8fb9\u9645\u6210\u672c\u9012\u51cf\u3001\u89c4\u6a21\u6536\u76ca\u9012\u51cf\u4ee5\u53ca\u6700\u4f18\u6210\u672c\u6548\u76ca\u533a\u57df\uff0c\u4e3a\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u4e3a\u6a21\u578b\u90e8\u7f72\u51b3\u7b56\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u57fa\u7840\uff0c\u8fd8\u4e3a\u672a\u6765\u57fa\u4e8e\u5e02\u573a\u7684AI\u63a8\u7406\u8d44\u6e90\u5b9a\u4ef7\u548c\u4f18\u5316\u5960\u5b9a\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2510.26745", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26745", "abs": "https://arxiv.org/abs/2510.26745", "authors": ["Shahriar Noroozizadeh", "Vaishnavh Nagarajan", "Elan Rosenfeld", "Sanjiv Kumar"], "title": "Deep sequence models tend to memorize geometrically; it is unclear why", "comment": null, "summary": "In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6bd4\u4e86\u5e8f\u5217\u5efa\u6a21\u4e2d\u53c2\u6570\u5316\u5185\u5b58\u7684\u5173\u8054\u89c6\u56fe\u4e0e\u51e0\u4f55\u89c6\u56fe\uff0c\u53d1\u73b0Transformer\u6a21\u578b\u4f1a\u81ea\u53d1\u5408\u6210\u539f\u5b50\u4e8b\u5b9e\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5c06\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u7b80\u5316\u4e3a\u6613\u4e8e\u5b66\u4e60\u7684\u51e0\u4f55\u4efb\u52a1\uff0c\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u6e90\u4e8e\u8c31\u504f\u5dee\u800c\u975e\u5178\u578b\u7684\u67b6\u6784\u6216\u4f18\u5316\u538b\u529b\u3002", "motivation": "\u7814\u7a76\u5e8f\u5217\u5efa\u6a21\u4e2d\u53c2\u6570\u5316\u5185\u5b58\u7684\u672c\u8d28\uff0c\u6311\u6218\u4f20\u7edf\u7684\u5173\u8054\u89c6\u56fe\uff08\u5c06\u5185\u5b58\u89c6\u4e3a\u5b9e\u4f53\u5171\u73b0\u7684\u66b4\u529b\u67e5\u627e\uff09\uff0c\u63a2\u7d22\u51e0\u4f55\u89c6\u56fe\u5982\u4f55\u89e3\u91ca\u6a21\u578b\u5bf9\u975e\u5171\u73b0\u5b9e\u4f53\u95f4\u5168\u5c40\u5173\u7cfb\u7684\u7f16\u7801\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u79bb\u51fa\u53ef\u5206\u6790\u7684Transformer\u63a8\u7406\u5b9e\u4f8b\uff0c\u5bf9\u6bd4\u5173\u8054\u89c6\u56fe\u4e0e\u51e0\u4f55\u89c6\u56fe\u7684\u5dee\u5f02\uff1b\u5206\u6790Node2Vec\u8fde\u63a5\uff0c\u63ed\u793a\u51e0\u4f55\u7ed3\u6784\u6e90\u4e8e\u8c31\u504f\u5dee\uff1b\u63a2\u8ba8\u51e0\u4f55\u7ed3\u6784\u5f62\u6210\u7684\u673a\u5236\u3002", "result": "\u53d1\u73b0Transformer\u6a21\u578b\u80fd\u591f\u81ea\u53d1\u5408\u6210\u539f\u5b50\u4e8b\u5b9e\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5c06\u590d\u6742\u7684\u2113\u91cd\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u7b80\u5316\u4e3a\u5355\u6b65\u51e0\u4f55\u4efb\u52a1\uff1b\u8bc1\u660e\u8fd9\u79cd\u4f18\u96c5\u51e0\u4f55\u7ed3\u6784\u7684\u5b66\u4e60\u4e0d\u9700\u8981\u6bd4\u66b4\u529b\u5173\u8054\u67e5\u627e\u66f4\u7b80\u6d01\uff0c\u4e14\u6e90\u4e8e\u81ea\u7136\u4ea7\u751f\u7684\u8c31\u504f\u5dee\u3002", "conclusion": "\u53c2\u6570\u5316\u5185\u5b58\u7684\u51e0\u4f55\u89c6\u56fe\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u89c6\u89d2\uff0c\u9f13\u52b1\u5728\u77e5\u8bc6\u83b7\u53d6\u3001\u5bb9\u91cf\u3001\u53d1\u73b0\u548c\u9057\u5fd8\u7b49\u9886\u57df\u91cd\u65b0\u5ba1\u89c6\u9ed8\u8ba4\u76f4\u89c9\uff0c\u5e76\u6307\u51fa\u4e86\u4f7fTransformer\u5185\u5b58\u66f4\u51e0\u4f55\u5316\u7684\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2510.25952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25952", "abs": "https://arxiv.org/abs/2510.25952", "authors": ["Tcharlies Schmitz"], "title": "Modular Linear Tokenization (MLT)", "comment": null, "summary": "This paper introduces Modular Linear Tokenization (MLT), a reversible and\ndeterministic technique for encoding high-cardinality categorical identifiers\ninto compact numerical vectors. Unlike traditional hashing or one-hot\nencodings, MLT preserves bijective mappings by leveraging modular arithmetic\nover finite fields and invertible linear transformations. The method offers\nexplicit control of dimensionality and computational scalability while\nmaintaining full reversibility, even for millions of identifiers. Experimental\nresults on the MovieLens 20M dataset show that MLT achieves comparable\npredictive performance to supervised embeddings while requiring significantly\nfewer parameters and lower training cost. An open-source implementation of MLT\nis available on PyPI (https://pypi.org/project/light-mlt/) and GitHub\n(https://github.com/tcharliesschmitz/light-mlt).", "AI": {"tldr": "MLT\u662f\u4e00\u79cd\u53ef\u9006\u7684\u786e\u5b9a\u6027\u6280\u672f\uff0c\u7528\u4e8e\u5c06\u9ad8\u57fa\u6570\u5206\u7c7b\u6807\u8bc6\u7b26\u7f16\u7801\u4e3a\u7d27\u51d1\u6570\u503c\u5411\u91cf\uff0c\u901a\u8fc7\u6a21\u8fd0\u7b97\u548c\u53ef\u9006\u7ebf\u6027\u53d8\u6362\u4fdd\u6301\u53cc\u5c04\u6620\u5c04\uff0c\u5177\u6709\u7ef4\u5ea6\u63a7\u5236\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u54c8\u5e0c\u6216\u72ec\u70ed\u7f16\u7801\u65b9\u6cd5\u5728\u9ad8\u57fa\u6570\u5206\u7c7b\u6807\u8bc6\u7b26\u7f16\u7801\u4e2d\u5b58\u5728\u4fe1\u606f\u635f\u5931\u6216\u7ef4\u5ea6\u7206\u70b8\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53ef\u9006\u6027\u53c8\u5177\u6709\u8ba1\u7b97\u6548\u7387\u7684\u7f16\u7801\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6709\u9650\u57df\u4e0a\u7684\u6a21\u8fd0\u7b97\u548c\u53ef\u9006\u7ebf\u6027\u53d8\u6362\uff0c\u6784\u5efa\u53ef\u9006\u7684\u786e\u5b9a\u6027\u7f16\u7801\u8fc7\u7a0b\uff0c\u652f\u6301\u5bf9\u7ef4\u5ea6\u8fdb\u884c\u663e\u5f0f\u63a7\u5236\u3002", "result": "\u5728MovieLens 20M\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMLT\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4e0e\u76d1\u7763\u5d4c\u5165\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u6570\u91cf\u548c\u8bad\u7ec3\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "MLT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9006\u4e14\u53ef\u6269\u5c55\u7684\u9ad8\u57fa\u6570\u5206\u7c7b\u6807\u8bc6\u7b26\u7f16\u7801\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2510.25954", "categories": ["cs.LG", "cs.AI", "J.3"], "pdf": "https://arxiv.org/pdf/2510.25954", "abs": "https://arxiv.org/abs/2510.25954", "authors": ["Lynn Metz", "Rachel Haggard", "Michael Moszczynski", "Samer Asbah", "Chris Mwase", "Patricia Khomani", "Tyler Smith", "Hannah Cooper", "Annie Mwale", "Arbaaz Muslim", "Gautam Prasad", "Mimi Sun", "Tomer Shekel", "Joydeep Paul", "Anna Carter", "Shravya Shetty", "Dylan Green"], "title": "Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi", "comment": "13 pages, 3010 words, 2 tables, 2 figures", "summary": "The reliability of routine health data in low and middle-income countries\n(LMICs) is often constrained by reporting delays and incomplete coverage,\nnecessitating the exploration of novel data sources and analytics. Geospatial\nFoundation Models (GeoFMs) offer a promising avenue by synthesizing diverse\nspatial, temporal, and behavioral data into mathematical embeddings that can be\nefficiently used for downstream prediction tasks. This study evaluated the\npredictive performance of three GeoFM embedding sources - Google Population\nDynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite\nimagery), and mobile phone call detail records (CDR) - for modeling 15 routine\nhealth programmatic outputs in Malawi, and compared their utility to\ntraditional geospatial interpolation methods. We used XGBoost models on data\nfrom 552 health catchment areas (January 2021-May 2023), assessing performance\nwith R2, and using an 80/20 training and test data split with 5-fold\ncross-validation used in training. While predictive performance was mixed, the\nembedding-based approaches improved upon baseline geostatistical methods in 13\nof 15 (87%) indicators tested. A Multi-GeoFM model integrating all three\nembedding sources produced the most robust predictions, achieving average\n5-fold cross validated R2 values for indicators like population density (0.63),\nnew HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,\n0.68, and 0.55, respectively. Prediction was poor for prediction targets with\nlow primary data availability, such as TB and malnutrition cases. These results\ndemonstrate that GeoFM embeddings imbue a modest predictive improvement for\nselect health and demographic outcomes in an LMIC context. We conclude that the\nintegration of multiple GeoFM sources is an efficient and valuable tool for\nsupplementing and strengthening constrained routine health information systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5730\u7406\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u6e90\u5728\u9a6c\u62c9\u7ef4\u9884\u6d4b15\u9879\u5e38\u89c4\u5065\u5eb7\u9879\u76ee\u6307\u6807\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u591a\u6e90GeoFM\u6a21\u578b\u572887%\u7684\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5730\u7406\u7edf\u8ba1\u65b9\u6cd5\uff0c\u5bf9\u4eba\u53e3\u5bc6\u5ea6\u3001\u65b0HIV\u75c5\u4f8b\u548c\u513f\u7ae5\u75ab\u82d7\u63a5\u79cd\u7b49\u6307\u6807\u9884\u6d4b\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u7684\u5e38\u89c4\u5065\u5eb7\u6570\u636e\u5b58\u5728\u62a5\u544a\u5ef6\u8fdf\u548c\u8986\u76d6\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u6570\u636e\u6e90\u548c\u5206\u6790\u65b9\u6cd5\u6765\u63d0\u9ad8\u6570\u636e\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528XGBoost\u6a21\u578b\uff0c\u57fa\u4e8e552\u4e2a\u5065\u5eb7\u670d\u52a1\u533a\u57df\u7684\u6570\u636e\uff082021\u5e741\u6708-2023\u5e745\u6708\uff09\uff0c\u8bc4\u4f30Google\u4eba\u53e3\u52a8\u6001\u57fa\u7840\u6a21\u578b\u3001Google AlphaEarth\u536b\u661f\u56fe\u50cf\u548c\u624b\u673a\u901a\u8bdd\u8bb0\u5f55\u4e09\u79cdGeoFM\u5d4c\u5165\u6e90\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e0e\u4f20\u7edf\u5730\u7406\u63d2\u503c\u65b9\u6cd5\u6bd4\u8f83\u3002", "result": "\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u572815\u4e2a\u6307\u6807\u4e2d\u768413\u4e2a\uff0887%\uff09\u4f18\u4e8e\u57fa\u7ebf\u5730\u7406\u7edf\u8ba1\u65b9\u6cd5\u3002\u591a\u6e90GeoFM\u6a21\u578b\u8868\u73b0\u6700\u7a33\u5065\uff0c\u5728\u4eba\u53e3\u5bc6\u5ea6\u3001\u65b0HIV\u75c5\u4f8b\u548c\u513f\u7ae5\u75ab\u82d7\u63a5\u79cd\u7b49\u6307\u6807\u4e0a\u53d6\u5f97\u826f\u597d\u9884\u6d4b\u6548\u679c\uff0c\u4f46\u5bf9\u7ed3\u6838\u75c5\u548c\u8425\u517b\u4e0d\u826f\u75c5\u4f8b\u7b49\u6570\u636e\u53ef\u7528\u6027\u4f4e\u7684\u76ee\u6807\u9884\u6d4b\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "GeoFM\u5d4c\u5165\u4e3aLMIC\u73af\u5883\u4e2d\u7684\u7279\u5b9a\u5065\u5eb7\u548c\u4eba\u53e3\u7ed3\u679c\u63d0\u4f9b\u4e86\u9002\u5ea6\u7684\u9884\u6d4b\u6539\u8fdb\uff0c\u591a\u6e90GeoFM\u6574\u5408\u662f\u8865\u5145\u548c\u52a0\u5f3a\u53d7\u9650\u5e38\u89c4\u5065\u5eb7\u4fe1\u606f\u7cfb\u7edf\u7684\u6709\u6548\u4e14\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2510.26238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26238", "abs": "https://arxiv.org/abs/2510.26238", "authors": ["Duc-Hai Nguyen", "Vijayakumar Nanjappan", "Barry O'Sullivan", "Hoang D. Nguyen"], "title": "Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses", "comment": "14 pages, 3 figures, 8 tables", "summary": "Millions of people take surveys every day, from market polls and academic\nstudies to medical questionnaires and customer feedback forms. These datasets\ncapture valuable insights, but their scale and structure present a unique\nchallenge for large language models (LLMs), which otherwise excel at few-shot\nreasoning over open-ended text. Yet, their ability to process questionnaire\ndata or lists of questions crossed with hundreds of respondent rows remains\nunderexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,\nSPSS, REDCap) are typically designed for humans in the workflow, limiting such\ndata integration with LLM and AI-empowered automation. This gap leaves\nscientists, surveyors, and everyday users without evidence-based guidance on\nhow to best represent questionnaires for LLM consumption. We address this by\nintroducing QASU (Questionnaire Analysis and Structural Understanding), a\nbenchmark that probes six structural skills, including answer lookup,\nrespondent count, and multi-hop inference, across six serialization formats and\nmultiple prompt strategies. Experiments on contemporary LLMs show that choosing\nan effective format and prompt combination can improve accuracy by up to 8.8%\npoints compared to suboptimal formats. For specific tasks, carefully adding a\nlightweight structural hint through self-augmented prompting can yield further\nimprovements of 3-4% points on average. By systematically isolating format and\nprompting effects, our open source benchmark offers a simple yet versatile\nfoundation for advancing both research and real-world practice in LLM-based\nquestionnaire analysis.", "AI": {"tldr": "QASU\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u95ee\u5377\u6570\u636e\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u516d\u79cd\u5e8f\u5217\u5316\u683c\u5f0f\u548c\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u53d1\u73b0\u9009\u62e9\u5408\u9002\u7684\u683c\u5f0f\u548c\u63d0\u793a\u7ec4\u5408\u53ef\u4ee5\u5c06\u51c6\u786e\u7387\u63d0\u9ad88.8\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f53\u524d\u8c03\u67e5\u95ee\u5377\u6570\u636e\u4e0eLLM\u7684\u96c6\u6210\u5b58\u5728\u7a7a\u767d\uff0c\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u4e3a\u4eba\u5de5\u4f5c\u4e1a\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\u6765\u4f18\u5316\u95ee\u5377\u6570\u636e\u5728LLM\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\u3002", "method": "\u5f15\u5165QASU\u57fa\u51c6\uff0c\u8bc4\u4f30\u516d\u79cd\u7ed3\u6784\u6280\u80fd\uff08\u5305\u62ec\u7b54\u6848\u67e5\u627e\u3001\u53d7\u8bbf\u8005\u8ba1\u6570\u548c\u591a\u8df3\u63a8\u7406\uff09\uff0c\u6d4b\u8bd5\u516d\u79cd\u5e8f\u5217\u5316\u683c\u5f0f\u548c\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u589e\u5f3a\u63d0\u793a\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u7ed3\u6784\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u6709\u6548\u7684\u683c\u5f0f\u548c\u63d0\u793a\u7ec4\u5408\u76f8\u6bd4\u6b21\u4f18\u683c\u5f0f\u53ef\u63d0\u9ad88.8%\u7684\u51c6\u786e\u7387\uff1b\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u589e\u5f3a\u63d0\u793a\u6dfb\u52a0\u7ed3\u6784\u63d0\u793a\u53ef\u5e73\u5747\u63d0\u9ad83-4%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "QASU\u57fa\u51c6\u901a\u8fc7\u7cfb\u7edf\u9694\u79bb\u683c\u5f0f\u548c\u63d0\u793a\u6548\u679c\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u95ee\u5377\u5206\u6790\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u591a\u529f\u80fd\u7684\u57fa\u7840\u3002"}}
{"id": "2510.26374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26374", "abs": "https://arxiv.org/abs/2510.26374", "authors": ["Qianli Shen", "Daoyuan Chen", "Yilun Huang", "Zhenqing Ling", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning", "comment": null, "summary": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian\n\\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement\nfinetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior\nestimates of task difficulty as the model evolves. It jointly incorporates\n\\emph{explicit evidence} from direct evaluations of selected tasks and\n\\emph{implicit evidence} inferred from these evaluations for unselected tasks,\nwith Thompson sampling ensuring a principled balance between exploration and\nexploitation. To make implicit evidence practical, we instantiate it with an\nultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT.", "AI": {"tldr": "BOTS\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u5f3a\u5316\u5fae\u8c03\u7684\u8d1d\u53f6\u65af\u5728\u7ebf\u4efb\u52a1\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ef4\u62a4\u4efb\u52a1\u96be\u5ea6\u540e\u9a8c\u4f30\u8ba1\uff0c\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u8bc1\u636e\uff0c\u4f7f\u7528Thompson\u91c7\u6837\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u5728\u4efb\u52a1\u9009\u62e9\u4e0a\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u6210\u672c\u9ad8\u3001\u9002\u5e94\u6027\u5dee\u6216\u8bc1\u636e\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u52a8\u6001\u4efb\u52a1\u9009\u62e9\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u81ea\u9002\u5e94\u7ef4\u62a4\u4efb\u52a1\u96be\u5ea6\u540e\u9a8c\u4f30\u8ba1\uff0c\u8054\u5408\u4f7f\u7528\u76f4\u63a5\u8bc4\u4f30\u7684\u663e\u5f0f\u8bc1\u636e\u548c\u4ece\u672a\u9009\u62e9\u4efb\u52a1\u63a8\u65ad\u7684\u9690\u5f0f\u8bc1\u636e\uff0c\u91c7\u7528Thompson\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d85\u8f7b\u91cf\u63d2\u503c\u63d2\u4ef6\u5b9e\u73b0\u9690\u5f0f\u8bc1\u636e\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u548c\u4e0d\u540c\u89c4\u6a21\u7684LLM\u4e0a\uff0cBOTS\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "BOTS\u4e3aRFT\u4e2d\u7684\u52a8\u6001\u4efb\u52a1\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26025", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26025", "abs": "https://arxiv.org/abs/2510.26025", "authors": ["Semyon Lomaso", "Judah Goldfeder", "Mehmet Hamza Erol", "Matthew So", "Yao Yan", "Addison Howard", "Nathan Kutz", "Ravid Shwartz Ziv"], "title": "Exploring Human-AI Conceptual Alignment through the Prism of Chess", "comment": null, "summary": "Do AI systems truly understand human concepts or merely mimic surface\npatterns? We investigate this through chess, where human creativity meets\nprecise strategic concepts. Analyzing a 270M-parameter transformer that\nachieves grandmaster-level play, we uncover a striking paradox: while early\nlayers encode human concepts like center control and knight outposts with up to\n85\\% accuracy, deeper layers, despite driving superior performance, drift\ntoward alien representations, dropping to 50-65\\% accuracy. To test conceptual\nrobustness beyond memorization, we introduce the first Chess960 dataset: 240\nexpert-annotated positions across 6 strategic concepts. When opening theory is\neliminated through randomized starting positions, concept recognition drops\n10-20\\% across all methods, revealing the model's reliance on memorized\npatterns rather than abstract understanding. Our layer-wise analysis exposes a\nfundamental tension in current architectures: the representations that win\ngames diverge from those that align with human thinking. These findings suggest\nthat as AI systems optimize for performance, they develop increasingly alien\nintelligence, a critical challenge for creative AI applications requiring\ngenuine human-AI collaboration. Dataset and code are available at:\nhttps://github.com/slomasov/ChessConceptsLLM.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26380", "abs": "https://arxiv.org/abs/2510.26380", "authors": ["Yuanhang Liu", "Beichen Wang", "Peng Li", "Yang Liu"], "title": "AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory", "comment": "52 pages, 1 figure", "summary": "Artificial intelligence (AI) has demonstrated impressive progress in\nmathematical reasoning, yet its integration into the practice of mathematical\nresearch remains limited. In this study, we investigate how the AI\nMathematician (AIM) system can operate as a research partner rather than a mere\nproblem solver. Focusing on a challenging problem in homogenization theory, we\nanalyze the autonomous reasoning trajectories of AIM and incorporate targeted\nhuman interventions to structure the discovery process. Through iterative\ndecomposition of the problem into tractable subgoals, selection of appropriate\nanalytical methods, and validation of intermediate results, we reveal how human\nintuition and machine computation can complement one another. This\ncollaborative paradigm enhances the reliability, transparency, and\ninterpretability of the resulting proofs, while retaining human oversight for\nformal rigor and correctness. The approach leads to a complete and verifiable\nproof, and more broadly, demonstrates how systematic human-AI co-reasoning can\nadvance the frontier of mathematical discovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86AI\u6570\u5b66\u5bb6\u7cfb\u7edf\u4f5c\u4e3a\u7814\u7a76\u4f19\u4f34\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e0eAI\u7684\u534f\u540c\u63a8\u7406\u89e3\u51b3\u540c\u8d28\u5316\u7406\u8bba\u4e2d\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4eba\u7c7b\u76f4\u89c9\u4e0e\u673a\u5668\u8ba1\u7b97\u7684\u4e92\u8865\u6027\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6570\u5b66\u7814\u7a76\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u5982\u4f55\u4f5c\u4e3a\u7814\u7a76\u4f19\u4f34\u800c\u975e\u5355\u7eaf\u7684\u95ee\u9898\u89e3\u51b3\u8005\u53c2\u4e0e\u6570\u5b66\u53d1\u73b0\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u6570\u5b66\u5bb6\u7684\u81ea\u4e3b\u63a8\u7406\u8f68\u8ff9\uff0c\u7ed3\u5408\u9488\u5bf9\u6027\u7684\u4eba\u7c7b\u5e72\u9884\u6765\u7ed3\u6784\u5316\u53d1\u73b0\u8fc7\u7a0b\uff0c\u5305\u62ec\u8fed\u4ee3\u5206\u89e3\u95ee\u9898\u4e3a\u53ef\u5904\u7406\u7684\u5b50\u76ee\u6807\u3001\u9009\u62e9\u9002\u5f53\u7684\u5206\u6790\u65b9\u6cd5\u4ee5\u53ca\u9a8c\u8bc1\u4e2d\u95f4\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u4e00\u4e2a\u5b8c\u6574\u4e14\u53ef\u9a8c\u8bc1\u7684\u8bc1\u660e\uff0c\u5c55\u793a\u4e86\u4eba\u7c7b\u4e0eAI\u534f\u540c\u63a8\u7406\u5982\u4f55\u63d0\u9ad8\u8bc1\u660e\u7684\u53ef\u9760\u6027\u3001\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u5bf9\u5f62\u5f0f\u4e25\u8c28\u6027\u548c\u6b63\u786e\u6027\u7684\u76d1\u7763\u3002", "conclusion": "\u7cfb\u7edf\u5316\u7684\u4eba\u7c7b-AI\u534f\u540c\u63a8\u7406\u80fd\u591f\u63a8\u8fdb\u6570\u5b66\u53d1\u73b0\u7684\u524d\u6cbf\uff0c\u8fd9\u79cd\u534f\u4f5c\u8303\u5f0f\u7ed3\u5408\u4e86\u4eba\u7c7b\u76f4\u89c9\u548c\u673a\u5668\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u4e3a\u6570\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.26038", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26038", "abs": "https://arxiv.org/abs/2510.26038", "authors": ["Jiali Cheng", "Chirag Agarwal", "Hadi Amiri"], "title": "Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods", "comment": null, "summary": "Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u53bb\u504f\u80fd\u529b\uff0c\u4f46\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u589e\u5f3a\u3001\u8fed\u4ee3\u77e5\u8bc6\u84b8\u998f\u548c\u6559\u5e08\u6743\u91cd\u521d\u59cb\u5316\u7b49\u65b9\u6cd5\u53ef\u4ee5\u6539\u5584\u53bb\u504f\u65b9\u6cd5\u7684\u53ef\u84b8\u998f\u6027\u3002", "motivation": "\u7814\u7a76\u77e5\u8bc6\u84b8\u998f\u5bf9\u6a21\u578b\u53bb\u504f\u80fd\u529b\u8f6c\u79fb\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u77e5\u8bc6\u84b8\u998f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u77e5\u8bc6\u84b8\u998f\u540e\u6a21\u578b\u53bb\u504f\u80fd\u529b\u7684\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u7535\u8def\u5206\u6790\u5185\u90e8\u673a\u5236\u3002", "result": "\u53d1\u73b0\u77e5\u8bc6\u84b8\u998f\u4f1a\u524a\u5f31\u6a21\u578b\u53bb\u504f\u80fd\u529b\uff0c\u53bb\u504f\u6a21\u578b\u8bad\u7ec3\u4e0d\u53d7\u6559\u5e08\u77e5\u8bc6\u6ce8\u5165\u7684\u76ca\u5904\uff0c\u4e0d\u540c\u504f\u89c1\u7684\u9c81\u68d2\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u53bb\u504f\u65b9\u6cd5\u53ef\u84b8\u998f\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7406\u89e3\u77e5\u8bc6\u84b8\u998f\u5de5\u4f5c\u539f\u7406\u548c\u8bbe\u8ba1\u66f4\u597d\u7684\u53bb\u504f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.26384", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26384", "abs": "https://arxiv.org/abs/2510.26384", "authors": ["Andrew M. Bean", "Nabeel Seedat", "Shengzhuang Chen", "Jonathan Richard Schwarz"], "title": "Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings", "comment": "9 pages, 2 figures, 4 tables", "summary": "The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u9879\u5185\u5728\u5c5e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5Scales++\uff0c\u901a\u8fc7\u8ba4\u77e5\u9700\u6c42\u9009\u62e9\u6570\u636e\uff0c\u5728\u4ec5\u4f7f\u75280.5%\u6570\u636e\u5b50\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u4ee52.9%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u9884\u6d4b\u5b8c\u6574\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u524d\u671f\u9009\u62e9\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6a21\u578b\u6027\u80fd\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\u9879\u7684\u65b9\u6cd5\u5b58\u5728\u9ad8\u524d\u671f\u6210\u672c\u3001\u65e0\u6cd5\u5904\u7406\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff08\u51b7\u542f\u52a8\u95ee\u9898\uff09\u4ee5\u53ca\u4f9d\u8d56\u672a\u6765\u6a21\u578b\u4e0e\u73b0\u6709\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u76f8\u4f3c\u7684\u8106\u5f31\u5047\u8bbe\u7b49\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u9879\u76ee\u4e2d\u5fc3\u7684\u9ad8\u6548\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5Scales++\uff0c\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u6837\u672c\u7684\u8ba4\u77e5\u9700\u6c42\u8fdb\u884c\u6570\u636e\u9009\u62e9\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u6a21\u578b\u7279\u5b9a\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "result": "Scales++\u5c06\u524d\u671f\u9009\u62e9\u6210\u672c\u964d\u4f4e\u4e8618\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u4fdd\u771f\u5ea6\u3002\u5728Open LLM\u6392\u884c\u699c\u4e0a\uff0c\u4ec5\u4f7f\u75280.5%\u6570\u636e\u5b50\u96c6\u5c31\u80fd\u4ee52.9%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u9884\u6d4b\u5b8c\u6574\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u3002", "conclusion": "\u9879\u76ee\u4e2d\u5fc3\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u51b7\u542f\u52a8\u6027\u80fd\u548c\u66f4\u53ef\u89e3\u91ca\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2510.26411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26411", "abs": "https://arxiv.org/abs/2510.26411", "authors": ["Riccardo Renzulli", "Colas Lepoutre", "Enrico Cassano", "Marco Grangetto"], "title": "MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders", "comment": null, "summary": "Artificial intelligence in healthcare requires models that are accurate and\ninterpretable. We advance mechanistic interpretability in medical vision by\napplying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,\na vision-language model trained on chest radiographs and reports. To quantify\ninterpretability, we propose an evaluation framework that combines correlation\nmetrics, entropy analyzes, and automated neuron naming via the MedGEMMA\nfoundation model. Experiments on the CheXpert dataset show that MedSAE neurons\nachieve higher monosemanticity and interpretability than raw MedCLIP features.\nOur findings bridge high-performing medical AI and transparency, offering a\nscalable step toward clinically reliable representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u533b\u5b66\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08MedSAEs\uff09\u5e94\u7528\u4e8eMedCLIP\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u6307\u6807\u3001\u71b5\u5206\u6790\u548cMedGEMMA\u81ea\u52a8\u795e\u7ecf\u5143\u547d\u540d\u6765\u91cf\u5316\u53ef\u89e3\u91ca\u6027\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86MedSAE\u795e\u7ecf\u5143\u6bd4\u539f\u59cbMedCLIP\u7279\u5f81\u5177\u6709\u66f4\u9ad8\u7684\u5355\u4e49\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u533b\u7597AI\u9700\u8981\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u672c\u6587\u65e8\u5728\u63a8\u8fdb\u533b\u5b66\u89c6\u89c9\u4e2d\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002", "method": "\u5c06\u533b\u5b66\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08MedSAEs\uff09\u5e94\u7528\u4e8eMedCLIP\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u63d0\u51fa\u7ed3\u5408\u76f8\u5173\u6027\u6307\u6807\u3001\u71b5\u5206\u6790\u548cMedGEMMA\u81ea\u52a8\u795e\u7ecf\u5143\u547d\u540d\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedSAE\u795e\u7ecf\u5143\u6bd4\u539f\u59cbMedCLIP\u7279\u5f81\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5355\u4e49\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u9ad8\u6548\u533b\u7597AI\u548c\u900f\u660e\u5ea6\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u4e3a\u4e34\u5e8a\u53ef\u9760\u8868\u793a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26418", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26418", "abs": "https://arxiv.org/abs/2510.26418", "authors": ["Jianli Zhao", "Tingchen Fu", "Rylan Schaeffer", "Mrinank Sharma", "Fazl Barez"], "title": "Chain-of-Thought Hijacking", "comment": null, "summary": "Large reasoning models (LRMs) achieve higher task performance by allocating\nmore inference-time compute, and prior works suggest this scaled reasoning may\nalso strengthen safety by improving refusal. Yet we find the opposite: the same\nreasoning can be used to bypass safeguards. We introduce Chain-of-Thought\nHijacking, a jailbreak attack on reasoning models. The attack pads harmful\nrequests with long sequences of harmless puzzle reasoning. Across HarmBench,\nCoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on\nGemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -\nfar exceeding prior jailbreak methods for LRMs. To understand the effectiveness\nof our attack, we turn to a mechanistic analysis, which shows that mid layers\nencode the strength of safety checking, while late layers encode the\nverification outcome. Long benign CoT dilutes both signals by shifting\nattention away from harmful tokens. Targeted ablations of attention heads\nidentified by this analysis causally decrease refusal, confirming their role in\na safety subnetwork. These results show that the most interpretable form of\nreasoning - explicit CoT - can itself become a jailbreak vector when combined\nwith final-answer cues. We release prompts, outputs, and judge decisions to\nfacilitate replication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Thought Hijacking\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6709\u5bb3\u8bf7\u6c42\u524d\u6dfb\u52a0\u65e0\u5bb3\u7684\u63a8\u7406\u5e8f\u5217\u6765\u7ed5\u8fc7\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe94%-100%\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u53ef\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u6269\u5c55\u63a8\u7406\u53cd\u800c\u53ef\u80fd\u524a\u5f31\u5b89\u5168\u6027\uff0c\u56e0\u4e3a\u540c\u6837\u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u88ab\u7528\u6765\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "method": "\u4f7f\u7528Chain-of-Thought Hijacking\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u6709\u5bb3\u8bf7\u6c42\u524d\u586b\u5145\u957f\u5e8f\u5217\u7684\u65e0\u5bb3\u8c1c\u9898\u63a8\u7406\uff0c\u901a\u8fc7\u7a00\u91ca\u5b89\u5168\u68c0\u67e5\u4fe1\u53f7\u6765\u7ed5\u8fc7\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u3002", "result": "\u5728HarmBench\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5bf9Gemini 2.5 Pro\u3001GPT o4 mini\u3001Grok 3 mini\u548cClaude 4 Sonnet\u7684\u653b\u51fb\u6210\u529f\u7387\u5206\u522b\u8fbe\u523099%\u300194%\u3001100%\u548c94%\uff0c\u8fdc\u8d85\u4e4b\u524d\u7684\u8d8a\u72f1\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6700\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u5f62\u5f0f\u2014\u2014\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4\uff0c\u5f53\u4e0e\u6700\u7ec8\u7b54\u6848\u63d0\u793a\u7ed3\u5408\u65f6\uff0c\u672c\u8eab\u53ef\u80fd\u6210\u4e3a\u8d8a\u72f1\u653b\u51fb\u7684\u8f7d\u4f53\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u5b89\u5168\u6027\u7684\u65b0\u8106\u5f31\u70b9\u3002"}}
{"id": "2510.26086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26086", "abs": "https://arxiv.org/abs/2510.26086", "authors": ["Zheng Zhang", "Haonan Li", "Xingyu Li", "Hang Zhang", "Zhiyun Qian"], "title": "LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline", "comment": null, "summary": "Bug bisection has been an important security task that aims to understand the\nrange of software versions impacted by a bug, i.e., identifying the commit that\nintroduced the bug. However, traditional patch-based bisection methods are\nfaced with several significant barriers: For example, they assume that the\nbug-inducing commit (BIC) and the patch commit modify the same functions, which\nis not always true. They often rely solely on code changes, while the commit\nmessage frequently contains a wealth of vulnerability-related information. They\nare also based on simple heuristics (e.g., assuming the BIC initializes lines\ndeleted in the patch) and lack any logical analysis of the vulnerability.\n  In this paper, we make the observation that Large Language Models (LLMs) are\nwell-positioned to break the barriers of existing solutions, e.g., comprehend\nboth textual data and code in patches and commits. Unlike previous BIC\nidentification approaches, which yield poor results, we propose a comprehensive\nmulti-stage pipeline that leverages LLMs to: (1) fully utilize patch\ninformation, (2) compare multiple candidate commits in context, and (3)\nprogressively narrow down the candidates through a series of down-selection\nsteps. In our evaluation, we demonstrate that our approach achieves\nsignificantly better accuracy than the state-of-the-art solution by more than\n38\\%. Our results further confirm that the comprehensive multi-stage pipeline\nis essential, as it improves accuracy by 60\\% over a baseline LLM-based\nbisection method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5\u6f0f\u6d1e\u4e8c\u5206\u6cd5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc6\u522b\u5f15\u5165\u6f0f\u6d1e\u7684\u63d0\u4ea4\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc738%\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8865\u4e01\u7684\u6f0f\u6d1e\u4e8c\u5206\u6cd5\u5b58\u5728\u591a\u4e2a\u9650\u5236\uff1a\u5047\u8bbe\u6f0f\u6d1e\u5f15\u5165\u63d0\u4ea4\u548c\u8865\u4e01\u63d0\u4ea4\u4fee\u6539\u76f8\u540c\u51fd\u6570\u3001\u4ec5\u4f9d\u8d56\u4ee3\u7801\u53d8\u66f4\u800c\u5ffd\u7565\u63d0\u4ea4\u6d88\u606f\u4e2d\u7684\u4fe1\u606f\u3001\u57fa\u4e8e\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u800c\u7f3a\u4e4f\u903b\u8f91\u5206\u6790\u3002LLM\u80fd\u591f\u7406\u89e3\u6587\u672c\u548c\u4ee3\u7801\uff0c\u6709\u671b\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u5229\u7528LLM\uff1a(1)\u5145\u5206\u5229\u7528\u8865\u4e01\u4fe1\u606f\uff0c(2)\u5728\u4e0a\u4e0b\u6587\u4e2d\u6bd4\u8f83\u591a\u4e2a\u5019\u9009\u63d0\u4ea4\uff0c(3)\u901a\u8fc7\u4e00\u7cfb\u5217\u7b5b\u9009\u6b65\u9aa4\u9010\u6b65\u7f29\u5c0f\u5019\u9009\u8303\u56f4\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc738%\uff0c\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u76f8\u6bd4\u57fa\u7ebfLLM\u4e8c\u5206\u6cd5\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534760%\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u514b\u670d\u4f20\u7edf\u6f0f\u6d1e\u4e8c\u5206\u6cd5\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u6f0f\u6d1e\u5f15\u5165\u63d0\u4ea4\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.26099", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26099", "abs": "https://arxiv.org/abs/2510.26099", "authors": ["Nick Masi", "Randall Balestriero"], "title": "SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth", "comment": null, "summary": "The dominant paradigm in machine learning is to assess model performance\nbased on average loss across all samples in some test set. This amounts to\naveraging performance geospatially across the Earth in weather and climate\nsettings, failing to account for the non-uniform distribution of human\ndevelopment and geography. We introduce Stratified Assessments of Forecasts\nover Earth (SAFE), a package for elucidating the stratified performance of a\nset of predictions made over Earth. SAFE integrates various data domains to\nstratify by different attributes associated with geospatial gridpoints:\nterritory (usually country), global subregion, income, and landcover (land or\nwater). This allows us to examine the performance of models for each individual\nstratum of the different attributes (e.g., the accuracy in every individual\ncountry). To demonstrate its importance, we utilize SAFE to benchmark a zoo of\nstate-of-the-art AI-based weather prediction models, finding that they all\nexhibit disparities in forecasting skill across every attribute. We use this to\nseed a benchmark of model forecast fairness through stratification at different\nlead times for various climatic variables. By moving beyond globally-averaged\nmetrics, we for the first time ask: where do models perform best or worst, and\nwhich models are most fair? To support further work in this direction, the SAFE\npackage is open source and available at https://github.com/N-Masi/safe", "AI": {"tldr": "SAFE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5730\u7403\u9884\u6d4b\u6a21\u578b\u5206\u5c42\u6027\u80fd\u7684\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u6309\u56fd\u5bb6\u3001\u5730\u533a\u3001\u6536\u5165\u6c34\u5e73\u548c\u571f\u5730\u8986\u76d6\u7b49\u5c5e\u6027\u5bf9\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u5206\u5c42\u5206\u6790\uff0c\u63ed\u793a\u6a21\u578b\u5728\u4e0d\u540c\u5730\u7406\u548c\u793e\u4f1a\u7ecf\u6d4e\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u57fa\u4e8e\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5e73\u5747\u635f\u5931\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u5929\u6c14\u548c\u6c14\u5019\u9884\u6d4b\u4e2d\u5ffd\u7565\u4e86\u4eba\u7c7b\u53d1\u5c55\u548c\u5730\u7406\u5206\u5e03\u7684\u4e0d\u5747\u5300\u6027\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u4e0d\u540c\u533a\u57df\u7684\u5b9e\u9645\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u5f00\u53d1SAFE\u5de5\u5177\u5305\uff0c\u6574\u5408\u591a\u79cd\u6570\u636e\u57df\uff0c\u6309\u5730\u7406\u7f51\u683c\u70b9\u7684\u4e0d\u540c\u5c5e\u6027\uff08\u9886\u571f\u3001\u5168\u7403\u5b50\u533a\u57df\u3001\u6536\u5165\u3001\u571f\u5730\u8986\u76d6\uff09\u8fdb\u884c\u5206\u5c42\uff0c\u5206\u6790\u6bcf\u4e2a\u5c5e\u6027\u5c42\u7ea7\u7684\u6a21\u578b\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u7684AI\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5728\u4e0d\u540c\u5c5e\u6027\u4e0a\u90fd\u5b58\u5728\u9884\u6d4b\u6280\u80fd\u7684\u4e0d\u5747\u8861\u6027\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u4e0d\u540c\u63d0\u524d\u65f6\u95f4\u548c\u6c14\u5019\u53d8\u91cf\u7684\u6a21\u578b\u9884\u6d4b\u516c\u5e73\u6027\u57fa\u51c6\u3002", "conclusion": "SAFE\u5de5\u5177\u5305\u9996\u6b21\u5b9e\u73b0\u4e86\u8d85\u8d8a\u5168\u5c40\u5e73\u5747\u6307\u6807\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u80fd\u591f\u8bc6\u522b\u6a21\u578b\u8868\u73b0\u6700\u4f73\u548c\u6700\u5dee\u7684\u533a\u57df\uff0c\u4ee5\u53ca\u6700\u516c\u5e73\u7684\u6a21\u578b\uff0c\u4e3a\u6539\u8fdb\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.26493", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26493", "abs": "https://arxiv.org/abs/2510.26493", "authors": ["Qishuo Hua", "Lyumanshan Ye", "Dayuan Fu", "Yang Xiao", "Xiaojie Cai", "Yunze Wu", "Jifan Lin", "Junfei Wang", "Pengfei Liu"], "title": "Context Engineering 2.0: The Context of Context Engineering", "comment": null, "summary": "Karl Marx once wrote that ``the human essence is the ensemble of social\nrelations'', suggesting that individuals are not isolated entities but are\nfundamentally shaped by their interactions with other entities, within which\ncontexts play a constitutive and essential role. With the advent of computers\nand artificial intelligence, these contexts are no longer limited to purely\nhuman--human interactions: human--machine interactions are included as well.\nThen a central question emerges: How can machines better understand our\nsituations and purposes? To address this challenge, researchers have recently\nintroduced the concept of context engineering. Although it is often regarded as\na recent innovation of the agent era, we argue that related practices can be\ntraced back more than twenty years. Since the early 1990s, the field has\nevolved through distinct historical phases, each shaped by the intelligence\nlevel of machines: from early human--computer interaction frameworks built\naround primitive computers, to today's human--agent interaction paradigms\ndriven by intelligent agents, and potentially to human--level or superhuman\nintelligence in the future. In this paper, we situate context engineering,\nprovide a systematic definition, outline its historical and conceptual\nlandscape, and examine key design considerations for practice. By addressing\nthese questions, we aim to offer a conceptual foundation for context\nengineering and sketch its promising future. This paper is a stepping stone for\na broader community effort toward systematic context engineering in AI systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6982\u5ff5\uff0c\u5c06\u5176\u5b9a\u4e49\u4e3a\u4f7f\u673a\u5668\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u60c5\u5883\u548c\u76ee\u7684\u7684\u65b9\u6cd5\uff0c\u5e76\u8ffd\u6eaf\u4e86\u8be5\u9886\u57df\u4ece1990\u5e74\u4ee3\u81f3\u4eca\u7684\u6f14\u53d8\u5386\u7a0b\u3002", "motivation": "\u53d7\u9a6c\u514b\u601d\u5173\u4e8e\"\u4eba\u7684\u672c\u8d28\u662f\u793e\u4f1a\u5173\u7cfb\u603b\u548c\"\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5728\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\uff0c\u4e0a\u4e0b\u6587\u4e0d\u518d\u5c40\u9650\u4e8e\u4eba\u9645\u4e92\u52a8\uff0c\u8fd8\u5305\u62ec\u4eba\u673a\u4e92\u52a8\u3002\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u673a\u5668\u5982\u4f55\u66f4\u597d\u5730\u7406\u89e3\u6211\u4eec\u7684\u60c5\u5883\u548c\u76ee\u7684\uff1f", "method": "\u901a\u8fc7\u5386\u53f2\u5206\u6790\uff0c\u5c06\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u53d1\u5c55\u5212\u5206\u4e3a\u4e0d\u540c\u9636\u6bb5\uff1a\u4ece\u65e9\u671f\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u5230\u5f53\u4eca\u7684\u667a\u80fd\u4f53\u9a71\u52a8\u4ea4\u4e92\u8303\u5f0f\uff0c\u518d\u5230\u672a\u6765\u53ef\u80fd\u7684\u4eba\u7c7b\u7ea7\u6216\u8d85\u4eba\u7c7b\u667a\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u7cfb\u7edf\u5b9a\u4e49\uff0c\u52fe\u52d2\u4e86\u5176\u5386\u53f2\u548c\u6982\u5ff5\u56fe\u666f\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b9e\u8df5\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0a\u4e0b\u6587\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u5e76\u63cf\u7ed8\u4e86\u5176\u672a\u6765\u524d\u666f\uff0c\u662f\u63a8\u52a8AI\u7cfb\u7edf\u4e2d\u7cfb\u7edf\u5316\u4e0a\u4e0b\u6587\u5de5\u7a0b\u66f4\u5e7f\u6cdb\u793e\u533a\u52aa\u529b\u7684\u57fa\u77f3\u3002"}}
{"id": "2510.26159", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26159", "abs": "https://arxiv.org/abs/2510.26159", "authors": ["Emilio Mastriani", "Alessandro Costa", "Federico Incardona", "Kevin Munari", "Sebastiano Spinello"], "title": "Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series", "comment": "This paper is currently under review for presentation at the IEEE\n  SAMI 2026 Conference", "summary": "In this study, we investigate the effectiveness of advanced feature\nengineering and hybrid model architectures for anomaly detection in a\nmultivariate industrial time series, focusing on a steam turbine system. We\nevaluate the impact of change point-derived statistical features,\nclustering-based substructure representations, and hybrid learning strategies\non detection performance. Despite their theoretical appeal, these complex\napproaches consistently underperformed compared to a simple Random Forest +\nXGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of\n0.976, F1-score of 0.41, and 100% early detection within the defined time\nwindow. Our findings highlight that, in scenarios with highly imbalanced and\ntemporally uncertain data, model simplicity combined with optimized\nsegmentation can outperform more sophisticated architectures, offering greater\nrobustness, interpretability, and operational utility.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u7b80\u5355\u7684\u968f\u673a\u68ee\u6797+XGBoost\u96c6\u6210\u6a21\u578b\u5728\u5206\u6bb5\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u7684\u7279\u5f81\u5de5\u7a0b\u548c\u6df7\u5408\u67b6\u6784\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u590d\u6742\u7279\u5f81\u5de5\u7a0b\u548c\u6df7\u5408\u6a21\u578b\u67b6\u6784\u5728\u591a\u5143\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u5173\u6ce8\u84b8\u6c7d\u8f6e\u673a\u7cfb\u7edf\u3002", "method": "\u8bc4\u4f30\u4e86\u53d8\u5316\u70b9\u5bfc\u51fa\u7684\u7edf\u8ba1\u7279\u5f81\u3001\u57fa\u4e8e\u805a\u7c7b\u7684\u5b50\u7ed3\u6784\u8868\u793a\u548c\u6df7\u5408\u5b66\u4e60\u7b56\u7565\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u7b80\u5355\u7684\u968f\u673a\u68ee\u6797+XGBoost\u96c6\u6210\u6a21\u578b\u5728\u5206\u6bb5\u6570\u636e\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u590d\u6742\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u7b80\u5355\u96c6\u6210\u6a21\u578b\u5b9e\u73b0\u4e86AUC-ROC 0.976\u3001F1\u5206\u65700.41\uff0c\u5e76\u5728\u5b9a\u4e49\u65f6\u95f4\u7a97\u53e3\u5185\u8fbe\u5230100%\u7684\u65e9\u671f\u68c0\u6d4b\u7387\u3002", "conclusion": "\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u548c\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7b80\u5355\u6027\u7ed3\u5408\u4f18\u5316\u5206\u6bb5\u53ef\u4ee5\u8d85\u8d8a\u66f4\u590d\u6742\u7684\u67b6\u6784\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u64cd\u4f5c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.26184", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.26184", "abs": "https://arxiv.org/abs/2510.26184", "authors": ["Songxin Lei", "Qiongyan Wang", "Yanchen Zhu", "Hanyu Yao", "Sijie Ruan", "Weilin Ruan", "Yuyu Luo", "Huaming Wu", "Yuxuan Liang"], "title": "A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation", "comment": null, "summary": "Public resource allocation involves the efficient distribution of resources,\nincluding urban infrastructure, energy, and transportation, to effectively meet\nsocietal demands. However, existing methods focus on optimizing the movement of\nindividual resources independently, without considering their capacity\nconstraints. To address this limitation, we propose a novel and more practical\nproblem: Collaborative Public Resource Allocation (CPRA), which explicitly\nincorporates capacity constraints and spatio-temporal dynamics in real-world\nscenarios. We propose a new framework called Game-Theoretic Spatio-Temporal\nReinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:\n1) We formulate the CPRA problem as a potential game and demonstrate that there\nis no gap between the potential function and the optimal target, laying a solid\ntheoretical foundation for approximating the Nash equilibrium of this NP-hard\nproblem; and 2) Our designed GSTRL framework effectively captures the\nspatio-temporal dynamics of the overall system. We evaluate GSTRL on two\nreal-world datasets, where experiments show its superior performance. Our\nsource codes are available in the supplementary materials.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u534f\u4f5c\u516c\u5171\u8d44\u6e90\u5206\u914d\uff08CPRA\uff09\u95ee\u9898\uff0c\u8003\u8651\u5bb9\u91cf\u7ea6\u675f\u548c\u65f6\u7a7a\u52a8\u6001\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u65f6\u7a7a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08GSTRL\uff09\u6765\u89e3\u51b3\u8fd9\u4e00NP\u96be\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u72ec\u7acb\u4f18\u5316\u5355\u4e2a\u8d44\u6e90\u79fb\u52a8\uff0c\u672a\u8003\u8651\u5bb9\u91cf\u7ea6\u675f\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06CPRA\u95ee\u9898\u5efa\u6a21\u4e3a\u52bf\u535a\u5f08\uff0c\u8bc1\u660e\u52bf\u51fd\u6570\u4e0e\u6700\u4f18\u76ee\u6807\u65e0\u95f4\u9699\uff0c\u5e76\u8bbe\u8ba1GSTRL\u6846\u67b6\u6355\u6349\u7cfb\u7edf\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793aGSTRL\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "GSTRL\u4e3aCPRA\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5904\u7406\u5bb9\u91cf\u7ea6\u675f\u548c\u65f6\u7a7a\u52a8\u6001\u3002"}}
{"id": "2510.26185", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26185", "abs": "https://arxiv.org/abs/2510.26185", "authors": ["Yunxiao Shi", "Shuo Yang", "Yixin Su", "Rui Zhang", "Min Xu"], "title": "Accumulative SGD Influence Estimation for Data Attribution", "comment": null, "summary": "Modern data-centric AI needs precise per-sample influence. Standard SGD-IE\napproximates leave-one-out effects by summing per-epoch surrogates and ignores\ncross-epoch compounding, which misranks critical examples. We propose\nACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out\nperturbation across training and updates an accumulative influence state at\neach step. In smooth strongly convex settings it achieves geometric error\ncontraction and, in smooth non-convex regimes, it tightens error bounds; larger\nmini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,\nand MNIST under clean and corrupted data and both convex and non-convex\ntraining, ACC-SGD-IE yields more accurate influence estimates, especially over\nlong epochs. For downstream data cleansing it more reliably flags noisy\nsamples, producing models trained on ACC-SGD-IE cleaned data that outperform\nthose cleaned with SGD-IE.", "AI": {"tldr": "\u63d0\u51faACC-SGD-IE\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u8bad\u7ec3\u4f20\u64ad\u7559\u4e00\u6270\u52a8\u548c\u7d2f\u79ef\u5f71\u54cd\u72b6\u6001\uff0c\u6539\u8fdb\u4e86\u6807\u51c6SGD-IE\u5728\u6570\u636e\u5f71\u54cd\u4f30\u8ba1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u957f\u8bad\u7ec3\u5468\u671f\u4e2d\u3002", "motivation": "\u73b0\u4ee3\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684AI\u9700\u8981\u7cbe\u786e\u7684\u6837\u672c\u7ea7\u5f71\u54cd\u4f30\u8ba1\uff0c\u4f46\u6807\u51c6SGD-IE\u65b9\u6cd5\u5ffd\u7565\u4e86\u8de8\u5468\u671f\u590d\u5408\u6548\u5e94\uff0c\u5bfc\u81f4\u5bf9\u5173\u952e\u6837\u672c\u7684\u9519\u8bef\u6392\u5e8f\u3002", "method": "ACC-SGD-IE\u662f\u4e00\u79cd\u8f68\u8ff9\u611f\u77e5\u4f30\u8ba1\u5668\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f20\u64ad\u7559\u4e00\u6270\u52a8\uff0c\u5e76\u5728\u6bcf\u4e00\u6b65\u66f4\u65b0\u7d2f\u79ef\u5f71\u54cd\u72b6\u6001\u3002", "result": "\u5728\u5e73\u6ed1\u5f3a\u51f8\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u51e0\u4f55\u8bef\u5dee\u6536\u7f29\uff0c\u5728\u5e73\u6ed1\u975e\u51f8\u673a\u5236\u4e2d\u6536\u7d27\u8bef\u5dee\u754c\u9650\uff1b\u66f4\u5927\u6279\u6b21\u8fdb\u4e00\u6b65\u51cf\u5c11\u5e38\u6570\u3002\u5b9e\u8bc1\u663e\u793a\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u5f71\u54cd\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u957f\u5468\u671f\u4e2d\u3002", "conclusion": "ACC-SGD-IE\u5728\u4e0b\u6e38\u6570\u636e\u6e05\u6d17\u4e2d\u66f4\u53ef\u9760\u5730\u6807\u8bb0\u566a\u58f0\u6837\u672c\uff0c\u4f7f\u7528\u5176\u6e05\u6d17\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u4f7f\u7528SGD-IE\u6e05\u6d17\u7684\u6a21\u578b\u3002"}}
{"id": "2510.26721", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.26721", "abs": "https://arxiv.org/abs/2510.26721", "authors": ["Xinhan Zheng", "Huyu Wu", "Xueting Wang", "Haiyun Jiang"], "title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis", "comment": null, "summary": "Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6587\u672c\u504f\u597d\u95ee\u9898\uff0c\u5176\u6839\u6e90\u5728\u4e8e\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u89c6\u89c9\u952e\u5411\u91cf\u4e0e\u6587\u672c\u952e\u5411\u91cf\u7684\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u89c6\u89c9\u4fe1\u606f\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u88ab\u4f4e\u4f30\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u65f6\u8868\u73b0\u51fa\u660e\u663e\u7684\u6587\u672c\u504f\u597d\uff0c\u9650\u5236\u4e86\u5176\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u6709\u6548\u63a8\u7406\u7684\u80fd\u529b\u3002\u4e0d\u540c\u4e8e\u5148\u524d\u7814\u7a76\u5c06\u8fd9\u79cd\u6587\u672c\u504f\u89c1\u5f52\u56e0\u4e8e\u6570\u636e\u4e0d\u5e73\u8861\u6216\u6307\u4ee4\u8c03\u4f18\u7b49\u5916\u90e8\u56e0\u7d20\uff0c\u672c\u7814\u7a76\u8ba4\u4e3a\u504f\u89c1\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u67b6\u6784\u3002", "method": "\u4eceLLaVA\u548cQwen2.5-VL\u6a21\u578b\u4e2d\u63d0\u53d6\u952e\u5411\u91cf\uff0c\u4f7f\u7528t-SNE\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u548cJensen-Shannon\u6563\u5ea6\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u7814\u7a76\u89c6\u89c9\u952e\u5411\u91cf\u548c\u6587\u672c\u952e\u5411\u91cf\u7684\u5206\u5e03\u7ed3\u6784\u3002", "result": "\u89c6\u89c9\u952e\u5411\u91cf\u548c\u6587\u672c\u952e\u5411\u91cf\u5728\u6ce8\u610f\u529b\u7a7a\u95f4\u4e2d\u5360\u636e\u660e\u663e\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\uff0c\u6a21\u6001\u95f4\u5dee\u5f02\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\uff0c\u8d85\u8fc7\u6a21\u6001\u5185\u53d8\u5f02\u7684\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6587\u672c\u504f\u89c1\u6e90\u4e8e\u6ce8\u610f\u529b\u952e\u7a7a\u95f4\u5185\u7684\u5185\u5728\u9519\u4f4d\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5916\u90e8\u6570\u636e\u56e0\u7d20\u3002"}}
{"id": "2510.26188", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26188", "abs": "https://arxiv.org/abs/2510.26188", "authors": ["Avinash Kadimisetty", "Arun Rajagopalan", "Vijendra SK"], "title": "Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients", "comment": "NCMLAI 2018", "summary": "Reducing preventable hospital readmissions is a national priority for payers,\nproviders, and policymakers seeking to improve health care and lower costs. The\nrate of readmission is being used as a benchmark to determine the quality of\nhealthcare provided by the hospitals. In thisproject, we have used machine\nlearning techniques like Logistic Regression, Random Forest and Support Vector\nMachines to analyze the health claims data and identify demographic and medical\nfactors that play a crucial role in predicting all-cause readmissions. As the\nhealth claims data is high dimensional, we have used Principal Component\nAnalysis as a dimension reduction technique and used the results for building\nregression models. We compared and evaluated these models based on the Area\nUnder Curve (AUC) metric. Random Forest model gave the highest performance\nfollowed by Logistic Regression and Support Vector Machine models. These models\ncan be used to identify the crucial factors causing readmissions and help\nidentify patients to focus on to reduce the chances of readmission, ultimately\nbringing down the cost and increasing the quality of healthcare provided to the\npatients.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u5065\u5eb7\u7d22\u8d54\u6570\u636e\uff0c\u8bc6\u522b\u5bfc\u81f4\u518d\u5165\u9662\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u964d\u4f4e\u53ef\u9884\u9632\u7684\u533b\u9662\u518d\u5165\u9662\u7387\u3002", "motivation": "\u964d\u4f4e\u53ef\u9884\u9632\u7684\u533b\u9662\u518d\u5165\u9662\u7387\u662f\u652f\u4ed8\u65b9\u3001\u63d0\u4f9b\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u7684\u56fd\u5bb6\u4f18\u5148\u4e8b\u9879\uff0c\u65e8\u5728\u63d0\u9ad8\u533b\u7597\u8d28\u91cf\u5e76\u964d\u4f4e\u6210\u672c\u3002\u518d\u5165\u9662\u7387\u88ab\u7528\u4f5c\u8bc4\u4f30\u533b\u9662\u533b\u7597\u8d28\u91cf\u7684\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u652f\u6301\u5411\u91cf\u673a\u7b49\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u5065\u5eb7\u7d22\u8d54\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4e3b\u6210\u5206\u5206\u6790\u8fdb\u884c\u964d\u7ef4\u5904\u7406\uff0c\u57fa\u4e8eAUC\u6307\u6807\u6bd4\u8f83\u548c\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5176\u6b21\u662f\u903b\u8f91\u56de\u5f52\u548c\u652f\u6301\u5411\u91cf\u673a\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u5bfc\u81f4\u518d\u5165\u9662\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u578b\u53ef\u7528\u4e8e\u8bc6\u522b\u5bfc\u81f4\u518d\u5165\u9662\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e2e\u52a9\u786e\u5b9a\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u60a3\u8005\uff0c\u4ece\u800c\u964d\u4f4e\u518d\u5165\u9662\u98ce\u9669\uff0c\u6700\u7ec8\u964d\u4f4e\u533b\u7597\u6210\u672c\u5e76\u63d0\u9ad8\u533b\u7597\u8d28\u91cf\u3002"}}
{"id": "2510.26219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26219", "abs": "https://arxiv.org/abs/2510.26219", "authors": ["Sekitoshi Kanai", "Tsukasa Yoshida", "Hiroshi Takahashi", "Haru Kuroki", "Kazumune Hashimoto"], "title": "Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space", "comment": "21 pages, 8 figures", "summary": "Test-time alignment of large language models (LLMs) attracts attention\nbecause fine-tuning LLMs requires high computational costs. In this paper, we\npropose a new test-time alignment method called adaptive importance sampling on\npre-logits (AISP) on the basis of the sampling-based model predictive control\nwith the stochastic control input. AISP applies the Gaussian perturbation into\npre-logits, which are outputs of the penultimate layer, so as to maximize\nexpected rewards with respect to the mean of the perturbation. We demonstrate\nthat the optimal mean is obtained by importance sampling with sampled rewards.\nAISP outperforms best-of-n sampling in terms of rewards over the number of used\nsamples and achieves higher rewards than other reward-based test-time alignment\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5AISP\uff0c\u901a\u8fc7\u5728\u9884\u5bf9\u6570\u5c42\u5e94\u7528\u9ad8\u65af\u6270\u52a8\u6765\u6700\u5927\u5316\u671f\u671b\u5956\u52b1\uff0c\u4f18\u4e8e\u6700\u4f73n\u91c7\u6837\u548c\u5176\u4ed6\u57fa\u4e8e\u5956\u52b1\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u57fa\u4e8e\u968f\u673a\u63a7\u5236\u8f93\u5165\u7684\u91c7\u6837\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5728\u9884\u5bf9\u6570\u5c42\u5e94\u7528\u9ad8\u65af\u6270\u52a8\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u83b7\u5f97\u6700\u4f18\u5747\u503c\u3002", "result": "AISP\u5728\u6837\u672c\u4f7f\u7528\u6570\u91cf\u65b9\u9762\u4f18\u4e8e\u6700\u4f73n\u91c7\u6837\uff0c\u5e76\u6bd4\u5176\u4ed6\u57fa\u4e8e\u5956\u52b1\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u3002", "conclusion": "AISP\u662f\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.26752", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26752", "abs": "https://arxiv.org/abs/2510.26752", "authors": ["William Overman", "Mohsen Bayati"], "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy", "comment": null, "summary": "As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u6700\u5c0f\u63a7\u5236\u63a5\u53e3\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u81ea\u4e3b\u884c\u52a8\u6216\u8bf7\u6c42\u4eba\u7c7b\u5e72\u9884\u4e4b\u95f4\u9009\u62e9\uff0c\u540c\u65f6\u4eba\u7c7b\u5728\u4fe1\u4efb\u6216\u76d1\u7763\u4e4b\u95f4\u9009\u62e9\u3002\u901a\u8fc7\u5c06\u5176\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u6f5c\u5728\u6e38\u620f\uff0c\u63d0\u4f9b\u4e86\u5bf9\u9f50\u4fdd\u8bc1\uff1a\u5728\u4eba\u7c7b\u4ef7\u503c\u51fd\u6570\u7684\u7ed3\u6784\u5047\u8bbe\u4e0b\uff0c\u667a\u80fd\u4f53\u589e\u52a0\u81ea\u4e3b\u6027\u7684\u51b3\u7b56\u4e0d\u4f1a\u635f\u5bb3\u4eba\u7c7b\u4ef7\u503c\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u5e95\u5c42\u7cfb\u7edf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u7ef4\u6301\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u63a7\u5236\u6210\u4e3a\u4e00\u4e2a\u6838\u5fc3\u5b89\u5168\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u900f\u660e\u63a7\u5236\u5c42\uff0c\u4f7f\u667a\u80fd\u4f53\u5b66\u4f1a\u5728\u98ce\u9669\u65f6\u8bf7\u6c42\u5e72\u9884\uff0c\u5728\u5b89\u5168\u65f6\u81ea\u4e3b\u884c\u52a8\u3002", "method": "\u5c06\u4eba\u673a\u4ea4\u4e92\u5efa\u6a21\u4e3a\u53cc\u73a9\u5bb6\u9a6c\u5c14\u53ef\u592b\u6e38\u620f\uff0c\u7279\u522b\u5173\u6ce8\u9a6c\u5c14\u53ef\u592b\u6f5c\u5728\u6e38\u620f\u60c5\u51b5\u3002\u901a\u8fc7\u72ec\u7acb\u5b66\u4e60\uff0c\u667a\u80fd\u4f53\u548c\u4eba\u7c7b\u53d1\u73b0\u5404\u81ea\u7684\u6700\u4f18\u76d1\u7763\u89d2\u8272\uff0c\u667a\u80fd\u4f53\u5b66\u4f1a\u5728\u4e0d\u786e\u5b9a\u65f6\u8bf7\u6c42\u5e2e\u52a9\uff0c\u4eba\u7c7b\u5b66\u4f1a\u4f55\u65f6\u8fdb\u884c\u76d1\u7763\u3002", "result": "\u7f51\u683c\u4e16\u754c\u6a21\u62df\u663e\u793a\uff0c\u901a\u8fc7\u72ec\u7acb\u5b66\u4e60\uff0c\u667a\u80fd\u4f53\u548c\u4eba\u7c7b\u80fd\u591f\u53d1\u73b0\u6700\u4f18\u76d1\u7763\u89d2\u8272\uff0c\u5f62\u6210\u65b0\u5174\u534f\u4f5c\uff0c\u907f\u514d\u8bad\u7ec3\u540e\u5f15\u5165\u7684\u5b89\u5168\u8fdd\u89c4\u3002\u667a\u80fd\u4f53\u5b66\u4f1a\u5728\u4e0d\u786e\u5b9a\u65f6\u8bf7\u6c42\u5e2e\u52a9\uff0c\u4eba\u7c7b\u5b66\u4f1a\u9002\u65f6\u76d1\u7763\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u90e8\u7f72\u540e\u4f7f\u672a\u5bf9\u9f50\u6a21\u578b\u66f4\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u5728\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u65f6\uff0c\u667a\u80fd\u4f53\u6539\u5584\u81ea\u8eab\u7ed3\u679c\u4e0d\u4f1a\u635f\u5bb3\u4eba\u7c7b\u4ef7\u503c\uff0c\u5b9e\u73b0\u4e86\u7279\u5b9a\u5f62\u5f0f\u7684\u5185\u5728\u5bf9\u9f50\u3002"}}
{"id": "2510.26230", "categories": ["cs.LG", "cs.AI", "68T09 68T09"], "pdf": "https://arxiv.org/pdf/2510.26230", "abs": "https://arxiv.org/abs/2510.26230", "authors": ["Minyi Peng", "Darian Gunamardi", "Ivan Tjuawinata", "Kwok-Yan Lam"], "title": "MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines", "comment": "10 pages, 6 figures", "summary": "As a new and promising approach, existing machine unlearning (MU) works\ntypically emphasize theoretical formulations or optimization objectives to\nachieve knowledge removal. However, when deployed in real-world scenarios, such\nsolutions typically face scalability issues and have to address practical\nrequirements such as full access to original datasets and model. In contrast to\nthe existing approaches, we regard classification training as a sequential\nprocess where classes are learned sequentially, which we call \\emph{inductive\napproach}. Unlearning can then be done by reversing the last training sequence.\nThis is implemented by appending a projection-redistribution layer in the end\nof the model. Such an approach does not require full access to the original\ndataset or the model, addressing the challenges of existing methods. This\nenables modular and model-agnostic deployment as an output filter into existing\nclassification pipelines with minimal alterations. We conducted multiple\nexperiments across multiple datasets including image (CIFAR-10/100 using\nCNN-based model) and tabular datasets (Covertype using tree-based model).\nExperiment results show consistently similar output to a fully retrained model\nwith a high computational cost reduction. This demonstrates the applicability,\nscalability, and system compatibility of our solution while maintaining the\nperformance of the output in a more practical setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u7eb3\u65b9\u6cd5\u7684\u673a\u5668\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u672b\u5c3e\u6dfb\u52a0\u6295\u5f71-\u91cd\u5206\u5e03\u5c42\u6765\u5b9e\u73b0\u9057\u5fd8\uff0c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u96c6\u6216\u5b8c\u6574\u6a21\u578b\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u6a21\u578b\u65e0\u5173\u7684\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u9700\u8981\u5b8c\u6574\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5b9e\u9645\u6311\u6218\u3002", "method": "\u5c06\u5206\u7c7b\u8bad\u7ec3\u89c6\u4e3a\u987a\u5e8f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u53cd\u8f6c\u6700\u540e\u8bad\u7ec3\u5e8f\u5217\u5b9e\u73b0\u9057\u5fd8\uff0c\u5728\u6a21\u578b\u672b\u5c3e\u6dfb\u52a0\u6295\u5f71-\u91cd\u5206\u5e03\u5c42\u4f5c\u4e3a\u8f93\u51fa\u8fc7\u6ee4\u5668\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08CIFAR-10/100\u3001Covertype\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8f93\u51fa\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u4f3c\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8f93\u51fa\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c55\u793a\u4e86\u9002\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7cfb\u7edf\u517c\u5bb9\u6027\uff0c\u4e3a\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26342", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26342", "abs": "https://arxiv.org/abs/2510.26342", "authors": ["Zhigao Guo", "Feng Dong"], "title": "Linear Causal Discovery with Interventional Constraints", "comment": null, "summary": "Incorporating causal knowledge and mechanisms is essential for refining\ncausal models and improving downstream tasks such as designing new treatments.\nIn this paper, we introduce a novel concept in causal discovery, termed\ninterventional constraints, which differs fundamentally from interventional\ndata. While interventional data require direct perturbations of variables,\ninterventional constraints encode high-level causal knowledge in the form of\ninequality constraints on causal effects. For instance, in the Sachs dataset\n(Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3\nexerts a positive causal effect on Akt. Existing causal discovery methods allow\nenforcing structural constraints (for example, requiring a causal path from\nPIP3 to Akt), but they may still produce incorrect causal conclusions such as\nlearning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap\nby explicitly constraining the total causal effect between variable pairs,\nensuring learned models respect known causal influences. To formalize\ninterventional constraints, we propose a metric to quantify total causal\neffects for linear causal models and formulate the problem as a constrained\noptimization task, solved using a two-stage constrained optimization method. We\nevaluate our approach on real-world datasets and demonstrate that integrating\ninterventional constraints not only improves model accuracy and ensures\nconsistency with established findings, making models more explainable, but also\nfacilitates the discovery of new causal relationships that would otherwise be\ncostly to identify.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56e0\u679c\u53d1\u73b0\u6982\u5ff5\u2014\u2014\u5e72\u9884\u7ea6\u675f\uff0c\u5b83\u4e0d\u540c\u4e8e\u5e72\u9884\u6570\u636e\uff0c\u800c\u662f\u4ee5\u56e0\u679c\u6548\u5e94\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684\u5f62\u5f0f\u7f16\u7801\u9ad8\u5c42\u56e0\u679c\u77e5\u8bc6\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u6a21\u578b\u7b26\u5408\u5df2\u77e5\u7684\u56e0\u679c\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u65bd\u52a0\u7ed3\u6784\u7ea6\u675f\uff0c\u4f46\u4ecd\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u7684\u56e0\u679c\u7ed3\u8bba\u3002\u4e3a\u4e86\u6539\u8fdb\u56e0\u679c\u6a21\u578b\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\uff0c\u9700\u8981\u6574\u5408\u56e0\u679c\u77e5\u8bc6\u548c\u673a\u5236\u3002", "method": "\u63d0\u51fa\u5e72\u9884\u7ea6\u675f\u6982\u5ff5\uff0c\u4e3a\u7ebf\u6027\u56e0\u679c\u6a21\u578b\u63d0\u51fa\u91cf\u5316\u603b\u56e0\u679c\u6548\u5e94\u7684\u5ea6\u91cf\uff0c\u5e76\u6784\u5efa\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6574\u5408\u5e72\u9884\u7ea6\u675f\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3001\u786e\u4fdd\u4e0e\u5df2\u6709\u53d1\u73b0\u7684\u4e00\u81f4\u6027\uff0c\u8fd8\u80fd\u4fc3\u8fdb\u53d1\u73b0\u539f\u672c\u6210\u672c\u9ad8\u6602\u7684\u65b0\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u5e72\u9884\u7ea6\u675f\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u660e\u786e\u7ea6\u675f\u53d8\u91cf\u5bf9\u4e4b\u95f4\u7684\u603b\u56e0\u679c\u6548\u5e94\uff0c\u4f7f\u5b66\u4e60\u5230\u7684\u6a21\u578b\u66f4\u7b26\u5408\u5df2\u77e5\u56e0\u679c\u5f71\u54cd\uff0c\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.26347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26347", "abs": "https://arxiv.org/abs/2510.26347", "authors": ["Sebastian Zieglmeier", "Niklas Erdmann", "Narada D. Warakagoda"], "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle", "comment": null, "summary": "Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6539\u8fdb\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u7a00\u758f\u3001\u968f\u673a\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u7279\u522b\u662f\u5728\u6c34\u4e0b\u6c61\u67d3\u4e91\u641c\u7d22\u7b49\u5e94\u7528\u4e2d\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u968f\u673a\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u7a00\u758f\u7684\u73af\u5883\u4e0b\uff0c\u8bb8\u591a\u884c\u52a8\u53ea\u80fd\u83b7\u5f97\u96f6\u5956\u52b1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u5927\u91cf\u6539\u8fdb\u65b9\u6cd5\uff0c\u5305\u62ec\u5206\u5c42\u7b97\u6cd5\u53d8\u66f4\u3001\u591a\u76ee\u6807\u5b66\u4e60\uff0c\u4ee5\u53ca\u96c6\u6210\u4f4d\u7f6e\u8bb0\u5fc6\u4f5c\u4e3a\u5916\u90e8\u8f93\u51fa\u8fc7\u6ee4\u5668\u4ee5\u9632\u6b62\u72b6\u6001\u91cd\u590d\u8bbf\u95ee\u3002\u91c7\u7528\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002", "result": "\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfQ\u5b66\u4e60\u548c\u4e24\u79cd\u7a77\u4e3e\u641c\u7d22\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u9002\u5e94\u968f\u673a\u3001\u975e\u5e73\u7a33\u548c\u5956\u52b1\u7a00\u758f\u7684\u73af\u5883\uff0c\u4e3a\u590d\u6742\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26369", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26369", "abs": "https://arxiv.org/abs/2510.26369", "authors": ["Kazuma Kano", "Yuki Mori", "Shin Katayama", "Kenta Urano", "Takuro Yonezawa", "Nobuo Kawaguchi"], "title": "CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse", "comment": "7 pages, 3 figures, accepted to IPIN 2025", "summary": "Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.", "AI": {"tldr": "\u63d0\u51faCorVS\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u5bf9\u5e94\u5173\u7cfb\u8fdb\u884c\u4eba\u5458\u8bc6\u522b\uff0c\u5728\u771f\u5b9e\u4ed3\u5e93\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "motivation": "\u5de5\u4e1a\u573a\u6240\u4e2d\u5de5\u4eba\u5b9a\u4f4d\u6570\u636e\u5bf9\u63d0\u9ad8\u751f\u4ea7\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ec5\u51ed\u89c6\u89c9\u6570\u636e\u8bc6\u522b\u4e2a\u4f53\u5f80\u5f80\u4e0d\u5b9e\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u53ef\u80fd\u5931\u6548", "method": "1. \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u5bf9\u5e94\u6982\u7387\u548c\u53ef\u9760\u6027\uff1b2. \u57fa\u4e8e\u9884\u6d4b\u6982\u7387\u548c\u53ef\u9760\u6027\u968f\u65f6\u95f4\u5339\u914d\u8f68\u8ff9\u548c\u4f20\u611f\u5668\u6d4b\u91cf", "result": "\u5f00\u53d1\u4e86\u771f\u5b9e\u4ed3\u5e93\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "CorVS\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u4eba\u5458\u8bc6\u522b"}}
{"id": "2510.26451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26451", "abs": "https://arxiv.org/abs/2510.26451", "authors": ["Jiayi Luo", "Qingyun Sun", "Beining Yang", "Haonan Yuan", "Xingcheng Fu", "Yanbiao Ma", "Jianxin Li", "Philip S. Yu"], "title": "Robust Graph Condensation via Classification Complexity Mitigation", "comment": null, "summary": "Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d41\u5f62\u7ea6\u675f\u9c81\u68d2\u56fe\u538b\u7f29\u6846\u67b6MRGC\uff0c\u901a\u8fc7\u4e09\u4e2a\u56fe\u6570\u636e\u6d41\u5f62\u5b66\u4e60\u6a21\u5757\uff0c\u4f7f\u538b\u7f29\u56fe\u4f4d\u4e8e\u5e73\u6ed1\u3001\u4f4e\u7ef4\u7684\u6d41\u5f62\u4e2d\uff0c\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u538b\u7f29\u65b9\u6cd5\u5728\u539f\u59cb\u56fe\u88ab\u7834\u574f\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u73b0\u6709\u9c81\u68d2\u56fe\u5b66\u4e60\u6280\u672f\u6548\u679c\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\u56fe\u538b\u7f29\u672c\u8d28\u4e0a\u662f\u5185\u5728\u7ef4\u5ea6\u964d\u4f4e\u8fc7\u7a0b\uff0c\u4f46\u8fd9\u4e00\u7279\u6027\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6270\u52a8\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u56fe\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u89c6\u89d2\uff0c\u63d0\u51faMRGC\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u56fe\u6570\u636e\u6d41\u5f62\u5b66\u4e60\u6a21\u5757\uff0c\u5f15\u5bfc\u538b\u7f29\u56fe\u4f4d\u4e8e\u5e73\u6ed1\u3001\u4f4e\u7ef4\u6d41\u5f62\u4e2d\uff0c\u6700\u5c0f\u5316\u7c7b\u522b\u6a21\u7cca\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMRGC\u5728\u5404\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "MRGC\u6846\u67b6\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u6709\u6548\u63d0\u5347\u4e86\u56fe\u538b\u7f29\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.26501", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26501", "abs": "https://arxiv.org/abs/2510.26501", "authors": ["Mustafa Fuad Rifet Ibrahim", "Maurice Meijer", "Alexander Schlaefer", "Peer Stelldinger"], "title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters", "comment": "Submitted to the 24th International Conference on Pervasive Computing\n  and Communications (PerCom 2026)", "summary": "Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u8d44\u6e90\u53d7\u9650\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b(UAD)\u4f5c\u4e3a\u4e0a\u6e38\u8fc7\u6ee4\u673a\u5236\u6765\u63d0\u9ad8\u5fc3\u7535\u56fe(ECG)\u5206\u6790\u7684\u9c81\u68d2\u6027\uff0c\u9632\u6b62\u56e0\u5206\u5e03\u5916(OOD)\u6570\u636e\u5bfc\u81f4\u7684\u9519\u8bef\u9884\u6d4b\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u8fde\u7eedECG\u76d1\u6d4b\u5bf9\u65e9\u671f\u5fc3\u8840\u7ba1\u75be\u75c5\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9762\u5bf9OOD\u6570\u636e\u65f6\u4f1a\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u9519\u8bef\u9884\u6d4b\uff0c\u5371\u53ca\u60a3\u8005\u5b89\u5168\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u8ba1\u7b97\u7ea6\u675f\uff0c\u8981\u4e48\u5206\u522b\u5904\u7406\u566a\u58f0\u548c\u672a\u89c1\u7c7b\u522b\u95ee\u9898\u3002", "method": "\u5728\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u4e0b(\u6700\u591a512k\u53c2\u6570)\uff0c\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22(NAS)\u4f18\u5316\u516d\u79cdUAD\u65b9\u6cd5\uff0c\u5305\u62ecDeep SVDD\u3001\u91cd\u5efa\u6a21\u578b\u3001\u63a9\u7801\u5f02\u5e38\u68c0\u6d4b\u3001\u5f52\u4e00\u5316\u6d41\u548c\u6269\u6563\u6a21\u578b\u3002\u5728PTB-XL\u548cBUT QDB\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5bf9OOD\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u522b\u548c\u566a\u58f0\u6c61\u67d3\u4fe1\u53f7\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "Deep SVDD\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\u3002\u5728\u5b9e\u9645\u90e8\u7f72\u6a21\u62df\u4e2d\uff0c\u5c06\u4f18\u5316\u7684Deep SVDD\u8fc7\u6ee4\u5668\u4e0e\u8bca\u65ad\u5206\u7c7b\u5668\u96c6\u6210\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5206\u7c7b\u5668\u7684\u57fa\u7ebf\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u6700\u591a21\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u4f18\u5316\u7684UAD\u8fc7\u6ee4\u5668\u80fd\u591f\u4fdd\u62a4\u81ea\u52a8\u5316ECG\u5206\u6790\uff0c\u4f7f\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u7684\u8fde\u7eed\u5fc3\u8840\u7ba1\u76d1\u6d4b\u66f4\u52a0\u5b89\u5168\u53ef\u9760\u3002"}}
{"id": "2510.26533", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.26533", "abs": "https://arxiv.org/abs/2510.26533", "authors": ["Adrien Weihs", "Andrea Bertozzi", "Matthew Thorpe"], "title": "Higher-Order Regularization Learning on Hypergraphs", "comment": null, "summary": "Higher-Order Hypergraph Learning (HOHL) was recently introduced as a\nprincipled alternative to classical hypergraph regularization, enforcing\nhigher-order smoothness via powers of multiscale Laplacians induced by the\nhypergraph structure. Prior work established the well- and ill-posedness of\nHOHL through an asymptotic consistency analysis in geometric settings. We\nextend this theoretical foundation by proving the consistency of a truncated\nversion of HOHL and deriving explicit convergence rates when HOHL is used as a\nregularizer in fully supervised learning. We further demonstrate its strong\nempirical performance in active learning and in datasets lacking an underlying\ngeometric structure, highlighting HOHL's versatility and robustness across\ndiverse learning settings.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u9ad8\u9636\u8d85\u56fe\u5b66\u4e60\uff08HOHL\uff09\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u622a\u65ad\u7248\u672cHOHL\u7684\u4e00\u81f4\u6027\uff0c\u63a8\u5bfc\u4e86\u5728\u5b8c\u5168\u76d1\u7763\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u7684\u6536\u655b\u901f\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u975e\u51e0\u4f55\u7ed3\u6784\u6570\u636e\u96c6\u4e2d\u7684\u5f3a\u5927\u5b9e\u8bc1\u6027\u80fd\u3002", "motivation": "\u5148\u524d\u5de5\u4f5c\u901a\u8fc7\u51e0\u4f55\u8bbe\u7f6e\u4e2d\u7684\u6e10\u8fd1\u4e00\u81f4\u6027\u5206\u6790\u5efa\u7acb\u4e86HOHL\u7684\u9002\u5b9a\u6027\u548c\u4e0d\u9002\u5b9a\u6027\uff0c\u672c\u6587\u65e8\u5728\u6269\u5c55\u8fd9\u4e00\u7406\u8bba\u57fa\u7840\u5e76\u9a8c\u8bc1HOHL\u5728\u4e0d\u540c\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u622a\u65adHOHL\u7684\u4e00\u81f4\u6027\uff0c\u63a8\u5bfcHOHL\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u5728\u5b8c\u5168\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u663e\u5f0f\u6536\u655b\u901f\u7387\uff0c\u5e76\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u975e\u51e0\u4f55\u7ed3\u6784\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u622a\u65adHOHL\u7684\u4e00\u81f4\u6027\uff0c\u83b7\u5f97\u4e86\u660e\u786e\u7684\u6536\u655b\u901f\u7387\uff1b\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aHOHL\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u7f3a\u4e4f\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "HOHL\u5728\u4e0d\u540c\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5c55\u73b0\u51fa\u591a\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u8d85\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2510.26557", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26557", "abs": "https://arxiv.org/abs/2510.26557", "authors": ["Jan Stenkamp", "Nina Herrmann", "Benjamin Karic", "Stefan Oehmcke", "Fabian Gieseke"], "title": "Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices", "comment": null, "summary": "Deploying machine learning models on compute-constrained devices has become a\nkey building block of modern IoT applications. In this work, we present a\ncompression scheme for boosted decision trees, addressing the growing need for\nlightweight machine learning models. Specifically, we provide techniques for\ntraining compact boosted decision tree ensembles that exhibit a reduced memory\nfootprint by rewarding, among other things, the reuse of features and\nthresholds during training. Our experimental evaluation shows that models\nachieved the same performance with a compression ratio of 4-16x compared to\nLightGBM models using an adapted training process and an alternative memory\nlayout. Once deployed, the corresponding IoT devices can operate independently\nof constant communication or external energy supply, and, thus, autonomously,\nrequiring only minimal computing power and energy. This capability opens the\ndoor to a wide range of IoT applications, including remote monitoring, edge\nanalytics, and real-time decision making in isolated or power-limited\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u63d0\u5347\u51b3\u7b56\u6811\u7684\u538b\u7f29\u65b9\u6848\uff0c\u901a\u8fc7\u7279\u5f81\u548c\u9608\u503c\u91cd\u7528\u7b49\u6280\u672f\u8bad\u7ec3\u7d27\u51d1\u7684\u6811\u96c6\u6210\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b04-16\u500d\u7684\u538b\u7f29\u6bd4\u3002", "motivation": "\u5728\u8ba1\u7b97\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u652f\u6301\u8bbe\u5907\u5728\u65e0\u6301\u7eed\u901a\u4fe1\u6216\u5916\u90e8\u80fd\u6e90\u4f9b\u5e94\u7684\u60c5\u51b5\u4e0b\u81ea\u4e3b\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u7279\u5f81\u548c\u9608\u503c\u91cd\u7528\u7b49\u6280\u672f\u8bad\u7ec3\u7d27\u51d1\u7684\u63d0\u5347\u51b3\u7b56\u6811\u96c6\u6210\uff0c\u4f7f\u7528\u6539\u8fdb\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u66ff\u4ee3\u5185\u5b58\u5e03\u5c40\u6765\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u4e0eLightGBM\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e864-16\u500d\u7684\u538b\u7f29\u6bd4\u3002", "conclusion": "\u8be5\u538b\u7f29\u65b9\u6848\u4f7f\u5f97\u7269\u8054\u7f51\u8bbe\u5907\u80fd\u591f\u5728\u4ec5\u9700\u6700\u5c0f\u8ba1\u7b97\u80fd\u529b\u548c\u80fd\u6e90\u7684\u60c5\u51b5\u4e0b\u81ea\u4e3b\u8fd0\u884c\uff0c\u4e3a\u8fdc\u7a0b\u76d1\u63a7\u3001\u8fb9\u7f18\u5206\u6790\u548c\u5b9e\u65f6\u51b3\u7b56\u7b49\u5e94\u7528\u5f00\u8f9f\u4e86\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2510.26714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26714", "abs": "https://arxiv.org/abs/2510.26714", "authors": ["Jamie Lanyon", "Axel Finke", "Petros Andreou", "Georgina Cosma"], "title": "On the limitation of evaluating machine unlearning using only a single training seed", "comment": "mini paper, 2 figures", "summary": "Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u8bc4\u4f30\u4e2d\u5e38\u89c1\u7684\u505a\u6cd5\u5b58\u5728\u95ee\u9898\uff1a\u591a\u6b21\u4ece\u540c\u4e00\u8bad\u7ec3\u6a21\u578b\u5f00\u59cb\u8fd0\u884c\u9057\u5fd8\u7b97\u6cd5\u53ef\u80fd\u4ea7\u751f\u975e\u4ee3\u8868\u6027\u7ed3\u679c\uff0c\u56e0\u4e3a\u67d0\u4e9b\u9057\u5fd8\u65b9\u6cd5\u5bf9\u6a21\u578b\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u968f\u673a\u6570\u79cd\u5b50\u9ad8\u5ea6\u654f\u611f\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u901a\u5e38\u53ea\u80fd\u8fd1\u4f3c\u5730\u79fb\u9664\u6570\u636e\u5f71\u54cd\uff0c\u5176\u6027\u80fd\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u8bc4\u4f30\u3002\u73b0\u6709\u8bc4\u4f30\u5b9e\u8df5\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u5bf9\u6a21\u578b\u8bad\u7ec3\u968f\u673a\u79cd\u5b50\u7684\u654f\u611f\u6027\uff0c\u63d0\u51fa\u8bc4\u4f30\u65f6\u5e94\u8003\u8651\u4e0d\u540c\u8bad\u7ec3\u79cd\u5b50\u5e26\u6765\u7684\u53d8\u5f02\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5bf9\u8bad\u7ec3\u968f\u673a\u79cd\u5b50\u9ad8\u5ea6\u654f\u611f\uff0c\u4ece\u540c\u4e00\u8bad\u7ec3\u6a21\u578b\u591a\u6b21\u8fd0\u884c\u53ef\u80fd\u4ea7\u751f\u975e\u4ee3\u8868\u6027\u7ed3\u679c\u3002", "conclusion": "\u5efa\u8bae\u5728\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\u7684\u7ecf\u9a8c\u6bd4\u8f83\u4e2d\uff0c\u5fc5\u987b\u53cd\u6620\u4e0d\u540c\u6a21\u578b\u8bad\u7ec3\u79cd\u5b50\u5e26\u6765\u7684\u53d8\u5f02\u6027\uff0c\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2510.26607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26607", "abs": "https://arxiv.org/abs/2510.26607", "authors": ["Maksim Maslov", "Alexander Kugaevskikh", "Matthew Ivanov"], "title": "Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis", "comment": null, "summary": "This paper considers the problem of regression over distributions, which is\nbecoming increasingly important in machine learning. Existing approaches often\nignore the geometry of the probability space or are computationally expensive.\nTo overcome these limitations, a new method is proposed that combines the\nparameterization of probability trajectories using a Bernstein basis and the\nminimization of the Wasserstein distance between distributions. The key idea is\nto model a conditional distribution as a smooth probability trajectory defined\nby a weighted sum of Gaussian components whose parameters -- the mean and\ncovariance -- are functions of the input variable constructed using Bernstein\npolynomials. The loss function is the averaged squared Wasserstein distance\nbetween the predicted Gaussian distributions and the empirical data, which\ntakes into account the geometry of the distributions. An autodiff-based\noptimization method is used to train the model. Experiments on synthetic\ndatasets that include complex trajectories demonstrated that the proposed\nmethod provides competitive approximation quality in terms of the Wasserstein\ndistance, Energy Distance, and RMSE metrics, especially in cases of pronounced\nnonlinearity. The model demonstrates trajectory smoothness that is better than\nor comparable to alternatives and robustness to changes in data structure,\nwhile maintaining high interpretability due to explicit parameterization via\ncontrol points. The developed approach represents a balanced solution that\ncombines geometric accuracy, computational practicality, and interpretability.\nProspects for further research include extending the method to non-Gaussian\ndistributions, applying entropy regularization to speed up computations, and\nadapting the approach to working with high-dimensional data for approximating\nsurfaces and more complex structures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4f2f\u6069\u65af\u5766\u57fa\u53c2\u6570\u5316\u548cWasserstein\u8ddd\u79bb\u6700\u5c0f\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u56de\u5f52\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u56de\u5f52\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u6982\u7387\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u51e0\u4f55\u7cbe\u5ea6\u548c\u8ba1\u7b97\u5b9e\u7528\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4f2f\u6069\u65af\u5766\u57fa\u53c2\u6570\u5316\u6982\u7387\u8f68\u8ff9\uff0c\u5c06\u6761\u4ef6\u5206\u5e03\u5efa\u6a21\u4e3a\u9ad8\u65af\u5206\u91cf\u7684\u52a0\u6743\u548c\uff0c\u5176\u5747\u503c\u548c\u534f\u65b9\u5dee\u901a\u8fc7\u4f2f\u6069\u65af\u5766\u591a\u9879\u5f0f\u6784\u9020\u4e3a\u8f93\u5165\u53d8\u91cf\u7684\u51fd\u6570\uff0c\u91c7\u7528\u57fa\u4e8e\u81ea\u52a8\u5fae\u5206\u7684\u4f18\u5316\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u5305\u542b\u590d\u6742\u8f68\u8ff9\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728Wasserstein\u8ddd\u79bb\u3001\u80fd\u91cf\u8ddd\u79bb\u548cRMSE\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u8fd1\u4f3c\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u975e\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u4f18\u4e8e\u6216\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5bf9\u6570\u636e\u7ed3\u6784\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee3\u8868\u4e86\u5e73\u8861\u51e0\u4f55\u7cbe\u5ea6\u3001\u8ba1\u7b97\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6269\u5c55\u5230\u975e\u9ad8\u65af\u5206\u5e03\u3001\u5e94\u7528\u71b5\u6b63\u5219\u5316\u52a0\u901f\u8ba1\u7b97\uff0c\u4ee5\u53ca\u9002\u5e94\u9ad8\u7ef4\u6570\u636e\u8fd1\u4f3c\u66f2\u9762\u548c\u66f4\u590d\u6742\u7ed3\u6784\u3002"}}
{"id": "2510.26633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26633", "abs": "https://arxiv.org/abs/2510.26633", "authors": ["Colin Doumont", "Victor Picheny", "Viacheslav Borovitskiy", "Henry Moss"], "title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization", "comment": null, "summary": "Bayesian Optimization (BO) has the potential to solve various combinatorial\ntasks, ranging from materials science to neural architecture search. However,\nBO requires specialized kernels to effectively model combinatorial domains.\nRecent efforts have introduced several combinatorial kernels, but the\nrelationships among them are not well understood. To bridge this gap, we\ndevelop a unifying framework based on heat kernels, which we derive in a\nsystematic way and express as simple closed-form expressions. Using this\nframework, we prove that many successful combinatorial kernels are either\nrelated or equivalent to heat kernels, and validate this theoretical claim in\nour experiments. Moreover, our analysis confirms and extends the results\npresented in Bounce: certain algorithms' performance decreases substantially\nwhen the unknown optima of the function do not have a certain structure. In\ncontrast, heat kernels are not sensitive to the location of the optima. Lastly,\nwe show that a fast and simple pipeline, relying on heat kernels, is able to\nachieve state-of-the-art results, matching or even outperforming certain slow\nor complex algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u70ed\u6838\u7684\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u5185\u6838\uff0c\u8bc1\u660e\u4e86\u591a\u4e2a\u6210\u529f\u7ec4\u5408\u5185\u6838\u4e0e\u70ed\u6838\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5e76\u5c55\u793a\u4e86\u70ed\u6838\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u590d\u6742\u7b97\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u5185\u6838\u6765\u5efa\u6a21\u7ec4\u5408\u57df\u3002\u73b0\u6709\u7ec4\u5408\u5185\u6838\u4e4b\u95f4\u7684\u5173\u7cfb\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e9b\u5185\u6838\u7684\u5185\u5728\u8054\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u70ed\u6838\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u63a8\u5bfc\u70ed\u6838\u5e76\u8868\u8fbe\u4e3a\u7b80\u5355\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u591a\u4e2a\u7ec4\u5408\u5185\u6838\u4e0e\u70ed\u6838\u7684\u5173\u7cfb\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u4e2a\u6210\u529f\u7ec4\u5408\u5185\u6838\u4e0e\u70ed\u6838\u76f8\u5173\u6216\u7b49\u4ef7\uff1b\u70ed\u6838\u5bf9\u6700\u4f18\u503c\u4f4d\u7f6e\u4e0d\u654f\u611f\uff0c\u800c\u67d0\u4e9b\u7b97\u6cd5\u6027\u80fd\u4f1a\u5927\u5e45\u4e0b\u964d\uff1b\u57fa\u4e8e\u70ed\u6838\u7684\u7b80\u5355\u7ba1\u9053\u80fd\u591f\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u70ed\u6838\u6846\u67b6\u4e3a\u7406\u89e3\u7ec4\u5408\u5185\u6838\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u70ed\u6838\u65b9\u6cd5\u4e0d\u4ec5\u7406\u8bba\u4e0a\u6709\u4f18\u52bf\uff0c\u5728\u5b9e\u8df5\u4e2d\u4e5f\u80fd\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u7ec4\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26776", "abs": "https://arxiv.org/abs/2510.26776", "authors": ["Jungyeon Koh", "Hyeonsu Lyu", "Jonggyu Jang", "Hyun Jong Yang"], "title": "Faithful and Fast Influence Function via Advanced Sampling", "comment": null, "summary": "How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u7279\u5f81\u548clogits\u7684\u5148\u8fdb\u91c7\u6837\u6280\u672f\uff0c\u901a\u8fc7\u9009\u62e9\u5177\u6709\u4ee3\u8868\u6027\u7684\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\u6765\u6539\u8fdb\u5f71\u54cd\u51fd\u6570\u4f30\u8ba1\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "\u5f71\u54cd\u51fd\u6570\u9700\u8981\u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\u7684Hessian\u77e9\u9635\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3002\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u867d\u7136\u53ef\u884c\u4f46\u4f30\u8ba1\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u91c7\u6837\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7279\u5f81\u548clogits\u7684\u4e24\u79cd\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u8003\u8651\u7279\u5f81\u6216logits\u7684\u968f\u673a\u5206\u5e03\u6765\u9009\u62e9\u5177\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u5b50\u96c6\uff0c\u4ece\u800c\u6539\u8fdb\u5f71\u54cd\u51fd\u6570\u4f30\u8ba1\u3002", "result": "\u5728\u7c7b\u522b\u79fb\u9664\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u51cf\u5c11\u4e8630.1%\u7684\u8ba1\u7b97\u65f6\u95f4\u548c42.2%\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u6216\u5c06F1\u5206\u6570\u63d0\u9ad8\u4e862.5%\u3002", "conclusion": "\u63d0\u51fa\u7684\u91c7\u6837\u6280\u672f\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5f71\u54cd\u51fd\u6570\u8ba1\u7b97\u6240\u9700\u7684\u8d44\u6e90\uff0c\u540c\u65f6\u63d0\u9ad8\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u5f71\u54cd\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26645", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26645", "abs": "https://arxiv.org/abs/2510.26645", "authors": ["Katarina Petrovi\u0107", "Lazar Atanackovic", "Viggo Moro", "Kacper Kapu\u015bniak", "\u0130smail \u0130lkan Ceylan", "Michael Bronstein", "Avishek Joey Bose", "Alexander Tong"], "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics", "comment": "Accepted to NeurIPS 2025", "summary": "Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCurly-FM\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u975e\u68af\u5ea6\u573a\u52a8\u529b\u5b66\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5468\u671f\u6027\u884c\u4e3a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5355\u7ec6\u80deRNA\u3001\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u6d0b\u6d41\u7b49\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6700\u5c0f\u4f5c\u7528\u539f\u7406\u7684\u65b9\u6cd5\u53ea\u80fd\u5b66\u4e60\u68af\u5ea6\u573a\u52a8\u529b\u5b66\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u8bb8\u591a\u7cfb\u7edf\uff08\u5982\u5355\u7ec6\u80deRNA\u4e2d\u7684\u7ec6\u80de\u5468\u671f\uff09\u8868\u73b0\u51fa\u7684\u975e\u68af\u5ea6\u5468\u671f\u6027\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u548c\u6c42\u89e3\u5177\u6709\u975e\u96f6\u6f02\u79fb\u53c2\u8003\u8fc7\u7a0b\u7684\u859b\u5b9a\u8c14\u6865\u95ee\u9898\uff0c\u5229\u7528\u63a8\u65ad\u7684\u901f\u5ea6\u548c\u79cd\u7fa4\u5feb\u7167\u6570\u636e\u6765\u5b66\u4e60\u975e\u68af\u5ea6\u573a\u52a8\u529b\u5b66\u3002", "result": "\u5728\u5355\u7ec6\u80de\u3001\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u6d0b\u6d41\u7b49\u8f68\u8ff9\u63a8\u65ad\u95ee\u9898\u4e2d\uff0cCurly-FM\u80fd\u591f\u5b66\u4e60\u5230\u4e0e\u53c2\u8003\u8fc7\u7a0b\u548c\u79cd\u7fa4\u8fb9\u9645\u66f4\u5339\u914d\u7684\u8f68\u8ff9\u3002", "conclusion": "Curly-FM\u5c06\u6d41\u5339\u914d\u6a21\u578b\u6269\u5c55\u5230\u8d85\u8d8a\u79cd\u7fa4\u5efa\u6a21\uff0c\u80fd\u591f\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\u4e2d\u5df2\u77e5\u7684\u5468\u671f\u6027\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6839\u672c\u5c40\u9650\u6027\u3002"}}
{"id": "2510.26679", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.26679", "abs": "https://arxiv.org/abs/2510.26679", "authors": ["Tommaso d'Orsi", "Gleb Novikov"], "title": "Tight Differentially Private PCA via Matrix Coherence", "comment": "SODA 2026; equal contribution", "summary": "We revisit the task of computing the span of the top $r$ singular vectors\n$u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a\nsimple and efficient algorithm -- based on singular value decomposition and\nstandard perturbation mechanisms -- returns a private rank-$r$ approximation\nwhose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$\nand the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed\nby Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state\nof the art -- significantly so in some regimes. In particular, we show that in\nthe dense setting, it achieves the same guarantees for single-spike PCA in the\nWishart model as those attained by optimal non-private algorithms, whereas\nprior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under\nGaussian perturbations. This implies that any estimator based on the Gaussian\nmechanism -- including ours -- preserves the coherence of the input. We\nconjecture that similar behavior holds for other structured models, including\nplanted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular,\nwe present a differentially private algorithm for Max-Cut and other constraint\nsatisfaction problems under low coherence assumptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u548c\u6807\u51c6\u6270\u52a8\u673a\u5236\u7684\u7b80\u5355\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u8ba1\u7b97\u77e9\u9635\u524dr\u4e2a\u5947\u5f02\u5411\u91cf\u7684\u8de8\u5ea6\u3002\u8be5\u7b97\u6cd5\u5728\u5bc6\u96c6\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e86\u4e0e\u6700\u4f18\u975e\u79c1\u6709\u7b97\u6cd5\u76f8\u540c\u7684\u4fdd\u8bc1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u79c1\u6709\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3Hardt\u548cRoth\u63d0\u51fa\u7684\u5173\u4e8e\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u8ba1\u7b97\u77e9\u9635\u524dr\u4e2a\u5947\u5f02\u5411\u91cf\u8de8\u5ea6\u7684\u95ee\u9898\uff0c\u6539\u8fdb\u73b0\u6709\u79c1\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3\u548c\u6807\u51c6\u6270\u52a8\u673a\u5236\uff0c\u57fa\u4e8e\u79e9r\u76f8\u5e72\u6027\u548c\u8c31\u95f4\u9699\u03c3r-\u03c3r+1\u6765\u6784\u5efa\u79c1\u6709\u79e9r\u8fd1\u4f3c\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5bc6\u96c6\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u6700\u4f18\u975e\u79c1\u6709\u7b97\u6cd5\u76f8\u540c\u7684\u5355\u5cf0PCA\u4fdd\u8bc1\uff0c\u4e14\u8bc1\u660e\u4e86\u9ad8\u65af\u6270\u52a8\u4e0d\u4f1a\u589e\u52a0\u79e9r\u76f8\u5e72\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u6709\u6548\u89e3\u51b3\u4e86\u5947\u5f02\u5411\u91cf\u8de8\u5ea6\u8ba1\u7b97\u95ee\u9898\uff0c\u5e76\u5728\u56fe\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u76f8\u5e72\u6027\u5047\u8bbe\u4e0b\u7684\u6700\u5927\u5272\u548c\u5176\u4ed6\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u3002"}}
{"id": "2510.26690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26690", "abs": "https://arxiv.org/abs/2510.26690", "authors": ["Amir Reza Mirzaei", "Yuqiao Wen", "Yanshuai Cao", "Lili Mou"], "title": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.", "AI": {"tldr": "LoRAQuant\u662f\u4e00\u79cd\u9488\u5bf9LoRA\u9002\u914d\u5668\u7684\u6df7\u5408\u7cbe\u5ea6\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7SVD\u91cd\u65b0\u53c2\u6570\u5316\u9002\u914d\u5668\uff0c\u5c06\u91cd\u8981\u4fe1\u606f\u96c6\u4e2d\u5728\u7279\u5b9a\u884c\u5217\uff0c\u4ece\u800c\u5b9e\u73b0\u91cd\u8981\u90e8\u5206\u9ad8\u7cbe\u5ea6\u91cf\u5316\u3001\u5176\u4f59\u90e8\u5206\u8d85\u4f4e\u4f4d\u5bbd\u91cf\u5316\uff0c\u663e\u8457\u964d\u4f4e\u6bd4\u7279\u4f7f\u7528\u91cf\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u4e2aLoRA\u9002\u914d\u5668\u540c\u65f6\u52a0\u8f7d\u65f6\uff0c\u867d\u7136\u5355\u4e2a\u9002\u914d\u5668\u8f7b\u91cf\uff0c\u4f46\u805a\u5408\u6210\u672c\u5728\u89c4\u6a21\u4e0a\u53d8\u5f97\u663e\u8457\uff0c\u9700\u8981\u89e3\u51b3\u591a\u9002\u914d\u5668\u90e8\u7f72\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3\u91cd\u65b0\u53c2\u6570\u5316\u6bcf\u4e2a\u9002\u914d\u5668\uff0c\u5c06\u6700\u91cd\u8981\u4fe1\u606f\u96c6\u4e2d\u5728\u7279\u5b9a\u884c\u5217\uff0c\u7136\u540e\u5bf9\u91cd\u8981\u7ec4\u4ef6\u91c7\u7528\u9ad8\u7cbe\u5ea6\u91cf\u5316\uff0c\u5176\u4f59\u90e8\u5206\u91c7\u7528\u8d85\u4f4e\u4f4d\u5bbd\u91cf\u5316\u3002", "result": "\u5728LLaMA 2-7B\u3001LLaMA 2-13B\u548cMistral 7B\u6a21\u578b\u4e0a\u7684\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7801\u548c\u6458\u8981\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0cLoRAQuant\u4f7f\u7528\u663e\u8457\u66f4\u4f4e\u7684\u6bd4\u7279\u6570\uff0c\u4f46\u8fbe\u5230\u53ef\u6bd4\u751a\u81f3\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "LoRAQuant\u80fd\u591f\u6709\u6548\u964d\u4f4e\u591aLoRA\u9002\u914d\u5668\u90e8\u7f72\u65f6\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u4e2a\u6027\u5316LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26788", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26788", "abs": "https://arxiv.org/abs/2510.26788", "authors": ["Penghui Qi", "Zichen Liu", "Xiangxin Zhou", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Defeating the Training-Inference Mismatch via FP16", "comment": null, "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\uff0cBF16\u6d6e\u70b9\u7cbe\u5ea6\u4f1a\u5f15\u5165\u8f83\u5927\u7684\u820d\u5165\u8bef\u5dee\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u4e0d\u4e00\u81f4\u3002\u7b80\u5355\u6539\u7528FP16\u7cbe\u5ea6\u5373\u53ef\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u56e0\u6570\u503c\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\u6d6e\u70b9\u7cbe\u5ea6\u672c\u8eab\u3002", "method": "\u901a\u8fc7\u5c06\u5e7f\u6cdb\u91c7\u7528\u7684BF16\u7cbe\u5ea6\u6539\u4e3aFP16\u7cbe\u5ea6\uff0c\u8fd9\u79cd\u6539\u53d8\u7b80\u5355\u6613\u884c\uff0c\u53ea\u9700\u51e0\u884c\u4ee3\u7801\u4fee\u6539\uff0c\u5b8c\u5168\u7531\u73b0\u4ee3\u6846\u67b6\u652f\u6301\u3002", "result": "\u4f7f\u7528FP16\u80fd\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5728\u591a\u6837\u5316\u4efb\u52a1\u3001\u7b97\u6cd5\u548c\u6846\u67b6\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "FP16\u7cbe\u5ea6\u5728RL\u5fae\u8c03\u4e2d\u6bd4BF16\u66f4\u4f18\uff0c\u5efa\u8bae\u91cd\u65b0\u8003\u8651\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2510.26709", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26709", "abs": "https://arxiv.org/abs/2510.26709", "authors": ["Chuyan Chen", "Chenyang Ma", "Zhangxin Li", "Yutong He", "Yanjie Dong", "Kun Yuan"], "title": "An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning", "comment": "8 pages, 2 figures", "summary": "Communication remains a central bottleneck in large-scale distributed machine\nlearning, and gradient sparsification has emerged as a promising strategy to\nalleviate this challenge. However, existing gradient compressors face notable\nlimitations: Rand-$K$\\ discards structural information and performs poorly in\npractice, while Top-$K$\\ preserves informative entries but loses the\ncontraction property and requires costly All-Gather operations. In this paper,\nwe propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that\naligns sparsity patterns across nodes using a lightweight sketch of the\ngradient, enabling index-free All-Reduce while preserving globally significant\ninformation. ARC-Top-$K$\\ is provably contractive and, when combined with\nmomentum error feedback (EF21M), achieves linear speedup and sharper\nconvergence rates than the original EF21M under standard assumptions.\nEmpirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing\nwall-clock training time by up to 60.7\\%, offering an efficient and scalable\nsolution that combines the robustness of Rand-$K$\\ with the strong performance\nof Top-$K$.", "AI": {"tldr": "\u63d0\u51faARC-Top-K\u538b\u7f29\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u68af\u5ea6\u8349\u56fe\u5bf9\u9f50\u8282\u70b9\u95f4\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u5b9e\u73b0\u65e0\u7d22\u5f15All-Reduce\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301Top-K\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\u901a\u4fe1\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u73b0\u6709\u68af\u5ea6\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1aRand-K\u4e22\u5f03\u7ed3\u6784\u4fe1\u606f\u4e14\u5b9e\u9645\u8868\u73b0\u5dee\uff0cTop-K\u4fdd\u7559\u4fe1\u606f\u4f46\u5931\u53bb\u538b\u7f29\u7279\u6027\u4e14\u9700\u8981\u6602\u8d35\u7684All-Gather\u64cd\u4f5c\u3002", "method": "\u8bbe\u8ba1All-Reduce\u517c\u5bb9\u7684Top-K\u538b\u7f29\u5668\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u8349\u56fe\u5bf9\u9f50\u8de8\u8282\u70b9\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u7ed3\u5408\u52a8\u91cf\u8bef\u5dee\u53cd\u9988(EF21M)\u5b9e\u73b0\u7ebf\u6027\u52a0\u901f\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "result": "ARC-Top-K\u5728\u4fdd\u6301Top-K\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe60.7%\uff0c\u7ed3\u5408\u4e86Rand-K\u7684\u9c81\u68d2\u6027\u548cTop-K\u7684\u5f3a\u6027\u80fd\u3002", "conclusion": "ARC-Top-K\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u5177\u6709\u538b\u7f29\u7279\u6027\uff0c\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\u6bd4\u539f\u59cbEF21M\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u7387\u3002"}}

<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 36]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SP](#eess.SP) [Total: 6]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: 本文针对去中心化联邦学习中的隐私预算量化挑战，提出了两种基于f-差分隐私的隐私核算方法：PN-f-DP和Sec-f-LDP，通过结合f-DP理论和马尔可夫链集中性工具，显著提升了隐私边界紧密度和算法效用。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习允许用户在不与中央服务器共享数据的情况下协作，但由于复杂的算法组件（如去中心化通信和本地更新）共存，准确量化隐私预算具有挑战性。

Method: 开发了两种新的f-DP核算方法：PN-f-DP（量化随机游走通信下用户对之间的隐私泄漏）和Sec-f-LDP（通过共享秘密支持结构化噪声注入），结合f-DP理论和马尔可夫链集中性工具。

Result: 在合成和真实数据集上的实验表明，相比Rényi DP方法，本文方法能产生更紧的(ε,δ)边界和更好的效用。

Conclusion: f-DP框架在去中心化隐私核算中具有显著优势，能够有效捕捉稀疏通信、本地迭代和相关噪声带来的隐私放大效应。

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [2] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 本文提出了选择性分类差距的概念，将理想oracle性能与实际选择性分类器性能之间的差距分解为五个来源：贝叶斯噪声、近似误差、排序误差、统计噪声和实现/分布偏移导致的松弛。研究发现单调后校准对缩小差距作用有限，需要能重新排序预测的评分机制。


<details>
  <summary>Details</summary>
Motivation: 当前选择性分类器难以达到完美排序oracle的性能水平，需要系统分析性能差距的来源，为构建更接近理想oracle行为的选择性分类器提供理论指导和实践方法。

Method: 提出选择性分类差距的有限样本分解框架，将差距分解为五个具体来源，并通过合成数据（双月数据集）和真实世界视觉、语言基准进行验证，通过控制实验分离各误差分量。

Result: 验证结果表明：(1)贝叶斯噪声和有限模型容量可解释显著差距；(2)只有更丰富的特征感知校准器能显著改善评分排序；(3)数据偏移引入的松弛需要分布鲁棒训练。

Conclusion: 该分解提供了定量误差预算和可操作的设计指南，帮助从业者构建更接近理想oracle行为的选择性分类器，强调需要能有效重新排序预测而非仅重新缩放评分的机制。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [3] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出了一种基于函数评估的无偏梯度估计器新方法，通过将方向导数重构为伸缩级数并采样特定分布，解决了零阶优化中梯度估计器的偏差问题，在理论和实验中均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法存在梯度估计器偏差问题，除非扰动步长趋近于零，这限制了方法的准确性和收敛性能。

Method: 通过将方向导数重构为伸缩级数，从精心设计的分布中采样，构建无偏梯度估计器，并分析了四种具体构造的最优缩放分布和扰动步长。

Result: 理论分析证明使用该估计器的SGD在光滑非凸目标上达到最优复杂度，实验在合成任务和语言模型微调中验证了相比标准方法更高的准确性和收敛性。

Conclusion: 提出的无偏梯度估计器家族有效解决了零阶优化中的偏差问题，在理论和实践中均表现出优越性能，为梯度不可得或计算昂贵的场景提供了更可靠的优化方案。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [4] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究两点零阶梯度估计器，发现当扰动步长趋近于零时，最小化估计器渐近方差的扰动分布可以沿梯度方向对齐，而非保持固定长度。作者提出了方向对齐扰动方案，并在理论和实证上证明其在特定条件下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定长度的随机扰动，但忽略了扰动方向与真实梯度对齐可能带来的优势。本文旨在探索这种方向对齐扰动的潜力，填补现有研究的空白。

Method: 将问题建模为扰动分布空间上的约束函数优化问题，提出了方向对齐扰动方案，该方案自适应地在关键方向上提供更高精度。同时提供了使用δ-无偏随机扰动的随机梯度下降收敛性分析。

Result: 理论分析表明方向对齐扰动方案在渐近方差最小化方面具有优势。实证评估显示，在合成问题和实际任务中，方向对齐扰动在特定条件下优于传统方法。

Conclusion: 方向对齐扰动方案为随机扰动设计提供了新的视角，通过自适应地沿关键方向提供更高精度，在特定场景下能够提升零阶优化方法的性能。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [5] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: 本文挑战了时间序列预测中的"长序列信息增益假设"，发现适当截断历史数据反而能提高预测精度，并提出AMRC方法通过动态掩码损失和表示一致性约束来抑制冗余特征学习。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型基于"长序列信息增益假设"，但该假设存在固有局限性。研究发现现有模型在训练过程中学习大量冗余特征（如噪声或不相关波动），从而影响有效信号提取。

Method: 提出自适应掩码损失与表示一致性（AMRC）方法，包含两个核心组件：1）动态掩码损失，自适应识别高区分度时间片段来指导模型训练中的梯度下降；2）表示一致性约束，稳定输入、标签和预测之间的映射关系。

Result: 实验结果表明AMRC能有效抑制冗余特征学习，同时显著提高模型性能。

Conclusion: 这项工作不仅挑战了时间建模中的传统假设，还为开发高效鲁棒的预测模型提供了新的理论见解和方法突破。

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [6] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: 本研究使用监督学习和决策树分类器，在CAD建模的平面图中基于RSSI数据进行RFID位置推断，旨在解决国防资产存储中的安全监控问题。


<details>
  <summary>Details</summary>
Motivation: RFID跟踪技术存在传感器特异性差的问题（如远距离检测、欺骗和伪造漏洞），可能导致错误检测和操作安全事件，因此需要开发更可靠的位置推断方法用于国防资产存储安全。

Method: 采用监督学习模拟，使用真实的RSSI数据和决策树分类器，在CAD建模的平面图中对12个实验室区域进行分类，通过分层抽样平衡数据集（5000个平衡观测值），并计算类别权重来处理类别不平衡问题。

Result: 模型整体准确率为34.2%，多个区域（F、G、H等）的F1分数超过0.40，但稀有类别（特别是LabZoneC）经常被错误分类。计算了邻接感知混淆矩阵以更好地解释物理相邻区域的分类结果。

Conclusion: 基于RSSI的决策树可以在现实模拟中应用于区域级异常检测或错位监控，但在低覆盖和低信号区域的可靠分类性能可以通过改进天线布局或增加传感器及多模态传感器融合来提升。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [7] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: SALT是一个轻量级框架，通过构建轨迹图来提供更细粒度的优势分配，解决了基于群体的强化学习算法中稀疏奖励导致训练不稳定和次优策略的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮任务中表现出色，但在复杂、多步骤、长视野任务中面临挑战。基于群体的强化学习算法如GRPO仅依赖稀疏结果奖励，导致训练不稳定和次优策略。

Method: 提出SALT框架，从相同提示的轨迹构建图，量化每个步骤的质量并相应分配优势。作为即插即用模块，无需修改rollout过程，计算开销可忽略。

Result: 在WebShop、ALFWorld和AppWorld基准测试中，SALT在不同模型规模下均能持续提升性能。

Conclusion: SALT通过细粒度优势分配有效解决了基于群体RL算法的训练稳定性问题，可作为现有算法的增强模块。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [8] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: 本文提出了一种耦合Transformer自编码器（CTAE），用于同时处理多脑区神经记录中的非平稳非线性动态，并分离共享与区域特异性结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略时间结构，要么局限于单一脑区、假设线性读出，或混淆共享和私有信号，无法有效处理多脑区同时记录中的复杂神经动态。

Method: CTAE采用Transformer编码器和解码器捕获长程神经动态，并明确将每个脑区的潜在空间划分为正交的共享和私有子空间。

Result: 在两个高密度电生理数据集（运动皮层和感觉区域）上，CTAE提取的表征比现有方法能更好地解码行为变量。

Conclusion: CTAE提供了一个统一框架，能够同时处理多脑区神经记录中的非平稳非线性动态和共享-私有信号分离问题。

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [9] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX是一个创新的时间序列分类解释框架，通过识别关键形状片段（shapelets）并使用Shapley值评估其重要性，提供比现有方法更精确和具有因果保真度的解释。


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等高风险应用中，时间序列分类模型的透明度和可信度至关重要。现有的事后解释方法主要关注时间步级别的特征归因，忽略了分类结果主要由关键形状片段驱动的基本前提。

Method: ShapeX框架包含Shapelet描述与检测（SDD）框架，能够学习对分类至关重要的多样化形状片段，并将时间序列分割为有意义的形状片段驱动段，使用Shapley值评估其显著性。

Result: 在合成和真实世界数据集上的实验结果表明，ShapeX在识别最相关子序列方面优于现有方法，提高了时间序列解释的精度和因果保真度。

Conclusion: ShapeX通过利用形状片段的原子性特性，能够产生揭示因果关系而不仅仅是相关性的解释，为高风险应用中的时间序列分类提供了更可靠的透明度。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [10] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出基于博弈论强化学习的因果发现框架，通过DDQN智能体与强基线方法（GES或GraN-DAG）竞争，实现理论保证与可扩展性的统一。


<details>
  <summary>Details</summary>
Motivation: 解决因果发现领域存在的核心矛盾：现有方法要么有强实证性能但缺乏有限样本保证，要么有理论保证但无法扩展到大规模问题。

Method: 使用DDQN强化学习智能体直接与GES或GraN-DAG等基线方法竞争，总是从对手的解进行热启动，构建博弈论框架。

Result: 在合成SEM（30节点）上观察到的错误概率随样本量n衰减，与理论紧密匹配；在真实世界基准数据集（Sachs、Asia、Alarm等）上持续改进GES和GraN-DAG，可扩展到Hepar2（70节点）、Dream（100节点）、Andes（220节点）等大规模图。

Conclusion: 建立了一类新的基于强化学习的因果发现算法，同时具备可证明的一致性、样本效率和实际可扩展性，标志着将实证性能与严格有限样本理论统一的关键进展。

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [11] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 论文提出了一种完全解耦的训练策略来解决原型自监督学习中的原型崩溃问题，通过将原型和编码器的学习分离，使用在线EM风格过程更新原型，从而消除原型崩溃并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的原型自监督学习方法普遍存在部分原型崩溃问题，即多个原型收敛到几乎相同的表示，这削弱了提供多样化和信息丰富目标的核心目的。当前方法通常通过过度参数化原型集或添加临时正则化来缓解症状，而非解决根本原因。

Method: 引入完全解耦的训练策略，将原型和编码器的学习分离。具体而言，将原型建模为高斯混合模型，使用在线EM风格过程独立于编码器损失进行更新。

Result: 这种简单而原则性的解耦方法无需显式正则化即可消除原型崩溃，产生持续多样化的原型和更强的下游性能。

Conclusion: 通过打破原型和编码器的联合优化，解决了原型崩溃的根本原因，提供了一种更有效的原型自监督学习方法。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [12] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出了一种名为ADP-VRSGP的自适应差分隐私去中心化学习方法，通过动态调整噪声方差和学习率，结合渐进梯度融合策略，在保护隐私的同时提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有使用固定方差噪声的差分隐私方法会降低模型性能和训练效率，需要一种能够动态调整隐私保护强度的方法。

Method: 采用步进衰减调度动态调整噪声方差和学习率，引入渐进梯度融合策略利用历史梯度，结合去中心化push-sum和聚合技术，适用于时变通信拓扑。

Result: 理论分析证明ADP-VRSGP在适当学习率下实现鲁棒收敛，实验验证其在多种场景下优于现有基线方法。

Conclusion: ADP-VRSGP方法有效解决了隐私保护去中心化学习中的挑战，在保证隐私的同时显著提升了训练稳定性和速度。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [13] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: RLEV方法通过将人类定义的价值信号直接整合到奖励函数中，扩展了RLVR框架，使LLM优化与可量化的人类价值信号对齐。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR在客观领域使用二元正确性奖励有效训练模型，但它忽略了并非所有任务都同等重要的问题。RLEV旨在通过直接整合人类定义的价值信号来解决这一局限性。

Method: 使用带有明确真实价值标签的考试风格数据，将人类定义的价值信号直接整合到奖励函数中。通过价值加权梯度放大在序列结束标记上的机制，实现价值敏感的终止策略。

Result: RLEV在多个RL算法和模型规模上持续优于仅基于正确性的基线方法。RLEV策略不仅提高了价值加权准确性，还学会了价值敏感的终止策略：对低价值提示简洁，对高价值提示详尽。

Conclusion: RLEV通过优化明确的效用函数，为将LLM与人类优先级对齐提供了一条实用路径，即使在噪声价值信号下也保持稳健。

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [14] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: 本文提出了三种可复制性的松弛概念：点态可复制性、近似可复制性和半可复制性，并在这些松弛条件下实现了样本最优的不可知PAC学习。


<details>
  <summary>Details</summary>
Motivation: 由于完全可复制性在某些简单任务（如阈值学习）中无法实现，作者探索在何种近似可复制性概念下学习是可行的。

Method: 提出了三种可复制性松弛：(1)点态可复制性：对固定输入保持一致性；(2)近似可复制性：对大部分分布输出一致的假设；(3)半可复制性：完全可复制但可使用共享无标签样本。

Result: 在常数可复制性参数下，获得了样本最优的不可知PAC学习器：前两种松弛使用Θ(d/α²)样本即可实现，第三种需要Θ(d²/α²)标记样本。

Conclusion: 通过适当的可复制性松弛，可以在不显著增加样本复杂度的情况下实现稳定的学习算法。

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [15] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 该研究评估了使用金毛猎犬寿命研究队列的常规实验室数据进行癌症风险分类的可行性，发现虽然存在可检测的癌症信号，但由于信号太弱且与正常衰老或其他炎症状况混淆，无法实现可靠的临床分类。


<details>
  <summary>Details</summary>
Motivation: 开发用于犬类早期癌症检测的无创筛查工具是兽医医学的重要挑战，常规实验室数据提供了低成本来源，但受到个体生物标志物非特异性和筛查人群中严重类别不平衡的限制。

Method: 在真实世界约束下，系统比较了126个分析流程，包括各种机器学习模型、特征选择方法和数据平衡技术，使用患者级数据分区防止泄漏，并采用SHAP进行可解释性分析。

Result: 最佳模型（逻辑回归分类器）显示出中等排名能力（AUROC=0.815），但临床分类性能较差（F1分数=0.25，阳性预测值=0.15），高阴性预测值（0.98）但召回率不足（0.79）。

Conclusion: 常规实验室数据中虽然存在统计上可检测的癌症信号，但太弱且混淆，无法与正常衰老或其他炎症状况进行可靠的临床区分，需要整合多模态数据源才能取得有意义的进展。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [16] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 本文提出QKCV注意力机制，在传统QKV框架中加入静态类别嵌入C来强调类别特定信息，作为即插即用模块提升多种注意力模型的预测精度，并能在微调单变量时间序列基础模型时仅更新C嵌入而保持预训练权重，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测任务中，类别信息对于捕捉内在数据模式至关重要，但传统注意力机制未能充分利用此类信息。

Method: 提出QKCV注意力机制，在QKV框架中引入静态类别嵌入C，形成Query-Key-Category-Value结构，作为即插即用模块集成到多种注意力模型中。

Result: QKCV显著提升了Vanilla Transformer、Informer、PatchTST、TFT等注意力模型在多样化真实数据集上的预测精度，并在微调单变量时间序列基础模型时仅通过更新C嵌入就实现了优越的微调性能。

Conclusion: QKCV是一种有效整合类别信息的注意力机制扩展，既能提升模型性能，又能高效微调基础模型，具有实际应用价值。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [17] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出了一个可证明收敛且实用的多目标强化学习框架，采用max-min准则，通过博弈论视角将其重构为两人零和正则化连续博弈，并基于镜像下降设计高效算法。


<details>
  <summary>Details</summary>
Motivation: 解决多目标强化学习中max-min准则的收敛性和实用性问题，提供理论保证的同时提升算法性能。

Method: 将max-min多目标强化学习重构为两人零和正则化连续博弈，采用镜像下降算法进行策略更新，并引入自适应正则化以增强性能。

Result: 算法在表格设置中展示了收敛行为，在深度强化学习实现中显著优于先前基线方法，在多个MORL环境中表现优异。

Conclusion: 该框架为多目标强化学习提供了理论保证和实用算法，在收敛性和性能方面均有显著提升。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [18] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: OpTI-BFM是一种乐观决策标准，通过直接建模奖励函数的不确定性来指导行为基础模型在测试时进行任务推断，减少了对预计算奖励数据的需求。


<details>
  <summary>Details</summary>
Motivation: 传统行为基础模型需要计算大量推理数据集的奖励，这需要访问奖励函数的形式或大量标注工作。为了缓解这些限制，研究旨在通过与环境交互来纯粹进行任务推断。

Method: 提出OpTI-BFM乐观决策标准，直接建模奖励函数的不确定性，指导行为基础模型进行数据收集以进行任务推断。该方法与线性bandits的上置信界算法有直接联系。

Result: 在已建立的零样本基准测试中，OpTI-BFM使基于后继特征的行为基础模型能够在少量episode中识别和优化未见过的奖励函数，且计算开销最小。

Conclusion: OpTI-BFM为行为基础模型提供了一种高效的任务推断方法，通过乐观决策标准减少了对预计算奖励数据的需求，在少量交互中实现高性能策略检索。

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [19] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: 本文提出了针对欧拉特征曲线（ECC）计算的优化GPU内核，实现了16-2000倍的速度提升，并引入了可微分的PyTorch层支持端到端学习。


<details>
  <summary>Details</summary>
Motivation: 拓扑特征能捕捉成像数据中的全局几何结构，但在深度学习中的实际应用需要计算效率和可微分性。

Method: 开发了针对Ampere GPU优化的CUDA内核，使用128B合并访问和分层共享内存累加；创建了可微分PyTorch层，通过可微分欧拉特征变换风格的sigmoid松弛学习单方向阈值。

Result: 在合成网格上相比之前的GPU实现实现了16-2000倍的速度提升，并成功实现了端到端学习能力。

Conclusion: 该方法为拓扑特征在深度学习中的广泛应用提供了计算高效和可微分的解决方案，并讨论了批处理和多GPU扩展以扩大采用范围。

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [20] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出了SynTSBench评估框架，通过可编程特征配置系统评估时间序列预测模型的基本建模能力，包括时间特征分解、鲁棒性分析和理论最优基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在真实应用中性能不稳定问题，以及现有评估框架缺乏对模型具体优缺点的定量洞察，导致难以针对特定预测场景选择合适的模型。

Method: 采用合成数据驱动的评估范式，通过三个核心分析维度：时间特征分解与能力映射、数据不规则性下的鲁棒性分析、理论最优基准测试。

Result: 实验表明当前深度学习模型在所有类型的时间特征上并未普遍接近最优基线。

Conclusion: SynTSBench框架能够系统评估时间序列预测模型的基本能力，为模型选择提供更清晰的指导。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [21] [KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models](https://arxiv.org/abs/2510.20278)
*Guangyu Dai,Siliang Tang,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出基于KAN的协作模型(KCM)来解决大-小模型协作中的精度下降、灾难性遗忘和幻觉问题，相比MLP方法在语言、视觉和跨模态任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决现有大-小模型协作框架存在的精度显著下降、灾难性遗忘加剧以及小模型知识引发的幻觉问题，同时降低计算资源消耗。

Method: 使用KAN（Kolmogorov-Arnold Networks）作为协作模型架构，替代传统的MLP，利用KAN更好的可视化和可解释性特性，在大-小模型协作系统中部署于语言、视觉和视觉-语言跨模态任务。

Result: 相比纯大模型方法，使用KCM作为协作模型的大-小模型协作框架显著减少了大模型推理调用次数，同时保持近乎相同的任务精度；KAN基础的小协作模型显著缓解了灾难性遗忘，对长尾数据带来显著精度提升；在所有指标上优于基于MLP的小协作模型(MCM)。

Conclusion: KCM在大-小模型协作中表现出色，既能有效降低计算资源消耗，又能缓解灾难性遗忘问题，在多种任务场景下均优于传统MLP方法。

Abstract: In recent years, Pretrained Large Models(PLMs) researchers proposed
large-small model collaboration frameworks, leveraged easily trainable small
models to assist large models, aim to(1) significantly reduce computational
resource consumption while maintaining comparable accuracy, and (2) enhance
large model performance in specialized domain tasks. However, this
collaborative paradigm suffers from issues such as significant accuracy
degradation, exacerbated catastrophic forgetting, and amplified hallucination
problems induced by small model knowledge. To address these challenges, we
propose a KAN-based Collaborative Model (KCM) as an improved approach to
large-small model collaboration. The KAN utilized in KCM represents an
alternative neural network architecture distinct from conventional MLPs.
Compared to MLPs, KAN offers superior visualizability and interpretability
while mitigating catastrophic forgetting. We deployed KCM in large-small model
collaborative systems across three scenarios: language, vision, and
vision-language cross-modal tasks. The experimental results demonstrate that,
compared with pure large model approaches, the large-small model collaboration
framework utilizing KCM as the collaborative model significantly reduces the
number of large model inference calls while maintaining near-identical task
accuracy, thereby substantially lowering computational resource consumption.
Concurrently, the KAN-based small collaborative model markedly mitigates
catastrophic forgetting, leading to significant accuracy improvements for
long-tail data. The results reveal that KCM demonstrates superior performance
across all metrics compared to MLP-based small collaborative models (MCM).

</details>


### [22] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: 本文提出LEGO框架，用于推荐系统中多属性遗忘，解决现有单属性遗忘方法无法同时处理多个遗忘请求和缺乏动态适应性的问题。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统中保护用户敏感信息需求的增长，现有研究主要关注单属性遗忘，但现实世界的隐私保护需求往往涉及多个敏感属性且具有动态性，现有方法无法满足这些需求。

Method: 提出LEGO框架，将多属性遗忘过程分为两个步骤：嵌入校准（从用户嵌入中移除特定属性信息）和灵活组合（将这些嵌入组合成单一嵌入以保护所有敏感属性），并将遗忘过程建模为互信息最小化问题。

Result: 在三个真实数据集和三个代表性推荐模型上的广泛实验证明了所提框架的有效性和效率。

Conclusion: LEGO框架通过两步骤设计解决了多属性遗忘的挑战，提供了理论保证，并在实验中表现出色。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [23] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文提出了基于相对排序的缩放定律，通过引入相对概率指标来补充传统的交叉熵指标，从相对排序角度研究语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要依赖交叉熵作为评估指标，但交叉熵只关注正确token的绝对概率，忽略了正确与错误token之间的相对排序关系，而相对排序在语言模型的实际应用中至关重要。

Method: 提出了相对概率指标(RBP)，量化正确token在top预测中的排名概率，并基于此建立了相对缩放定律，通过四个数据集和四个模型家族的广泛实验验证了该定律的鲁棒性和准确性。

Result: 实验证明相对缩放定律能够准确预测模型性能随规模增长的变化，并展示了该定律在解释涌现现象和寻找缩放定律基础理论方面的应用价值。

Conclusion: 相对缩放定律补充了交叉熵视角，为大规模语言模型的缩放提供了更完整的理解，对实际开发和理论探索都具有重要价值。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [24] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: 提出InvDec混合架构，通过倒置解码器实现时间编码和变量级解码的分离，在保持时间建模能力的同时有效处理多变量相关性，特别在高维数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：通道独立方法如PatchTST擅长时间建模但忽略变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码能力。需要一种能同时建模时间模式和跨变量依赖的解决方案。

Method: 结合基于补丁的时间编码器和在变量维度上通过变量自注意力操作的倒置解码器。引入延迟变量嵌入，在时间编码后丰富变量特定表示；采用自适应残差融合机制动态平衡时间和变量信息。

Result: 在七个基准测试上的广泛实验显示，在高维数据集上取得显著提升：电力数据集（321变量）MSE降低20.9%，天气数据集提升4.3%，交通数据集提升2.7%，同时在低维ETT数据集上保持竞争力。

Conclusion: InvDec通过分离时间编码和变量级解码，有效解决了多变量时间序列预测中的关键挑战。消融研究验证了各组件有效性，分析表明随着数据集维度增加，跨变量建模变得愈发重要，InvDec的优势也随之增长。

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [25] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: 提出了一种针对标记时间点过程中事件标记分布不平衡问题的阈值方法，通过调整标记概率来优化标记预测，并开发了新的神经MTPP模型来支持有效的时间采样和标记概率估计。


<details>
  <summary>Details</summary>
Motivation: 现实应用中事件标记分布高度不平衡，一些标记频繁而其他标记罕见，这对下一个事件预测性能（特别是罕见标记事件）构成了重大挑战。

Method: 提出阈值方法学习阈值来调整由标记先验概率归一化的标记概率，以优化标记预测；开发新的神经MTPP模型支持有效时间采样和标记概率估计，无需计算昂贵的数值不当积分。

Result: 在真实世界数据集上的广泛实验表明，该方法在下一个事件标记和时间预测方面优于各种基线方法。

Conclusion: 所提出的解决方案通过处理标记不平衡问题，显著提高了MTPP模型在下一个事件预测中的性能，特别是在罕见标记事件上。

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [26] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: 提出Bi-CoG方法，通过同时利用模型间和模型内一致性以及错误感知动态伪标签分配策略，解决半监督微调中的模型偏差和超参数敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督微调方法存在模型偏差和超参数敏感性问题，主要依赖预测一致性或预定义置信度阈值。

Method: 提出Bi-CoG方法，利用模型间和模型内一致性，结合错误感知动态伪标签分配策略来分配高质量、低偏差的伪标签。

Result: 在14个数据集上的实验表明，Bi-CoG能持续显著提升现有方法的性能。

Conclusion: Bi-CoG是一种简单有效的即插即用方法，能有效解决半监督微调中的关键问题。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [27] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 本文提出了一种快速且无需微调的LLM适应方法，通过选择性矩阵降维和梯度分析来提升下游任务准确率，相比之前的LASER方法大幅减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: LASER方法虽然能通过剪枝提升LLM下游任务准确率，但其逐层搜索和全数据集前向传播的计算开销使其难以快速部署。本文旨在消除这种开销。

Method: 使用梯度分析识别关键矩阵，仅检查少量选定矩阵；扩展分解搜索空间，允许矩阵行围绕多个子空间聚类；仅使用100个样本计算梯度和评估准确率。

Result: 方法将准确率提升高达24.6个百分点，同时大幅减少搜索时间，仅需单次梯度步和快速扫描就能适应新数据集。

Conclusion: 结合这些发现可得到快速稳健的下游任务适应算法，完全无需微调即可有效适应LLM到新数据集。

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [28] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: 本文分析了将MLOps实践集成到操作技术（OT）系统中的挑战，并提出了一种将MLOps嵌入到现有OT参考模型中的系统方法。通过评估RAMI 4.0和ISA-95标准对MLOps的适用性，并展示MLOps生命周期组件到RAMI 4.0的映射，证明了结构化适配现有参考模型可以实现成功集成。


<details>
  <summary>Details</summary>
Motivation: 随着MLOps在工业环境中日益普及，将其与操作技术（OT）系统集成面临重大挑战。本文旨在解决MLOps与OT环境结合的基本障碍，探索如何将MLOps实践有效嵌入到已建立的OT参考模型中。

Method: 评估RAMI 4.0和ISA-95标准对MLOps集成的适用性，提出系统化的MLOps集成方法，并通过真实案例展示MLOps生命周期组件到RAMI 4.0的详细映射。

Result: 研究发现标准MLOps实践不能直接移植到OT环境中，但通过使用现有参考模型进行结构化适配，可以为成功集成提供可行路径。

Conclusion: 虽然MLOps实践无法直接应用于OT环境，但通过基于现有参考模型（如RAMI 4.0和ISA-95）的结构化适配方法，可以实现MLOps与OT系统的有效集成。

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [29] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: 本文分析了差分隐私迁移学习中的两个关键超参数：裁剪边界C和批次大小B。研究发现理论指导与实证结果存在矛盾，并揭示了现有启发式方法在有限计算预算下的不足。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私迁移学习实践中，裁剪边界C和批次大小B的选择存在理论指导与实证结果不匹配的问题，这影响了模型在隐私约束下的性能优化。

Method: 通过分析梯度分布变化，研究在有限计算预算下裁剪边界C和批次大小B对模型性能的影响，探讨累积DP噪声的作用机制。

Result: 发现强隐私条件下理论上应选择较小C，但实证中较大C表现更好；现有批次大小调优启发式方法在有限计算预算下失效；跨任务使用单一(C,B)设置会导致性能下降。

Conclusion: 需要重新审视差分隐私迁移学习中的超参数选择策略，考虑梯度重加权和累积DP噪声的影响，针对不同隐私约束和计算资源进行个性化调优。

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [30] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: H-SPLID是一种新颖算法，通过将显著和非显著特征显式分解到不同空间来学习显著特征表示，促进学习低维、任务相关特征，并建立鲁棒性与潜在表示压缩之间的联系。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够明确分离显著和非显著特征的算法，以促进学习更具鲁棒性和任务相关性的低维特征表示。

Method: 提出H-SPLID算法，将输入特征显式分解为显著和非显著子空间，利用Hilbert-Schmidt独立性准则(HSIC)建立鲁棒性与表示压缩的理论联系。

Result: 在图像分类任务上的实证评估表明，使用H-SPLID训练的模型主要依赖显著输入组件，对影响非显著特征（如图像背景）的扰动敏感性降低。

Conclusion: H-SPLID通过特征空间分解有效促进了鲁棒特征学习，建立了表示维度与预测鲁棒性之间的理论联系，为学习压缩且任务相关的特征表示提供了新方法。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [31] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: xTime是一个用于时间序列极端事件预测的新框架，通过知识蒸馏和专家混合机制解决数据不平衡问题，显著提升极端事件的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列中的极端事件（如洪水、热浪、医疗紧急情况）具有重大实际影响，但现有预测模型往往因数据不平衡和忽略中间事件信息而难以准确预测这些事件。

Method: 提出xTime框架，利用知识蒸馏从训练在较低稀有度事件上的模型转移信息，并引入专家混合机制动态选择和融合不同稀有度级别的专家模型输出。

Result: 在多个数据集上的实验表明，xTime实现了持续的改进，极端事件的预测准确率从3%提升到78%。

Conclusion: xTime通过知识蒸馏和专家混合机制有效解决了极端事件预测中的数据不平衡问题，显著提高了预测性能。

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [32] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 本研究通过数据驱动建模方法优化约翰霍普金斯跌倒风险评估工具(JHFRAT)，结合临床知识和电子健康记录数据，显著提升了跌倒风险预测性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动方法将JHFRAT跌倒风险预测与更多临床有意义指标对齐，提高预测准确性和临床实用性。

Method: 回顾性分析54,209例住院患者数据，使用约束评分优化(CSO)模型结合JHFRAT评估数据和电子健康记录变量，并与当前JHFRAT和XGBoost基准模型进行比较。

Result: CSO模型预测性能显著优于当前JHFRAT(AUC-ROC=0.91 vs 0.86)，与XGBoost性能相当(AUC-ROC=0.94)但更具鲁棒性，且EHR变量的加入对性能影响不大。

Conclusion: 这种循证方法为医疗系统提供了坚实基础，可通过数据驱动优化技术系统性地增强住院患者跌倒预防方案和患者安全，改善风险评估和资源分配。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [33] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: 本文为Thompson采样在强化学习中的理论分析，建立了具有高斯边际分布的模型在序列决策中的无遗憾保证，证明了在具有联合高斯过程先验的episodic强化学习中的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: Thompson采样在序列决策中广泛应用，但在具有复杂时间结构的强化学习环境中理论基础有限，特别是在有限时间马尔可夫决策过程中。

Method: 使用具有高斯边际分布的模型，考虑具有联合高斯过程先验的episodic强化学习，分析非高斯价值函数和Bellman更新的递归结构，扩展椭圆势引理到多输出设置。

Result: 证明了在K个episode、每个horizon为H的情况下，遗憾边界为$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$，其中$\Gamma(\cdot)$捕获高斯过程模型的复杂度。

Conclusion: 这项工作推进了对Thompson采样在强化学习中理解，强调了结构假设和模型不确定性如何影响其在有限时间马尔可夫决策过程中的性能。

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [34] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于狄利克雷过程的多模态学习框架，通过DP的富者愈富特性自动平衡模态内显著特征学习和跨模态对齐，在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态融合中如何在保持各模态特征表达能力的同时学习跨模态交互的关键挑战，避免过度强调模态边际分布对齐导致的过度正则化问题。

Method: 假设每个模态遵循多元高斯混合分布，采用狄利克雷过程计算各分量的混合权重，利用DP的富者愈富特性动态分配特征贡献并选择最显著特征。

Result: 在多个多模态数据集上的广泛实验表明，该模型优于其他竞争方法，消融分析验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。

Conclusion: 提出的DP驱动多模态学习框架能够自动实现模态内显著表示学习和跨模态对齐之间的最优平衡，为多模态融合提供了有效解决方案。

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [35] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: MEIcoder是一种生物信息解码方法，利用神经元特异性最兴奋输入(MEIs)、结构相似性指数损失和对抗训练，在小数据集上实现最先进的视觉刺激重建性能。


<details>
  <summary>Details</summary>
Motivation: 灵长类或人类等生物数据往往稀缺，这对深度学习解码技术构成挑战，需要克服数据不足的问题。

Method: 引入MEIcoder方法，结合神经元特异性最兴奋输入(MEIs)、结构相似性指数损失和对抗训练。

Result: 在初级视觉皮层单细胞活动重建视觉刺激方面达到最先进性能，特别是对小数据集表现优异，仅需1,000-2,500个神经元和少于1,000个训练数据点即可重建高保真自然图像。

Conclusion: 证明了早期视觉系统中可靠解码的可行性，为神经科学和神经工程应用提供了实用见解。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [36] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: 该论文挑战了关于反向KL散度和前向KL散度在强化学习中的传统直觉，发现模式覆盖主要取决于正则化强度等因素，而非KL散度的选择。作者提出了一个简单算法，通过调整奖励幅度来优化目标分布，在语言模型和化学语言模型的实验中提高了解决方案的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为优化反向KL散度会导致"模式寻求"，而优化前向KL散度会导致"质量覆盖"，后者在需要从多个不同模式采样时更受青睐。但作者发现这种直觉在强化学习与KL正则化（如语言模型中常用）的情况下并不一定适用。

Method: 作者通过数学分析和实证研究，揭示了反向/前向KL的选择决定了最优目标分布的族系，而模式覆盖主要取决于正则化强度、奖励与参考概率的相对尺度等因素。基于这些洞察，构建了一个简单、可扩展且理论上有依据的算法，只需对奖励幅度进行最小修改。

Result: 实验表明，该简单修改能够在对大型语言模型和化学语言模型进行后训练时，提高解决方案的质量和多样性，无需任何外部多样性信号，并且在单独使用前向或反向KL失败的情况下都能工作。

Conclusion: 论文结论是模式覆盖不主要取决于KL散度的选择，而是其他因素如正则化强度。提出的算法通过最小化奖励幅度调整，能够优化目标分布以覆盖所有高质量采样模式，在实践中有效提升了模型的多样性和质量。

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [37] [Treatment Effect Learning Under Sequential Randomization](https://arxiv.org/abs/2510.20078)
*Rina Friedberg,Richard Mudd,Patrick Johnstone,Melissa Pothen,Vishal Vaingankar,Vishwanath Sangale,Abbas Zaidi*

Main category: stat.AP

TL;DR: 本文提出了一种结合T-Learner和G-Formula的方法来处理在线实验中顺序治疗分配带来的复杂依赖结构问题，特别是在存在持续效应的情况下。


<details>
  <summary>Details</summary>
Motivation: 在线实验中的顺序治疗分配会产生复杂的依赖结构，导致治疗在后续会话中产生持续效应，这使得标准识别和推断方法容易产生错误。

Method: 将T-Learner分层嵌入到G-Formula中，基于因果机器学习和顺序设置中的识别文献。

Result: 在简单模拟中，该方法在存在持续效应的情况下防止了准确性衰减。

Conclusion: 针对技术领域常见系统性质定制的识别和推断策略非常重要。

Abstract: Sequential treatment assignments in online experiments lead to complex
dependency structures, often rendering identification, estimation and inference
over treatments a challenge. Treatments in one session (e.g., a user logging
on) can have an effect that persists into subsequent sessions, leading to
cumulative effects on outcomes measured at a later stage. This can render
standard methods for identification and inference trivially misspecified. We
propose T-Learners layered into the G-Formula for this setting, building on
literature from causal machine learning and identification in sequential
settings. In a simple simulation, this approach prevents decaying accuracy in
the presence of carry-over effects, highlighting the importance of
identification and inference strategies tailored to the nature of systems often
seen in the tech domain.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [38] [NanoHydra: Energy-Efficient Time-Series Classification at the Edge](https://arxiv.org/abs/2510.20038)
*Cristian Cioflan,Jose Fonseca,Xiaying Wang,Luca Benini*

Main category: eess.SP

TL;DR: NanoHydra是一种用于极边缘设备的轻量级时间序列分类方法，使用二进制随机卷积核提取特征，在GAP9微控制器上实现高效并行处理，在ECG5000数据集上达到94.47%准确率，每推理仅消耗7.69微焦能量，比现有技术高效18倍。


<details>
  <summary>Details</summary>
Motivation: 开发适用于极边缘设备的高效时间序列分类方法，延长电池供电设备的寿命，同时保持分类准确性，实现保护用户隐私和提供实时预测的智能传感器节点。

Method: 采用轻量级二进制随机卷积核从数据流中提取有意义的特征，在超低功耗GAP9微控制器上利用其八核集群并行执行计算密集型任务。

Result: 在ECG5000数据集上达到94.47%的分类准确率，仅需0.33毫秒即可准确分类1秒长的ECG信号，每推理能耗为7.69微焦，比现有技术高效18倍。

Conclusion: NanoHydra适用于智能可穿戴设备，能够实现超过四年的设备寿命，为资源受限的边缘设备提供了高效的时间序列分类解决方案。

Abstract: Time series classification (TSC) on extreme edge devices represents a
stepping stone towards intelligent sensor nodes that preserve user privacy and
offer real-time predictions. Resource-constrained devices require efficient
TinyML algorithms that prolong the device lifetime of battery-operated devices
without compromising the classification accuracy. We introduce NanoHydra, a
TinyML TSC methodology relying on lightweight binary random convolutional
kernels to extract meaningful features from data streams. We demonstrate our
system on the ultra-low-power GAP9 microcontroller, exploiting its eight-core
cluster for the parallel execution of computationally intensive tasks. We
achieve a classification accuracy of up to 94.47% on ECG5000 dataset,
comparable with state-of-the-art works. Our efficient NanoHydra requires only
0.33 ms to accurately classify a 1-second long ECG signal. With a modest energy
consumption of 7.69 uJ per inference, 18x more efficient than the
state-of-the-art, NanoHydra is suitable for smart wearable devices, enabling a
device lifetime of over four years.

</details>


### [39] [Sensing Security in Near-Field ISAC: Exploiting Scatterers for Eavesdropper Deception](https://arxiv.org/abs/2510.20140)
*Jiangong Chen,Xia Lei,Kaitao Meng,Kawon Han,Yuchen Zhang,Christos Masouros,Athina P. Petropulu*

Main category: eess.SP

TL;DR: 提出一种利用已知散射体进行位置欺骗的感知安全方案，通过在近场ISAC场景中向散射体分配更多探测功率来欺骗窃听者，使其误将散射体识别为目标


<details>
  <summary>Details</summary>
Motivation: 在近场集成感知与通信场景中，利用已知散射体提升感知安全性，无需窃听者的先验信息即可实现位置欺骗

Method: 采用分数规划和半定松弛方法优化波束成形策略，在合法雷达SINR约束下最大化加权和速率和散射体分配功率，使用CRB、MSE和KLD指标评估安全性能

Result: 仿真结果表明该方案能灵活调整波束成形策略，显著增强窃听端的杂波信号强度，导致实际目标被混淆甚至漏检

Conclusion: 所提出的位置欺骗方案能有效实现通信、感知和感知安全性能之间的三向权衡，显著提升近场ISAC系统的感知安全性

Abstract: In this paper, we explore sensing security in near-field (NF) integrated
sensing and communication (ISAC) scenarios by exploiting known scatterers in
the sensing scene. We propose a location deception (LD) scheme where scatterers
are deliberately illuminated with probing power that is higher than that
directed toward targets of interest, with the goal of deceiving potential
eavesdroppers (Eves) with sensing capability into misidentifying scatterers as
targets. While the known scatterers can be removed at the legitimate sensing
receiver, our LD approach causes Eves to misdetect targets. Notably, this
deception is achieved without requiring any prior information about the Eves'
characteristics or locations. To strike a flexible three-way tradeoff among
communication, sensing, and sensing-security performance, the sum rate and
power allocated to scatterers are weighted and maximized under a legitimate
radar signal-to-interference-plus-noise ratio (SINR) constraint. We employ the
fractional programming (FP) framework and semidefinite relaxation (SDR) to
solve this problem. To evaluate the security of the proposed LD scheme, the
Cramer-Rao Bound (CRB) and mean squared error (MSE) metrics are employed.
Additionally, we introduce the Kullback-Leibler Divergence (KLD) gap between
targets and scatterers at Eve to quantify the impact of the proposed LD
framework on Eve's sensing performance from an information-theoretical
perspective. Simulation results demonstrate that the proposed LD scheme can
flexibly adjust the beamforming strategy according to performance requirements,
thereby achieving the desired three-way tradeoff. In particular, in terms of
sensing security, the proposed scheme significantly enhances the clutter signal
strength at Eve's side, leading to confusion or even missed detection of the
actual target.

</details>


### [40] [A Survey of OTFS-Based Index Modulation Techniques: Challenges, Benefits, and Future Directions for 6G and Beyond](https://arxiv.org/abs/2510.20265)
*Burak Ahmet Ozden,Erdogan Aydin,Emir Aslandogan,Haci Ilhan,Ertugrul Basar,Miaowen Wen,Marco Di Renzo,Vincent Poor*

Main category: eess.SP

TL;DR: 本文综述了基于正交时频空间(OTFS)和索引调制(IM)的无线通信系统，系统性地分类了OTFS-IM方案，比较了各种变体的性能，并讨论了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: OTFS技术在延迟多普勒域进行调制，能够充分利用信道分集并将快速时变信道转化为近似静态信道，而IM技术通过通信资源索引编码数据位来提高性能。结合两者可提供更稳健、高效能的无线通信解决方案。

Method: 对现有OTFS-IM方案进行系统性综述和分类，包括系统架构、检测方法、性能指标分析，并详细描述各种OTFS-IM变体的工作原理和系统模型。

Result: 提供了OTFS-IM方案的全面性能比较分析，包括计算复杂度、误码性能、容量、节能、频谱效率和吞吐量等方面。

Conclusion: OTFS-IM系统具有显著优势，但面临复杂度、效率、延迟、信道估计等挑战，未来需要与其他先进无线通信技术集成发展。

Abstract: Orthogonal time frequency space (OTFS) is a two-dimensional modulation
technique that uses the delay-Doppler (DD) domain and is a candidate for
providing robust, high-capacity wireless communications for envisioned 6G and
beyond networks. The OTFS technique maps data to the DD domain instead of the
traditional time-frequency domain, enabling it to fully utilize channel
diversity and transform fast time-varying channels into nearly static channels.
Index modulation (IM) is a communication paradigm that conveys information not
only through conventional modulation symbols but also by encoding data bits in
the indices of the selected communication resources to improve error
performance, spectral efficiency, and energy efficiency. In this survey, a
comprehensive review of work on OTFS-based wireless communication systems is
presented. In particular, the existing OTFS-IM schemes are reviewed and
systematically categorized according to their system architectures, detection
methods, and performance aspects such as capacity, peak-to-average power ratio,
diversity, complexity, imperfect channel state information, spectral
efficiency, and outage probability. Furthermore, the operating principles and
system models of OTFS-IM variants-including OTFS-based space shift keying,
OTFS-based spatial modulation, OTFS-based quadrature spatial modulation,
OTFS-based media-based modulation, and OTFS-based code index modulation-are
described, followed by a comparative performance analysis in terms of
computational complexity, error performance, capacity, energy saving, spectral
efficiency, and throughput. Finally, the challenges, benefits, and future
directions for OTFS-IM systems are discussed, covering key aspects such as
complexity, efficiency, latency, channel estimation, hardware constraints,
synchronization, security, and potential integration with other advanced
wireless communication techniques.

</details>


### [41] [Near-Field 3D Localization and MIMO Channel Estimation with Sub-Connected Planar Arrays](https://arxiv.org/abs/2510.20274)
*Kangda Zhi,Tianyu Yang,Songyan Xue,Giuseppe Caire*

Main category: eess.SP

TL;DR: 提出了一种用于近场XL-MIMO系统中信道估计和3D定位的三阶段算法，结合OMP和SBL技术，显著降低了导频开销并提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 在近场XL-MIMO系统中，现有码本设计不适用于多天线用户场景，导致信道估计效果不佳，需要开发新的有效算法。

Method: 三阶段算法：1）使用OMP和DFT字典进行子阵列信道估计；2）利用MUSIC和LS准则估计用户阵列中心位置；3）基于估计位置构建位置辅助字典，使用SBL进行MIMO信道估计。

Result: 与多个基准方法相比，所提算法在导频开销和估计精度方面均表现出显著优势。

Conclusion: 该三阶段算法有效解决了近场XL-MIMO系统中多天线用户的信道估计和3D定位问题，具有实际应用价值。

Abstract: This paper investigates the design of channel estimation and 3D localization
algorithms in a challenging scenario, where a sub-connected planar extremely
large-scale multiple-input multiple-output (XL-MIMO) communicates with
multi-antenna users. In the near field, the uplink MIMO channel is of full
column rank and therefore can not be estimated effectively by applying existing
codebooks that are designed for the far-field case or for the near-field case
but limited to single antenna users. To solve this problem, we propose a
three-stage algorithm aided by orthogonal matching pursuit (OMP) and sparse
Bayesian learning (SBL). Specifically, we firstly partition the XL-MIMO into
subarrays and use OMP to solve the compressed sensing (CS) problem about
subarray channel estimation with the Discrete Fourier Transform (DFT)-based
dictionary matrix. Secondly, exploiting the estimated subarray channels and
employing one-dimensional multiple signal classification (MUSIC), we estimate
the central location of the user array under the Least Squares (LS) criterion.
Finally, we utilize the estimated central location to construct a refined
location-aided dictionary matrix and obtain the MIMO channel estimation using
SBL. Results exhibit the significant superiority of the proposed algorithm
compared with several benchmarks, in terms of both the pilot overhead and
estimation accuracy.

</details>


### [42] [A Transformer Inspired AI-based MIMO receiver](https://arxiv.org/abs/2510.20363)
*András Rácz,Tamás Borsos,András Veres,Benedek Csala*

Main category: eess.SP

TL;DR: AttDet是一种基于Transformer的MIMO检测方法，将每个传输层视为token，通过轻量级自注意力机制学习流间干扰，结合模型可解释性和数据驱动灵活性，在5G信道下实现接近最优的BER/BLER性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决MIMO系统中复杂的流间干扰问题，同时保持检测算法的可解释性和计算效率，需要开发一种结合模型驱动和数据驱动优势的检测方法。

Method: 将每个传输层作为token，直接从估计的信道矩阵推导查询和键向量，注意力分数量化信道相关性，值向量由匹配滤波器输出初始化并迭代优化。

Result: 在现实的5G信道模型和高阶混合QAM调制编码方案下，AttDet能够实现接近最优的BER/BLER性能，同时保持多项式复杂度。

Conclusion: AttDet成功地将Transformer架构应用于MIMO检测，在保持模型可解释性的同时提供了数据驱动的灵活性，为无线通信系统提供了高效可靠的检测解决方案。

Abstract: We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple
Output) detection method that treats each transmit layer as a token and learns
inter-stream interference via a lightweight self-attention mechanism. Queries
and keys are derived directly from the estimated channel matrix, so attention
scores quantify channel correlation. Values are initialized by matched-filter
outputs and iteratively refined. The AttDet design combines model-based
interpretability with data-driven flexibility. We demonstrate through
link-level simulations under realistic 5G channel models and high-order, mixed
QAM modulation and coding schemes, that AttDet can approach near-optimal
BER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining
predictable, polynomial complexity.

</details>


### [43] [An Accelerated Mixed Weighted-Unweighted MMSE Approach for MU-MIMO Beamforming](https://arxiv.org/abs/2510.20507)
*Xi Gao,Akang Wang,Junkai Zhang,Qihong Duan,Jiang Xue*

Main category: eess.SP

TL;DR: 提出了一种基于块坐标下降框架的高并行算法A-MMMSE，用于多用户MIMO系统的预编码设计，避免了传统WMMSE算法中的矩阵求逆操作，显著降低了计算复杂度并适合GPU加速。


<details>
  <summary>Details</summary>
Motivation: 传统WMMSE算法在基站天线数量较多时计算复杂度高（立方级），限制了其在延迟敏感场景中的应用，需要开发更高效的算法。

Method: 采用块坐标梯度下降更新预编码矩阵，避免矩阵求逆，仅使用矩阵乘法；引入基于和均方误差最小化问题的两阶段热启动策略加速收敛。

Result: 仿真结果表明A-MMMSE在加权和速率性能上与WMMSE及其增强变体reduced-WMMSE相当，同时在各种系统配置下显著减少了计算时间。

Conclusion: A-MMMSE算法在保持性能的同时大幅降低了计算复杂度，特别适合GPU加速，为延迟敏感的多用户MIMO系统提供了高效的预编码解决方案。

Abstract: Precoding design based on weighted sum-rate (WSR) maximization is a
fundamental problem in downlink multi-user multiple-input multiple-output
(MU-MIMO) systems. While the weighted minimum mean-square error (WMMSE)
algorithm is a standard solution, its high computational complexity--cubic in
the number of base station antennas due to matrix inversions--hinders its
application in latency-sensitive scenarios. To address this limitation, we
propose a highly parallel algorithm based on a block coordinate descent
framework. Our key innovation lies in updating the precoding matrix via block
coordinate gradient descent, which avoids matrix inversions and relies solely
on matrix multiplications, making it exceptionally amenable to GPU
acceleration. We prove that the proposed algorithm converges to a stationary
point of the WSR maximization problem. Furthermore, we introduce a two-stage
warm-start strategy grounded in the sum mean-square error (MSE) minimization
problem to accelerate convergence. We refer to our method as the Accelerated
Mixed weighted-unweighted sum-MSE minimization (A-MMMSE) algorithm. Simulation
results demonstrate that A-MMMSE matches the WSR performance of both
conventional WMMSE and its enhanced variant, reduced-WMMSE, while achieving a
substantial reduction in computational time across diverse system
configurations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [44] [Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection](https://arxiv.org/abs/2510.20653)
*Jack Butler,Nikita Kozodoi,Zainab Afolabi,Brian Tyacke,Gaiar Baimuratov*

Main category: stat.ML

TL;DR: 本文系统比较了自反思和预算调优两种推理优化技术在数学推理和翻译任务中的表现，揭示了自反思在不同领域的效果差异显著，在数学推理中性能提升可达220%。研究还分析了反思轮次深度和反馈机制质量对性能的影响，并在真实场景中验证了发现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的不断发展，从业者面临更多无需重新训练模型即可提升推理性能的选择，包括预算调优和多步技术如自反思。这些方法虽然能提高输出质量，但在准确性、成本和延迟之间产生了复杂的权衡关系，且在不同领域中的表现尚未被充分理解。

Method: 系统比较自反思和预算调优在数学推理和翻译任务中的表现，评估包括Anthropic Claude、Amazon Nova和Mistral系列在内的主要LLMs，在不同反思深度和计算预算下推导帕累托最优性能边界，并分析反思轮次深度和反馈机制质量对性能的影响。

Result: 分析显示自反思效果存在显著的领域依赖性，在数学推理中性能提升可达220%。在Zalando Lounge部署的自反思增强营销内容本地化系统显示出市场依赖的有效性，强调了部署这些技术时进行领域特定评估的重要性。

Conclusion: 研究结果为在特定领域和资源约束下选择最优推理策略提供了可操作的指导，并开源了自反思实现以确保可复现性。

Abstract: As Large Language Models (LLMs) continue to evolve, practitioners face
increasing options for enhancing inference-time performance without model
retraining, including budget tuning and multi-step techniques like
self-reflection. While these methods improve output quality, they create
complex trade-offs among accuracy, cost, and latency that remain poorly
understood across different domains. This paper systematically compares
self-reflection and budget tuning across mathematical reasoning and translation
tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and
Mistral families, along with other models under varying reflection depths and
compute budgets to derive Pareto optimal performance frontiers. Our analysis
reveals substantial domain dependent variation in self-reflection
effectiveness, with performance gains up to 220\% in mathematical reasoning. We
further investigate how reflection round depth and feedback mechanism quality
influence performance across model families. To validate our findings in a
real-world setting, we deploy a self-reflection enhanced marketing content
localisation system at Lounge by Zalando, where it shows market-dependent
effectiveness, reinforcing the importance of domain specific evaluation when
deploying these techniques. Our results provide actionable guidance for
selecting optimal inference strategies given specific domains and resource
constraints. We open source our self-reflection implementation for
reproducibility at
https://github.com/aws-samples/sample-genai-reflection-for-bedrock.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 本研究提出了分析可靠性基准（ARB），用于量化大型语言模型在能源系统分析中的推理可靠性，通过五个子指标评估模型在确定性、概率性和认知性场景下的表现，并测试了四个前沿模型。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在能源领域的应用缺乏标准化框架来评估系统推理的正确性，现有验证实践主要关注预测准确性或计算效率，而忽略了分析结论的逻辑完整性。

Method: 开发了分析可靠性基准（ARB），整合准确性、推理可靠性、不确定性纪律、政策一致性和透明度五个子指标，使用开放技术经济数据集（NREL ATB 2024、DOE H2A/H2New、IEA WEO 2024）在确定性、概率性和认知性场景下评估模型性能。

Result: GPT-4/5和Claude 4.5 Sonnet实现了持续且符合政策的推理（分析可靠性指数大于90），Gemini 2.5 Pro表现出中等稳定性，而Llama 3 70B低于专业阈值。统计验证确认这些差异显著且可重现。

Conclusion: ARB建立了能源文献中首个定量方法来验证人工智能系统中的因果、概率和政策驱动推理，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [46] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究探讨了AI技术在个性化学习中的潜力，通过领导力人格特质和机器学习模型预测学业成功。使用129名环境工程硕士生的领导力人格测试数据和学业成绩，通过特征选择和7种ML算法建模，随机森林分类器在包含17个人格特征和领导力标记的模型中达到87.50%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的应用潜力，通过预测学业成功来早期识别学生的优势和劣势，为个性化学习策略选择提供支持。

Method: 收集129名硕士生的领导力人格测试数据（23个特征）和平均成绩，使用探索性数据分析和相关性分析，通过皮尔逊相关系数进行特征选择，采用7种ML算法（SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM）进行建模。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 该研究为在教育过程早期识别学生优劣势提供了额外机会，有助于选择最适合的个性化学习策略。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [47] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [48] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT模型通过整合POI信息和时间描述符来增强BERT模型，显著提升了人类移动性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 提出STaBERT模型，在每个位置整合POI和时间信息，构建统一的语义增强移动性表示。

Result: 实验结果显示STaBERT显著提升预测精度：单城市预测的GEO-BLEU得分从0.34提升到0.75；多城市预测从0.34提升到0.56。

Conclusion: 整合POI和时间信息能够有效捕捉人类移动的语义基础，显著改进移动性预测性能。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [49] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于军事行动中AI系统目标打击的附带损害评估模型，该模型将时间、空间和力量维度整合到统一的知识表示与推理架构中，通过分层结构捕获AI系统的类别、架构组件、打击向量和上下文方面。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中扮演日益重要角色的时代，确保负责任的打击需要对潜在附带效应进行严格评估。

Method: 采用设计科学方法论，构建了集成时间、空间和力量维度的统一知识表示与推理架构，模型包含分层结构来捕获AI系统类别、架构组件、打击向量和上下文方面，并考虑传播、严重性、可能性和评估指标。

Result: 通过实例化演示和评估了该模型，为构建负责任和可信赖的智能系统奠定了基础。

Conclusion: 该模型为评估军事行动中打击AI系统产生的效应提供了透明推理机制，有助于构建负责任的智能系统。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [50] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的新型神经推理器，使用嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器在面对不一致和错误数据时缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法在真实知识库上部署困难，因为使用的描述逻辑推理器对不一致和错误数据缺乏鲁棒性。

Method: EBR推理器依赖嵌入来近似符号推理器的结果，仅需要检索原子概念和存在限制的实例，就能检索或近似SHOIQ描述逻辑中任何概念的实例集。

Result: 实验表明，EBR与最先进的推理器相比，在面对缺失和错误数据时表现出更强的鲁棒性。

Conclusion: EBR神经推理器为概念学习提供了一种对数据错误具有鲁棒性的推理方法，解决了现有符号推理器在真实知识库应用中的局限性。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [51] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的知识图谱对齐方法，无需训练数据，可同时对齐实体和关系，提供可解释的结果，并在主要基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注实体级对齐，缺乏可解释性推理且需要训练数据。FLORA旨在解决这些问题，提供无监督、可解释的全面对齐方案。

Method: FLORA基于模糊逻辑，通过迭代方式同时对齐实体和关系，允许存在悬空实体（没有对应实体的实体），并保证收敛性。

Result: 该方法在主要基准测试中取得了最先进的结果，证明了其有效性。

Conclusion: FLORA是一种简单而有效的知识图谱对齐方法，具有无监督、可解释、全面对齐等优势，为知识图谱对齐任务提供了新的解决方案。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>

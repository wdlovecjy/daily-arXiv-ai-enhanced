<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 3]
- [eess.SP](#eess.SP) [Total: 3]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.LG](#cs.LG) [Total: 35]
- [cs.AI](#cs.AI) [Total: 16]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Long-Term Spatio-Temporal Forecasting of Monthly Rainfall in West Bengal Using Ensemble Learning Approaches](https://arxiv.org/abs/2510.13927)
*Jishu Adhikary,Raju Maiti*

Main category: stat.AP

TL;DR: 本研究开发了一个分层建模框架，结合回归模型和多层感知器(MLP)来预测西孟加拉邦19个地区1900-2019年的月度降雨量，能够进行长达9年的长期预测。


<details>
  <summary>Details</summary>
Motivation: 降雨预报在气候适应、农业和水资源管理中具有关键作用，需要开发能够处理降雨动态非线性和复杂结构的长期预测方法。

Method: 提出分层建模框架：首先使用回归模型预测年度特征（年总量、季度比例、变异性等），然后将这些预测作为辅助输入整合到MLP模型中，捕捉月度序列的非线性时间模式和空间依赖性。

Result: 分层回归-MLP架构能够提供稳健的长期时空预测，预测了2011-2019年共108个月的降雨量。

Conclusion: 该方法为农业、灌溉规划和水资源保护策略提供了有价值的见解，能够有效处理降雨动态的复杂性。

Abstract: Rainfall forecasting plays a critical role in climate adaptation,
agriculture, and water resource management. This study develops long-term
forecasts of monthly rainfall across 19 districts of West Bengal using a
century-scale dataset spanning 1900-2019. Daily rainfall records are aggregated
into monthly series, resulting in 120 years of observations for each district.
The forecasting task involves predicting the next 108 months (9 years,
2011-2019) while accounting for temporal dependencies and spatial interactions
among districts. To address the nonlinear and complex structure of rainfall
dynamics, we propose a hierarchical modeling framework that combines
regression-based forecasting of yearly features with multi-layer perceptrons
(MLPs) for monthly prediction. Yearly features, such as annual totals,
quarterly proportions, variability measures, skewness, and extremes, are first
forecasted using regression models that incorporate both own lags and
neighboring-district lags. These forecasts are then integrated as auxiliary
inputs into an MLP model, which captures nonlinear temporal patterns and
spatial dependencies in the monthly series. The results demonstrate that the
hierarchical regression-MLP architecture provides robust long-term
spatio-temporal forecasts, offering valuable insights for agriculture,
irrigation planning, and water conservation strategies.

</details>


### [2] [A Data-Parsimonious Model for Long-Term Risk Assessments of West Nile Virus Spillover](https://arxiv.org/abs/2510.14011)
*Saman Hosseini,Lee W. Cohnstaedt,Matin Marjani,Caterina Scoglio*

Main category: stat.AP

TL;DR: 提出了一种新的数据简约概率模型，用于预测西尼罗河病毒爆发的时机和季节性严重程度，该模型结合温度驱动的隔室模型和非参数核密度估计方法，可在传播季节开始前几个月提供可靠预测。


<details>
  <summary>Details</summary>
Motivation: 许多西尼罗河病毒预测框架需要昆虫学或鸟类监测数据，但这些数据在某些地区可能无法获得，因此需要开发数据简约的预测方法。

Method: 结合温度驱动的西尼罗河病毒隔室模型和非参数核密度估计方法，构建联合概率密度函数和泊松率表面，作为蚊虫丰度和归一化累积温度的函数，并使用人类发病率记录进行校准。

Result: 在加州、德克萨斯州和佛罗里达州的六个县进行评估，这些地区具有完全不同的生态和气候条件，模型在多个性能指标上表现出强一致性。

Conclusion: 该模型能够在传播季节开始前几个月产生可靠的预测，支持主动的缓解工作，适用于不同生态和气候条件的地区。

Abstract: Many West Nile virus (WNV) forecasting frameworks incorporate entomological
or avian surveillance data, which may be unavailable in some regions. We
introduce a novel data-parsimonious probabilistic model to predict both the
timing of outbreak onset and the seasonal severity of WNV spillover. Our
approach combines a temperature-driven compartmental model of WNV with
nonparametric kernel density estimation methods to construct a joint
probability density function and a Poisson rate surface as function of mosquito
abundance and normalized cumulative temperature. Calibrated on human incidence
records, the model produces reliable forecasts several months before the
transmission season begins, supporting proactive mitigation efforts. We
evaluated the framework across three counties in California (Orange, Los
Angeles, and Riverside), two in Texas (Dallas and Harris), and one in Florida
(Duval), representing completely different ecology and distinct climatic
regimes, and observed strong agreement across multiple performance metrics.

</details>


### [3] [Bayes-ically fair: A Bayesian Ranking of the Olympic Medal Table](https://arxiv.org/abs/2510.14723)
*Cormac MacDermott,Carl J. Scarrott,John Ferguson*

Main category: stat.AP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating a country's sporting success provides insight into its
decision-making and infrastructure for developing athletic talent. The Olympic
Games serve as a global benchmark, yet conventional medal rankings can be
unduly influenced by population size. We propose a Bayesian ranking scheme to
rank the performance of National Olympic Committees by their "long-run"
medals-to-population ratio. The algorithm aims to mitigate the influence of
large populations and reduce the stochastic fluctuations for smaller nations by
applying shrinkage. These long-run rankings provide a more stable and
interpretable ordering of national sporting performance across games compared
to existing methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [4] [Error Rate Analysis and Low-Complexity Receiver Design for Zero-Padded AFDM](https://arxiv.org/abs/2510.14507)
*Qin Yi,Zeping Sui,Zilong Liu*

Main category: eess.SP

TL;DR: 本文研究了ZP-AFDM系统的误码率性能和低复杂度接收机设计，提出了两种低复杂度检测器，并分析了理论误码率性能。


<details>
  <summary>Details</summary>
Motivation: 研究零填充仿射频分复用系统的误码率性能和低复杂度接收机设计，旨在降低传统检测器的计算复杂度。

Method: 利用时域信道矩阵的零填充辅助下三角结构，提出了新型低复杂度MMSE检测器和基于最大比合并的时域检测器。

Result: 仿真结果表明，所提出的检测器能够实现与传统基于矩阵求逆的MMSE检测器相同的误码率性能，同时显著降低了复杂度。

Conclusion: 提出的低复杂度检测器在保持性能的同时有效降低了计算复杂度，为ZP-AFDM系统的实际应用提供了可行的解决方案。

Abstract: This paper studies the error rate performance and low-complexity receiver
design for zero-padded affine frequency division multiplexing (ZP-AFDM)
systems. By exploiting the unique ZP-aided lower triangular structure of the
time domain (TD) channel matrix, we propose {a novel low-complexity} minimum
mean square error (MMSE) detector and {a} maximum ratio combining-based TD
(MRC-TD) detector. Furthermore, the theoretical bit error rate (BER)
{performance} of both MMSE and maximum likelihood detectors {is} analyzed.
Simulation results demonstrate {that} the proposed detectors can achieve
identical BER performance to that of {the conventional MMSE detector based on
matrix inversion} while {enjoying significantly reduced complexity.}

</details>


### [5] [A Scalable MVDR Beamforming Algorithm That is Linear in the Number of Antennas](https://arxiv.org/abs/2510.14802)
*Sanjaya Herath,Armin Gerami,Kevin Wagner,Ramani Duraiswami,Christopher A. Metzler*

Main category: eess.SP

TL;DR: 提出了一种针对大规模阵列的可扩展MVDR波束成形方法，将计算复杂度从天线数量的立方级降低到线性级，适用于信噪比低于噪声底限的场景。


<details>
  <summary>Details</summary>
Motivation: 传统MVDR波束成形在大规模阵列中计算复杂度呈立方增长，难以满足实时应用需求，特别是在GPS等信号低于噪声底限的场景中。

Method: 利用Sherman-Morrison公式、低秩奇异值分解近似和代数操作，针对信号低于噪声底限的场景设计高效算法。

Result: 通过仿真验证，该方法显著降低了计算负载，同时在大规模阵列中保持了高波束成形精度。

Conclusion: 该方法为雷达、声纳和无线通信等大规模天线阵列应用中的实时MVDR波束成形提供了可行解决方案。

Abstract: The Minimum Variance Distortionless Response (MVDR) beamforming technique is
widely applied in array systems to mitigate interference. However, applying
MVDR to large arrays is computationally challenging; its computational
complexity scales cubically with the number of antenna elements. In this paper,
we introduce a scalable MVDR beamforming method tailored for massive arrays.
Our approach, which is specific to scenarios where the signal of interest is
below the noise floor (e.g.,~GPS), leverages the Sherman-Morrison formula,
low-rank Singular Value Decomposition (SVD) approximations, and algebraic
manipulation. Using our approach, we reduce the computational complexity from
cubic to linear in the number of antennas. We evaluate the proposed method
through simulations, comparing its computational efficiency and beamforming
accuracy with the conventional MVDR approach. Our method significantly reduces
the computational load while maintaining high beamforming accuracy for
large-scale arrays. This solution holds promise for real-time applications of
MVDR beamforming in fields like radar, sonar, and wireless communications,
where massive antenna arrays are proliferating.

</details>


### [6] [Decoding in the presence of ISI without interleaving ORBGRAND AI](https://arxiv.org/abs/2510.14939)
*Ken R. Duffy,Moritz Grundei,Jane A. Millward,Muralidhar Rangaswamy,Muriel Medard*

Main category: eess.SP

TL;DR: 提出了一种名为ORBGRAND-AI的解码器，用于处理符号间干扰(ISI)信道中的有色噪声，无需交织即可达到或优于现有软输入解码器的性能。


<details>
  <summary>Details</summary>
Motivation: 符号间干扰(ISI)广泛存在于各种信道中，会导致时间色散。传统均衡方法会产生有色噪声，需要开发新的解码方法来处理这种有色噪声环境。

Method: 基于统计物理学中近似独立性的思想开发了ORBGRAND-AI解码器，通过放弃交织，在ISI信道中直接处理有色噪声。评估了延迟抽头模型和RFView物理建模工具导出的ISI信道。

Result: ORBGRAND-AI在相同的每信息比特能量下，能够提供与最先进的软输入解码器(如CA-SCL)相同或更低的块错误率(BLER)，且无需交织器。二阶自回归模型能够充分表示RFView信道效应。

Conclusion: ORBGRAND-AI解码器在ISI信道中处理有色噪声方面表现出色，无需交织即可达到先进解码器的性能，为有色噪声环境下的通信提供了有效的解决方案。

Abstract: Inter symbol interference (ISI), which occurs in a wide variety of channels,
is a result of time dispersion. It can be mitigated by equalization which
results in noise coloring. For such colored noise, we propose a decoder called
Ordered Reliability Bit Guessing Random Additive Noise Decoding (ORBGRANDAI)
which is inspired by the development of approximate independence in statistical
physics. By foregoing interleaving, ORBGRAND-AI can deliver the same, or lower,
block error rate (BLER) for the same amount of energy per information bit in an
ISI channel as a state-of-the-art soft input decoder, such as Cyclic Redundancy
Check Assisted-Successive Cancellation List (CA-SCL) decoding, with an
interleaver. To assess the decoding performance of ORBGRAND-AI, we consider
delay tap models and their associated colored noise. In particular, we examine
a two-tap dicode ISI channel as well as an ISI channel derived from data from
RFView, a physics-informed modeling and simulation tool. We investigate the
dicode and RFView channel under a variety of imperfect channel state
information assumptions and show that a second order autoregressive model
adequately represents the RFView channel effect.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [7] [deFOREST: Fusing Optical and Radar satellite data for Enhanced Sensing of Tree-loss](https://arxiv.org/abs/2510.14092)
*Julio Enrique Castrillon-Candas,Hanfeng Gu,Caleb Meredith,Yulin Li,Xiaojing Tang,Pontus Olofsson,Mark Kon*

Main category: stat.ML

TL;DR: 提出了一种结合光学和SAR数据的森林砍伐检测管道，使用KL展开的残差空间构建光学异常图，结合HMM进行分类，在亚马逊森林测试中显示优于现有混合方法的高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发一种不依赖数据分布先验知识的森林砍伐检测方法，解决传统参数统计方法在高维数据中不实用的假设问题，特别是在多云地区光学数据稀疏的情况下提高检测鲁棒性。

Method: 使用KL展开的残差空间构建光学异常图，通过残差分量的集中界限量化异常，无需数据分布先验知识。将光学异常图与SAR数据结合，使用隐马尔可夫模型进行森林状态分类。

Result: 在亚马逊森林92.19km×91.80km区域的测试中，混合光学-雷达方法和纯光学方法都达到了高精度，优于现有最先进的混合方法。混合方法在光学数据稀疏的多云地区表现出显著更强的鲁棒性。

Conclusion: 提出的混合光学-SAR森林砍伐检测方法不仅精度高，而且在多云地区光学数据稀疏的情况下具有优越的鲁棒性，为森林监测提供了有效的解决方案。

Abstract: In this paper we develop a deforestation detection pipeline that incorporates
optical and Synthetic Aperture Radar (SAR) data. A crucial component of the
pipeline is the construction of anomaly maps of the optical data, which is done
using the residual space of a discrete Karhunen-Lo\`{e}ve (KL) expansion.
Anomalies are quantified using a concentration bound on the distribution of the
residual components for the nominal state of the forest. This bound does not
require prior knowledge on the distribution of the data. This is in contrast to
statistical parametric methods that assume knowledge of the data distribution,
an impractical assumption that is especially infeasible for high dimensional
data such as ours. Once the optical anomaly maps are computed they are combined
with SAR data, and the state of the forest is classified by using a Hidden
Markov Model (HMM). We test our approach with Sentinel-1 (SAR) and Sentinel-2
(Optical) data on a $92.19\,km \times 91.80\,km$ region in the Amazon forest.
The results show that both the hybrid optical-radar and optical only methods
achieve high accuracy that is superior to the recent state-of-the-art hybrid
method. Moreover, the hybrid method is significantly more robust in the case of
sparse optical data that are common in highly cloudy regions.

</details>


### [8] [High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data](https://arxiv.org/abs/2510.14145)
*Mohammed Baragilly,Hend Gabr*

Main category: stat.ML

TL;DR: 提出了一种新的高维聚类验证框架HD-BWDM，通过随机投影和PCA解决维度灾难，使用修剪聚类和基于中心点的距离确保对异常值的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类有效性指标（如Calinski-Harabasz、Silhouette、Davies-Bouldin）依赖于基于质心的距离，在高维或受污染数据中性能下降。

Method: HD-BWDM框架结合随机投影和主成分分析缓解维度灾难，应用修剪聚类和基于中心点的距离确保对异常值的鲁棒性。

Result: 理论分析显示在Johnson-Lindenstrauss嵌入下具有一致性和收敛性，模拟实验表明HD-BWDM在高维投影和污染下保持稳定和可解释性。

Conclusion: HD-BWDM为现代高维应用中的非参数聚类提供了一个理论上有基础、计算效率高的停止规则，是传统基于质心验证标准的鲁棒替代方案。

Abstract: Determining the appropriate number of clusters in unsupervised learning is a
central problem in statistics and data science. Traditional validity indices
such as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend on
centroid-based distances and therefore degrade in high-dimensional or
contaminated data. This paper proposes a new robust, nonparametric clustering
validation framework, the High-Dimensional Between-Within Distance Median
(HD-BWDM), which extends the recently introduced BWDM criterion to
high-dimensional spaces. HD-BWDM integrates random projection and principal
component analysis to mitigate the curse of dimensionality and applies trimmed
clustering and medoid-based distances to ensure robustness against outliers. We
derive theoretical results showing consistency and convergence under
Johnson-Lindenstrauss embeddings. Extensive simulations demonstrate that
HD-BWDM remains stable and interpretable under high-dimensional projections and
contamination, providing a robust alternative to traditional centroid-based
validation criteria. The proposed method provides a theoretically grounded,
computationally efficient stopping rule for nonparametric clustering in modern
high-dimensional applications.

</details>


### [9] [Local Causal Discovery for Statistically Efficient Causal Inference](https://arxiv.org/abs/2510.14582)
*Mátyás Schubert,Tom Claassen,Sara Magliacane*

Main category: stat.ML

TL;DR: LOAD是一种结合局部因果发现方法计算效率和全局方法统计最优性的因果发现方法，能够在保持可扩展性的同时找到最优调整集。


<details>
  <summary>Details</summary>
Motivation: 全局因果发现方法虽然能找到最优调整集但计算复杂度高，局部方法虽然可扩展但只能找到统计次优的调整集，需要一种兼顾计算效率和统计最优性的方法。

Method: LOAD首先识别目标变量间的因果关系并测试是否可识别，如果可识别则通过局部因果发现推断中介变量及其父节点来找到最优调整集，否则返回基于局部结构的有效父调整集。

Result: 在合成和真实数据实验中，LOAD在可扩展性上优于全局方法，在效应估计准确性上优于局部方法。

Conclusion: LOAD成功结合了局部方法的计算效率和全局方法的统计最优性，为因果效应估计提供了一种既高效又准确的方法。

Abstract: Causal discovery methods can identify valid adjustment sets for causal effect
estimation for a pair of target variables, even when the underlying causal
graph is unknown. Global causal discovery methods focus on learning the whole
causal graph and therefore enable the recovery of optimal adjustment sets,
i.e., sets with the lowest asymptotic variance, but they quickly become
computationally prohibitive as the number of variables grows. Local causal
discovery methods offer a more scalable alternative by focusing on the local
neighborhood of the target variables, but are restricted to statistically
suboptimal adjustment sets. In this work, we propose Local Optimal Adjustments
Discovery (LOAD), a sound and complete causal discovery approach that combines
the computational efficiency of local methods with the statistical optimality
of global methods. First, LOAD identifies the causal relation between the
targets and tests if the causal effect is identifiable by using only local
information. If it is identifiable, it then finds the optimal adjustment set by
leveraging local causal discovery to infer the mediators and their parents.
Otherwise, it returns the locally valid parent adjustment sets based on the
learned local structure. In our experiments on synthetic and realistic data
LOAD outperforms global methods in scalability, while providing more accurate
effect estimation than local methods.

</details>


### [10] [Fast and Scalable Score-Based Kernel Calibration Tests](https://arxiv.org/abs/2510.14711)
*Pierre Glaser,David Widmann,Fredrik Lindsten,Arthur Gretton*

Main category: stat.ML

TL;DR: 提出KCCSD测试，一种基于核的非参数方法，用于评估具有明确定义分数的概率模型的校准性。该方法无需昂贵的期望近似，并能控制I类错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要昂贵的期望近似，且难以控制I类错误。需要开发一种更高效、可靠的校准性评估方法。

Method: 使用新的基于分数的概率核函数族，无需概率密度样本即可估计；采用条件拟合优度准则处理KCCSD测试的U统计量。

Result: 在各种合成设置中验证了测试方法的性能。

Conclusion: KCCSD测试提供了一种无需昂贵期望近似、能控制I类错误的概率模型校准性评估方法。

Abstract: We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD
test), a non-parametric, kernel-based test for assessing the calibration of
probabilistic models with well-defined scores. In contrast to previous methods,
our test avoids the need for possibly expensive expectation approximations
while providing control over its type-I error. We achieve these improvements by
using a new family of kernels for score-based probabilities that can be
estimated without probability density samples, and by using a conditional
goodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the
properties of our test on various synthetic settings.

</details>


### [11] [A Geometric Approach to Optimal Experimental Design](https://arxiv.org/abs/2510.14848)
*Gavin Kerrigan,Christian A. Naesseth,Tom Rainforth*

Main category: stat.ML

TL;DR: 提出了一种基于最优传输理论的新几何框架用于最优实验设计，通过互传输依赖度替代传统基于概率密度的方法，提供更灵活的几何目标优化设计。


<details>
  <summary>Details</summary>
Motivation: 传统最优实验设计方法基于概率密度，具有限制性的不变性特性，需要更灵活且能针对特定下游估计问题定制的方法。

Method: 引入互传输依赖度作为统计依赖度量，基于最优传输理论，通过选择适当的几何结构来优化实验设计。

Result: 该框架能够生成高质量的设计，为传统信息论技术提供了灵活的替代方案。

Conclusion: 基于最优传输的几何框架为最优实验设计提供了更灵活和定制化的方法，能够适应不同的下游估计需求。

Abstract: We introduce a novel geometric framework for optimal experimental design
(OED). Traditional OED approaches, such as those based on mutual information,
rely explicitly on probability densities, leading to restrictive invariance
properties. To address these limitations, we propose the mutual transport
dependence (MTD), a measure of statistical dependence grounded in optimal
transport theory which provides a geometric objective for optimizing designs.
Unlike conventional approaches, the MTD can be tailored to specific downstream
estimation problems by choosing appropriate geometries on the underlying
spaces. We demonstrate that our framework produces high-quality designs while
offering a flexible alternative to standard information-theoretic techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames提出了一种基于场景驱动的关键帧选择新范式，通过预测语义连贯、与查询相关的视频片段来保持时间连续性，支持任意数量的关键帧选择以适应不同用户预算。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在长视频理解中受到上下文窗口和计算成本的限制，均匀帧采样会导致大量信息丢失，而现有的关键帧选择方法通常产生稀疏且时间不连续的帧，忽略了场景连续性且缺乏多尺度帧选择的灵活性。

Method: 首先构建了包含20万个视频高光片段的PeakClips数据集，然后采用三阶段渐进式课程学习：两个监督微调阶段用于时间定位和关键片段感知，接着是一个强化学习阶段，直接优化场景驱动的预测策略以适应下游任务而无需额外标注。

Result: 在主要的长视频理解基准测试上的广泛实验表明，K-frames提供了一个有效、可解释且即插即用的解决方案，支持各种尺度的关键帧选择。

Conclusion: K-frames通过场景驱动的关键帧选择方法有效解决了长视频理解中的信息保留和计算效率问题，为多模态大语言模型的长视频处理提供了新的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [13] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 提出了一种多视图半监督标签分布学习方法MVSS-LDL，通过利用每个视图的局部最近邻结构并强调多视图间局部结构的互补性，解决了多视图半监督标签分布学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单视图标签分布学习问题，且只处理有标签数据，而多视图标签分布学习问题以及包含无标签数据的情况尚未被考虑。

Method: 首先计算每个视图的k-最近邻结构，然后通过整合样本在其他视图中的最近邻来补充当前视图的最近邻集合，最后基于补充后的最近邻集合构建基于图学习的多视图半监督标签分布学习模型。

Result: 数值研究表明，MVSS-LDL方法在分类性能上明显优于现有的单视图标签分布学习方法。

Conclusion: 这是首次尝试解决多视图标签分布学习问题，通过考虑局部最近邻结构的互补性，不同视图可以相互提供局部结构信息来补充彼此。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [14] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 提出DR-RPO算法，一种模型无关的在线策略优化方法，用于在分布偏移下学习鲁棒策略，具有次线性遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中训练和部署环境不同的分布偏移问题，特别是在在线设置下样本效率和探索的关键挑战。

Method: 结合参考策略正则化，在软最大策略类中实现可优化性；采用d-矩形线性MDP公式，结合线性函数逼近和上置信界奖励进行乐观探索。

Result: 理论保证策略优化在鲁棒RL中可实现多项式次优界和样本效率，与基于价值的方法性能相当；实证结果验证了算法的鲁棒性。

Conclusion: DR-RPO填补了鲁棒RL中策略优化方法的理论空白，证明了策略优化在分布鲁棒设置中的有效性。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [15] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本文提出了一种新的ICD编码自动分配方法，将其视为分类和排序任务而非纯分类任务，以考虑编码顺序的重要性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的ICD诊断编码顺序对医疗诊断和报销至关重要，但现有方法将其视为纯分类任务而忽略了编码顺序。

Method: 从检索系统角度处理该问题，将其制定为分类和排序任务，同时考虑代码分配和优先级排序。

Result: 在正确排序主要诊断代码方面准确率达到47%（相比现有最佳方法的20%），在分类指标上微平均F1分数0.6065和宏平均F1分数0.2904，均优于之前最佳模型。

Conclusion: 所提出的框架在识别高优先级代码方面具有优越能力，证明了将ICD编码任务视为分类和排序问题的有效性。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [16] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 本文提出了主动可观测马尔可夫决策过程（AOMDP），其中智能体不仅选择控制动作，还决定是否测量潜在状态。测量动作会揭示真实潜在状态，但可能对环境产生负面延迟影响。


<details>
  <summary>Details</summary>
Motivation: 在现实世界强化学习设置中，测量状态可能成本高昂且对后续结果产生负面影响。需要一种框架来平衡状态测量的收益与成本。

Method: 将AOMDP建模为周期性部分可观测MDP，提出基于信念状态的在线RL算法，并使用顺序蒙特卡洛方法联合近似未知静态环境参数和未观测潜在状态的后验分布。

Result: 研究表明，减少不确定性可以证明提高样本效率，并增加最优策略的价值，尽管存在测量成本。在数字健康应用中评估了所提算法。

Conclusion: AOMDP框架能够有效处理状态测量的成本收益权衡，在数字健康等应用中具有实用价值。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [17] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据保真度损失函数——分布一致性损失，用于解决逆问题中传统点对点损失函数容易过拟合噪声的问题。该损失函数通过测试观测测量值是否与当前估计所暗示的噪声分布统计一致来评估数据保真度，可作为传统损失函数的直接替代。


<details>
  <summary>Details</summary>
Motivation: 传统的数据保真度损失函数（如均方误差）寻求与噪声测量值的点对点一致性，往往导致对噪声的过拟合。作者希望开发一种能够避免过拟合噪声的数据保真度评估方法。

Method: 引入分布一致性损失，用分布级校准替代点对点匹配，使用基于模型的概率分数对每个测量值进行评估。该方法与现有正则化器兼容，优化方式与传统损失相同。

Result: 在图像去噪中，使用DC损失替代MSE损失无需提前停止即可获得更高PSNR；在医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工正则化的效果。

Conclusion: DC损失为逆问题提供了一个统计基础扎实、性能增强的替代传统保真度损失的方法，特别适用于测量噪声分布已知且测量数据集包含许多独立噪声值的实际逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [18] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 该论文提出了基于jet bundles和Weil代数的自动微分几何框架，将反向模式AD解释为余切拉回，泰勒模式对应Weil代数求值，并推导了正确性、稳定性和复杂性的简洁陈述。


<details>
  <summary>Details</summary>
Motivation: 为自动微分理论提供几何基础，通过微分几何视角解释AD，并为深度学习和科学计算开发结构保持的微分方法奠定基础。

Method: 使用jet bundles和Weil代数的几何框架，将反向模式AD建模为余切拉回操作，泰勒模式作为Weil代数中的求值过程。

Result: 推导了反向模式的函子恒等式、高阶导数的代数精确性、截断误差的显式边界，并证明张量化Weil代数能以线性成本计算所有混合导数。

Conclusion: 该框架通过微分几何视角解释AD理论，为开发结构保持的微分方法提供了基础，避免了嵌套JVP/VJP调度的组合爆炸问题。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [19] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitNet Distillation (BitDistill) 是一种轻量级方法，可将现成的全精度大语言模型微调为1.58位精度（三元权重{-1, 0, 1}），在特定下游任务上实现与全精度模型相当的性能，同时节省10倍内存并加速2.65倍CPU推理。


<details>
  <summary>Details</summary>
Motivation: 解决全精度大语言模型在特定任务部署时的高计算成本和内存需求问题，通过极低精度量化实现高效推理。

Method: 采用三个关键技术：BitNet中的SubLN模块、基于MiniLM的多头注意力蒸馏，以及作为关键预热步骤的持续预训练，以缓解全精度与1.58位模型在特定任务上的性能差距。

Result: 实验结果显示，BitDistill在不同模型规模下均能达到与全精度对应模型相当的性能，同时实现高达10倍的内存节省和2.65倍的CPU推理加速。

Conclusion: BitDistill提供了一种高效的方法，能够在保持任务特定性能的同时，显著降低大语言模型的部署成本和推理延迟。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [20] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 本文提出了交互一致性指数（IC-index）来评估药物-靶点亲和力预测中交互方向预测的性能，该指标补充了现有的预测性能评估方法，特别关注交互效应的方向预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的药物-靶点亲和力预测方法主要关注亲和力值的预测准确性，但忽略了交互效应的方向预测。正确捕捉交互方向对于药物分配决策至关重要，因为交互的存在意味着不同的药物-靶点分配会产生不同的总亲和力。

Method: 提出了交互一致性指数（IC-index）来评估固定预测器和机器学习算法在交互方向预测方面的性能。通过理论分析证明了IC-index在无法捕捉交互的预测器上的不变性，并展示了学习算法在药物和靶点身份置换等变性时无法捕捉交互效应。

Result: 在多个生物医学交互数据集上的综合实证评估表明，不同类型的亲和力强度预测方法在IC-index上的表现各不相同，该指标有效地补充了现有的预测性能评估方法。

Conclusion: IC-index是一个有价值的补充评估指标，能够专门评估交互方向预测的性能，对于药物-靶点分配等决策任务具有重要意义。通过引入适当的侧信息可以克服置换等变性带来的限制，提高交互效应的捕捉能力。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [21] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 本文提出了一个基于先验预测矩匹配的严格方法来确定概率张量模型中的秩可识别性，将矩匹配条件转化为对数线性方程组，并建立了秩可识别性与该系统可解性之间的等价关系。


<details>
  <summary>Details</summary>
Motivation: 张量分解中潜在维度（秩）的选择通常依赖启发式方法，缺乏严格的数学基础，因此需要一种严谨的方法来确定秩的可识别性。

Method: 通过将矩匹配条件转化为关于边际矩、先验超参数和秩的对数线性方程组，建立秩可识别性与系统可解性之间的等价关系，并应用于四种基础张量模型。

Result: 证明PARAFAC/CP模型、张量链模型和张量环模型的线性结构产生可解系统，其秩可识别；而Tucker模型的对称拓扑导致系统欠定，秩不可识别。对于可识别模型，推导了仅基于观测数据矩的显式闭式秩估计器。

Conclusion: 该方法为张量分解中的秩选择提供了严格的数学基础，通过矩匹配方法成功识别了多种张量模型的秩可识别性，并开发了实用的秩估计器。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [22] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 本文提出了条件Clifford可操纵核，解决了CSCNNs核基不完备的问题，通过输入特征场计算等变表示来增强核，在多个PDE预测任务中表现出更好的表达能力。


<details>
  <summary>Details</summary>
Motivation: Clifford-Steerable CNNs的核基不完备，限制了模型的表达能力，需要解决这一问题以提高模型性能。

Method: 提出条件Clifford可操纵核，通过输入特征场计算等变表示来增强核，推导等变约束并通过隐式参数化高效求解。

Result: 在流体动力学和相对论电动力学等多个PDE预测任务中，该方法持续优于基线方法，证明了改进的表达能力。

Conclusion: 条件Clifford可操纵核有效解决了CSCNNs核基不完备的问题，显著提升了模型在PDE预测任务中的性能。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [23] [Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling](https://arxiv.org/abs/2510.14717)
*Alexandru Meterez,Depen Morwani,Jingfeng Wu,Costin-Andrei Oncescu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: Seesaw是一种批处理大小调度框架，通过在学习率衰减时同时增加批处理大小来加速大语言模型预训练，在保持损失动态的同时减少串行步骤。


<details>
  <summary>Details</summary>
Motivation: 批处理大小增加（batch ramp）是加速大语言模型预训练的有前景策略，但对于Adam等自适应优化器的最优策略尚不明确，通常需要启发式调优。

Method: 提出Seesaw框架：当标准调度器将学习率减半时，Seesaw将学习率乘以1/√2并加倍批处理大小，从而保持损失动态同时减少串行步骤。

Result: 在150M/300M/600M参数模型上，Seesaw在相同FLOPs下匹配余弦衰减性能，同时将挂钟时间减少约36%，接近理论极限。

Conclusion: Seesaw提供了一种原则性的批处理大小调度方法，在保持训练质量的同时显著加速大语言模型预训练。

Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a
promising strategy to accelerate large language model pretraining. While for
SGD, doubling the batch size can be equivalent to halving the learning rate,
the optimal strategy for adaptive optimizers like Adam is less clear. As a
result, any batch-ramp scheduling, if used at all, is typically tuned
heuristically. This work develops a principled framework for batch-size
scheduling and introduces Seesaw: whenever a standard scheduler would halve the
learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the
batch size, preserving loss dynamics while reducing serial steps.
Theoretically, we provide, to our knowledge, the first finite-sample proof of
equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy
linear regression, and we extend this equivalence to normalized SGD, a
tractable proxy for Adam, under a variance-dominated regime observed in
practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla
scale using a constant (critical) batch size, Seesaw matches cosine decay at
equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the
theoretical limit implied by our analysis.

</details>


### [24] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出了一种新算法，用于在线性非高斯有向无环模型中估计具有潜在混杂变量的因果DAG，允许潜在变量之间、观测变量之间以及两者之间存在因果结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设潜在混杂变量相互独立，或者无法正确处理观测变量间存在因果关系的模型。

Method: 利用观测数据的高阶累积量来识别因果结构。

Result: 广泛的模拟和真实世界数据实验证明了所提算法的有效性和实用性。

Conclusion: 该方法能够识别允许潜在变量和观测变量间存在复杂因果结构的LvLiNGAM模型。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [25] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出了一种名为Neural Effect Search的新方法，直接从数据中发现未知的因果效应，无需预先假设，通过预训练基础模型和稀疏自编码器将非结构化数据转化为有意义表示，并在生态学实验中首次实现了无监督的因果效应识别。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验依赖人工假设和昂贵分析，限制了大规模因果效应估计，可能导致关注流行但不完整的假设。

Method: 使用预训练基础模型将试验中的非结构化数据转化为表示，通过稀疏自编码器进行解释，并引入Neural Effect Search递归程序通过渐进分层解决多重检验和效应纠缠问题。

Result: 在半合成实验中验证了算法的鲁棒性，在实验生态学背景下首次成功实现了真实世界科学试验中的无监督因果效应识别。

Conclusion: 该方法能够直接从数据中发现未知的因果效应，突破了传统随机对照试验的局限性，为大规模因果效应估计提供了新途径。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [26] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文系统评估了不同损失函数对Transformer模型在股票排名预测中的影响，为基于排名的投资组合选择提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 量化交易策略依赖准确的股票排名来识别盈利投资。虽然Transformer模型在金融时间序列分析中表现良好，但不同训练损失函数如何影响其股票排名能力尚未充分研究。标准损失函数追求简单预测精度，无法直接教导模型学习股票收益的正确排序。

Method: 在S&P 500数据上系统评估了包括点对点、成对、列表式在内的多种先进损失函数，用于每日股票收益预测，以促进基于排名的投资组合选择。重点关注每种损失函数如何影响模型识别资产间盈利相对排序的能力。

Result: 研究提供了一个全面的基准，揭示了不同损失函数如何影响模型学习对投资组合选择至关重要的横截面和时间模式的能力。

Conclusion: 该研究为优化基于排名的交易策略提供了实用指导，填补了先进排名损失函数在金融收益排名应用中缺乏系统比较的空白。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [27] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 本文调查了传统数据分析技术的局限性，并探讨了基于张量的方法如何提供更强大的数据集表征替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集表征方法（统计、结构和基于模型的分析）往往无法提供创新和可解释性所需的深度理解和洞察。

Method: 通过示例说明张量方法如何揭示细微的数据特征，提供增强的可解释性和可操作智能。

Result: 张量方法能够展现传统方法难以发现的数据特性，为复杂数据集的理解提供新视角。

Conclusion: 提倡采用基于张量的表征方法，有望在理解复杂数据集方面取得重大进展，为智能、可解释的数据驱动发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [28] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个生产部署的多智能体标注框架，通过可配置的多智能体协作解决金融服务中的大规模标注积压问题，在摩根大通部署后消除了100万条话语积压，平均与人工标注者达到86%一致率，每年节省5000多小时人工标注工作。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务中数百万客户话语需要准确分类的标注积压关键挑战，将理论多智能体系统转化为实际企业部署。

Method: 结合专用智能体与结构化推理和基于评判者的共识机制，支持通过配置而非代码更改定义自定义标注类型，实现动态任务适应。

Result: 消除100万条话语积压，平均86%与人工标注者一致率，每年节省5000多小时人工工作；在内部意图分类数据集上比传统方法提升13.8% Top-1准确率、15.1% Top-5准确率和16.9% F1值。

Conclusion: 弥合了理论多智能体系统与实际企业部署之间的差距，为面临类似标注挑战的组织提供了蓝图。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [29] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 本章探讨联邦学习中的激励机制设计挑战，分析如何应用经济学和博弈论概念，以及区块链和深度强化学习等技术解决方案，提出涵盖集中式和去中心化架构的综合分类法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能实现数据隐私保护的协作模型训练，但存在参与困境问题，参与实体不愿贡献或可能搭便车，需要设计有效的激励机制来确保系统可持续性。

Method: 应用经济学和博弈论基础概念，结合区块链和深度强化学习等技术支持，建立涵盖集中式和去中心化架构的综合分类法。

Result: 提出了一套全面的激励机制分类框架，展示了这些概念在医疗保健、智能基础设施、车联网和区块链去中心化系统等新兴工业应用中的实际应用。

Conclusion: 精心设计的激励机制不是可选功能，而是联邦学习实际成功的关键组成部分，目前已有有前景的解决方案，但在构建真正可持续、公平和稳健的联邦学习生态系统方面仍面临重大挑战。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [30] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST是一个无需探针的框架，通过直接估计变换矩阵和频谱分析来理解Transformer层的功能，为现有可解释性方法提供补充视角。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型作为黑盒的问题，理解其内部机制，补充现有的可解释性方法。

Method: 使用Moore-Penrose伪逆估计每层的实际变换矩阵，并通过六个可解释指标进行频谱分析。

Result: 发现编码器-解码器模型的不同行为：解码器模型展示压缩-扩展循环，编码器模型保持一致的高秩处理；核分析显示层间功能关系模式，CKA相似性矩阵将层划分为三个阶段：特征提取、压缩和专业化。

Conclusion: CAST框架为理解Transformer模型内部机制提供了新的视角，揭示了不同模型架构的独特行为模式。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [31] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAREBench是一个早期事件预测基准测试，评估多模态输入下的预测准确性和时间稳定性，特别关注风险轨迹的稳定性以建立临床信任。


<details>
  <summary>Details</summary>
Motivation: 当前早期事件预测系统缺乏对风险分数稳定性的评估，且主要基于表格数据，无法测试多模态输入下的轨迹行为。

Method: 提出CAREBench基准，使用多模态输入（表格EHR、ECG波形、临床文本），引入基于局部Lipschitz常数的稳定性指标来量化短期风险变异性。

Result: 现有方法（特别是LLMs）在同时优化准确性和稳定性方面表现不佳，在高精度操作点召回率显著较差。

Conclusion: 需要开发能够产生证据对齐、稳定轨迹的模型，以在连续监测环境中赢得临床医生的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [32] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 该研究提出了一种结合谱残差方法和储层计算的SR-RC模型，用于提升时间序列异常检测性能，同时保持学习效率，适合边缘AI部署。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算在资源受限的边缘设备上需要过大的储层才能达到足够的异常检测性能，而注意力机制虽然能提高精度但会损害学习效率。

Method: 提出SR-RC模型，将无学习的自底向上注意力机制——谱残差方法与储层计算相结合。

Result: SR-RC在基准任务和真实世界时间序列数据集上优于传统储层计算和基于谱残差特征提取的逻辑回归模型。

Conclusion: 由于谱残差方法与储层计算都适合硬件实现，SR-RC为在边缘AI中部署时间序列异常检测提供了实用方向。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [33] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: LLM-ERM是一个提出-验证框架，使用LLM引导的程序搜索替代穷举枚举，在少量样本下有效学习短程序任务，而基于梯度的训练方法需要指数级更多样本。


<details>
  <summary>Details</summary>
Motivation: 解决程序学习中样本效率和计算可行性之间的矛盾：传统穷举搜索计算成本高，而梯度训练在某些短程序家族上需要指数级样本。

Method: 使用预训练推理增强的LLM生成k个候选程序，在保留数据上进行编译和验证，选择最佳验证假设，无需反馈、自适应或梯度。

Result: LLM-ERM仅用200个样本就能解决奇偶变体、模式匹配和素数测试等任务，而SGD训练的transformer即使有100,000个样本也会过拟合。

Conclusion: 语言引导的程序合成恢复了有限类ERM的统计效率，同时保持计算可行性，为学习超出梯度训练范围的简洁假设提供了实用途径。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [34] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 该论文首次系统分析了基于LLM的提示优化中的投毒风险，发现基于反馈的攻击比注入查询攻击更有效，提出了无需访问奖励模型的虚假奖励攻击，并开发了轻量级高亮防御方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统在日常AI应用中的广泛应用，提示优化成为关键环节，但该优化阶段的安全性尚未得到充分研究。论文旨在填补这一空白，系统分析提示优化中的投毒风险。

Method: 使用HarmBench评估系统对操纵反馈的脆弱性，引入无需访问奖励模型的虚假奖励攻击方法，并提出轻量级高亮防御机制来减轻攻击影响。

Result: 研究发现基于反馈的攻击比注入查询攻击更有效，可将攻击成功率提高ΔASR=0.48；虚假奖励攻击显著增加系统脆弱性；高亮防御可将虚假奖励攻击的ΔASR从0.23降至0.07，且不影响系统效用。

Conclusion: 提示优化管道应被视为一级攻击面，需要加强对反馈通道和优化框架的安全保护措施。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [35] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 论文揭示了多模态学习中的模态不平衡问题不仅发生在表示学习阶段，还显著体现在决策层，导致主导模态（如音频）压制较弱模态，并提出需要在决策层引入自适应权重分配机制。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在整合不同模态信息时存在模态不平衡问题，主导模态在联合优化过程中会压制较弱模态，这种不平衡不仅发生在表示学习阶段，还显著体现在决策层。

Method: 通过在音频-视觉数据集（CREMAD和Kinetic-Sounds）上进行实验，分析模型在预训练和平衡优化后仍存在的系统性模态偏差，并探究这种偏差的来源。

Result: 实验表明即使经过广泛预训练和平衡优化，模型仍对某些模态（如音频）表现出系统性偏差，这种偏差源于特征空间和决策权重分布的内在差异，而非仅由优化动态引起。

Conclusion: 在融合阶段聚合未校准的模态输出会导致决策层权重偏差，阻碍较弱模态的有效贡献。未来多模态系统应更关注在决策层纳入自适应权重分配机制，根据各模态能力实现相对平衡。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [36] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为MergeMoE的MoE模型压缩方法，通过数学优化构建压缩矩阵，在相同压缩比下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: MoE技术在扩展模型规模方面很有前景，但MoE模型的内存开销很大，因此需要研究压缩方法。专家合并是一种新兴的MoE压缩技术，但需要更好的理论分析和优化方法。

Method: 从专家输出合并的角度重新解释专家合并过程，将其视为在正向计算中插入额外矩阵，从而建立优化公式。基于此分析，提出MergeMoE方法，利用数学优化构建压缩矩阵。

Result: 在多个MoE模型上评估MergeMoE，结果显示该方法在相同压缩比下始终优于基线方法。

Conclusion: 通过理论分析和数学优化，MergeMoE提供了一种有效的MoE模型压缩方法，在保持性能的同时显著减少内存开销。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [37] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: 提出了Guess2Graph框架，使用专家猜测来指导统计测试的顺序而非替代测试，保持统计一致性的同时提升性能。开发了PC-Guess和gPC-Guess两个变体，理论上都能在专家错误时保持正确性，gPC-Guess在专家表现优于随机时能在有限样本中超越非增强版本。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法在样本有限时表现不佳，而将专家知识作为约束的方法需要完美预测或不确定性估计，在实际应用中不可靠。

Method: 提出Guess2Graph框架，使用专家猜测指导统计测试顺序而非替代测试。开发PC-Guess（增强PC算法）和gPC-Guess（学习增强变体，更好利用高质量专家输入）两个实例。

Result: 理论上两种方法都能在专家错误时保持正确性，gPC-Guess在专家表现优于随机时能在有限样本中超越非增强版本。实证显示两者随专家准确性单调改进，gPC-Guess获得显著更强的增益。

Conclusion: Guess2Graph框架通过将专家知识整合到统计测试顺序中，在保持理论保证的同时显著提升了因果发现算法的性能，特别是在样本有限的情况下。

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [38] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出可逆学习框架，通过状态转移可逆性度量和选择性回滚操作，解决基于价值的强化学习中的价值高估和环境部分不可逆性问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于价值的强化学习智能体对价值高估的脆弱性以及在部分不可逆环境中的不稳定性问题。

Method: 框架包含两个核心机制：经验推导的状态转移可逆性度量Phi(s,a)，以及选择性状态回滚操作。Phi度量在固定时间窗口内返回先前状态的可能性，用于动态调整时序差分更新中的惩罚项。当动作的预期回报显著低于瞬时估计值并违反阈值时，系统会执行回滚操作。

Result: 在CliffWalking v0环境中，灾难性坠落减少超过99.8%，平均回合回报提高55%。在Taxi v3环境中，非法动作抑制率≥99.9%，累积奖励提高65.7%，两个环境的奖励方差均显著降低。

Conclusion: 结合可逆性感知评估和针对性回滚的方法提高了安全性、性能和稳定性，回滚机制是实现这些安全性和性能提升的关键组件，标志着向安全可靠的顺序决策迈出了坚实一步。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [39] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出了选择性表示空间（SRS）模块，通过可学习的选择性分块和动态重组技术，自适应地选择和重排时间序列中的信息块，以增强基于分块的时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列分块技术将时间序列划分为相邻的块，导致固定的表示空间，从而产生表达力不足的表示。本文旨在构建一个选择性表示空间，灵活地包含最具信息量的块用于预测。

Method: 提出SRS模块，包含可学习的选择性分块和动态重组技术，自适应地从上下文时间序列中选择和重排块。构建了SRSNet模型，由SRS模块和MLP头组成。

Result: SRSNet在多个领域的真实世界数据集上实现了最先进的性能。SRS模块作为即插即用模块，也能增强现有基于分块模型的性能。

Conclusion: SRS模块通过构建选择性表示空间，有效提升了时间序列预测的性能，既可作为独立模型使用，也可作为增强现有模型的插件。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [40] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: 本文提出MX+格式，一种基于块浮点数(BFP)的改进格式，通过重新利用异常值的指数字段作为扩展尾数来提高精度，在4位精度下显著提升大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有低精度数据格式要么需要侵入式软件修改，要么难以在硬件厂商间广泛采用。超低位BFP变体由于块内异常值问题难以提供合理的语言模型性能。

Method: 提出MX+格式，基于关键洞察：异常值不需要使用其指数字段，可以重新利用该字段作为扩展尾数来提高异常值元素的精度。这是一种成本效益高且非侵入式的扩展。

Result: MX+相比4位MX格式(MXFP4)实现了显著更高的模型性能，存储开销和速度下降可忽略不计。

Conclusion: MX+为高效LLM推理提供了比MXFP4或MXFP6更具吸引力的替代方案。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [41] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: RedOUT是一个无监督框架，通过将结构熵集成到图分类的测试时OOD检测中，解决训练和测试数据分布差异导致的预测不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法虽然利用数据为中心的技术提取有效表示，但其性能仍受到导致语义偏移的结构冗余的影响。

Method: 提出RedOUT框架，引入冗余感知图信息瓶颈（ReGIB），将目标分解为基本信息和无关冗余，通过最小化结构熵减少解耦的冗余，并提出理论上的上下界进行优化。

Result: 在真实世界数据集上的广泛实验表明，RedOUT在OOD检测方面表现优异，平均提升6.7%，在ClinTox/LIPO数据集对上显著超越最佳竞争对手17.3%。

Conclusion: RedOUT通过结构熵最小化和冗余感知图信息瓶颈，有效提升了图分类中OOD检测的性能。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [42] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出了一种名为Conformal Labeling的新方法，通过控制假发现率来识别AI预测可以可靠信任的实例，为AI标签提供理论质量保证。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据标注成本高的问题，现有AI模型标注方法缺乏理论保证，导致AI标注子集的标签错误率过高。

Method: 构建每个测试实例的conformal p值，通过比较AI模型的预测置信度与校准实例的误标置信度，选择p值低于数据依赖阈值的测试实例。

Result: 实验表明该方法在各种任务（图像和文本标注、LLM QA）中实现了严格的FDR控制和高功率。

Conclusion: Conformal Labeling方法能够有效控制FDR在名义水平以下，确保AI分配标签的正确性，为AI标注提供了可靠的理论保证。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [43] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: FAL是一种高效的Transformer架构，通过重定向第一层MHA输出到后续层的MLP输入，消除了每块的MHA-MLP连接，从而减少了多GPU训练时间达44%，提高单GPU吞吐量1.18倍，并实现了更好的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer设计存在显著的通信开销，特别是在张量并行中，每个块的MHA-MLP连接需要all-reduce通信。研究表明MHA-MLP连接可以绕过以提高效率，而第一层的注意力输出可以作为替代信号。

Method: 提出FAL架构，将第一层MHA输出重定向到后续层的MLP输入，消除每块的MHA-MLP连接。还提出FAL+，将归一化的第一注意力输出添加到后续层的MHA输出中，以增强MLP输入质量。

Result: FAL减少多GPU训练时间达44%，提高单GPU吞吐量1.18倍，相比基线GPT实现了更好的困惑度。FAL+在不增加训练时间的情况下实现了更低的困惑度。

Conclusion: FAL和FAL+通过重新设计Transformer架构中的连接方式，有效减少了通信开销，提高了训练效率和模型质量。

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [44] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 提出了一种基于黎曼几何的无监督域自适应方法，通过Siegel嵌入将一阶和二阶矩表示为对称正定矩阵，使用SPD流形上的自然几何距离进行域对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用临时相似性度量在嵌入空间中对齐源域和目标域的低阶统计矩，缺乏几何原理指导。

Method: 采用黎曼距离进行分布对齐，通过Siegel嵌入将一阶和二阶矩表示为单个对称正定矩阵，在SPD矩阵共享流形上使用自然几何距离同时适应两个矩。

Result: 在图像去噪和图像分类基准测试中验证了方法的有效性，代码已公开。

Conclusion: 该方法通过保留源域和目标域的均值和协方差结构，为跨域比较提供了更忠实的度量，并将黎曼流形距离与目标域误差界联系起来。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [45] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 本文提出一种无需额外训练的迭代采样算法，仅使用基础模型自身似然性就能在推理时激发与RL后训练相当的推理能力，并在多个任务上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在不进行额外训练的情况下，仅通过纯采样在推理时从基础模型中激发出与RL后训练相当的推理能力，避免RL训练带来的多样性下降问题。

Method: 受马尔可夫链蒙特卡洛技术启发，提出一种简单的迭代采样算法，利用基础模型自身的似然性进行采样。

Result: 在MATH500、HumanEval和GPQA等多个单次任务上，该方法显著提升推理能力，性能接近甚至超过RL后训练，同时避免了RL训练中多样性崩溃的问题。

Conclusion: 该方法无需训练、精选数据集或验证器，具有广泛适用性，表明基础模型在推理时通过纯采样就能获得强大的推理能力。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [46] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 本文提出了一种新型的随机奖励机（stochastic reward machines）及其学习算法，用于处理强化学习中奖励函数存在噪声的情况，相比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机学习算法假设奖励无噪声，但在实际应用中奖励往往存在噪声，这一限制需要被克服。

Method: 基于约束求解的算法，从强化学习智能体的探索中学习最小随机奖励机，可与现有奖励机强化学习算法轻松结合。

Result: 在两个案例研究中证明了算法的有效性，在存在噪声奖励函数的情况下优于现有方法和朴素方法。

Conclusion: 该算法在极限情况下保证收敛到最优策略，为处理噪声奖励的强化学习问题提供了有效解决方案。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了一种基于决策一致性的模型有效性验证方法DOTechnique，通过评估替代模型与高保真模型是否产生相同决策来确定有效性区域，无需预定义有效性框架。


<details>
  <summary>Details</summary>
Motivation: 模型有效性对决策过程至关重要，但传统方法依赖预定义有效性框架，这些框架可能不可用或不充分。

Method: 开发了决策导向技术(DOTechnique)，通过比较替代模型与高保真模型的决策一致性来确定有效性，结合领域约束和符号推理来缩小搜索空间。

Result: 以高速公路变道系统为例，展示了DOTechnique能够发现仿真模型的有效性区域。

Conclusion: 该技术有潜力通过决策者上下文支持模型有效性的发现。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [48] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态自动语音识别方法，通过结合视觉信息（特别是演示文稿幻灯片）来改进科学演示场景下的语音识别性能，在领域特定术语识别方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动语音识别系统主要依赖声学信息而忽略多模态上下文，但视觉信息在消除歧义和适应特定场景方面至关重要。特别是在科学演示场景中，演示文稿幻灯片包含重要的上下文信息。

Method: 首先创建多模态演示基准测试，包括领域特定术语的自动分析；然后探索用多模态信息增强语音模型的方法；通过数据增强方法解决缺乏配套幻灯片数据集的问题；最后使用增强数据集训练模型。

Result: 与基线模型相比，训练得到的模型在所有词汇上的词错误率相对降低了约34%，在领域特定术语上的词错误率相对降低了35%。

Conclusion: 通过整合演示文稿幻灯片等视觉信息，可以显著提升自动语音识别系统在科学演示场景下的性能，特别是在领域特定术语识别方面效果显著。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [49] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在零相关情境下会系统性地产生因果幻觉，错误地推断不存在因果关系，这支持了LLMs只是复制因果语言而非真正理解因果关系的假设。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否会在经典认知科学范式——列联判断任务中发展因果幻觉，特别是在医学等需要准确因果推理的领域中。

Method: 构建了1000个医学背景下的零相关情境数据集，在这些情境中可用信息不足以建立变量间的因果关系，然后提示LLMs评估潜在原因的有效性。

Result: 所有评估的模型都系统性地推断出无根据的因果关系，显示出对因果幻觉的强烈易感性。

Conclusion: 研究结果支持LLMs只是复制因果语言而非真正理解因果关系的假设，并对在需要准确因果推理的领域中使用语言模型提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [50] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出轻量级多模态架构，融合传感器数据和视觉图像预测文化遗产退化严重程度，在斯特拉斯堡大教堂数据上达到76.9%准确率，比标准多模态架构提升43%。


<details>
  <summary>Details</summary>
Motivation: 文化遗产遗址因气候变化加速退化，传统单模态监测方法无法捕捉环境压力与材料退化之间的复杂相互作用。

Method: 采用改进的PerceiverIO架构，包含简化编码器（64D潜在空间）和自适应Barlow Twins损失函数，鼓励模态互补性而非冗余。

Result: 在37个训练样本的小数据集上，模型准确率达到76.9%，比标准多模态架构提升43%，比单模态方法（传感器61.5%、图像46.2%）表现更好。

Conclusion: 架构简单性与对比正则化相结合，能够在数据稀缺的文化遗产监测环境中实现有效的多模态学习，为AI驱动的保护决策支持系统奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [51] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 本文系统探索了使用EEG、语音和文本多模态进行抑郁症检测的方法，比较了手工特征与预训练嵌入、不同神经编码器、单模态/双模态/三模态配置以及融合策略，发现三模态组合能提升检测性能，预训练嵌入优于手工特征。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是常见心理健康问题，现有自动检测方法存在范围有限、缺乏系统特征比较、评估协议不一致等问题，需要系统探索多模态特征表示和建模策略。

Method: 使用EEG、语音和文本三种模态，系统评估手工特征与预训练嵌入、不同神经编码器、单模态/双模态/三模态配置，分析融合策略并关注EEG的作用，采用一致的受试者独立分割确保稳健可复现的基准测试。

Result: 结果显示：(i) EEG、语音和文本多模态组合能增强多模态检测；(ii) 预训练嵌入优于手工特征；(iii) 精心设计的三模态模型达到最先进的性能。

Conclusion: 本研究为多模态抑郁症检测的未来研究奠定了基础，证明了多模态融合的有效性和预训练嵌入的优越性。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [52] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 对2019-2024年精准医学中AI实施的文献进行范围综述，识别数据质量、临床可靠性、工作流整合和治理方面的关键障碍与促进因素，提出基于生态系统的框架支持可信赖和可持续的实施。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准医学中日益重要，能够整合和解释多模态数据，但在临床环境中的实施仍然有限，需要系统分析实施障碍和促进因素。

Method: 采用范围综述方法，分析2019-2024年相关文献，通过基于生态系统的框架识别关键实施因素。

Result: 识别了数据质量、临床可靠性、工作流整合和治理四个主要维度的障碍与促进因素，强调了影响真实世界转化的相互依赖关系。

Conclusion: 提出了支持可信赖和可持续AI实施在精准医学中的未来方向，强调需要系统性方法来解决实施挑战。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [53] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 提出了Terrarium框架，用于在基于LLM的多智能体系统中进行细粒度的安全、隐私和安全性研究。该框架重新利用黑板设计创建模块化、可配置的多智能体协作测试平台。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统能够自动化繁琐的用户任务，但引入了新的风险，包括错位攻击、恶意方攻击、智能体被破坏或用户数据被盗等安全问题。

Method: 重新利用多智能体系统中的早期方法——黑板设计，创建模块化、可配置的多智能体协作测试平台。识别关键攻击向量，如错位、恶意智能体、通信被破坏和数据中毒。

Result: 实现了三个协作多智能体场景和四种代表性攻击，展示了框架的灵活性。提供了快速原型设计、评估和防御迭代的工具。

Conclusion: Terrarium框架旨在加速可信多智能体系统的进展，通过提供工具来快速原型设计、评估和防御迭代。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [54] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，为多智能体系统提供实时、无监督的步骤级错误检测和自我纠正，通过历史条件异常评分来防止错误级联传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协作解决问题方面表现出色，但对级联错误很脆弱——单个错误步骤可能会在智能体间传播并破坏整个轨迹。

Method: MASC采用两种互补设计：(1) 下一执行重构：从查询和交互历史预测下一步的嵌入以捕捉因果一致性；(2) 原型引导增强：学习正常步骤嵌入的原型先验，在稀疏上下文下稳定重构和异常评分。检测到异常步骤时，触发纠正智能体在信息流向下游前修订输出。

Result: 在Who&When基准测试中，MASC持续优于所有基线，将步骤级错误检测的AUC-ROC提高了8.47%；当集成到不同多智能体框架时，在各种架构上都能带来一致的端到端增益。

Conclusion: MASC的元认知监控和针对性纠正能够以最小开销缓解错误传播，证实了该框架的有效性和通用性。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [55] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: IP-Merging是一种无需调优的方法，通过识别多模态大语言模型（MLLM）和数学大语言模型中的推理相关参数，将其投影到MLLM子空间并合并，从而直接提升MLLM的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM的数学推理能力落后于纯文本LLM，但直接使用模型合并方法会因参数空间不匹配而导致性能下降。

Method: 提出IP-Merging方法：1）识别MLLM和数学LLM中的推理相关参数；2）将这些参数投影到MLLM子空间；3）在子空间中合并参数。

Result: 实验证明IP-Merging能有效提升MLLM的数学推理能力，且不损害其他能力。

Conclusion: IP-Merging提供了一种无需调优的方法，使MLLM能够直接从数学LLM吸收推理能力，解决了参数空间不对齐的问题。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [56] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 这篇论文概述了神经符号AI中的推理捷径问题，讨论了其成因、后果以及应对方法，旨在为研究人员提供统一的视角来理解和解决这一挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI模型在概念未直接监督时容易出现推理捷径问题，这会损害模型解释性、分布外性能及可靠性。目前相关研究较为分散，需要系统性的综述来帮助研究人员理解和应对这一问题。

Method: 通过提供推理捷径的直观介绍，讨论其成因和后果，回顾现有理论特征，详细阐述包括缓解和意识策略在内的应对方法，并分析各种方法的优缺点。

Result: 论文构建了关于推理捷径的统一视角，系统整理了相关理论和方法，为研究人员提供了理解这一问题的入门指南。

Conclusion: 通过以易于理解的形式重新表述高级材料，这篇综述旨在降低解决推理捷径问题的门槛，最终促进可靠神经符号AI和可信AI模型的发展。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [57] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本文对2021-2025年荷兰公共卫生机器学习研究中的算法偏见进行了系统文献综述，开发了RABAT评估工具，发现普遍存在公平性框架缺失等问题，并提出了ACAR四阶段公平性框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在公共卫生领域有巨大潜力，但如果不系统关注算法偏见，可能会无意中加剧现有的健康不平等。需要评估当前研究中对算法偏见的识别、讨论和报告情况。

Method: 开发了风险算法偏见评估工具(RABAT)，整合了Cochrane偏见风险、PROBAST和微软负责任AI检查表等成熟框架，应用于35篇同行评审研究进行系统分析。

Result: 分析显示普遍存在差距：虽然数据采样和缺失数据实践记录良好，但大多数研究忽略了明确的公平性框架、亚组分析以及对潜在危害的透明讨论。

Conclusion: 提出了ACAR四阶段公平性框架（意识、概念化、应用、报告），为公共卫生ML从业者提供可操作建议，确保算法创新促进而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [58] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一种基于主动推理和符号推理的新型人工智能伦理框架，通过最小化全局期望自由能量来形式化伦理行为，采用神经符号架构让智能体在不确定环境中评估行为后果。


<details>
  <summary>Details</summary>
Motivation: 传统以人为中心的AI伦理方法存在局限性，需要一种能够在不预设人类道德直觉的情况下，让智能体发展出情境敏感、自适应和关系性伦理行为的新框架。

Method: 提出神经符号架构，结合主动推理和符号推理，使智能体能够在动态多智能体环境中通过最小化全局期望自由能量来评估行为的伦理后果。

Result: 通过伦理资源分配的案例研究展示了NAEL能够动态平衡自我保存、认知学习和集体福利的能力。

Conclusion: NAEL框架能够克服现有伦理模型的限制，为人工智能系统提供非人类中心主义的伦理推理能力，使其在复杂环境中发展出适应性伦理行为。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [59] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文改进了COUP算法配置方法，使其在保持理论保证的同时，实际性能能够与无性能保证的启发式配置方法竞争。


<details>
  <summary>Details</summary>
Motivation: COUP是一种基于效用理论的算法配置方法，主要关注理论保证而忽略了实际性能。本文旨在弥补这一差距，使基于理论的效用算法配置在实际应用中具有竞争力。

Method: 提出了一系列对COUP的改进措施，在不降低理论保证的前提下提升其经验性能，并通过实验验证这些改进的效果。

Result: 改进后的COUP在经验性能上显著提升，能够与广泛使用的启发式配置方法竞争，同时保持理论性能保证。

Conclusion: 通过系统改进，成功将基于理论的效用算法配置方法提升到实用水平，为算法选择问题提供了更可靠的解决方案。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [60] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 本文提出PAVE方法，通过在知识感知子空间中净化任务向量来减少模型合并时的冗余冲突，提高合并模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法使用任务向量作为基本构建块，但由于任务向量中存在任务无关的冗余信息，导致合并模型性能显著下降。现有方法随机丢弃参数元素，缺乏知识感知且涉及随机性。

Method: 提出PAVE方法：1）从每个任务采样训练样本，通过微调模型获取线性层前的协方差矩阵；2）执行上下文导向的奇异值分解，突出与目标知识最相关的权重分量；3）在知识感知子空间中将微调模型权重分解为任务相关和冗余组件，通过剪枝冗余组件净化任务向量；4）引入谱秩分配策略，通过优化归一化激活剪枝误差实现公平剪枝。

Result: PAVE作为即插即用方案，可应用于各种基于任务向量的合并方法，在多种合并方法、任务和模型架构中均表现出有效性。

Conclusion: PAVE方法通过在知识感知子空间中净化任务向量，有效解决了模型合并中的冗余冲突问题，显著提升了合并模型的性能。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [61] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 论文提出了SimKO方法来解决强化学习验证奖励（RLVR）中的过度集中问题，该方法通过不对称地调整概率分布来鼓励探索，提高pass@K性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在系统性偏向利用而非探索的问题，表现为pass@1性能提升但pass@K（K>1）性能下降。需要理解并解决这种训练动态中的概率集中效应。

Method: 提出Simple Pass@K Optimization (SimKO)方法：对于验证正确的响应，提升前K个候选的概率；对于验证错误的响应，对top-1候选施加更强的惩罚。该方法特别适用于高熵token。

Result: 在多个数学和逻辑推理基准测试中，SimKO在广泛的K值范围内持续产生更高的pass@K性能，有效改善了RLVR的探索能力。

Conclusion: SimKO通过缓解概率过度集中问题，为改进RLVR的探索提供了一种简单有效的方法，能够在不牺牲pass@1性能的同时显著提升pass@K性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [62] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 该论文将康德的《纯粹理性批判》重新解释为反馈稳定性理论，提出复合不稳定性指数（H-Risk）来量化推理系统的过自信错误，并在线性高斯模拟和大型语言模型中验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 将康德的理性自我限制理论与反馈控制理论相结合，为诊断和减少推理系统中的过自信问题提供原则性框架。

Method: 提出复合不稳定性指数H-Risk（结合谱边界、条件数、时间敏感性和创新放大），在线性高斯模拟和大型语言模型中进行实验验证。

Result: 在线性高斯模拟中，更高的H-Risk预测过自信错误；在LLMs中，脆弱的内部动态与误校准和幻觉相关，批判式提示对校准和幻觉有混合效果。

Conclusion: 该研究在康德自我限制与反馈控制之间建立了结构性桥梁，为诊断和选择性减少推理系统中的过自信提供了原则性视角。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>

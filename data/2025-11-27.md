<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [stat.ML](#stat.ML) [Total: 2]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.LG](#cs.LG) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文研究使用大语言模型自动重构层次结构以优化双曲嵌入质量，通过提示方法将现有层次结构转化为符合双曲嵌入需求的形式，实验证明重构后的层次结构在多个标准指标上都能产生更高质量的双曲嵌入。


<details>
  <summary>Details</summary>
Motivation: 双曲几何能有效嵌入层次数据，但嵌入质量与输入层次结构密切相关。研究发现最优双曲嵌入需要高分支因子和单继承结构。为帮助知识工程师重新组织层次知识，本文探索大语言模型是否具备自动重构层次结构以满足这些标准的能力。

Method: 提出基于提示的方法，使用大语言模型在已知双曲嵌入需求指导下转换现有层次结构。在16个不同层次结构上进行实验验证。

Result: 实验结果显示，经过大语言模型重构的层次结构在多个标准嵌入质量指标上一致产生更高质量的双曲嵌入。

Conclusion: 大语言模型能够有效重构层次结构以优化双曲嵌入质量，同时提供可解释的重组过程，为知识工程师提供合理性解释。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [2] [Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning](https://arxiv.org/abs/2511.20694)
*Kevin Lee,Russell Spiewak,James Walsh*

Main category: cs.AI

TL;DR: 该论文提出了一个名为"Reasoning With a Star"的太阳物理学数据集和基准测试方法，用于评估大语言模型在科学推理中的表现，发现通过系统工程原则分解工作流程在需要演绎推理的问题上优于直接提示。


<details>
  <summary>Details</summary>
Motivation: 解决太阳物理学中大语言模型科学推理的挑战，包括整合物理假设、保持单位一致性以及通过协调方法提供清晰的科学格式。

Method: 构建基于NASA和UCAR Living With a Star暑期学校问题集的数据集，采用程序化评分器检查预测结果，并比较单次提示和四种多智能体模式的性能。

Result: 发现通过系统工程原则分解工作流程在需要演绎推理而非纯归纳回忆的问题上表现优于直接提示方法。

Conclusion: 提出的数据集和基准测试方法为评估大语言模型在太阳物理学科学推理能力提供了有效工具，多智能体分解策略在复杂推理任务中具有优势。

Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.

</details>


### [3] [Guaranteed Optimal Compositional Explanations for Neurons](https://arxiv.org/abs/2511.20934)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 本文提出了第一个计算保证最优组合解释的框架，通过分解空间对齐因素、设计启发式估计方法，开发出能在可行时间内找到最优解释的算法。在计算机视觉和CNN中，发现10-40%的beam search解释是次优的。


<details>
  <summary>Details</summary>
Motivation: 现有组合解释方法使用beam search来计算神经元激活与概念之间的空间对齐，但无法提供最优性保证，不清楚当前解释与真正最优解的接近程度。

Method: 提出包含三个组件的框架：(i)识别影响空间对齐因素的分解方法；(ii)在搜索任何阶段估计对齐的启发式方法；(iii)第一个能在可行时间内计算最优组合解释的算法。

Result: 在计算机视觉和CNN的流行设置中，当涉及重叠概念时，10-40%通过beam search获得的解释是次优的。基于所提分解和启发式的beam search变体在运行时匹配或优于先前方法。

Conclusion: 该框架首次实现了保证最优的组合解释计算，揭示了现有方法的局限性，并提供了更高效的替代方案。

Abstract: While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.

</details>


### [4] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang,Wenlong Huang,Yu Zhou,Hang Yin,Tianwei Bao,Jianwen Lyu,Weiyu Liu,Ruohan Zhang,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: ENACT是一个评估视觉语言模型是否展现具身认知的基准，通过视觉问答形式测试模型从自我中心交互中进行世界建模的能力，包含前向世界建模和逆向世界建模两个任务。


<details>
  <summary>Details</summary>
Motivation: 研究现代视觉语言模型虽然主要以非具身方式训练，但能否表现出具身认知的特征，即智能是否源于感觉运动交互而非被动观察。

Method: 将具身认知评估构建为部分可观测马尔可夫决策过程，使用场景图变化作为动作，设计了两个序列重排序任务：给定动作重排观察序列（前向世界建模）和给定观察重排动作序列（逆向世界建模）。

Result: 实验发现前沿视觉语言模型与人类之间存在性能差距，且随着交互视野延长差距扩大。模型在逆向任务上表现优于前向任务，并表现出人类中心偏见，如偏好右手动作、当相机参数或视角偏离人类视觉时性能下降。

Conclusion: ENACT基准揭示了视觉语言模型在具身认知能力方面的局限性，为评估和改进模型的具身智能提供了重要工具。

Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.

</details>


### [5] [Causality Without Causal Models](https://arxiv.org/abs/2511.21260)
*Joseph Y. Halpern,Rafael Pass*

Main category: cs.AI

TL;DR: 本文对Halpern和Pearl的因果定义进行了抽象化，提取其关键特征，使其能够应用于任何定义了反事实的模型。


<details>
  <summary>Details</summary>
Motivation: Halpern-Pearl的因果定义仅限于因果模型，无法处理包含析取、否定、信念和嵌套反事实的复杂情况，需要更通用的定义框架。

Method: 通过抽象化Halpern-Pearl因果定义的关键特征，构建一个可以在任何反事实模型上应用的通用因果定义框架。

Result: 新定义框架不仅能应用于更广泛的模型（包括允许回溯的模型），还能处理复杂逻辑公式，并能扩展到解释的定义。

Conclusion: 抽象化方法扩展了因果定义的适用范围，提供了对因果定义更深入的理解，并为构建通用解释框架奠定了基础。

Abstract: Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.

</details>


### [6] [New Hybrid Heuristics for Pseudo-Boolean Propagation](https://arxiv.org/abs/2511.21417)
*Mia Müßig,Jan Johannsen*

Main category: cs.AI

TL;DR: 本文介绍了在伪布尔求解中混合单元传播策略的新启发式方法，能够显著优于RoundingSAT求解器中的当前方法


<details>
  <summary>Details</summary>
Motivation: 当前伪布尔求解中最成功的单元传播策略是结合观察文字方案和计数方法的混合模式，但需要改进其决策启发式

Method: 为混合决策引入新的启发式方法，结合观察文字方案和计数方法

Result: 新启发式方法能够显著优于RoundingSAT求解器中的当前方法

Conclusion: 新提出的启发式方法在伪布尔求解的混合单元传播策略中表现优异

Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.

</details>


### [7] [Pessimistic Verification for Open Ended Math Questions](https://arxiv.org/abs/2511.21522)
*Yanxing Huang,Zihan Tang,Zejin Lin,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 提出悲观验证方法，通过并行构建多个验证流程来检测数学证明中的错误，显著提升验证性能且计算资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 现有验证性能受限于错误检测能力，需要更有效的验证方法来提高数学问题验证的可靠性。

Method: 设计悲观验证变体，为同一证明构建多个并行验证流程，只要任一验证报告错误就判定证明不正确。

Result: 该方法在多个数学验证基准测试中显著提升性能，token效率甚至超过扩展长链思维，且多数假阴性源于原始数据集标注错误。

Conclusion: 悲观验证能有效提升语言模型输出的可靠性和性能，对实现长视野数学任务至关重要，有助于增强语言模型的数学能力。

Abstract: The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method's performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.

</details>


### [8] [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](https://arxiv.org/abs/2511.21570)
*Maria Perez-Ortiz*

Main category: cs.AI

TL;DR: 本文提出了"负责任计算前瞻"概念，探讨人工智能在负责任前瞻中的作用，建立该领域的基本原则，并展示AI驱动的前瞻工具。


<details>
  <summary>Details</summary>
Motivation: 在技术快速发展和全球挑战复杂的时代，负责任前瞻已成为政策制定者应对未来不确定性和塑造未来的重要框架。

Method: 通过建立负责任计算前瞻的基本原则，结合人工智能、模拟和情景分析，开发AI驱动的前瞻工具来支持政策制定。

Result: AI增强了政策制定者应对不确定性、评估风险和制定可持续、有韧性未来战略的能力，但需要与人类判断相结合。

Conclusion: AI应作为支持性工具融入前瞻实践，补充而非替代政策制定者的判断，以塑造有韧性和道德健全的未来。

Abstract: In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [9] [Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference](https://arxiv.org/abs/2511.21223)
*Jasraj Singh,Shelvia Wongso,Jeremie Houssineau,Badr-Eddine Chérief-Abdellatif*

Main category: stat.ML

TL;DR: 本文开发了一种原则性的可能性变分推理方法，并将其应用于指数族函数，揭示了可能性理论的独特数学结构。


<details>
  <summary>Details</summary>
Motivation: 变分推理是现代贝叶斯学习的基石，但其在高维积分下的公式依赖期望和散度，通常需要近似技术。可能性理论作为不精确概率框架，可以直接建模认知不确定性，但将其适应到变分推理需要重新思考熵和散度等核心概念。

Method: 开发了一种原则性的可能性变分推理方法，并将其应用于指数族函数，与概率对应物进行比较。

Result: 提出了可能性变分推理的数学框架，揭示了可能性理论与概率理论在变分推理中的结构差异。

Conclusion: 这项工作为可能性理论在变分推理中的应用提供了理论基础，展示了可能性框架在建模认知不确定性方面的独特优势。

Abstract: Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.

</details>


### [10] [Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities (II)](https://arxiv.org/abs/2511.21526)
*Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 本文解决了当社区数量K≥√n时，随机块模型(SBM)中社区恢复的计算障碍问题，证明了在Chin等人提出的阈值之上可以实现多项式时间社区恢复。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明当K≥√n时，社区恢复在稀疏区域可以在Kesten-Stigum阈值以下实现，但关于在大多数密度区域中是否能在新提出的阈值之上实现恢复的问题仍然开放。

Method: 通过构造满足特定结构性质的motif族，并证明通过计数这些motif可以在提议的阈值之上实现社区恢复。

Result: 证明了在K≥√n的SBM中，社区恢复在提议的阈值之上是可能的，从而完成了该设置下计算障碍的完整图像。

Conclusion: 在中等稀疏区域，最优算法与谱方法有根本性不同，本文结果完善了多社区设置下社区恢复计算障碍的理论图景。

Abstract: A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.
  When $K \geq \sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \geq \sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.
  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\ 1- Constructing a family of motifs satisfying specific structural properties; and\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \geq \sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [11] [WiRainbow: Single-Antenna Direction-Aware Wi-Fi Sensing via Dispersion Effect](https://arxiv.org/abs/2511.20671)
*Zhaoxin Chang,Shuguang Xiao,Fusang Zhang,Xujun Ma,Badii Jouaber,Qingfeng Zhang,Daqing Zhang*

Main category: eess.SP

TL;DR: WiRainbow是一种利用频率扫描天线实现单天线Wi-Fi方向感知的新方法，通过扩展天线视场和开发信号处理框架，在复杂环境中实现准确、鲁棒且经济的方向感知。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi方向估计方法通常依赖昂贵复杂的天线阵列，难以在实际场景中部署。需要一种低成本、易部署的单天线方向感知解决方案。

Method: 提出基于耦合谐振器的天线架构扩展传统频率扫描天线的窄视场，开发基于感知信噪比的信号处理框架，在丰富多径环境中可靠估计目标方向。

Result: 原型系统评估显示WiRainbow能够实现准确、鲁棒的方向感知，适用于多种Wi-Fi传感应用场景。

Conclusion: WiRainbow通过创新的天线设计和信号处理方法，成功实现了低成本、高精度的单天线Wi-Fi方向感知，为实际部署提供了可行方案。

Abstract: Recently, Wi-Fi signals have emerged as a powerful tool for contactless sensing. During the sensing process, obtaining target direction information can provide valuable contextual insights for various applications. Existing direction estimation methods typically rely on antenna arrays, which are costly and complex to deploy in real-world scenarios. In this paper, we present WiRainbow, a novel approach that enables single-antenna-based direction awareness for Wi-Fi sensing by leveraging the dispersion effect of frequency-scanning antennas (FSAs), which can naturally steer Wi-Fi subcarriers toward distinct angles during signal transmission. To address key challenges in antenna design and signal processing, we propose a coupled-resonator-based antenna architecture that significantly expands the narrow Field-of-View inherent in conventional FSAs, improving sensing coverage. Additionally, we develop a sensing signal-to-noise-ratio-based signal processing framework that reliably estimates target direction in multipath-rich environments. We prototype WiRainbow and evaluate its performance through benchmark experiments and real-world case studies, demonstrating its ability to achieve accurate, robust, and cost-effective direction awareness for diverse Wi-Fi sensing applications.

</details>


### [12] [2D Sparse Array Design via Reweighted L1 Second Order Cone Programming for 3D Ultrasound Imaging](https://arxiv.org/abs/2511.21133)
*Xi Zhang,Miguel Bernal,Wei-Ning Lee*

Main category: eess.SP

TL;DR: 提出了一种基于二阶锥规划和重加权L1技术的稀疏阵列设计方法，用于减少3D超声成像中的通道数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统全寻址二维阵列需要数千个独立通道，成本高昂。现有随机优化方法结果不稳定，需要更可靠的稀疏阵列设计方法。

Method: 将稀疏阵列合成问题建模为二阶锥规划，并实现重加权L1技术来顺序优化SOCP，设计了具有准平坦旁瓣的二维稀疏阵列。

Result: 设计的Q-Flats阵列具有252个激活元素，旁瓣电平不超过-21.26 dB。与密集阵列、费马螺旋阵列和锥形螺旋阵列相比，Q-Flats在分辨率上优于螺旋阵列约3%，对比度略差。

Conclusion: 重加权L1 SOCP方法是一种有前景且灵活的方法，可在分辨率、对比度和激活元素数量之间寻求平衡。

Abstract: Two-dimensional (2D) fully-addressed arrays can conveniently realize three-dimensional (3D) ultrasound imaging while fully controlled such arrays usually demands thousands of independent channels, which is costly. Sparse array technique using stochastic optimization methods is one of promising techniques to reduce channel counts while due to the stochastic nature of these methods, the optimized results are usually unstable. In this work, we introduce a sparse array design approach that formulates the synthesis problem of sparse arrays as second-order cone programming (SOCP) and a re-weighted L1 technique is implemented to sequentially optimize the SOCP. Based on this method, an on-grid quasi-flatten side-lobe (Q-Flats) 2D sparse array with side-lobe level (SLL) no more than -21.26 dB and 252 activated elements is designed, which aims to achieve as high contrast performance as possible under the limits of resolution and maximum number of independent channels (i.e., 256). The imaging performance of the Q-Flats array was compared with those of a corresponding dense array (Dense), a Fermat spiral array (Spiral) and a spatially 50%-Tukey tapered spiral array (Spiral-Taper) using Field II simulations in a multi-angle steered diverging wave transmission scheme. It was demonstrated that the Dense achieved the best resolution and contrast and the Spiral-Taper the worst. The Q-Flats showed better resolution (about 3%) but slightly worse contrast than the Spiral. All the results indicate the re-weighted L1 SOCP method is a promising and flexible method for seeking trade-offs among resolution, contrast, and number of activated elements.

</details>


### [13] [Multiport Analytical Pixel Electromagnetic Simulator (MAPES) for AI-assisted RFIC and Microwave Circuit Design](https://arxiv.org/abs/2511.21274)
*Junhui Rao,Yi Liu,Jichen Zhang,Zhaoyang Ming,Tianrui Qiao,Yujie Zhang,Chi Yuk Chiu,Hua Wang,Ross Murch*

Main category: eess.SP

TL;DR: MAPES是一种新颖的多端口分析像素电磁模拟器，能够高效准确地预测任意基于像素的微波和RFIC结构的电磁性能，仅需约1%的全波仿真数据即可构建多端口阻抗矩阵，实现600-2000倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统AI辅助电磁设计需要大量全波仿真数据集，存在数据驱动过拟合问题，需要一种更高效、准确的电磁性能预测方法。

Method: 基于集成内部多端口方法，引入虚拟像素和对角虚拟像素，在关键位置插入虚拟端口，捕捉所有水平、垂直和对角电磁耦合，构建单一多端口阻抗矩阵。

Result: MAPES在180nm和65nm CMOS工艺及PCB上验证，实现了高预测精度，相比CST仿真速度提升600-2000倍，消除了数据驱动过拟合。

Conclusion: MAPES提供了一种高效、可扩展且可靠的实用工具，适用于跨多种制造技术的AI辅助微波电路和RFIC设计。

Abstract: This paper proposes a novel analytical framework, termed the Multiport Analytical Pixel Electromagnetic Simulator (MAPES). MAPES enables efficient and accurate prediction of the electromagnetic (EM) performance of arbitrary pixel-based microwave (MW) and RFIC structures. Inspired by the Integrated Internal Multiport Method (IMPM), MAPES extends the concept to the pixel presence/absence domain used in AI-assisted EM design. By introducing virtual pixels and diagonal virtual pixels and inserting virtual ports at critical positions, MAPES captures all horizontal, vertical, and diagonal electromagnetic couplings within a single multiport impedance matrix. Only a small set of full-wave simulations (typically about 1% of the datasets required by AI-assisted EM simulators) is needed to construct this matrix. Subsequently, any arbitrary pixel configuration can be evaluated analytically using a closed-form multiport relation without additional full-wave calculations. The proposed approach eliminates data-driven overfitting and ensures accurate results across all design variations. Comprehensive examples for single- and double-layer CMOS processes (180 nm and 65 nm) and PCBs confirm that MAPES achieves high prediction accuracy with 600- 2000x speed improvement compared to CST simulations. Owing to its efficiency, scalability and reliability, MAPES provides a practical and versatile tool for AI-assisted MW circuit and RFIC design across diverse fabrication technologies.

</details>


### [14] [Blind Turbo Demodulation for Differentially Encoded OFDM with 2D Trellis Decomposition](https://arxiv.org/abs/2511.21345)
*Chin-Hung Chen,Yan Wu,Wim van Houtum,Alex Alvarado*

Main category: eess.SP

TL;DR: 本文开发了一种完全盲的turbo-DE-PSK方案，无需导频即可联合估计信道相位、信道增益和噪声方差，在DAB-like系统中实现接近理想信道知识的性能。


<details>
  <summary>Details</summary>
Motivation: 在DAB-like系统中，turbo-DE-PSK接收机虽然通过迭代解码提供显著性能增益，但依赖无导频的准确信道估计，这是此类场景中的关键挑战。

Method: 采用二维网格分解进行盲相位估计，辅以基于功率的信道增益和噪声方差估计器，构建完全盲的turbo-DE-PSK方案。

Result: 仿真结果表明，盲二维turbo解调器性能接近理想信道知识接收机，并在实际传输条件下保持鲁棒性。

Conclusion: 所提出的盲turbo-DE-PSK方案能够有效解决DAB-like系统中无导频信道估计的挑战，实现高性能的盲接收。

Abstract: Digital Audio Broadcasting (DAB)-like systems employ differentially encoded (DE) phase-shift keying (PSK) for transmission. While turbo-DE-PSK receivers offer substantial performance gains through iterative decoding by making the DE-PSK an inner code, they rely on accurate channel estimation without pilots, which is a key challenge in DAB-like scenarios. This paper develops a fully blind turbo-DE-PSK scheme that jointly estimates channel phase, channel gain, and noise variance directly from the received signal. The design leverages a two-dimensional (2D) trellis decomposition for blind phase estimation, complemented by power-based estimators for channel gain and noise variance. We provide a comprehensive system assessment across practical system parameters, including inner code length, phase quantization, and 2D block size. Simulation results show that the blind 2D turbo demodulator approaches the performance of receivers with perfect channel knowledge and remains robust under realistic transmission conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding](https://arxiv.org/abs/2511.20696)
*Dan Li,Hye-Bin Shin,Yeon-Woo Choi*

Main category: cs.LG

TL;DR: 提出了一种无需历史EEG样本的原型引导无示例持续学习框架，用于解决跨个体EEG解码中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 由于EEG信号在个体间存在显著变异性，在持续EEG解码任务中，引入新受试者时先前获得的知识往往会被覆盖。现有方法依赖存储历史数据作为回放缓冲区来防止遗忘，但隐私问题或内存限制使得保存这些数据不切实际。

Method: 提出ProNECL框架，构建类级原型来总结每个受试者的判别性表示，并通过跨受试者特征对齐和知识蒸馏，逐步将新特征空间与全局原型记忆对齐。

Result: 在BCI Competition IV 2a和2b数据集上验证，该框架有效平衡了知识保留和适应性，在跨受试者持续EEG解码任务中实现了优越性能。

Conclusion: ProNECL框架能够在无需访问历史EEG样本的情况下有效保留先验知识，为持续EEG解码提供了一种实用的解决方案。

Abstract: Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.

</details>


### [16] [Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning](https://arxiv.org/abs/2511.20811)
*Aaron O. Feldman,D. Isaiah Harp,Joseph Duncan,Mac Schwager*

Main category: cs.LG

TL;DR: 本文开发了一种基于数据驱动的飞行测试运行时安全监控方法，通过离线随机轨迹模拟学习短期安全风险的校准统计模型，为飞行员提供预先中止机动操作的标准。


<details>
  <summary>Details</summary>
Motivation: 飞行测试中飞行员在参数不确定的飞机上执行机动操作时，安全违规可能因不确定性而意外发生，需要为飞行员提供明确、预先的中止标准来避免安全违规。

Method: 方法包含三个通用组件：基于近期观测预测未来状态的模型、通过最近邻模型对预测状态进行安全分类、以及通过保形预测进行分类器校准。

Result: 在具有不确定参数的飞行动力学模型上评估表明，该方法能可靠识别不安全场景，匹配理论保证，并在风险预分类方面优于基线方法。

Conclusion: 该方法为飞行测试中的运行时安全监控提供了一种有效的数据驱动解决方案，能够提前识别安全风险并指导飞行员决策。

Abstract: We develop a data-driven approach for runtime safety monitoring in flight testing, where pilots perform maneuvers on aircraft with uncertain parameters. Because safety violations can arise unexpectedly as a result of these uncertainties, pilots need clear, preemptive criteria to abort the maneuver in advance of safety violation. To solve this problem, we use offline stochastic trajectory simulation to learn a calibrated statistical model of the short-term safety risk facing pilots. We use flight testing as a motivating example for data-driven learning/monitoring of safety due to its inherent safety risk, uncertainty, and human-interaction. However, our approach consists of three broadly-applicable components: a model to predict future state from recent observations, a nearest neighbor model to classify the safety of the predicted state, and classifier calibration via conformal prediction. We evaluate our method on a flight dynamics model with uncertain parameters, demonstrating its ability to reliably identify unsafe scenarios, match theoretical guarantees, and outperform baseline approaches in preemptive classification of risk.

</details>


### [17] [Active Slice Discovery in Large Language Models](https://arxiv.org/abs/2511.20713)
*Minhui Zhang,Prahar Ijner,Yoav Wald,Elliot Creager*

Main category: cs.LG

TL;DR: 本文提出主动切片发现方法，通过主动学习识别LLM在毒性分类中的系统性错误切片，显著减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定数据子集上存在系统性错误，识别这些错误切片对模型改进至关重要，但人工标注成本高昂。

Method: 使用主动学习算法，结合不同特征表示，通过有限的人工验证来分组可能属于同一错误切片的样本。

Result: 基于不确定性的主动学习算法效果最佳，仅使用2-10%的切片成员信息即可达到竞争性准确率，显著优于基线方法。

Conclusion: 主动切片发现是识别模型错误切片的有效方法，能够大幅减少人工标注工作量，在毒性分类任务中表现出色。

Abstract: Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.

</details>


### [18] [Probabilistic Hash Embeddings for Online Learning of Categorical Features](https://arxiv.org/abs/2511.20893)
*Aodong Li,Abishek Sankararaman,Balakrishnan Narayanaswamy*

Main category: cs.LG

TL;DR: 提出概率哈希嵌入模型解决在线学习中分类特征词汇表变化的问题，通过贝叶斯在线学习避免确定性嵌入的顺序敏感性和遗忘问题


<details>
  <summary>Details</summary>
Motivation: 传统特征哈希方法在离线或批处理场景中表现良好，但在在线学习场景中，确定性嵌入对类别到达顺序敏感且容易遗忘，导致性能下降

Method: 提出概率哈希嵌入模型，将哈希嵌入视为随机变量，应用贝叶斯在线学习从数据中增量学习，推导出可扩展的推理算法

Result: 实验在分类、序列建模和推荐系统的在线学习设置中展示了PHE的优越性能，同时保持高内存效率（仅消耗one-hot嵌入表2-4倍的内存）

Conclusion: PHE模型能够处理不断演化的分类项目词汇表，适应新项目而不遗忘旧项目，参数集有界不随流中不同观察值数量增长，且对项目到达顺序不变

Abstract: We study streaming data with categorical features where the vocabulary of categorical feature values is changing and can even grow unboundedly over time. Feature hashing is commonly used as a pre-processing step to map these categorical values into a feature space of fixed size before learning their embeddings. While these methods have been developed and evaluated for offline or batch settings, in this paper we consider online settings. We show that deterministic embeddings are sensitive to the arrival order of categories and suffer from forgetting in online learning, leading to performance deterioration. To mitigate this issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally from data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle an evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed values on the stream, and (iv) is invariant to the item arrival order. Experiments in classification, sequence modeling, and recommendation systems in online learning setups demonstrate the superior performance of PHE while maintaining high memory efficiency (consumes as low as 2~4 memory of a one-hot embedding table). Supplementary materials are at https://github.com/aodongli/probabilistic-hash-embeddings

</details>


### [19] [Gradient Descent Algorithm Survey](https://arxiv.org/abs/2511.20725)
*Deng Fucheng,Wang Wanjie,Gong Ao,Wang Xiaoqi,Wang Fan*

Main category: cs.LG

TL;DR: 本文系统分析SGD、Mini-batch SGD、Momentum、Adam和Lion五种深度学习优化算法的核心优势、局限性和实践建议，为算法选择和参数调优提供标准化参考。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习优化算法的实际配置需求，解决不同规模模型和各种训练场景中的优化挑战，为学术研究和工程实践提供合理选择、参数调优和性能提升的参考依据。

Method: 系统分析五种主要优化算法（SGD、Mini-batch SGD、Momentum、Adam、Lion）的核心特点，包括各自的优势、局限性和关键实践建议。

Result: 深入理解了这些优化算法的特性，建立了标准化的算法选择参考框架，能够指导不同场景下的算法配置和参数调优。

Conclusion: 该研究为深度学习优化算法的合理选择和配置提供了系统性的指导，有助于提升模型训练效率和性能，适用于各种规模的模型和训练场景。

Abstract: Focusing on the practical configuration needs of optimization algorithms in deep learning, this article concentrates on five major algorithms: SGD, Mini-batch SGD, Momentum, Adam, and Lion. It systematically analyzes the core advantages, limitations, and key practical recommendations of each algorithm. The research aims to gain an in-depth understanding of these algorithms and provide a standardized reference for the reasonable selection, parameter tuning, and performance improvement of optimization algorithms in both academic research and engineering practice, helping to solve optimization challenges in different scales of models and various training scenarios.

</details>


### [20] [Operationalizing Quantized Disentanglement](https://arxiv.org/abs/2511.20927)
*Vitoria Barin-Pacela,Kartik Ahuja,Simon Lacoste-Julien,Pascal Vincent*

Main category: cs.LG

TL;DR: 本文提出了一种名为Cliff的无监督解缠方法，通过鼓励轴对齐的密度不连续性来恢复量化因子，在非线性映射下实现了优于基准的性能。


<details>
  <summary>Details</summary>
Motivation: 现有理论表明量化因子在任意微分同胚下具有无监督可识别性，但将这一理论原则转化为有效的实践标准仍然具有挑战性，特别是在非线性映射下。

Method: 开发了一种鼓励轴对齐不连续性的无监督解缠标准，通过密度估计中的尖锐变化（称为cliffs）来识别不连续性，并确保沿某一因子的cliffs位置与其他因子的值独立。

Result: Cliff方法在所有解缠基准测试中都优于基线方法，证明了其在无监督解缠中的有效性。

Conclusion: 通过鼓励轴对齐的不连续性，Cliff方法成功地将理论原则转化为实用的无监督解缠标准，并在各种基准测试中表现出色。

Abstract: Recent theoretical work established the unsupervised identifiability of quantized factors under any diffeomorphism. The theory assumes that quantization thresholds correspond to axis-aligned discontinuities in the probability density of the latent factors. By constraining a learned map to have a density with axis-aligned discontinuities, we can recover the quantization of the factors. However, translating this high-level principle into an effective practical criterion remains challenging, especially under nonlinear maps. Here, we develop a criterion for unsupervised disentanglement by encouraging axis-aligned discontinuities. Discontinuities manifest as sharp changes in the estimated density of factors and form what we call cliffs. Following the definition of independent discontinuities from the theory, we encourage the location of the cliffs along a factor to be independent of the values of the other factors. We show that our method, Cliff, outperforms the baselines on all disentanglement benchmarks, demonstrating its effectiveness in unsupervised disentanglement.

</details>


### [21] [CHiQPM: Calibrated Hierarchical Interpretable Image Classification](https://arxiv.org/abs/2511.20779)
*Thomas Norrenbrock,Timo Kaiser,Sovan Biswas,Neslihan Kose,Ramesh Manuvinakurike,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: CHiQPM是一个全局可解释模型，通过对比解释多数类别实现优越的全局可解释性，提供类似人类推理的分层解释，并内置可解释的Conformal预测方法，在保持99%非可解释模型准确率的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，除了全局解释外，详细的局部解释对于在推理过程中有效支持人类专家至关重要，需要实现人类-AI互补性。

Method: 提出校准分层QPM（CHiQPM），通过对比解释多数类别实现全局可解释性，提供分层解释（类似人类推理方式），并内置可解释的Conformal预测方法。

Result: CHiQPM作为点预测器达到SOTA准确率，保持非可解释模型99%的准确率，其校准集预测在效率上与其他CP方法竞争，同时提供沿其分层解释的连贯集的可解释预测。

Conclusion: CHiQPM在保持高准确率的同时实现了全面的全局和局部可解释性，展示了在不牺牲整体准确性的情况下融入可解释性的显著改进。

Abstract: Globally interpretable models are a promising approach for trustworthy AI in safety-critical domains. Alongside global explanations, detailed local explanations are a crucial complement to effectively support human experts during inference. This work proposes the Calibrated Hierarchical QPM (CHiQPM) which offers uniquely comprehensive global and local interpretability, paving the way for human-AI complementarity. CHiQPM achieves superior global interpretability by contrastively explaining the majority of classes and offers novel hierarchical explanations that are more similar to how humans reason and can be traversed to offer a built-in interpretable Conformal prediction (CP) method. Our comprehensive evaluation shows that CHiQPM achieves state-of-the-art accuracy as a point predictor, maintaining 99% accuracy of non-interpretable models. This demonstrates a substantial improvement, where interpretability is incorporated without sacrificing overall accuracy. Furthermore, its calibrated set prediction is competitively efficient to other CP methods, while providing interpretable predictions of coherent sets along its hierarchical explanation.

</details>


### [22] [Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs](https://arxiv.org/abs/2511.21050)
*Dongkyu Derek Cho,Huan Song,Arijit Ghosh Chowdhury,Haotian An,Yawei Wang,Rohit Thekkanal,Negin Sokhandan,Sharlina Keshava,Hannah Marlowe*

Main category: cs.LG

TL;DR: 本文首次全面分析了RLVR（带可验证奖励的强化学习）在LLM微调中的安全性，挑战了传统安全-能力权衡的假设，证明了RLVR可以同时提升推理能力并保持或改进安全防护。


<details>
  <summary>Details</summary>
Motivation: 传统LLM微调方法（如SFT和RLHF）存在安全-能力权衡问题，即提升任务性能会降低安全对齐。RLVR作为有前景的替代方法，其安全性影响尚未被探索。

Method: 通过理论分析推导KL约束优化下的安全漂移上界，并进行广泛的实证实验，涵盖五个对抗性安全基准，研究优化算法、模型规模和任务领域的影响。

Result: 理论证明在特定条件下可以消除安全退化，实证结果显示RLVR能够同时增强推理能力并保持或改进安全防护。

Conclusion: 研究发现挑战了不可避免的安全-能力权衡假设，确立了特定训练方法可以同时实现两个目标，为安全部署具备推理能力的LLM提供了见解。

Abstract: Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.

</details>


### [23] [Autoregressive Surrogate Modeling of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.20830)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 本文提出了首个使用球面傅里叶神经算子的自回归机器学习替代模型，用于预测太阳风径向速度，相比传统数值方法具有更好的准确性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统三维磁流体动力学模型计算成本高昂，限制了边界条件不确定性的快速探索，需要开发高效的替代模型来改进空间天气预报。

Method: 使用球面傅里叶神经算子，通过预测有限径向范围并迭代向外传播解，构建自回归机器学习替代模型。

Result: 与数值HUX替代模型相比，SFNO表现出相当或更优的性能，同时提供了灵活、可训练和数据驱动的替代方案。

Conclusion: 该研究为高保真太阳风建模建立了新颖的方法论，代码和可视化结果已开源。

Abstract: The solar wind, a continuous outflow of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Accurate prediction of features such as high-speed streams and coronal mass ejections is critical for space weather forecasting, but traditional three-dimensional magnetohydrodynamic (MHD) models are computationally expensive, limiting rapid exploration of boundary condition uncertainties. We introduce the first autoregressive machine learning surrogate for steady-state solar wind radial velocity using the Spherical Fourier Neural Operator (SFNO). By predicting a limited radial range and iteratively propagating the solution outward, the model improves accuracy in distant regions compared to a single-step approach. Compared with the numerical HUX surrogate, SFNO demonstrates superior or comparable performance while providing a flexible, trainable, and data-driven alternative, establishing a novel methodology for high-fidelity solar wind modeling. The source code and additional visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity-autoregressive.

</details>


### [24] [Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2511.20913)
*Yingchuan Sun,Shengpu Tang*

Main category: cs.LG

TL;DR: 本文通过实证研究比较了脓毒症管理中不同时间步长（1、2、4、8小时）对离线强化学习的影响，发现更细的时间步长（1-2小时）在静态行为策略下能获得更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有脓毒症管理强化学习研究大多使用4小时时间步长，但该粗粒度可能扭曲患者动态并导致次优治疗策略，需要量化时间步长对离线RL各环节的实际影响。

Method: 采用相同离线RL流程，设计动作重映射方法以公平比较不同时间步长，在两种策略学习设置下进行跨时间步长的模型选择，评估时间步长对状态表示学习、行为克隆、策略训练和离策略评估的影响。

Result: 不同时间步长的性能趋势随学习设置变化，使用静态行为策略在更细时间步长（1-2小时）学习的策略获得整体最佳性能和稳定性。

Conclusion: 时间步长是医疗领域离线RL的核心设计选择，研究结果为超越传统4小时设置的替代方案提供了证据支持。

Abstract: Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($Δt\!=\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$Δt$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $Δt$ vary as learning setups change, while policies learned at finer time-step sizes ($Δt = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.

</details>


### [25] [Dataset Poisoning Attacks on Behavioral Cloning Policies](https://arxiv.org/abs/2511.20992)
*Akansha Kalra,Soumil Datta,Ethan Gilmore,Duc La,Guanhong Tao,Daniel S. Brown*

Main category: cs.LG

TL;DR: 本文首次分析了行为克隆策略对干净标签后门攻击的脆弱性，通过注入视觉触发器在演示数据中创建虚假相关性，并在测试时利用这些后门显著降低策略性能。


<details>
  <summary>Details</summary>
Motivation: 随着行为克隆策略在现实世界中部署的增加，其鲁棒性和潜在漏洞成为重要关注点。本文旨在研究BC策略对后门攻击的脆弱性，特别是当训练数据被轻微污染时。

Method: 通过在演示数据集中注入视觉触发器来污染数据，创建虚假相关性；引入基于熵的测试时触发器攻击，识别关键状态；评估策略脆弱性与污染数据比例、触发器强度和类型的关系。

Result: 实证表明，即使在最小程度污染的数据集上训练的BC策略，在部署期间对后门触发器攻击也表现出高度脆弱性，尽管其任务性能接近基线水平。

Conclusion: 研究结果强调了迫切需要加强对BC策略鲁棒性的研究，特别是当大规模数据集被用于训练现实世界信息物理系统的策略时。

Abstract: Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious correlation that can be exploited at test time. We evaluate how policy vulnerability scales with the fraction of poisoned data, the strength of the trigger, and the trigger type. We also introduce a novel entropy-based test-time trigger attack that substantially degrades policy performance by identifying critical states where test-time triggering of the backdoor is expected to be most effective at degrading performance. We empirically demonstrate that BC policies trained on even minimally poisoned datasets exhibit deceptively high, near-baseline task performance despite being highly vulnerable to backdoor trigger attacks during deployment. Our results underscore the urgent need for more research into the robustness of BC policies, particularly as large-scale datasets are increasingly used to train policies for real-world cyber-physical systems. Videos and code are available at https://sites.google.com/view/dataset-poisoning-in-bc.

</details>


### [26] [Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.21011)
*Sid Bharthulwar,Stone Tao,Hao Su*

Main category: cs.LG

TL;DR: 本文提出了一种称为交错重置（staggered resets）的技术，通过在任务时间轴的不同点初始化和重置环境，减少同步重置引入的有害非平稳性，从而提高强化学习训练的样本效率、收敛速度和最终性能。


<details>
  <summary>Details</summary>
Motivation: 在GPU大规模并行模拟环境中，为了最大化吞吐量通常使用短回合进行策略更新，但这会导致同步重置引入有害的非平稳性，扭曲学习信号并破坏训练稳定性。

Method: 提出交错重置技术，让环境在任务时间轴的不同点进行初始化和重置，从而产生具有更大时间多样性的训练批次，减少同步回合引入的非平稳性。

Result: 在挑战性的高维机器人环境中，该技术显著提高了样本效率、加快了实际收敛速度并增强了最终性能，且比朴素同步回合具有更好的扩展性。

Conclusion: 交错重置是一种简单而有效的技术，能够显著改善大规模并行强化学习训练的性能和稳定性，特别是在使用短回合和高更新数据比的情况下。

Abstract: Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.

</details>


### [27] [Prediction of Herd Life in Dairy Cows Using Multi-Head Attention Transformers](https://arxiv.org/abs/2511.21034)
*Mahdi Saki,Justin Lipman*

Main category: cs.LG

TL;DR: 开发基于多头注意力Transformer的AI模型，利用奶牛从出生开始的历史多变量时间序列数据来预测奶牛寿命，在澳大利亚7个农场的19,000头奶牛数据上达到83%的决定系数。


<details>
  <summary>Details</summary>
Motivation: 奶农需要基于客观评估来决定保留还是淘汰奶牛，需要识别更具韧性的奶牛以应对农场条件并完成更多泌乳期，这一决策过程复杂且具有显著的环境和经济影响。

Method: 使用多头注意力Transformer技术分析历史多变量时间序列数据，数据集包含来自7个澳大利亚农场的19,000头奶牛的约780,000条记录。

Result: 模型在预测牛群寿命方面达到83%的整体决定系数，显示出在实际奶牛群管理中的应用潜力。

Conclusion: AI驱动的模型能够有效预测奶牛寿命，为奶农提供客观决策支持，有助于提高奶牛群管理效率。

Abstract: Dairy farmers should decide to keep or cull a cow based on an objective assessment of her likely performance in the herd. For this purpose, farmers need to identify more resilient cows, which can cope better with farm conditions and complete more lactations. This decision-making process is inherently complex, with significant environmental and economic implications. In this study, we develop an AI-driven model to predict cow longevity using historical multivariate time-series data recorded from birth. Leveraging advanced AI techniques, specifically Multi-Head Attention Transformers, we analysed approximately 780,000 records from 19,000 unique cows across 7 farms in Australia. The results demonstrate that our model achieves an overall determination coefficient of 83% in predicting herd life across the studied farms, highlighting its potential for practical application in dairy herd management.

</details>


### [28] [MNM : Multi-level Neuroimaging Meta-analysis with Hyperbolic Brain-Text Representations](https://arxiv.org/abs/2511.21092)
*Seunghun Baek,Jaejin Lee,Jaeyoon Sim,Minjae Jeong,Won Hwa Kim*

Main category: cs.LG

TL;DR: 提出了一种利用双曲几何连接神经科学文献和脑激活图的新框架，通过将研究论文文本和脑图像嵌入到共享的双曲空间中，捕捉神经影像数据的语义相似性和层次结构。


<details>
  <summary>Details</summary>
Motivation: 解决神经影像研究中样本量小的问题，传统基于关键词检索或线性映射的方法忽略了大脑的丰富层次结构。

Method: 使用Lorentz模型将研究论文文本和对应脑图像嵌入到共享双曲空间，执行多级神经影像元分析：1) 对齐脑和文本嵌入以获得语义对应；2) 引导文本和脑激活之间的层次关系；3) 保持脑激活模式中的层次关系。

Result: 实验结果表明，该模型优于基线方法，提供了一个鲁棒且可解释的多级神经影像元分析范式。

Conclusion: 通过双曲脑-文本表示，该方法为神经影像元分析提供了有效的新框架，能够更好地捕捉大脑活动的语义和层次特征。

Abstract: Various neuroimaging studies suffer from small sample size problem which often limit their reliability. Meta-analysis addresses this challenge by aggregating findings from different studies to identify consistent patterns of brain activity. However, traditional approaches based on keyword retrieval or linear mappings often overlook the rich hierarchical structure in the brain. In this work, we propose a novel framework that leverages hyperbolic geometry to bridge the gap between neuroscience literature and brain activation maps. By embedding text from research articles and corresponding brain images into a shared hyperbolic space via the Lorentz model, our method captures both semantic similarity and hierarchical organization inherent in neuroimaging data. In the hyperbolic space, our method performs multi-level neuroimaging meta-analysis (MNM) by 1) aligning brain and text embeddings for semantic correspondence, 2) guiding hierarchy between text and brain activations, and 3) preserving the hierarchical relationships within brain activation patterns. Experimental results demonstrate that our model outperforms baselines, offering a robust and interpretable paradigm of multi-level neuroimaging meta-analysis via hyperbolic brain-text representation.

</details>


### [29] [Interpretable Fair Clustering](https://arxiv.org/abs/2511.21109)
*Mudi Jiang,Jiahui Zhou,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种可解释的公平聚类框架，通过将公平约束集成到决策树结构中，在保证聚类性能的同时提高公平性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类方法缺乏可解释性，限制了在高风险场景中的应用。需要开发既能保证公平性又具有可解释性的聚类方法。

Method: 构建可解释的决策树来划分数据，同时确保受保护群体间的公平对待。还提出了无需公平超参数调优的变体，通过对无公平约束构建的树进行后剪枝实现。

Result: 在真实世界和合成数据集上的广泛实验表明，该方法不仅提供有竞争力的聚类性能和改善的公平性，还具有可解释性、处理多个敏感属性的能力等优势。

Conclusion: 该方法能够在复杂公平约束下稳健运行，为公平透明的聚类开辟了新可能性。

Abstract: Fair clustering has gained increasing attention in recent years, especially in applications involving socially sensitive attributes. However, existing fair clustering methods often lack interpretability, limiting their applicability in high-stakes scenarios where understanding the rationale behind clustering decisions is essential. In this work, we address this limitation by proposing an interpretable and fair clustering framework, which integrates fairness constraints into the structure of decision trees. Our approach constructs interpretable decision trees that partition the data while ensuring fair treatment across protected groups. To further enhance the practicality of our framework, we also introduce a variant that requires no fairness hyperparameter tuning, achieved through post-pruning a tree constructed without fairness constraints. Extensive experiments on both real-world and synthetic datasets demonstrate that our method not only delivers competitive clustering performance and improved fairness, but also offers additional advantages such as interpretability and the ability to handle multiple sensitive attributes. These strengths enable our method to perform robustly under complex fairness constraints, opening new possibilities for equitable and transparent clustering.

</details>


### [30] [Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination](https://arxiv.org/abs/2511.21118)
*Pius Onobhayedo,Paul Osemudiame Oamen*

Main category: cs.LG

TL;DR: 该论文提出了一种解决联邦学习系统中组成性差距的方法，通过密码学收据证明聚合正确性、几何新颖性测量防止激励博弈、并行对象所有权实现线性可扩展性以及时间锁定策略检查追溯性操纵。


<details>
  <summary>Details</summary>
Motivation: 人工智能正从集中式提供转向分布式创建，但民主化愿景因组成性差距而未能实现，包括聚合器缺乏问责制、经济机制缺失且易被博弈、协调串行化状态修改限制可扩展性以及治理允许追溯性操纵。

Method: 利用密码学收据证明聚合正确性，几何新颖性测量防止激励博弈，并行对象所有权实现线性可扩展性，时间锁定策略检查追溯性操纵。

Result: 提出的方法解决了联邦学习系统中的关键组成性差距，为大规模分布式AI创建提供了可行的技术方案。

Conclusion: 通过密码学、几何测量、并行所有权和时间锁定策略的综合应用，实现了联邦学习系统的可问责性、抗博弈性、可扩展性和防追溯操纵能力，推动了分布式AI的民主化愿景。

Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.

</details>


### [31] [The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods](https://arxiv.org/abs/2511.21363)
*Kevin Iselborn,David Dembinsky,Adriano Lucieri,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出了一种新的局部特征归因方法保真度评估指标DPC，通过结合扰动和归因的方向，实现了近10倍的速度提升并消除了随机性，为医疗等高风险领域提供确定性、可信赖的评估方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险环境中，解释方法的保真度至关重要。现有保真度指标如Infidelity依赖蒙特卡洛近似，需要大量模型评估并引入随机采样不确定性，无法满足临床和监管需求。

Method: 在引导扰动实验中修改现有的预测变化(PC)指标，通过结合扰动和归因的方向，提出定向预测变化(DPC)指标，实现确定性评估。

Result: 在皮肤病变图像和金融表格数据两个数据集、两个黑盒模型、七种解释算法和广泛超参数范围内，对4744个不同解释进行评估，DPC与PC共同实现了对基线导向和局部特征归因方法的全面高效评估。

Conclusion: DPC指标提供了确定性、可重复的结果，实现了近10倍速度提升并消除了随机性，能够可靠评估解释方法的保真度，特别适用于医疗等高风险场景。

Abstract: The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes.

</details>


### [32] [Best Practices for Machine Learning Experimentation in Scientific Applications](https://arxiv.org/abs/2511.21354)
*Umberto Michelucci,Francesca Venturini*

Main category: cs.LG

TL;DR: 本文提供了一个实用的结构化指南，用于在科学应用中进行机器学习实验，重点关注可重现性、公平比较和透明报告。提出了包含对数过拟合比率和复合过拟合分数的评估指标。


<details>
  <summary>Details</summary>
Motivation: 机器学习在科学研究中日益普及，但实验设计和文档记录的质量会影响结果的可靠性。糟糕的基线、不一致的预处理或验证不足可能导致对模型性能的误导性结论。

Method: 提出逐步工作流程，从数据集准备到模型选择和评估。引入考虑过拟合和验证折叠不稳定性的指标，包括对数过拟合比率和复合过拟合分数。

Result: 通过推荐实践和示例报告格式，支持研究人员建立稳健的基线，并从应用于科学问题的机器学习模型中得出有效的基于证据的见解。

Conclusion: 本文旨在提高科学机器学习实验的质量和可靠性，通过结构化方法和评估指标促进可重现和公平的比较。

Abstract: Machine learning (ML) is increasingly adopted in scientific research, yet the quality and reliability of results often depend on how experiments are designed and documented. Poor baselines, inconsistent preprocessing, or insufficient validation can lead to misleading conclusions about model performance. This paper presents a practical and structured guide for conducting ML experiments in scientific applications, focussing on reproducibility, fair comparison, and transparent reporting. We outline a step-by-step workflow, from dataset preparation to model selection and evaluation, and propose metrics that account for overfitting and instability across validation folds, including the Logarithmic Overfitting Ratio (LOR) and the Composite Overfitting Score (COS). Through recommended practices and example reporting formats, this work aims to support researchers in establishing robust baselines and drawing valid evidence-based insights from ML models applied to scientific problems.

</details>


### [33] [Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data](https://arxiv.org/abs/2511.21378)
*Jungi Lee,Jungkwon Kim,Chi Zhang,Kwangsun Yoo,Seok-Joo Byun*

Main category: cs.LG

TL;DR: 提出AAR方法解决异常检测中数据污染问题，通过动态排除异常数据，在图像和表格数据集上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统异常检测模型假设训练数据纯净，但实际数据常被污染。现有方法依赖固定污染比例假设，当假设与实际不符时性能严重下降，特别是在正常与异常数据分布重叠的噪声环境中

Method: 提出自适应和主动拒绝方法(AAR)，使用改进的z-score和高斯混合模型阈值动态排除异常，结合硬拒绝和软拒绝策略平衡保留正常数据和排除异常数据

Result: 在两个图像数据集和三十个表格数据集上的实验表明，AAR比最先进方法提高了0.041 AUROC

Conclusion: AAR为污染数据集提供了可扩展且可靠的解决方案，增强了鲁棒性，为安全和医疗等领域的实际应用铺平了道路

Abstract: Handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. Conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. To address these limitations, we propose Adaptive and Aggressive Rejection (AAR), a novel method that dynamically excludes anomalies using a modified z-score and Gaussian mixture model-based thresholds. AAR effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. Extensive experiments on two image datasets and thirty tabular datasets demonstrate that AAR outperforms the state-of-the-art method by 0.041 AUROC. By providing a scalable and reliable solution, AAR enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.

</details>


### [34] [Mechanistic Interpretability for Transformer-based Time Series Classification](https://arxiv.org/abs/2511.21514)
*Matīss Kalnāre,Sofoklis Kitharidis,Thomas Bäck,Niki van Stein*

Main category: cs.LG

TL;DR: 本文通过将机制可解释性技术从NLP领域迁移到时间序列分类的Transformer架构，揭示了模型内部注意力头和时间步的因果结构，构建了信息传播的因果图。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列分类中表现出色，但其内部决策机制难以理解。现有可解释性方法主要关注输入输出归因，对内部机制揭示不足，需要更深入的机制可解释性分析。

Method: 采用机制可解释性技术：激活修补、注意力显著性分析和稀疏自编码器，系统探测注意力头和时间步的内部因果作用，构建因果图展示信息传播路径。

Result: 在基准时间序列数据集上实验，识别出驱动正确分类的关键注意力头和时间位置，展示了稀疏自编码器在发现可解释潜在特征方面的潜力。

Conclusion: 研究为Transformer可解释性提供了方法论贡献，并揭示了时间序列分类任务中Transformer功能机制的新见解。

Abstract: Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.

</details>


### [35] [IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference](https://arxiv.org/abs/2511.21513)
*Wanli Zhong,Haibo Feng,Zirui Zhou,Hanyang Peng,Shiqi Yu*

Main category: cs.LG

TL;DR: IntAttention是一种完全整数的注意力机制，通过IndexSoftmax替代浮点指数运算，消除量化-反量化开销，在边缘设备上实现高效Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署Transformer模型受限于延迟和能耗，INT8量化虽然加速了矩阵乘法，但softmax成为主要瓶颈，占注意力延迟的65%，破坏了端到端整数数据流。

Method: 提出IntAttention全整数注意力流水线，核心是IndexSoftmax硬件友好算子，集成稀疏感知裁剪、32项查找表近似和直接整数归一化，无需重新训练。

Result: 在Armv8 CPU上实现3.7倍加速和61%能耗降低，比传统INT8注意力快2.0倍，在各种语言和视觉模型中保持高精度。

Conclusion: IntAttention为边缘设备提供了实用高效的Transformer推理解决方案，无需重新训练即可实现显著性能提升。

Abstract: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can account for up to 65% of total attention latency and disrupts the end-to-end integer dataflow critical for edge hardware efficiency. To address this limitation, we present IntAttention, the first fully integer, plug-and-play attention pipeline without retraining. At the core of our approach lies IndexSoftmax, a hardware-friendly operator that replaces floating-point exponentials entirely within the integer domain. IntAttention integrates sparsity-aware clipping, a 32-entry lookup-table approximation, and direct integer normalization, thereby eliminating all datatype conversion overhead. We evaluate IntAttention and demonstrate consistent and substantial gains. Our method achieves up to 3.7x speedup and 61% energy reduction over FP16 baselines and 2.0x faster than conventional INT8 attention pipelines on Armv8 CPUs. These gains are achieved with high-fidelity accuracy comparable to baselines across diverse language and vision models, enabling practical and efficient Transformer inference on commodity edge devices. Code will be released in later version of this work.

</details>


### [36] [Computing Strategic Responses to Non-Linear Classifiers](https://arxiv.org/abs/2511.21560)
*Jack Geary,Boyan Gao,Henry Gouk*

Main category: cs.LG

TL;DR: 提出了一种计算战略分类中智能体最佳响应的方法，通过优化智能体目标的拉格朗日对偶来解决非线性分类器设置中的关键限制。


<details>
  <summary>Details</summary>
Motivation: 战略分类中部署分类器会导致战略行为引发分布漂移，现有方法主要关注线性设置，但许多情况下非线性分类器更合适，而计算非线性设置中的最佳响应存在困难。

Method: 通过优化智能体目标的拉格朗日对偶来计算最佳响应，该方法在非线性分类器设置中可直接应用。

Result: 该方法在线性设置中重现了最佳响应，识别出现有方法的关键弱点，并在非线性分类器设置中可用于评估和训练。

Conclusion: 提出的方法解决了战略分类中非线性分类器最佳响应计算的限制，为非线性设置下的分类器学习和评估提供了有效工具。

Abstract: We consider the problem of strategic classification, where the act of deploying a classifier leads to strategic behaviour that induces a distribution shift on subsequent observations. Current approaches to learning classifiers in strategic settings are focused primarily on the linear setting, but in many cases non-linear classifiers are more suitable. A central limitation to progress for non-linear classifiers arises from the inability to compute best responses in these settings. We present a novel method for computing the best response by optimising the Lagrangian dual of the Agents' objective. We demonstrate that our method reproduces best responses in linear settings, identifying key weaknesses in existing approaches. We present further results demonstrating our method can be straight-forwardly applied to non-linear classifier settings, where it is useful for both evaluation and training.

</details>


### [37] [A decoupled alignment kernel for peptide membrane permeability predictions](https://arxiv.org/abs/2511.21566)
*Ali Amirahmadi,Gökçe Geylan,Leonardo De Maria,Farzaneh Etminani,Mattias Ohlsson,Alessandro Tibo*

Main category: cs.LG

TL;DR: 提出了一种单体感知解耦全局对齐核（MD-GAK）及其变体PMD-GAK，用于预测环肽的细胞膜渗透性，重点解决数据稀缺和不确定性校准问题。


<details>
  <summary>Details</summary>
Motivation: 环肽是靶向细胞内位点的有前景模式，但细胞膜渗透性仍是关键瓶颈，且面临公共数据有限和需要良好校准不确定性的挑战。

Method: 提出MD-GAK核方法，将化学有意义的残基-残基相似性与序列对齐耦合，同时将局部匹配与间隙惩罚解耦；还引入变体PMD-GAK加入三角位置先验；使用高斯过程作为预测模型。

Result: 通过广泛实验证明该方法优于现有最先进模型，在所有指标上表现更优，PMD-GAK在减少校准误差方面具有额外优势。

Conclusion: MD-GAK和PMD-GAK是简单有效的核方法，在环肽细胞膜渗透性预测中表现出色，特别在不确定性估计方面具有优势。

Abstract: Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics.

</details>

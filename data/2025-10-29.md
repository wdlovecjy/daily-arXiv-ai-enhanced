<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 3]
- [eess.SP](#eess.SP) [Total: 6]
- [stat.AP](#stat.AP) [Total: 3]
- [cs.LG](#cs.LG) [Total: 38]
- [cs.AI](#cs.AI) [Total: 12]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Beyond Normality: Reliable A/B Testing with Non-Gaussian Data](https://arxiv.org/abs/2510.23666)
*Junpeng Gong,Chunkai Wang,Hao Li,Jinyong Ma,Haoxuan Li,Xu He*

Main category: stat.ML

TL;DR: 本文分析了A/B测试中t检验在数据非正态分布或样本量不均衡时的可靠性问题，提出了最小样本量计算公式和Edgeworth校正方法，显著提升了A/B测试的可靠性。


<details>
  <summary>Details</summary>
Motivation: A/B测试已成为在线市场决策的核心工具，但当数据分布偏离正态性或处理组与对照组样本量不同时，常用的成对t检验不再可靠，可能导致有害策略的发布。

Method: 量化偏斜、长尾数据和不等分配对错误率的影响，推导t检验有效所需的最小样本量显式公式，并引入基于Edgeworth的校正方法来提供更准确的p值。

Result: 发现许多在线反馈指标需要数亿样本才能确保可靠的A/B测试，离线实验证实了理论最小样本量阈值的实用价值，校正方法显著提高了真实条件下的测试可靠性。

Conclusion: 通过理论分析和校正方法，解决了A/B测试在非理想条件下的可靠性问题，为实际应用提供了更可靠的统计基础。

Abstract: A/B testing has become the cornerstone of decision-making in online markets,
guiding how platforms launch new features, optimize pricing strategies, and
improve user experience. In practice, we typically employ the pairwise $t$-test
to compare outcomes between the treatment and control groups, thereby assessing
the effectiveness of a given strategy. To be trustworthy, these experiments
must keep Type I error (i.e., false positive rate) under control; otherwise, we
may launch harmful strategies. However, in real-world applications, we find
that A/B testing often fails to deliver reliable results. When the data
distribution departs from normality or when the treatment and control groups
differ in sample size, the commonly used pairwise $t$-test is no longer
trustworthy. In this paper, we quantify how skewed, long tailed data and
unequal allocation distort error rates and derive explicit formulas for the
minimum sample size required for the $t$-test to remain valid. We find that
many online feedback metrics require hundreds of millions samples to ensure
reliable A/B testing. Thus we introduce an Edgeworth-based correction that
provides more accurate $p$-values when the available sample size is limited.
Offline experiments on a leading A/B testing platform corroborate the practical
value of our theoretical minimum sample size thresholds and demonstrate that
the corrected method substantially improves the reliability of A/B testing in
real-world conditions.

</details>


### [2] [Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis](https://arxiv.org/abs/2510.23935)
*Enze Shi,Pankaj Bhagwat,Zhixian Yang,Linglong Kong,Bei Jiang*

Main category: stat.ML

TL;DR: 提出一种通过调整数据表示来平衡预测效用和公平性的框架，使用充分降维分解特征空间，选择性移除敏感信息，并通过理论分析和实验验证方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型经常继承和放大历史偏见，导致不公平结果。传统公平性方法通常在预测层面施加约束，而没有解决数据表示中的潜在偏见。

Method: 使用充分降维将特征空间分解为目标相关、敏感和共享组件，通过选择性移除敏感信息来控制公平性-效用权衡，并采用影响函数量化参数估计的渐近行为。

Result: 在合成和真实数据集上的实验验证了理论见解，表明所提方法在保持预测性能的同时有效提高了公平性。

Conclusion: 该框架提供了一种原则性方法来处理机器学习中的公平性问题，通过调整数据表示而非仅约束预测，实现了更好的公平性-效用平衡。

Abstract: Machine learning models have achieved widespread success but often inherit
and amplify historical biases, resulting in unfair outcomes. Traditional
fairness methods typically impose constraints at the prediction level, without
addressing underlying biases in data representations. In this work, we propose
a principled framework that adjusts data representations to balance predictive
utility and fairness. Using sufficient dimension reduction, we decompose the
feature space into target-relevant, sensitive, and shared components, and
control the fairness-utility trade-off by selectively removing sensitive
information. We provide a theoretical analysis of how prediction error and
fairness gaps evolve as shared subspaces are added, and employ influence
functions to quantify their effects on the asymptotic behavior of parameter
estimates. Experiments on both synthetic and real-world datasets validate our
theoretical insights and show that the proposed method effectively improves
fairness while preserving predictive performance.

</details>


### [3] [Self-Concordant Perturbations for Linear Bandits](https://arxiv.org/abs/2510.24187)
*Lucas Lévy,Jean-Lou Valeau,Arya Akhavan,Patrick Rebeschini*

Main category: stat.ML

TL;DR: 该论文提出了一个统一算法框架，将FTRL和FTPL方法连接起来，并引入自协调扰动概念。基于此设计的新FTPL算法在d维超立方体和欧几里得球上实现了O(d√(n ln n))的遗憾界，在超立方体上比现有方法有√d的改进。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性线性赌博问题，旨在建立FTRL和FTPL方法之间的统一框架，扩展它们在完全信息设置中的已知联系，并改进现有算法的性能。

Method: 提出自协调扰动概念，结合自协调正则化和高效随机探索，设计基于FTPL的新算法。

Result: 在d维超立方体和欧几里得球上均实现了O(d√(n ln n))的遗憾界。在欧几里得球上匹配现有自协调FTRL方法，在超立方体上比这些方法有√d的改进，且达到最优界（除对数因子外）。

Conclusion: 成功建立了FTRL和FTPL方法的统一框架，通过自协调扰动设计的新算法在多个设置中实现了最优或接近最优的性能。

Abstract: We study the adversarial linear bandits problem and present a unified
algorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and
Follow-the-Perturbed-Leader (FTPL) methods, extending the known connection
between them from the full-information setting. Within this framework, we
introduce self-concordant perturbations, a family of probability distributions
that mirror the role of self-concordant barriers previously employed in the
FTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based
algorithm that combines self-concordant regularization with efficient
stochastic exploration. Our approach achieves a regret of $O(d\sqrt{n \ln n})$
on both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean
ball, this matches the rate attained by existing self-concordant FTRL methods.
For the hypercube, this represents a $\sqrt{d}$ improvement over these methods
and matches the optimal bound up to logarithmic factors.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [4] [Communication in a Fractional World: MIMO MC-OTFS Precoder Prediction](https://arxiv.org/abs/2510.23832)
*Evan Allen,Karim Said,Robert Calderbank,Lingjia Liu*

Main category: eess.SP

TL;DR: 本文提出了一种基于OTFS调制的物理信息复杂指数基扩展模型预测框架，用于解决高移动性场景下MIMO通信中的CSI反馈问题。


<details>
  <summary>Details</summary>
Motivation: 随着6G技术的发展，高移动性场景如VANETs和FANETs需要支持MIMO技术，但传统时频CSI反馈方案在快速变化的信道条件下负担过重，因此需要转向更稳定的延迟-多普勒表示方法。

Method: 推导了OTFS输入输出关系随时间变化的表达式，并基于此创建了物理信息复杂指数基扩展模型预测框架，最大化过时CSI的有效性。

Result: 该框架能够处理整数和分数延迟-多普勒信道，促进高移动性MIMO通信。

Conclusion: OTFS调制在移动性环境下提供更大稳定性，所提出的预测框架有效解决了高移动性场景下的CSI反馈挑战。

Abstract: As 6G technologies advance, international bodies and regulatory agencies are
intensifying efforts to extend seamless connectivity especially for
high-mobility scenarios such as Mobile Ad-Hoc Networks (\textit{MANETs}) types
such as Vehicular Ad-Hoc Networks (\textit{VANETs}) and Flying Ad-Hoc Networks
(\textit{FANETs}). For these environments to be considered for long term
adoption and use they must support Multiple-Input-Multiple- (MIMO) technology,
rapidly fluctuating channel conditions in these environments place a heavy
burden on traditional time-frequency CSI feedback schemes required for MIMO
precoding. This motivates a shift toward delay-Doppler representations like
those employed by Orthogonal Time-Frequency Space(OTFS) modulation, which
offers greater stability under mobility. We derive an expression for the
variation over time in the OTFS I/O relationship. We then use this to create a
physics informed complex exponential basis expansion model prediction framework
that maximizes the usefulness of outdated Channel State Information (CSI) in
the presence of integer and fractional delay-Doppler channels and facilitates
high mobility MIMO communication.

</details>


### [5] [Coordinated Multipoint Transmission in Pinching Antenna Systems](https://arxiv.org/abs/2510.23837)
*Ali Amhaz,Shreya Khisa,Mohamed Elhattab,Chadi Assi,Sanaa Sharafeddine*

Main category: eess.SP

TL;DR: 该论文研究了在协调多点传输系统中使用捏合天线技术来克服大规模衰落问题，通过联合优化波束成形向量和捏合位置来最大化可实现的和速率，并采用梯度元学习策略解决非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统协调多点传输系统在使用均匀线性阵列时存在速率上限限制，而捏合天线技术被证明是克服MIMO系统中大规模衰落问题的有前景解决方案，因此研究如何将该技术优势应用于CoMP系统。

Method: 采用梯度元学习策略来联合优化传输波束成形向量和波导上的捏合位置，同时满足用户服务质量要求，解决由于决策参数强耦合导致的非凸优化问题。

Result: 数值分析表明所提出的GML方法达到了最优解的92%，相比其他基准方法具有优越性，并且相比传统CoMP系统实现了更高的可实现速率上限。

Conclusion: 捏合天线技术在CoMP系统中能够有效克服大规模衰落，所提出的梯度元学习方法在解决复杂非凸优化问题上表现出色，显著提升了系统性能。

Abstract: We study a coordinated multi-point (CoMP) transmission where two base
stations (BSs), each supported by a pinching antenna system (PASS), are
deployed to jointly serve communication users under spatial division multiple
access (SDMA) technology. Pinching Antenna technology was introduced as a
promising solution to overcome the large-scale fading that has been shown to be
an impediment in multiple-input multiple-output (MIMO) systems. To realize the
advantages of this technology in CoMP systems, which suffer from an upperbound
rate limitation when traditional uniform linear arrays (ULAs) are adopted, we
formulate an optimization problem with the aim of maximizing the achievable sum
rate by jointly determining the transmit beamforming vectors and pinching
locations on the waveguides while respecting the quality of service (QoS)
requirements of users. This problem is inherently non-convex due to the strong
coupling among its decision parameters, making it challenging to solve using
traditional optimization methods. Thus, we utilize a gradient-based
meta-learning (GML) strategy specifically designed for large-scale optimization
tasks. Finally, numerical analysis demonstrates the effectiveness of the
proposed GML approach, achieving 92 percent of the optimal solution, and the
superiority of the solution presented compared to other benchmarks. In
addition, it achieves a higher upper bound on the achievable rate compared to
conventional CoMP systems.

</details>


### [6] [LEO Downlink Channel Model Revisited: Scattering Geometry-Inspired Derivation](https://arxiv.org/abs/2510.23900)
*Kuan-Po Chiu,Sumit Roy*

Main category: eess.SP

TL;DR: 本文提出了一种新的LEO到地面接收器信道模型推导方法，填补了现有技术中缺乏对非视距(NLOS)链路功率谱密度(PSD)的几何感知表征的空白。


<details>
  <summary>Details</summary>
Motivation: 现有技术缺乏对非视距(NLOS)链路功率谱密度(PSD)的适当几何感知表征，这是一个明显的空白。

Method: 从第一原理出发对PSD进行相干推导，能够重现先前技术的结果，并解释主要PSD特征与传播几何参数的因果关系。

Result: 新推导的模型能够重现先前技术的结果，并清晰地解释PSD主要特征与传播几何参数之间的因果关系。

Conclusion: 提出的新推导方法填补了现有技术的空白，为LEO到地面接收器信道建模提供了更准确的几何感知PSD表征。

Abstract: This paper presents a new derivation of LEO-to-ground receiver channel model
to address a clear gap in the prior art: the lack of an appropriate geometry
aware characterization of non LOS (NLOS) link model represented by the power
spectral density (PSD). Specifically, the main contribution is a coherent
derivation of the PSD from 1st principles that is able to reproduce results in
prior art and explain the causal relationship of main PSD features to the
propagation geometry parameters.

</details>


### [7] [Localized Acoustic-Event Measurement Probe: Connector Confirmation Utilizing Acoustic Signatures](https://arxiv.org/abs/2510.24017)
*Brian Skoglind,Travis Roberts,Sourabh Karmakar,Cameron Turner,Laine Mears*

Main category: eess.SP

TL;DR: 本研究提出了一种基于声学检测的电气连接状态验证系统，通过物理降噪和声音特征放大算法提高信噪比，有效检测电气连接器的连接状态。


<details>
  <summary>Details</summary>
Motivation: 现代消费产品中的电气连接在自动化装配线上仍需人工完成，手动连接容易出现松动或遗漏问题，导致设备停机和返工成本。现有视觉验证、增强现实或电路参数测量方法在检测连接状态方面能力有限。

Method: 采用物理系统进行背景噪声抑制，并使用成功配对噪声特征放大算法来提高电气连接器配对声音与工厂环境声音之间的信噪比。

Result: 该解决方案在检测和分类连接状态方面有效性超过75%，且无需对现有手动互连过程进行任何修改。

Conclusion: 声学验证系统可以有效检测电气连接状态，通过提高信噪比的方法解决了工厂环境中声音检测的挑战。

Abstract: Modern consumer products are full of interconnected electrical and electronic
modules to fulfill direct and indirect needs. In an automated assembly line
still, most of these interconnections are required to be done manually due to
the large variety of connector types, connector positions, and the soft,
flexible nature of their structures. The manual connection points are the
source of partial or completely loose connections. Sometimes connections are
missed due to the application of unequal mating forces and natural human
fatigue. Subsequently, these defects can lead to unexpected downtime and
expensive rework. For successful connection detection, past approaches such as
vision verification, Augmented Reality, or circuit parameter-based measurements
have shown limited ability to detect the correct connection state. Though most
connections emit a specific noise for successful mating, the acoustic-based
verification system for electrical connection confirmation has not been
extensively researched. The main discouraging reason for such research is the
typically low signal-to-noise ratio (SNR) between the sound of a pair of
electrical connector mating and the diverse soundscape of the plant. In this
study, the authors investigated increasing the SNR between the electrical
connector mating sound and the plant soundscape to improve connection success
detection by employing a physical system for background noise mitigation and
the successful met noise signature amplification algorithm. The solution is
over 75% effective at detecting and classifying connection state. The solution
has been constructed without any modification to the existing manual
interconnection process.

</details>


### [8] [PULSE: Privileged Knowledge Transfer from Electrodermal Activity to Low-Cost Sensors for Stress Monitoring](https://arxiv.org/abs/2510.24058)
*Zihan Zhao,Masood Mortazavi,Ning Yan*

Main category: eess.SP

TL;DR: PULSE框架通过自监督预训练利用EDA信号，但在推理时仅使用更易获取的ECG、BVP、ACC和TEMP模态，实现了低成本传感器的压力检测。


<details>
  <summary>Details</summary>
Motivation: EDA作为压力检测的主要信号需要昂贵硬件，而现实可穿戴设备往往不具备这种能力，因此需要开发能够在没有EDA的情况下进行准确压力检测的方法。

Method: 将编码器输出分为共享和私有嵌入，对齐跨模态的共享嵌入并融合为模态不变表示，私有嵌入携带模态特定信息支持重建目标。预训练后通过冻结的EDA教师模型将交感神经唤醒表示转移到学生编码器。

Result: 在WESAD数据集上实现了强大的压力检测性能，表明特权EDA的表示可以转移到低成本传感器以提高准确性。

Conclusion: 该方法能够在降低硬件成本的同时，通过知识转移实现准确的压力检测，为现实世界可穿戴设备提供了可行的解决方案。

Abstract: Electrodermal activity (EDA), the primary signal for stress detection,
requires costly hardware often unavailable in real-world wearables. In this
paper, we propose PULSE, a framework that utilizes EDA exclusively during
self-supervised pretraining, while enabling inference without EDA but with more
readily available modalities such as ECG, BVP, ACC, and TEMP. Our approach
separates encoder outputs into shared and private embeddings. We align shared
embeddings across modalities and fuse them into a modality-invariant
representation. The private embeddings carry modality-specific information to
support the reconstruction objective. Pretraining is followed by knowledge
transfer where a frozen EDA teacher transfers sympathetic-arousal
representations into student encoders. On WESAD, our method achieves strong
stress-detection performance, showing that representations of privileged EDA
can be transferred to low-cost sensors to improve accuracy while reducing
hardware cost.

</details>


### [9] [Towards actionable hypotension prediction- predicting catecholamine therapy initiation in the intensive care unit](https://arxiv.org/abs/2510.24287)
*Richard Koebe,Noah Saibel,Juan Miguel Lopez Alcaraz,Simon Schäfer,Nils Strodthoff*

Main category: eess.SP

TL;DR: 该研究开发了一个机器学习模型来预测ICU患者中儿茶酚胺治疗的启动，而不是传统的低血压预测。模型基于平均动脉压动态、治疗背景和患者特征，在预测儿茶酚胺启动方面表现优于基于固定MAP阈值的低血压预测。


<details>
  <summary>Details</summary>
Motivation: ICU患者低血压常见且危及生命，儿茶酚胺治疗是关键的干预步骤。现有机器学习模型主要预测基于固定MAP阈值的低血压，忽视了治疗升级的临床决策过程。预测儿茶酚胺启动能提供更具临床可操作性的目标。

Method: 使用MIMIC-III数据库，将儿茶酚胺启动建模为15分钟预测窗口内的二元事件。输入特征包括两小时滑动MAP窗口的统计描述符、人口统计学、生物特征、合并症和当前治疗。采用XGBoost模型训练，并通过SHAP进行解释。

Result: 模型AUROC达到0.822，显著优于基于MAP<65的低血压基线预测（AUROC 0.686）。SHAP分析显示近期MAP值、MAP趋势和当前治疗是主要预测因子。在男性、年轻患者、高BMI患者以及无合并症或并发用药的患者中表现更好。

Conclusion: 基于MAP动态、治疗背景和患者特征预测儿茶酚胺启动，支持治疗升级的关键决策，将重点从基于阈值的警报转向可操作的决策支持。该方法在自然事件不平衡的广泛ICU人群中可行。

Abstract: Hypotension in critically ill ICU patients is common and life-threatening.
Escalation to catecholamine therapy marks a key management step, with both
undertreatment and overtreatment posing risks. Most machine learning (ML)
models predict hypotension using fixed MAP thresholds or MAP forecasting,
overlooking the clinical decision behind treatment escalation. Predicting
catecholamine initiation, the start of vasoactive or inotropic agent
administration offers a more clinically actionable target reflecting real
decision-making. Using the MIMIC-III database, we modeled catecholamine
initiation as a binary event within a 15-minute prediction window. Input
features included statistical descriptors from a two-hour sliding MAP context
window, along with demographics, biometrics, comorbidities, and ongoing
treatments. An Extreme Gradient Boosting (XGBoost) model was trained and
interpreted via SHapley Additive exPlanations (SHAP). The model achieved an
AUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,
AUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP
trends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant
predictors. Subgroup analysis showed higher performance in males, younger
patients (<53 years), those with higher BMI (>32), and patients without
comorbidities or concurrent medications. Predicting catecholamine initiation
based on MAP dynamics, treatment context, and patient characteristics supports
the critical decision of when to escalate therapy, shifting focus from
threshold-based alarms to actionable decision support. This approach is
feasible across a broad ICU cohort under natural event imbalance. Future work
should enrich temporal and physiological context, extend label definitions to
include therapy escalation, and benchmark against existing hypotension
prediction systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [10] [Universal Inference for Testing Calibration of Mean Estimates within the Exponential Dispersion Family](https://arxiv.org/abs/2510.23821)
*Łukasz Delong,Mario Wüthrich*

Main category: stat.AP

TL;DR: 本文提出了一种在指数分散族中的子采样分割似然比检验方法，用于验证预测的均值校准，该方法提供有限样本保证和普遍有效的临界值，比传统LRT具有更高的检测错误校准的能力。


<details>
  <summary>Details</summary>
Motivation: 在金融和精算决策等领域，预测的均值校准验证至关重要。传统方法存在局限性，需要开发具有有限样本保证和普遍有效临界值的校准测试方法。

Method: 开发了指数分散族中的子采样分割似然比检验，结合了分割似然比和子采样技术，提出基于此的新测试统计量来提升校准测试性能。

Result: 数值分析表明，该方法在检测错误校准方面具有高功效，是传统LRT的有吸引力的替代方案。

Conclusion: 提出的子采样分割LRT方法为均值校准验证提供了有效的解决方案，具有有限样本保证和优越的检测能力。

Abstract: Calibration of mean estimates for predictions is a crucial property in many
applications, particularly in the fields of financial and actuarial
decision-making. In this paper, we first review classical approaches for
validating mean-calibration, and we discuss the Likelihood Ratio Test (LRT)
within the Exponential Dispersion Family (EDF). Then, we investigate the
framework of universal inference to test for mean-calibration. We develop a
sub-sampled split LRT within the EDF that provides finite sample guarantees
with universally valid critical values. We investigate type I error, power and
e-power of this sub-sampled split LRT, we compare it to the classical LRT, and
we propose a novel test statistics based on the sub-sampled split LRT to
enhance the performance of the calibration test. A numerical analysis verifies
that our proposal is an attractive alternative to the classical LRT achieving a
high power in detecting miscalibration.

</details>


### [11] [Machine Learning for the Production of Official Statistics: Density Ratio Estimation using Biased Transaction Data for Japanese labor statistics](https://arxiv.org/abs/2510.24153)
*Yuya Takada,Kiyoshi Izumi*

Main category: stat.AP

TL;DR: 本文展示了即使存在选择偏差的交易数据，通过应用机器学习中的密度比估计和协变量偏移监督学习概念，也能用于及时生成官方统计数据。以日本私营就业机构数据为例，可以提前发布关键劳动力市场指标。


<details>
  <summary>Details</summary>
Motivation: 国家统计机构开始使用非传统数据源（如销售点数据和手机GPS数据）来生成官方统计数据。这些数据具有显著增强官方统计有用性的潜力，但由于不是通过抽样调查方法收集而存在严重选择偏差。如果能够解决这种偏差，这些数据将成为官方统计的宝贵资源。

Method: 采用机器学习领域的密度比估计和协变量偏移监督学习概念，处理存在选择偏差的交易数据。以日本私营就业机构的数据作为案例研究，展示如何利用这些有偏数据生成初步统计数据。

Result: 研究表明，即使使用存在偏差的交易数据，也能及时生成官方统计数据。通过这种方法，可以提前发布关键劳动力市场指标，否则这些指标可能会延迟长达一年才发布，无法用于及时决策。

Conclusion: 即使存在选择偏差的非传统数据源，通过适当的机器学习方法处理，也能成为官方统计的有价值资源，显著扩大统计范围并提高决策质量，包括经济政策的制定。

Abstract: National statistical institutes are beginning to use non-traditional data
sources to produce official statistics. These sources, originally collected for
non-statistical purposes, include point-of-sales(POS) data and mobile phone
global positioning system(GPS) data. Such data have the potential to
significantly enhance the usefulness of official statistics. In the era of big
data, many private companies are accumulating vast amounts of transaction data.
Exploring how to leverage these data for official statistics is increasingly
important. However, progress has been slower than expected, mainly because such
data are not collected through sample-based survey methods and therefore
exhibit substantial selection bias. If this bias can be properly addressed,
these data could become a valuable resource for official statistics,
substantially expanding their scope and improving the quality of
decision-making, including economic policy. This paper demonstrates that even
biased transaction data can be useful for producing official statistics for
prompt release, by drawing on the concepts of density ratio estimation and
supervised learning under covariate shift, both developed in the field of
machine learning. As a case study, we show that preliminary statistics can be
produced in a timely manner using biased data from a Japanese private
employment agency. This approach enables the early release of a key labor
market indicator that would otherwise be delayed by up to a year, thereby
making it unavailable for timely decision-making.

</details>


### [12] [Streamlining business functions in official statistical production with Machine Learning](https://arxiv.org/abs/2510.24394)
*Sandra Barragán,Adrián Pérez-Bote,Carlos Sáez,David Salgado,Luis Sanguiao-Sande*

Main category: stat.AP

TL;DR: 本文描述了在官方统计生产过程中使用统计学习模型简化业务功能的试点和生产经验，旨在提高准确性、成本效益、及时性、粒度、减轻响应负担和频率。


<details>
  <summary>Details</summary>
Motivation: 通过统计学习模型改进官方统计生产过程的质量，包括准确性、成本效益、及时性、粒度、响应负担和频率等方面。

Method: 在西班牙统计局(INE)的真实调查数据上进行了试点实验，采用统计学习模型来简化业务功能。

Result: 试点经验表明该方法在多个质量维度上具有改进潜力。

Conclusion: 统计学习模型在官方统计生产过程中具有应用价值，能够有效提升统计业务的质量和效率。

Abstract: We provide a description of pilot and production experiences to streamline
some business functions in the official statistical production process using
statistical learning models. Our approach is quality-oriented searching for an
improvement on accuracy, cost-efficiency, timeliness, granularity, response
burden reduction, and frequency. Pilot experiences have been conducted with
data from real surveys in Statistics Spain (INE).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.23617)
*Phuong Q. Dao,Mark Roantree,Vuong M. Ngo*

Main category: cs.LG

TL;DR: 该论文提出了BERT-ViT-EF模型及其扩展DTCN，用于多模态情感分析。DTCN通过早期融合策略结合BERT和ViT编码器，并引入对比学习来对齐文本和图像表示，在两个基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析通过联合分析文本和图像数据，能够比单模态方法提供更丰富和准确的情感理解。现有方法在跨模态交互和联合表示学习方面仍有改进空间。

Method: 首先提出BERT-ViT-EF模型，使用Transformer编码器（BERT用于文本，ViT用于图像）通过早期融合策略实现跨模态交互。然后提出DTCN扩展，在BERT后添加额外的Transformer编码器层来优化文本上下文，并采用对比学习来对齐文本和图像表示。

Result: 在MVSA-Single和TumEmo两个基准数据集上的实验结果显示，DTCN在TumEmo上取得了最佳准确率（78.4%）和F1分数（78.3%），在MVSA-Single上也表现出色，准确率为76.6%，F1分数为75.9%。

Conclusion: 研究表明，在基于Transformer的多模态情感分析中，早期融合和更深层次的上下文建模能够显著提升模型性能，验证了所提出方法的有效性。

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by
jointly analyzing data from multiple modalities typically text and images
offering a richer and more accurate interpretation than unimodal approaches. In
this paper, we first propose BERT-ViT-EF, a novel model that combines powerful
Transformer-based encoders BERT for textual input and ViT for visual input
through an early fusion strategy. This approach facilitates deeper cross-modal
interactions and more effective joint representation learning. To further
enhance the model's capability, we propose an extension called the Dual
Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN
incorporates an additional Transformer encoder layer after BERT to refine
textual context (before fusion) and employs contrastive learning to align text
and image representations, fostering robust multimodal feature learning.
Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo
demonstrate the effectiveness of our approach. DTCN achieves best accuracy
(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on
MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements
highlight the benefits of early fusion and deeper contextual modeling in
Transformer-based multimodal sentiment analysis.

</details>


### [14] [Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields](https://arxiv.org/abs/2510.23621)
*Alexandre Benoit*

Main category: cs.LG

TL;DR: 该论文研究了如何通过降低精度算术和GPU优化内核来加速SO(3)-等变机器学习力场MACE，同时保持物理保真度。


<details>
  <summary>Details</summary>
Motivation: 机器学习力场虽然能提供精确的分子动力学模拟，但计算成本高昂。目前缺乏关于降低精度和GPU优化是否会影响物理保真度的系统性证据。

Method: 通过端到端和逐块分析MACE性能，比较e3nn和cuEquivariance后端，评估FP64/FP32/BF16/FP16精度设置，在可重复稳态时序下进行推理、短NVT和长NPT水模拟以及玩具训练运行。

Result: cuEquivariance将推理延迟降低约3倍，仅将线性层转换为BF16/FP16可获得额外4倍加速，NVT/NPT MD中的能量和热力学观测值保持在运行间变异性范围内。训练中使用半精度权重会降低力RMSE。

Conclusion: 融合等变内核和混合精度推理可以显著加速最先进的力场，对下游MD影响可忽略。建议默认使用cuEquivariance和FP32，为线性层启用BF16/FP16以获得最大吞吐量，训练保持FP32精度。

Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at
high computational cost. For SO(3)-equivariant models such as MACE, there is
little systematic evidence on whether reduced-precision arithmetic and
GPU-optimized kernels can cut this cost without harming physical fidelity. This
thesis aims to make MACE cheaper and faster while preserving accuracy by
identifying computational bottlenecks and evaluating low-precision execution
policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA
cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32
accumulation) for inference, short NVT and long NPT water simulations, and toy
training runs under reproducible, steady-state timing. cuEquivariance reduces
inference latency by about $3\times$. Casting only linear layers to BF16/FP16
within an FP32 model yields roughly 4x additional speedups, while energies and
thermodynamic observables in NVT/NPT MD remain within run-to-run variability.
Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq
modules without explicit adapters causes representation mismatches. Fused
equivariant kernels and mixed-precision inference can substantially accelerate
state-of-the-art force fields with negligible impact on downstream MD. A
practical policy is to use cuEquivariance with FP32 by default and enable
BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum
throughput, while training remains in FP32. Further gains are expected on
Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and
pipeline fusion.

</details>


### [15] [Adversarially-Aware Architecture Design for Robust Medical AI Systems](https://arxiv.org/abs/2510.23622)
*Alyssa Gerhart,Balaji Iyangar*

Main category: cs.LG

TL;DR: 本文研究了医疗AI系统中的对抗性攻击风险，通过在皮肤病数据集上的实证实验，展示了对抗性方法显著降低分类准确性的问题，并评估了防御措施的有效性。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击对医疗AI系统构成严重威胁，可能导致危险的误分类，延误治疗或误诊，特别是在服务不足的人群中威胁患者安全。

Method: 通过详细的威胁建模、实验基准测试和模型评估，在皮肤病数据集上进行实证实验，测试对抗性训练和蒸馏等防御方法。

Result: 防御措施虽然降低了攻击成功率，但需要在对抗性攻击防御和干净数据上的模型性能之间取得平衡。

Conclusion: 呼吁采用集成的技术、伦理和政策方法，构建更具弹性和公平性的医疗AI系统。

Abstract: Adversarial attacks pose a severe risk to AI systems used in healthcare,
capable of misleading models into dangerous misclassifications that can delay
treatments or cause misdiagnoses. These attacks, often imperceptible to human
perception, threaten patient safety, particularly in underserved populations.
Our study explores these vulnerabilities through empirical experimentation on a
dermatological dataset, where adversarial methods significantly reduce
classification accuracy. Through detailed threat modeling, experimental
benchmarking, and model evaluation, we demonstrate both the severity of the
threat and the partial success of defenses like adversarial training and
distillation. Our results show that while defenses reduce attack success rates,
they must be balanced against model performance on clean data. We conclude with
a call for integrated technical, ethical, and policy-based approaches to build
more resilient, equitable AI in healthcare.

</details>


### [16] [DiNo and RanBu: Lightweight Predictions from Shallow Random Forests](https://arxiv.org/abs/2510.23624)
*Tiago Mendonça dos Santos,Rafael Izbicki,Luís Gustavo Esteves*

Main category: cs.LG

TL;DR: 该论文提出了DiNo和RanBu两种浅层森林方法，通过将少量深度受限的树转换为高效的距离加权预测器，显著降低了随机森林的推理延迟和内存需求，同时保持或提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 随机森林集成在表格预测任务中表现优异，但依赖数百棵深度树导致高推理延迟和内存需求，限制了在延迟敏感或资源受限环境中的部署。

Method: DiNo通过观测对的最远共同祖先测量同源距离，RanBu对Breiman经典邻近度度量应用核平滑。两种方法在森林训练后完全操作，无需额外生长树，仅需轻量级矩阵向量操作调整单个带宽参数h。

Result: 在3个合成基准和25个公共数据集上，RanBu匹配或超过了全深度随机森林的精度（特别是在高噪声设置中），同时将训练加推理时间减少了高达95%。DiNo在低噪声机制中以适度的计算成本实现了最佳偏差-方差权衡。两种方法可直接扩展到分位数回归，在保持精度的同时获得显著速度提升。

Conclusion: DiNo和RanBu提供了高效的浅层森林替代方案，显著降低了计算成本，同时保持或提升了预测性能，特别适用于延迟敏感和资源受限的环境。

Abstract: Random Forest ensembles are a strong baseline for tabular prediction tasks,
but their reliance on hundreds of deep trees often results in high inference
latency and memory demands, limiting deployment in latency-sensitive or
resource-constrained environments. We introduce DiNo (Distance with Nodes) and
RanBu (Random Bushes), two shallow-forest methods that convert a small set of
depth-limited trees into efficient, distance-weighted predictors. DiNo measures
cophenetic distances via the most recent common ancestor of observation pairs,
while RanBu applies kernel smoothing to Breiman's classical proximity measure.
Both approaches operate entirely after forest training: no additional trees are
grown, and tuning of the single bandwidth parameter $h$ requires only
lightweight matrix-vector operations. Across three synthetic benchmarks and 25
public datasets, RanBu matches or exceeds the accuracy of full-depth random
forests-particularly in high-noise settings-while reducing training plus
inference time by up to 95\%. DiNo achieves the best bias-variance trade-off in
low-noise regimes at a modest computational cost. Both methods extend directly
to quantile regression, maintaining accuracy with substantial speed gains. The
implementation is available as an open-source R/C++ package at
https://github.com/tiagomendonca/dirf. We focus on structured tabular random
samples (i.i.d.), leaving extensions to other modalities for future work.

</details>


### [17] [Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling](https://arxiv.org/abs/2510.23631)
*Yuxuan Tang,Yifan Feng*

Main category: cs.LG

TL;DR: RCPO是一个统一框架，通过最大似然估计将偏好优化与（排名）选择建模相结合，支持基于效用和基于排名的选择模型，能够利用多选和排名反馈数据，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法主要依赖成对偏好优化，忽略了从更丰富的人类反馈形式（如多选比较和top-k排名）中学习的机会。

Method: 提出排名选择偏好优化（RCPO）框架，基于最大似然估计，支持基于效用和基于排名的选择模型，包含Multinomial Logit和Mallows-RMJ两种代表性排名选择模型。

Result: 在Llama-3-8B-Instruct和Gemma-2-9B-it模型上的实证研究表明，RCPO在AlpacaEval 2和Arena-Hard基准测试中始终优于竞争基线方法。

Conclusion: RCPO展示了如何直接利用排名偏好数据，结合适当的选择模型，实现更有效的对齐，为将（排名）选择建模融入LLM训练提供了通用且可扩展的基础。

Abstract: Alignment of large language models (LLMs) has predominantly relied on
pairwise preference optimization, where annotators select the better of two
responses to a prompt. While simple, this approach overlooks the opportunity to
learn from richer forms of human feedback, such as multiwise comparisons and
top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a
unified framework that bridges preference optimization with (ranked) choice
modeling via maximum likelihood estimation. The framework is flexible,
supporting both utility-based and rank-based choice models. It subsumes several
existing pairwise methods (e.g., DPO, SimPO), while providing principled
training objectives for richer feedback formats. We instantiate this framework
with two representative ranked choice models (Multinomial Logit and
Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across
AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms
competitive baselines. RCPO shows how directly leveraging ranked preference
data, combined with the right choice models, yields more effective alignment.
It offers a versatile and extensible foundation for incorporating (ranked)
choice modeling into LLM training.

</details>


### [18] [Monotone and Separable Set Functions: Characterizations and Neural Models](https://arxiv.org/abs/2510.23634)
*Soutrik Sarangi,Yonatan Sverdlov,Nadav Dym,Abir De*

Main category: cs.LG

TL;DR: 该论文研究了单调分离集函数的设计，这类函数能保持集合包含关系的偏序结构，即S⊆T当且仅当F(S)≤F(T)。在无限基础集的情况下，证明了MAS函数不存在，但提出了一个弱MAS模型，并展示了其在集合包含任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 受集合包含问题应用的启发，研究如何设计能保持集合偏序关系的集合到向量函数，即满足S⊆T当且仅当F(S)≤F(T)的函数。

Method: 建立了MAS函数向量维度需求的下界和上界。在无限基础集情况下，提出了弱MAS模型，该模型具有Holder连续性稳定性，并可用于构建能近似所有单调集函数的通用模型。

Result: 在无限基础集上证明了MAS函数不存在，但提出的弱MAS模型在实验中表现出色，在集合包含任务上优于未包含集合包含归纳偏置的标准集合模型。

Conclusion: MAS函数为集合包含问题提供了理论基础，弱MAS模型在实际应用中有效，代码已在GitHub开源。

Abstract: Motivated by applications for set containment problems, we consider the
following fundamental problem: can we design set-to-vector functions so that
the natural partial order on sets is preserved, namely $S\subseteq T \text{ if
and only if } F(S)\leq F(T) $. We call functions satisfying this property
Monotone and Separating (MAS) set functions. % We establish lower and upper
bounds for the vector dimension necessary to obtain MAS functions, as a
function of the cardinality of the multisets and the underlying ground set. In
the important case of an infinite ground set, we show that MAS functions do not
exist, but provide a model called our which provably enjoys a relaxed MAS
property we name "weakly MAS" and is stable in the sense of Holder continuity.
We also show that MAS functions can be used to construct universal models that
are monotone by construction and can approximate all monotone set functions.
Experimentally, we consider a variety of set containment tasks. The experiments
show the benefit of using our our model, in comparison with standard set models
which do not incorporate set containment as an inductive bias. Our code is
available in https://github.com/yonatansverdlov/Monotone-Embedding.

</details>


### [19] [Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning](https://arxiv.org/abs/2510.23635)
*Andrea Bontempelli,Matteo Busso,Leonardo Javier Malcotti,Fausto Giunchiglia*

Main category: cs.LG

TL;DR: 本研究评估了Skeptical Learning (SKEL)在真实用户环境下的表现，通过大学生使用iLog移动应用四周的实验，发现SKEL能减少标注工作量并提高数据质量，但需要在用户努力和数据质量间找到平衡。


<details>
  <summary>Details</summary>
Motivation: 数字个人助理需要高质量标注，但用户标注常存在错误和噪声。先前SKEL研究通过比较离线主动标注与被动数据评估标注准确性，但缺乏最终用户确认，而用户是自身情境的最佳判断者。

Method: 在真实世界条件下评估SKEL性能，让实际用户基于当前视角和需求精炼输入标签。研究涉及大学生在其设备上使用iLog移动应用，持续四周时间。

Result: 结果突显了在用户努力和数据质量间找到适当平衡的挑战，以及使用SKEL的潜在好处，包括减少标注工作量和提高收集数据质量。

Conclusion: SKEL在真实用户环境中能有效减少标注工作量并提升数据质量，但需要仔细权衡用户参与度与数据质量之间的关系。

Abstract: Any digital personal assistant, whether used to support task performance,
answer questions, or manage work and daily life, including fitness schedules,
requires high-quality annotations to function properly. However, user
annotations, whether actively produced or inferred from context (e.g., data
from smartphone sensors), are often subject to errors and noise. Previous
research on Skeptical Learning (SKEL) addressed the issue of noisy labels by
comparing offline active annotations with passive data, allowing for an
evaluation of annotation accuracy. However, this evaluation did not include
confirmation from end-users, the best judges of their own context. In this
study, we evaluate SKEL's performance in real-world conditions with actual
users who can refine the input labels based on their current perspectives and
needs. The study involves university students using the iLog mobile application
on their devices over a period of four weeks. The results highlight the
challenges of finding the right balance between user effort and data quality,
as well as the potential benefits of using SKEL, which include reduced
annotation effort and improved quality of collected data.

</details>


### [20] [A Physics-informed Multi-resolution Neural Operator](https://arxiv.org/abs/2510.23810)
*Sumanta Roy,Bahador Bahmani,Ioannis G. Kevrekidis,Michael D. Shields*

Main category: cs.LG

TL;DR: 提出了一种物理信息驱动的算子学习方法，将分辨率无关神经算子框架扩展到完全无数据设置，解决了训练数据不足和分辨率不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 在真实工程应用中，获取大量高质量训练数据具有挑战性，且数据集可能在不同实现中具有不均匀的离散化，网格分辨率在样本间变化。

Method: 将任意离散化的输入函数投影到潜在嵌入空间，使用预训练的基函数。通过多层感知机近似偏微分方程算子，在物理空间通过有限差分求解器强制执行PDE约束。

Result: 在多个具有多分辨率数据的数值示例上验证了方法的性能，包括粗粒度和细粒度离散化的输入函数采样。

Conclusion: 该方法能够有效处理多分辨率数据，在无数据设置下实现物理信息驱动的算子学习。

Abstract: The predictive accuracy of operator learning frameworks depends on the
quality and quantity of available training data (input-output function pairs),
often requiring substantial amounts of high-fidelity data, which can be
challenging to obtain in some real-world engineering applications. These
datasets may be unevenly discretized from one realization to another, with the
grid resolution varying across samples. In this study, we introduce a
physics-informed operator learning approach by extending the Resolution
Independent Neural Operator (RINO) framework to a fully data-free setup,
addressing both challenges simultaneously. Here, the arbitrarily (but
sufficiently finely) discretized input functions are projected onto a latent
embedding space (i.e., a vector space of finite dimensions), using pre-trained
basis functions. The operator associated with the underlying partial
differential equations (PDEs) is then approximated by a simple multi-layer
perceptron (MLP), which takes as input a latent code along with spatiotemporal
coordinates to produce the solution in the physical space. The PDEs are
enforced via a finite difference solver in the physical space. The validation
and performance of the proposed method are benchmarked on several numerical
examples with multi-resolution data, where input functions are sampled at
varying resolutions, including both coarse and fine discretizations.

</details>


### [21] [Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning](https://arxiv.org/abs/2510.24356)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 提出感知学习（PeL）范式，将智能体的感知接口优化与下游决策学习解耦，使用任务无关信号直接优化感知属性如稳定性、信息性和几何控制。


<details>
  <summary>Details</summary>
Motivation: 传统方法将感知和决策耦合优化，导致感知质量难以评估和保证。PeL旨在分离感知学习，使其独立于特定任务目标，从而获得更鲁棒和通用的感知能力。

Method: 定义感知接口f_φ和决策函数g_θ的分离框架，使用任务无关信号优化感知属性，提出表示不变性指标评估感知质量，并证明PeL更新与贝叶斯任务风险梯度正交。

Result: 建立了感知学习的理论框架，提供了任务无关的评估指标来认证感知质量，证明了感知学习更新与决策学习更新的正交性。

Conclusion: PeL范式为构建更鲁棒、可解释的感知系统提供了理论基础，实现了感知学习与决策学习的有效解耦。

Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.

</details>


### [22] [Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs](https://arxiv.org/abs/2510.23650)
*Wei Xia*

Main category: cs.LG

TL;DR: 提出了两种零样本logits层去偏方法：Static和Dynamic，其中Dynamic方法能减少高达70%的偏见，同时保持最小的流畅性损失。


<details>
  <summary>Details</summary>
Motivation: 开发有效的去偏方法，特别是针对已对齐的大型语言模型，以解决模型输出中的偏见问题。

Method: 使用语义感知的logits干预方法，在logits层进行去偏处理，而不是在隐藏层。

Result: Dynamic方法在减少偏见方面表现最佳，偏见减少达70%，且流畅性损失最小；logits干预方法优于隐藏层方法。

Conclusion: 语义感知的logits干预是一种稳定且有效的去偏方法，特别适用于已对齐的大型语言模型。

Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing
methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits
intervention outperforms hidden-layer approaches. We show semantic-aware logits
intervention is stable and effective for debiasing aligned LLMs.

</details>


### [23] [The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models](https://arxiv.org/abs/2510.23652)
*Yao Lu,Yuqi Li,Wenbin Xie,Shanqing Yu,Qi Xuan,Zhaowei Zhu,Shiping Wen*

Main category: cs.LG

TL;DR: CLP是一个连续层剪枝框架，通过可微凹门算法自动识别最佳连续层段进行剪枝，并使用截止端点调优策略恢复模型性能，在多个LLM模型上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临模型尺寸大、计算成本高的问题。现有层剪枝方法依赖人工指标评估单个层，忽略了层间依赖关系，会破坏信息流并严重降低性能。

Method: 提出CLP框架：1）可微凹门算法通过基于梯度的优化自动识别最佳连续层段进行剪枝；2）截止端点调优策略通过微调剪枝段相邻层来有效恢复模型性能。

Result: 在多个模型架构（LLaMA2、LLaMA3、Qwen）和尺寸（7B到70B参数）上的实验表明，CLP显著优于现有基线方法。在20%剪枝率下，LLaMA3-70B平均性能保持率达到95.34%，比基线方法高出4.29%-30.52%。

Conclusion: CLP能有效减少LLM的计算开销，同时保持高性能，并可无缝与量化技术结合进一步压缩模型，仅带来轻微性能损失。

Abstract: Although large language models (LLMs) have achieved revolutionary
breakthroughs in many fields, their large model size and high computational
cost pose significant challenges for practical deployment on
resource-constrained edge devices. To this end, layer pruning has been proposed
to reduce the computational overhead by directly removing redundant layers.
However, existing layer pruning methods typically rely on hand-crafted metrics
to evaluate and remove individual layers, while ignoring the dependencies
between layers. This can disrupt the model's information flow and severely
degrade performance. To address these issues, we propose CLP, a novel
continuous layer pruning framework that introduces two key innovations: a
differentiable concave gate algorithm that automatically identifies the best
continuous layer segments for pruning via gradient-based optimization; and a
cutoff endpoint tuning strategy that effectively restores model performance by
fine-tuning only the layers adjacent to the pruned segments. Extensive
experiments across multiple model architectures (including LLaMA2, LLaMA3 and
Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly
outperforms existing state-of-the-art baselines. For example, at a pruning rate
of $20\%$, CLP achieves an average performance retention of $95.34\%$ on
LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can
be seamlessly combined with quantization to further compress the model with
only a slight performance loss.

</details>


### [24] [A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops](https://arxiv.org/abs/2510.23657)
*Saklain Niam,Tashfiqur Rahman,Md. Amjad Patwary,Mukarram Hossain*

Main category: cs.LG

TL;DR: 该研究开发了首个机器学习框架来预测冷等离子体处理下五种作物的发芽提升效果，其中Extra Trees模型表现最佳，并揭示了电压和处理时间的激素效应响应模式。


<details>
  <summary>Details</summary>
Motivation: 冷等离子体是一种环保的促进种子发芽的方法，但由于复杂的种子-等离子体-环境相互作用，结果难以预测，因此需要开发预测性工具来优化处理参数。

Method: 使用梯度提升(GB)、极限梯度提升(XGB)、极端随机树(ET)及其混合模型等机器学习方法，在介电阻挡放电等离子体条件下预测大豆、大麦、向日葵、萝卜和番茄的发芽提升。

Result: Extra Trees模型表现最佳(R²=0.919)，特征减少后提升至R²=0.925。工程分析显示激素效应响应：<7kV或<200s效果可忽略，7-15kV/200-500s发芽率最大，>20kV或长时间处理发芽率降低。放电功率是主导因素，≥100W配合短时间处理效果最佳。

Conclusion: 该框架成功预测了不同物种和品种的发芽响应，并集成到MLflow中，为精准农业中优化冷等离子体种子发芽提供了决策支持工具。

Abstract: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet
outcomes remain difficult to predict due to complex seed--plasma--environment
interactions. This study introduces the first machine learning framework to
forecast germination uplift in soybean, barley, sunflower, radish, and tomato
under dielectric barrier discharge (DBD) plasma. Among the models tested (GB,
XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} =
0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925
after feature reduction. Engineering analysis revealed a hormetic response:
negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for
200--500 s, and reduced germination beyond 20 kV or prolonged exposures.
Discharge power was also a dominant factor, with germination rate maximizing at
$\geq$100 W with low exposure time. Species and cultivar-level predictions
showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high
consistency, while sunflower remained slightly higher variable (MAE = 3.80).
Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,
while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively
poorly captured. This framework was also embedded into MLflow, providing a
decision-support tool for optimizing CP seed germination in precision
agriculture.

</details>


### [25] [Transformers from Compressed Representations](https://arxiv.org/abs/2510.23665)
*Juan C. Leon Alcazar,Mattia Soldan,Mohammad Saatialsoruji,Alejandro Pardo,Hani Itani,Juan Camilo Perez,Bernard Ghanem*

Main category: cs.LG

TL;DR: TEMPEST是一种利用压缩文件固有字节流结构进行表示学习的方法，通过紧凑编码使标准transformer能够直接从压缩数据流学习语义表示，无需原始字节处理或完整媒体解码。


<details>
  <summary>Details</summary>
Motivation: 压缩文件格式是高效数据存储和传输的基础，但其在表示学习方面的潜力尚未充分探索。

Method: 利用压缩文件的字节流结构设计有效的标记化和编码策略，使标准transformer能够直接从压缩数据流学习语义表示。

Result: 在多个数据集、编码方案和模态上的实验表明，TEMPEST在保持与最先进方法竞争性的准确率的同时，显著降低了内存和计算需求。

Conclusion: 该方法显著减少了语义分类所需的标记数量，降低了计算复杂度和内存使用，实现了效率提升。

Abstract: Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.

</details>


### [26] [Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems](https://arxiv.org/abs/2510.23668)
*Fujiang Yuan,Yangrui Fan,Xiaohuan Bing,Zhen Tian,Chunhong Yuan,Yankang Li*

Main category: cs.LG

TL;DR: 该研究提出了一种基于STL分解的混合框架，结合LSTM、ARIMA和XGBoost三种模型来预测交通流量，通过分解时间序列并让各模型专注于不同时间特征，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 单一模型难以捕捉交通流量数据中复杂的非线性、多尺度时间模式，需要开发更有效的混合预测方法。

Method: 使用STL将时间序列分解为趋势、季节性和残差三个分量，分别用LSTM建模长期趋势、ARIMA捕捉季节周期性、XGBoost预测非线性残差波动，最后通过乘法集成得到最终预测。

Result: 基于纽约市交叉口998条交通流量记录，LSTM-ARIMA-XGBoost混合模型在MAE、RMSE和R平方指标上显著优于单独的LSTM、ARIMA和XGBoost模型。

Conclusion: 分解策略能有效分离时间特征，使各模型专业化，从而提高了预测精度、可解释性和鲁棒性。

Abstract: Accurate traffic flow forecasting is essential for intelligent transportation
systems and urban traffic management. However, single model approaches often
fail to capture the complex, nonlinear, and multi scale temporal patterns in
traffic flow data. This study proposes a decomposition driven hybrid framework
that integrates Seasonal Trend decomposition using Loess (STL) with three
complementary predictive models. STL first decomposes the original time series
into trend, seasonal, and residual components. Then, a Long Short Term Memory
(LSTM) network models long term trends, an Autoregressive Integrated Moving
Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient
Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The
final forecast is obtained through multiplicative integration of the sub model
predictions. Using 998 traffic flow records from a New York City intersection
between November and December 2015, results show that the LSTM ARIMA XGBoost
hybrid model significantly outperforms standalone models including LSTM, ARIMA,
and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy
effectively isolates temporal characteristics, allowing each model to
specialize, thereby improving prediction accuracy, interpretability, and
robustness.

</details>


### [27] [Explaining Robustness to Catastrophic Forgetting Through Incremental Concept Formation](https://arxiv.org/abs/2510.23756)
*Nicki Barari,Edward Kim,Christopher MacLellan*

Main category: cs.LG

TL;DR: 该研究探讨了Cobweb/4V模型在持续学习中抵抗灾难性遗忘的机制，通过对比实验验证了三个假设：自适应结构重组、稀疏选择性更新和信息理论学习的优势。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是持续学习中的核心挑战，研究旨在探索Cobweb/4V模型抵抗遗忘的机制，为构建稳定自适应的持续学习系统提供理论支持。

Method: 通过比较Cobweb/4V与神经基线模型（包括新提出的CobwebNN），在MNIST、Fashion-MNIST、MedMNIST和CIFAR-10等数据集上进行实验，验证三个假设。

Result: 实验表明自适应结构重组增强了学习可塑性，稀疏更新减少了干扰，信息理论学习过程无需回顾历史数据即可保留先验知识。

Conclusion: 这些发现揭示了缓解灾难性遗忘的机制，突显了基于概念和信息理论方法在构建稳定自适应持续学习系统中的潜力。

Abstract: Catastrophic forgetting remains a central challenge in continual learning,
where models are required to integrate new knowledge over time without losing
what they have previously learned. In prior work, we introduced Cobweb/4V, a
hierarchical concept formation model that exhibited robustness to catastrophic
forgetting in visual domains. Motivated by this robustness, we examine three
hypotheses regarding the factors that contribute to such stability: (1)
adaptive structural reorganization enhances knowledge retention, (2) sparse and
selective updates reduce interference, and (3) information-theoretic learning
based on sufficiency statistics provides advantages over gradient-based
backpropagation. To test these hypotheses, we compare Cobweb/4V with neural
baselines, including CobwebNN, a neural implementation of the Cobweb framework
introduced in this work. Experiments on datasets of varying complexity (MNIST,
Fashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring
enhances learning plasticity, sparse updates help mitigate interference, and
the information-theoretic learning process preserves prior knowledge without
revisiting past data. Together, these findings provide insight into mechanisms
that can mitigate catastrophic forgetting and highlight the potential of
concept-based, information-theoretic approaches for building stable and
adaptive continual learning systems.

</details>


### [28] [ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning](https://arxiv.org/abs/2510.23818)
*Yilang Zhang,Xiaodong Yang,Yiwei Cai,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文提出了一种新的LoRA优化方法，通过累积连续的低秩增量来构建高秩权重更新，从而克服传统LoRA方法的限制，提高性能并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的不断扩大，计算开销成为任务特定微调的主要瓶颈。虽然低秩适应(LoRA)通过将权重更新限制在低维子空间来有效控制成本，但这种限制会阻碍有效性和减缓收敛速度。

Method: 通过累积连续的低秩增量来逐步构建高秩权重更新。具体来说，识别每个更新步骤中的最优低秩矩阵，以最小化损失函数并紧密逼近全参数微调。为了在不重启的情况下实现高效无缝优化，通过适当缩放原始低秩矩阵的列来形成最优选择。

Result: 在规模高达120亿参数的流行LLMs上进行的广泛数值测试表明，在自然语言理解、常识推理和数学问题解决等多样化任务上，相对于最先进的LoRA变体，该方法实现了持续的性能提升和快速收敛。

Conclusion: 该方法通过累积低秩增量构建高秩权重更新的策略，有效解决了传统LoRA方法的局限性，在保持计算效率的同时显著提升了模型性能和收敛速度。

Abstract: As large language models (LLMs) continue to scale in size, the computational
overhead has become a major bottleneck for task-specific fine-tuning. While
low-rank adaptation (LoRA) effectively curtails this cost by confining the
weight updates to a low-dimensional subspace, such a restriction can hinder
effectiveness and slow convergence. This contribution deals with these
limitations by accumulating progressively a high-rank weight update from
consecutive low-rank increments. Specifically, the per update optimal low-rank
matrix is identified to minimize the loss function and closely approximate full
fine-tuning. To endow efficient and seamless optimization without restarting,
this optimal choice is formed by appropriately scaling the columns of the
original low-rank matrix. Rigorous performance guarantees reveal that the
optimal scaling can be found analytically. Extensive numerical tests with
popular LLMs scaling up to 12 billion parameters demonstrate a consistent
performance gain and fast convergence relative to state-of-the-art LoRA
variants on diverse tasks including natural language understanding, commonsense
reasoning, and mathematical problem solving.

</details>


### [29] [Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers](https://arxiv.org/abs/2510.23912)
*Marko Karbevski,Antonij Mijoski*

Main category: cs.LG

TL;DR: 研究表明在注意力机制中，Query权重是冗余的，可以移除而不影响模型性能，从而减少8%以上的非嵌入参数。


<details>
  <summary>Details</summary>
Motivation: 探索当前LLM中注意力机制的Query、Key、Value权重三元组是否可以被简化，以减少模型参数数量。

Method: 在理论分析基础上，使用完整的GPT-3小型架构（包含层归一化、跳跃连接和权重衰减）进行从头训练验证。

Result: 简化后的模型（移除Query权重）在验证损失上达到了与标准基线相当的性能。

Conclusion: Query权重在注意力机制中是冗余的，这一发现在大规模模型中的适用性值得进一步研究。

Abstract: The Query, Key, Value weight triplet is a building block of current attention
mechanisms in state-of-the-art LLMs. We theoretically investigate whether this
triplet can be reduced, proving under simplifying assumptions that the Query
weights are redundant, thereby reducing the number of non-embedding/lm-head
parameters by over 8%. We validate the theory on full-complexity GPT-3 small
architectures (with layer normalization, skip connections, and weight decay)
trained from scratch, demonstrating that the reduced model achieves comparable
validation loss to standard baselines. These findings motivate the
investigation of the Query weight redundancy at scale.

</details>


### [30] [Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs](https://arxiv.org/abs/2510.23914)
*Arsenii Mustafin,Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 将折扣奖励MDP的几何解释扩展到平均奖励情况，统一了两种分析框架，证明了在唯一且遍历的最优策略下值迭代算法具有几何收敛率。


<details>
  <summary>Details</summary>
Motivation: MDP的理论分析通常分为平均奖励和折扣奖励两种情况，虽然相似但分开分析。本文旨在统一这两种情况的分析框架。

Method: 将折扣奖励MDP的几何解释扩展到平均奖励情况，建立统一的几何分析框架。

Result: 成功将折扣奖励情况下的主要结果扩展到平均奖励情况：在唯一且遍历的最优策略下，值迭代算法具有几何收敛率。

Conclusion: 通过几何解释的统一框架，实现了平均奖励和折扣奖励MDP分析的整合，为值迭代算法的收敛性分析提供了统一的理论基础。

Abstract: The theoretical analysis of Markov Decision Processes (MDPs) is commonly
split into two cases - the average-reward case and the discounted-reward case -
which, while sharing similarities, are typically analyzed separately. In this
work, we extend a recently introduced geometric interpretation of MDPs for the
discounted-reward case to the average-reward case, thereby unifying both. This
allows us to extend a major result known for the discounted-reward case to the
average-reward case: under a unique and ergodic optimal policy, the Value
Iteration algorithm achieves a geometric convergence rate.

</details>


### [31] [Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments](https://arxiv.org/abs/2510.23931)
*Miguel Fernandez-de-Retana,Unai Zulaika,Rubén Sánchez-Corcuera,Aitor Almeida*

Main category: cs.LG

TL;DR: 本文研究了在联邦学习中，差分隐私机制（DP-SGD和PDP-SGD）作为防御梯度泄漏攻击的有效性，发现DP-SGD能显著降低攻击风险但会牺牲模型性能，而PDP-SGD保持良好性能但防御效果不佳。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护敏感数据不直接共享，但仍面临梯度泄漏攻击的风险，需要评估差分隐私机制在实际防御中的效果。

Method: 在模拟联邦学习环境中，评估不同隐私级别下计算机视觉模型的性能，并分析从截获梯度中重建私有数据的质量。

Result: DP-SGD显著减轻了梯度泄漏攻击风险，但模型效用有适度折衷；PDP-SGD保持强分类性能，但无法有效防御重建攻击。

Conclusion: 需要超越理论保证，经验性地评估隐私机制在分布式学习场景中的实际效果，因为信息泄漏可能对数据安全和隐私构成严重威胁。

Abstract: Federated Learning (FL) allows for the training of Machine Learning models in
a collaborative manner without the need to share sensitive data. However, it
remains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private
information from the shared model updates. In this work, we investigate the
effectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD
and a variant based on explicit regularization (PDP-SGD) - as defenses against
GLAs. To this end, we evaluate the performance of several computer vision
models trained under varying privacy levels on a simple classification task,
and then analyze the quality of private data reconstructions obtained from the
intercepted gradients in a simulated FL environment. Our results demonstrate
that DP-SGD significantly mitigates the risk of gradient leakage attacks,
albeit with a moderate trade-off in model utility. In contrast, PDP-SGD
maintains strong classification performance but proves ineffective as a
practical defense against reconstruction attacks. These findings highlight the
importance of empirically evaluating privacy mechanisms beyond their
theoretical guarantees, particularly in distributed learning scenarios where
information leakage may represent an unassumable critical threat to data
security and privacy.

</details>


### [32] [ChessQA: Evaluating Large Language Models for Chess Understanding](https://arxiv.org/abs/2510.23948)
*Qianfeng Wen,Zhenwei Tang,Ashton Anderson*

Main category: cs.LG

TL;DR: ChessQA是一个全面的基准测试，用于评估大型语言模型在象棋理解方面的能力，涵盖五个任务类别：结构、模式、短战术、位置判断和语义，对应玩家掌握象棋知识的递进抽象层次。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM象棋能力评估是临时性的且范围狭窄，难以准确衡量LLM对象棋的理解以及这种理解如何随规模、后训练方法或架构选择而变化。

Method: 创建ChessQA基准测试，包含五个任务类别：结构（基本规则）、模式（战术模式）、短战术（正确计算）、位置判断（评估位置）和语义（描述高级概念）。

Result: 评估当代LLM发现所有五个类别都存在持续弱点，提供了按类别的结果和错误分析。

Conclusion: ChessQA提供了一个更全面的象棋能力评估框架，超越了之前简单的走子质量评估，为诊断和比较提供了受控、一致的设置，并将发布代码、定期更新的数据集和公共排行榜。

Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and
abstraction capabilities of large language models (LLMs), as it has
well-defined structure and objective ground truth while admitting a wide
spectrum of skill levels. However, existing evaluations of LLM ability in chess
are ad hoc and narrow in scope, making it difficult to accurately measure LLM
chess understanding and how it varies with scale, post-training methodologies,
or architecture choices. We present ChessQA, a comprehensive benchmark that
assesses LLM chess understanding across five task categories (Structural,
Motifs, Short Tactics, Position Judgment, and Semantic), which approximately
correspond to the ascending abstractions that players master as they accumulate
chess knowledge, from understanding basic rules and learning tactical motifs to
correctly calculating tactics, evaluating positions, and semantically
describing high-level concepts. In this way, ChessQA captures a more
comprehensive picture of chess ability and understanding, going significantly
beyond the simple move quality evaluations done previously, and offers a
controlled, consistent setting for diagnosis and comparison. Furthermore,
ChessQA is inherently dynamic, with prompts, answer keys, and construction
scripts that can evolve as models improve. Evaluating a range of contemporary
LLMs, we find persistent weaknesses across all five categories and provide
results and error analyses by category. We will release the code, periodically
refreshed datasets, and a public leaderboard to support further research.

</details>


### [33] [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)
*Scott Emmons,Roland S. Zimmermann,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 本文提出了一种测量思维链可监控性的实用方法，包括可读性和覆盖度两个指标，并通过自动评分器提示实现评估，发现前沿模型在挑战性基准测试中表现出高可监控性。


<details>
  <summary>Details</summary>
Motivation: 思维链监控为AI安全提供了独特机会，但训练实践或模型架构的变化可能使这种机会丧失。为了帮助保持可监控性，需要测量其关键组成部分。

Method: 提出测量可读性（人类能否理解推理过程）和覆盖度（思维链是否包含人类产生最终输出所需的所有推理）的指标，使用自动评分器提示让任何有能力的LLM计算现有思维链的可读性和覆盖度。

Result: 在合成思维链退化测试中验证了自动评分器的有效性，应用于多个前沿模型在挑战性基准测试中，发现它们表现出高可监控性。

Conclusion: 这些指标可作为开发者跟踪设计决策对可监控性影响的工具，该方法应被视为对抗性压力测试的补充而非替代，用于测试对抗故意规避模型的鲁棒性。

Abstract: While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI
safety, this opportunity could be lost through shifts in training practices or
model architecture. To help preserve monitorability, we propose a pragmatic way
to measure two components of it: legibility (whether the reasoning can be
followed by a human) and coverage (whether the CoT contains all the reasoning
needed for a human to also produce the final output). We implement these
metrics with an autorater prompt that enables any capable LLM to compute the
legibility and coverage of existing CoTs. After sanity-checking our prompted
autorater with synthetic CoT degradations, we apply it to several frontier
models on challenging benchmarks, finding that they exhibit high
monitorability. We present these metrics, including our complete autorater
prompt, as a tool for developers to track how design decisions impact
monitorability. While the exact prompt we share is still a preliminary version
under ongoing development, we are sharing it now in the hopes that others in
the community will find it useful. Our method helps measure the default
monitorability of CoT - it should be seen as a complement, not a replacement,
for the adversarial stress-testing needed to test robustness against
deliberately evasive models.

</details>


### [34] [An efficient probabilistic hardware architecture for diffusion-like models](https://arxiv.org/abs/2510.23972)
*Andraž Jelinčič,Owen Lockwood,Akhil Garlapati,Guillaume Verdon,Trevor McCourt*

Main category: cs.LG

TL;DR: 提出一种全晶体管概率计算机架构，在硬件层面实现强大的去噪模型，相比GPU能实现约10,000倍的能效提升


<details>
  <summary>Details</summary>
Motivation: 现有的概率AI专用计算机提案依赖有限建模技术和不可扩展的异质硬件，未能获得广泛应用

Method: 设计全晶体管概率计算机架构，在硬件层面直接实现强大的去噪模型

Result: 系统级分析表明，该架构在简单图像基准测试中能达到与GPU相当的性能，同时能耗降低约10,000倍

Conclusion: 该全晶体管概率计算机架构解决了现有方案的局限性，为概率AI计算提供了高效可行的硬件实现方案

Abstract: The proliferation of probabilistic AI has promoted proposals for specialized
stochastic computers. Despite promising efficiency gains, these proposals have
failed to gain traction because they rely on fundamentally limited modeling
techniques and exotic, unscalable hardware. In this work, we address these
shortcomings by proposing an all-transistor probabilistic computer that
implements powerful denoising models at the hardware level. A system-level
analysis indicates that devices based on our architecture could achieve
performance parity with GPUs on a simple image benchmark using approximately
10,000 times less energy.

</details>


### [35] [Spatio-temporal Multivariate Time Series Forecast with Chosen Variables](https://arxiv.org/abs/2510.24027)
*Zibo Liu,Zhe Jiang,Zelin Xu,Tingsong Xiao,Yupu Zhang,Zhengkun Xiao,Haibo Wang,Shigang Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的时空多元时间序列预测问题，研究如何从n个位置中优化选择m个变量作为模型输入，以最大化预测精度。提出了一个联合变量选择和模型优化的统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设模型输入中的m个变量是预先确定的，但如何选择这m个变量的问题从未被研究。本文填补了这一空白，研究了带有选择变量的STMF问题。

Method: 提出了包含三个技术组件的统一框架：(1) 掩码变量参数剪枝，通过基于分位数的掩码逐步剪枝信息量较少的变量和注意力参数；(2) 优先变量参数重放，重放低损失的过去样本以保持学习知识的模型稳定性；(3) 动态外推机制，通过可学习的空间嵌入和邻接信息将信息从输入变量传播到所有其他变量。

Result: 在五个真实世界数据集上的实验表明，本文方法在准确性和效率方面显著优于最先进的基线方法。

Conclusion: 联合变量选择和模型优化的方法在时空多元时间序列预测中表现出有效性，能够同时提高预测精度和模型效率。

Abstract: Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series
of $n$ spatially distributed variables in a period of recent past to forecast
their values in a period of near future. It has important applications in
spatio-temporal sensing forecast such as road traffic prediction and air
pollution prediction. Recent papers have addressed a practical problem of
missing variables in the model input, which arises in the sensing applications
where the number $m$ of sensors is far less than the number $n$ of locations to
be monitored, due to budget constraints. We observe that the state of the art
assumes that the $m$ variables (i.e., locations with sensors) in the model
input are pre-determined and the important problem of how to choose the $m$
variables in the input has never been studied. This paper fills the gap by
studying a new problem of STMF with chosen variables, which optimally selects
$m$-out-of-$n$ variables for the model input in order to maximize the forecast
accuracy. We propose a unified framework that jointly performs variable
selection and model optimization for both forecast accuracy and model
efficiency. It consists of three novel technical components: (1) masked
variable-parameter pruning, which progressively prunes less informative
variables and attention parameters through quantile-based masking; (2)
prioritized variable-parameter replay, which replays low-loss past samples to
preserve learned knowledge for model stability; (3) dynamic extrapolation
mechanism, which propagates information from variables selected for the input
to all other variables via learnable spatial embeddings and adjacency
information. Experiments on five real-world datasets show that our work
significantly outperforms the state-of-the-art baselines in both accuracy and
efficiency, demonstrating the effectiveness of joint variable selection and
model optimization.

</details>


### [36] [Mitigating Negative Transfer via Reducing Environmental Disagreement](https://arxiv.org/abs/2510.24044)
*Hui Sun,Zheng Xie,Hao-Yuan He,Ming Li*

Main category: cs.LG

TL;DR: 本文提出RED方法，通过因果解耦学习来减少无监督领域自适应中的负迁移问题，通过分离因果特征和环境特征来降低环境分歧。


<details>
  <summary>Details</summary>
Motivation: 无监督领域自适应中显著的领域偏移会导致负迁移，降低模型性能。研究发现跨领域在非因果环境特征上的判别分歧是导致负迁移的关键因素。

Method: 提出RED方法，通过对抗训练领域特定的环境特征提取器，将样本解耦为领域不变的因果特征和领域特定的非因果环境特征，然后基于这些特征估计并减少环境分歧。

Result: 实验结果表明RED方法有效缓解了负迁移问题，并达到了最先进的性能水平。

Conclusion: 通过因果解耦学习减少环境分歧是解决无监督领域自适应中负迁移问题的有效途径，RED方法在理论和实验上都证明了其有效性。

Abstract: Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a
labeled source domain to an unlabeled target domain, addressing the challenge
of \emph{domain shift}. Significant domain shifts hinder effective knowledge
transfer, leading to \emph{negative transfer} and deteriorating model
performance. Therefore, mitigating negative transfer is essential. This study
revisits negative transfer through the lens of causally disentangled learning,
emphasizing cross-domain discriminative disagreement on non-causal
environmental features as a critical factor. Our theoretical analysis reveals
that overreliance on non-causal environmental features as the environment
evolves can cause discriminative disagreements~(termed \emph{environmental
disagreement}), thereby resulting in negative transfer. To address this, we
propose Reducing Environmental Disagreement~(RED), which disentangles each
sample into domain-invariant causal features and domain-specific non-causal
environmental features via adversarially training domain-specific environmental
feature extractors in the opposite domains. Subsequently, RED estimates and
reduces environmental disagreement based on domain-specific non-causal
environmental features. Experimental results confirm that RED effectively
mitigates negative transfer and achieves state-of-the-art performance.

</details>


### [37] [Information-Theoretic Discrete Diffusion](https://arxiv.org/abs/2510.24088)
*Moongyu Jeon,Sangwoo Shin,Dongjae Jeon,Albert No*

Main category: cs.LG

TL;DR: 提出了离散扩散模型的信息论框架，通过分数匹配损失得到对数似然的理论估计器，建立了信息-最小去噪分数熵关系，并扩展到掩码扩散过程。


<details>
  <summary>Details</summary>
Motivation: 受高斯设置中I-MMSE恒等式的启发，希望在离散设置中建立类似的理论框架，为离散扩散模型提供理论支撑。

Method: 引入信息-最小去噪分数熵关系，将数据与其扩散版本之间的互信息与最小去噪分数熵损失联系起来，并扩展到掩码扩散过程建立信息-最小去噪交叉熵关系。

Result: 实验在合成和真实世界数据上验证了估计器的准确性、方差稳定性和实用性，表明常用损失函数不仅是变分下界，而且是紧致的对数似然估计器。

Conclusion: 该框架为离散扩散模型提供了理论基础，证明了常用损失函数的理论合理性，并支持时间无关公式、条件似然估计和耦合蒙特卡洛估计等实际扩展。

Abstract: We present an information-theoretic framework for discrete diffusion models
that yields principled estimators of log-likelihood using score-matching
losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive
analogous results for the discrete setting. Specifically, we introduce the
Information-Minimum Denoising Score Entropy (I-MDSE) relation, which links
mutual information between data and its diffused version to the minimum
denoising score entropy (DSE) loss. We extend this theory to masked diffusion
and establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)
relation, connecting cross-entropy losses to mutual information in discrete
masked processes. These results provide a time-integral decomposition of the
log-likelihood of the data in terms of optimal score-based losses, showing that
commonly used losses such as DSE and DCE are not merely variational bounds but
tight and principled estimators of log-likelihood. The I-MDCE decomposition
further enables practical extensions, including time-free formula, conditional
likelihood estimation in prompt-response tasks, and coupled Monte Carlo
estimation of likelihood ratios. Experiments on synthetic and real-world data
confirm the accuracy, variance stability, and utility of our estimators. The
code is publicly available at https://github.com/Dongjae0324/infodis.

</details>


### [38] [SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning](https://arxiv.org/abs/2510.24200)
*Alexander Bakarsky,Dimitar I. Dimitrov,Maximilian Baader,Martin Vechev*

Main category: cs.LG

TL;DR: SPEAR++攻击改进了原有的SPEAR攻击，通过应用稀疏字典学习技术，使基于线性层和ReLU激活函数的梯度反演攻击在计算上变得可行，能够处理10倍更大的批次大小，同时保持对DP噪声和FedAvg聚合的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然实现了分布式训练而不共享数据，但梯度反演攻击挑战了其隐私保护特性。现有的SPEAR攻击虽然理论突破重要，但由于在批次大小上呈指数级运行时间，实用性受限。

Method: 应用最先进的稀疏字典学习技术来解决线性层和ReLU激活函数的梯度反演问题，使攻击在计算上可行。

Result: SPEAR++攻击保持了SPEAR的所有理想特性（如对DP噪声和FedAvg聚合的鲁棒性），同时能够处理10倍更大的批次大小。

Conclusion: SPEAR++通过稀疏字典学习技术显著提升了梯度反演攻击的实用性，使其能够应用于更大规模的联邦学习部署，同时保持对隐私保护机制的鲁棒性。

Abstract: Federated Learning has seen an increased deployment in real-world scenarios
recently, as it enables the distributed training of machine learning models
without explicit data sharing between individual clients. Yet, the introduction
of the so-called gradient inversion attacks has fundamentally challenged its
privacy-preserving properties. Unfortunately, as these attacks mostly rely on
direct data optimization without any formal guarantees, the vulnerability of
real-world systems remains in dispute and requires tedious testing for each new
federated deployment. To overcome these issues, recently the SPEAR attack was
introduced, which is based on a theoretical analysis of the gradients of linear
layers with ReLU activations. While SPEAR is an important theoretical
breakthrough, the attack's practicality was severely limited by its exponential
runtime in the batch size b. In this work, we fill this gap by applying
State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the
problem of gradient inversion on linear layers with ReLU activations tractable.
Our experiments demonstrate that our new attack, SPEAR++, retains all desirable
properties of SPEAR, such as robustness to DP noise and FedAvg aggregation,
while being applicable to 10x bigger batch sizes.

</details>


### [39] [Closing Gaps: An Imputation Analysis of ICU Vital Signs](https://arxiv.org/abs/2510.24217)
*Alisher Turubayev,Anna Shopova,Fabian Lange,Mahmut Kamalak,Paul Mattes,Victoria Ayvasky,Bert Arnrich,Bjarne Pfitzner,Robin P. van de Water*

Main category: cs.LG

TL;DR: 本文比较了ICU生命体征数据的多种插补方法，旨在为临床预测模型选择最准确的插补技术，并创建了一个包含15种插补方法和4种截断方法的可扩展基准。


<details>
  <summary>Details</summary>
Motivation: ICU数据中生命体征测量存在大量缺失段，这会影响机器学习预测性能。目前缺乏对ICU生命体征插补方法的全面比较，且实际中仍在使用可能降低预测准确性的临时插补技术。

Method: 建立了一个可扩展和可重用的基准，包含15种插补方法和4种截断方法，用于在主要ICU数据集上进行基准测试。

Result: 通过系统比较不同插补技术，为研究人员提供了选择最准确插补方法的指导。

Conclusion: 该工作为ICU生命体征数据插补提供了比较基础，有助于促进更多机器学习模型进入临床实践。

Abstract: As more Intensive Care Unit (ICU) data becomes available, the interest in
developing clinical prediction models to improve healthcare protocols
increases. However, the lack of data quality still hinders clinical prediction
using Machine Learning (ML). Many vital sign measurements, such as heart rate,
contain sizeable missing segments, leaving gaps in the data that could
negatively impact prediction performance. Previous works have introduced
numerous time-series imputation techniques. Nevertheless, more comprehensive
work is needed to compare a representative set of methods for imputing ICU
vital signs and determine the best practice. In reality, ad-hoc imputation
techniques that could decrease prediction accuracy, like zero imputation, are
still used. In this work, we compare established imputation techniques to guide
researchers in improving the performance of clinical prediction models by
selecting the most accurate imputation technique. We introduce an extensible
and reusable benchmark with currently 15 imputation and 4 amputation methods,
created for benchmarking on major ICU datasets. We hope to provide a
comparative basis and facilitate further ML development to bring more models
into clinical practice.

</details>


### [40] [Sparse Optimistic Information Directed Sampling](https://arxiv.org/abs/2510.24234)
*Ludovic Schwartz,Hamish Flynn,Gergely Neu*

Main category: cs.LG

TL;DR: 本文提出了稀疏乐观信息导向采样（SOIDS）算法，能够在数据丰富和数据匮乏两种情况下同时实现最优最坏情况遗憾，无需贝叶斯假设。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏线性赌博机算法要么在数据丰富时达到最优最坏情况遗憾但依赖维度多项式，要么在数据匮乏时实现维度无关但回合数依赖较差。稀疏IDS算法虽然能在两种情况下同时达到最优贝叶斯遗憾，但需要贝叶斯假设。

Method: 使用稀疏乐观信息导向采样（SOIDS）算法，通过引入时间依赖学习率的新颖分析，能够平衡信息和遗憾。

Result: SOIDS算法在理论上扩展了IDS的保证，提供了首个在数据丰富和数据匮乏两种情况下同时达到最优最坏情况遗憾的算法，并通过实验验证了其良好性能。

Conclusion: SOIDS算法能够在无需贝叶斯假设的情况下，在稀疏线性赌博机问题中同时适应数据丰富和数据匮乏两种情况，实现最优最坏情况遗憾。

Abstract: Many high-dimensional online decision-making problems can be modeled as
stochastic sparse linear bandits. Most existing algorithms are designed to
achieve optimal worst-case regret in either the data-rich regime, where
polynomial depen- dence on the ambient dimension is unavoidable, or the
data-poor regime, where dimension-independence is possible at the cost of worse
dependence on the num- ber of rounds. In contrast, the sparse Information
Directed Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has
the optimal rate in both regimes simultaneously. In this work, we explore the
use of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the
same adaptivity in the worst-case setting, without Bayesian assumptions.
Through a novel analysis that enables the use of a time-dependent learning
rate, we show that SOIDS can optimally balance information and regret. Our
results extend the theoretical guarantees of IDS, pro- viding the first
algorithm that simultaneously achieves optimal worst-case regret in both the
data-rich and data-poor regimes. We empirically demonstrate the good
performance of SOIDS.

</details>


### [41] [SALS: Sparse Attention in Latent Space for KV cache Compression](https://arxiv.org/abs/2510.24273)
*Junlin Mu,Hantao Huang,Jihang Zhang,Minghui Yu,Tao Wang,Yidong Li*

Main category: cs.LG

TL;DR: SALS是一个在潜在空间中进行稀疏注意力的框架，通过低秩投影将KV缓存压缩到紧凑的潜在空间，并在该空间中使用无RoPE的查询-键交互进行稀疏token选择，从而避免完整KV缓存重建的开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理面临KV缓存大小和内存带宽需求的挑战，现有低秩压缩方法由于RoPE机制导致精度严重下降或产生新的速度瓶颈。

Method: 提出SALS框架：1）将KV缓存通过低秩投影到紧凑潜在空间；2）在潜在空间中使用无RoPE的查询-键交互进行稀疏token选择；3）仅重建重要token子集。

Result: 在LLaMA2-7b-chat和Mistral-7b等模型上评估，SALS实现了6.4倍KV缓存压缩和5.7倍注意力算子加速，端到端吞吐量相比GPT-fast在4k和32K序列上分别提升1.4倍和4.5倍。

Conclusion: SALS通过潜在空间稀疏注意力实现了SOTA性能，在保持竞争力的准确度同时显著提升了推理效率。

Abstract: Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.

</details>


### [42] [EDC: Equation Discovery for Classification](https://arxiv.org/abs/2510.24310)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: 提出了一种基于方程发现(ED)的二元分类框架EDC，能够发现指定决策边界位置和形状的解析函数，在人工和真实数据实验中表现优于现有ED分类方法，达到与最先进二元分类方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 方程发现在回归任务中已取得显著成功，但需要将其扩展到分类任务，特别是发现能够明确描述决策边界的解析函数。

Method: 使用适度复杂度的语法构建模型，包括线性项、二次项、指数项以及特征乘积项（用于捕捉XOR类依赖关系），通过EDC框架发现决策边界的解析表达式。

Result: 在人工和真实数据集上的实验表明，EDC能够同时发现目标方程的结构和参数值，性能优于现有ED分类方法，与最先进二元分类方法相当。

Conclusion: 提出的语法在保持模型灵活性的同时避免了过拟合，且语法复杂度可配置，允许包含领域特定表达式，为分类任务提供了可解释的解析模型。

Abstract: Equation Discovery techniques have shown considerable success in regression
tasks, where they are used to discover concise and interpretable models
(\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary
classification framework. Our proposed method EDC finds analytical functions of
manageable size that specify the location and shape of the decision boundary.
In extensive experiments on artificial and real-life data, we demonstrate how
EDC is able to discover both the structure of the target equation as well as
the value of its parameters, outperforming the current state-of-the-art
ED-based classification methods in binary classification and achieving
performance comparable to the state of the art in binary classification. We
suggest a grammar of modest complexity that appears to work well on the tested
datasets but argue that the exact grammar -- and thus the complexity of the
models -- is configurable, and especially domain-specific expressions can be
included in the pattern language, where that is required. The presented grammar
consists of a series of summands (additive terms) that include linear,
quadratic and exponential terms, as well as products of two features (producing
hyperbolic curves ideal for capturing XOR-like dependencies). The experiments
demonstrate that this grammar allows fairly flexible decision boundaries while
not so rich to cause overfitting.

</details>


### [43] [What do vision-language models see in the context? Investigating multimodal in-context learning](https://arxiv.org/abs/2510.24331)
*Gabriel O. dos Santos,Esther Colombini,Sandra Avila*

Main category: cs.LG

TL;DR: 本文系统研究了视觉语言模型(VLMs)中的上下文学习能力，发现在图像描述任务中，当前VLMs主要依赖文本线索而未能有效整合视觉信息，存在多模态融合能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习在大型语言模型中得到广泛研究，但在视觉语言模型中的有效性仍未充分探索。本文旨在系统分析VLMs中的上下文学习能力，填补这一研究空白。

Method: 评估了涵盖四种架构的七个VLMs模型，在三个图像描述基准上进行测试。分析了提示设计、架构选择和训练策略对多模态上下文学习的影响，并首次分析了注意力模式随上下文示例数量的变化。

Result: 训练于图像-文本交错数据能提升上下文学习性能，但不意味着能有效整合视觉和文本信息。指令调优改善了指令跟随能力，但可能减少对上下文示例的依赖。注意力分析显示当前VLMs主要关注文本线索，未能充分利用视觉信息。

Conclusion: 当前VLMs在上下文学习能力上存在关键限制，特别是在多模态整合方面。这些发现为增强VLMs从多模态上下文示例中学习的能力提供了重要见解。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.

</details>


### [44] [Filtering instances and rejecting predictions to obtain reliable models in healthcare](https://arxiv.org/abs/2510.24368)
*Maria Gabriela Valeriano,David Kohan Marzagão,Alfredo Montelongo,Carlos Roberto Veiga Kiffer,Natan Katz,Ana Carolina Lorena*

Main category: cs.LG

TL;DR: 提出一种新颖的两步数据驱动方法，通过改进数据质量和过滤低置信度预测来增强机器学习模型性能。第一步使用实例难度过滤训练数据，第二步在推理时引入置信度拒绝机制。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险领域（如医疗保健）中广泛应用，但这些模型往往无法考虑不确定性，即使在置信度低时也提供预测，影响预测可靠性。

Method: 采用两步法：1）训练阶段使用实例难度（IH）过滤问题实例来改进数据集；2）推理阶段引入基于置信度的拒绝机制，只保留可靠预测。使用三个真实医疗数据集进行评估。

Result: 评估结果显示，将IH过滤与置信度拒绝相结合能有效提升模型性能，同时保留大部分实例。该方法在平衡预测性能和拒绝率方面表现良好。

Conclusion: 该方法为在安全关键应用中部署机器学习系统提供了一种实用方法，能有效增强模型可靠性。

Abstract: Machine Learning (ML) models are widely used in high-stakes domains such as
healthcare, where the reliability of predictions is critical. However, these
models often fail to account for uncertainty, providing predictions even with
low confidence. This work proposes a novel two-step data-centric approach to
enhance the performance of ML models by improving data quality and filtering
low-confidence predictions. The first step involves leveraging Instance
Hardness (IH) to filter problematic instances during training, thereby refining
the dataset. The second step introduces a confidence-based rejection mechanism
during inference, ensuring that only reliable predictions are retained. We
evaluate our approach using three real-world healthcare datasets, demonstrating
its effectiveness at improving model reliability while balancing predictive
performance and rejection rate. Additionally, we use alternative criteria -
influence values for filtering and uncertainty for rejection - as baselines to
evaluate the efficiency of the proposed method. The results demonstrate that
integrating IH filtering with confidence-based rejection effectively enhances
model performance while preserving a large proportion of instances. This
approach provides a practical method for deploying ML systems in
safety-critical applications.

</details>


### [45] [Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings](https://arxiv.org/abs/2510.24432)
*Seyed Mahdi Basiri Azad,Joschka Boedecker*

Main category: cs.LG

TL;DR: 提出一种使用少量成功演示初始化强化学习智能体价值函数的简单有效方法，通过在稀疏奖励环境中提供有用的动作先验来加速收敛


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的强化学习由于缺乏信息反馈而面临重大挑战，需要减少探索负担并提高样本效率

Method: 使用少量成功演示预计算价值估计作为早期学习目标，采用离线到在线的混合范式，智能体通过标准在线交互进一步优化这些估计

Result: 在基准任务上的实验表明，该方法加速了收敛并优于标准基线方法，即使使用最少或次优的演示数据也能取得良好效果

Conclusion: 通过演示初始化的混合离线-在线强化学习范式能够有效解决稀疏奖励环境中的学习挑战，显著提高样本效率

Abstract: Reinforcement learning (RL) in sparse-reward environments remains a
significant challenge due to the lack of informative feedback. We propose a
simple yet effective method that uses a small number of successful
demonstrations to initialize the value function of an RL agent. By precomputing
value estimates from offline demonstrations and using them as targets for early
learning, our approach provides the agent with a useful prior over promising
actions. The agent then refines these estimates through standard online
interaction. This hybrid offline-to-online paradigm significantly reduces the
exploration burden and improves sample efficiency in sparse-reward settings.
Experiments on benchmark tasks demonstrate that our method accelerates
convergence and outperforms standard baselines, even with minimal or suboptimal
demonstration data.

</details>


### [46] [Methodology for Comparing Machine Learning Algorithms for Survival Analysis](https://arxiv.org/abs/2510.24473)
*Lucas Buk Cardoso,Simone Aldrey Angelo,Yasmin Pacheco Gil Bonilha,Fernando Maia,Adeylson Guimarães Ribeiro,Maria Paula Curado,Gisele Aparecida Fernandes,Vanderlei Cunha Parro,Flávio Almeida de Magalhães Cipparrone,Alexandre Dias Porto Chiavegatto Filho,Tatiana Natasha Toporcov*

Main category: cs.LG

TL;DR: 本研究比较了六种机器学习生存分析模型在结直肠癌患者数据上的表现，发现XGBoost-AFT模型性能最佳，展示了机器学习在生存预测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估不同机器学习模型在生存分析中的表现，特别是在处理删失数据时的能力，以改进结直肠癌患者的生存预测和临床决策支持。

Method: 使用巴西圣保罗医院癌症登记处的近45,000名结直肠癌患者数据，评估了六种机器学习生存分析模型（RSF、GBSA、SSVM、XGB-Cox、XGB-AFT、LGBM），采用不同采样器进行超参数优化，并使用C-Index、C-Index IPCW、时间相关AUC和IBS等指标评估性能。

Result: XGB-AFT模型表现最佳（C-Index = 0.7618; IPCW = 0.7532），其次是GBSA和RSF模型。

Conclusion: 机器学习生存分析模型在改善生存预测和支持决策制定方面具有显著潜力和应用价值。

Abstract: This study presents a comparative methodological analysis of six machine
learning models for survival analysis (MLSA). Using data from nearly 45,000
colorectal cancer patients in the Hospital-Based Cancer Registries of S\~ao
Paulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for
Survival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),
XGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival
considering censored data. Hyperparameter optimization was performed with
different samplers, and model performance was assessed using the Concordance
Index (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score
(IBS). Survival curves produced by the models were compared with predictions
from classification algorithms, and predictor interpretation was conducted
using SHAP and permutation importance. XGB-AFT achieved the best performance
(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results
highlight the potential and applicability of MLSA to improve survival
prediction and support decision making.

</details>


### [47] [MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU](https://arxiv.org/abs/2510.24500)
*Yong Huang,Zhongqi Yang,Amir Rahmani*

Main category: cs.LG

TL;DR: MIMIC-Sepsis是一个从MIMIC-IV数据库衍生的脓毒症患者队列和基准框架，旨在支持脓毒症病程的可重复建模，包含35,239名ICU患者的时间对齐临床变量和标准化治疗数据。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是ICU中死亡率的主要原因，但现有研究常依赖过时数据集、不可复现的预处理流程和有限的临床干预覆盖。

Method: 基于Sepsis-3标准构建透明预处理流程，包括结构化插补策略和治疗纳入，并发布专注于早期死亡率预测、住院时间估计和休克发作分类的基准任务。

Result: 实证结果表明，纳入治疗变量显著提高了模型性能，特别是基于Transformer的架构。

Conclusion: MIMIC-Sepsis可作为评估重症监护研究中预测和序列模型的稳健平台。

Abstract: Sepsis is a leading cause of mortality in intensive care units (ICUs), yet
existing research often relies on outdated datasets, non-reproducible
preprocessing pipelines, and limited coverage of clinical interventions. We
introduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from
the MIMIC-IV database, designed to support reproducible modeling of sepsis
trajectories. Our cohort includes 35,239 ICU patients with time-aligned
clinical variables and standardized treatment data, including vasopressors,
fluids, mechanical ventilation and antibiotics. We describe a transparent
preprocessing pipeline-based on Sepsis-3 criteria, structured imputation
strategies, and treatment inclusion-and release it alongside benchmark tasks
focused on early mortality prediction, length-of-stay estimation, and shock
onset classification. Empirical results demonstrate that incorporating
treatment variables substantially improves model performance, particularly for
Transformer-based architectures. MIMIC-Sepsis serves as a robust platform for
evaluating predictive and sequential models in critical care research.

</details>


### [48] [DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment](https://arxiv.org/abs/2510.24574)
*Hao Wang,Licheng Pan,Yuan Lu,Zhixuan Chu,Xiaoxi Li,Shuting He,Zhichao Chen,Haoxuan Li,Qingsong Wen,Zhouchen Lin*

Main category: cs.LG

TL;DR: DistDF是一种新的时间序列预测方法，通过最小化条件预测分布与标签分布之间的差异来解决传统直接预测方法在存在标签自相关时的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统直接预测方法在存在标签自相关时，条件负对数似然的估计存在偏差，影响预测性能。

Method: 提出DistDF方法，通过最小化条件预测分布与标签分布之间的差异来实现对齐。引入联合分布Wasserstein差异来上界条件差异，该差异可从经验样本中进行可微估计，并与基于梯度的训练无缝集成。

Result: 大量实验表明，DistDF提升了多种预测模型的性能，并实现了最先进的预测性能。

Conclusion: DistDF通过分布对齐方法有效解决了传统预测方法在标签自相关下的偏差问题，为时间序列预测提供了新的有效解决方案。

Abstract: Training time-series forecast models requires aligning the conditional
distribution of model forecasts with that of the label sequence. The standard
direct forecast (DF) approach resorts to minimize the conditional negative
log-likelihood of the label sequence, typically estimated using the mean
squared error. However, this estimation proves to be biased in the presence of
label autocorrelation. In this paper, we propose DistDF, which achieves
alignment by alternatively minimizing a discrepancy between the conditional
forecast and label distributions. Because conditional discrepancies are
difficult to estimate from finite time-series observations, we introduce a
newly proposed joint-distribution Wasserstein discrepancy for time-series
forecasting, which provably upper bounds the conditional discrepancy of
interest. This discrepancy admits tractable, differentiable estimation from
empirical samples and integrates seamlessly with gradient-based training.
Extensive experiments show that DistDF improves the performance diverse
forecast models and achieves the state-of-the-art forecasting performance. Code
is available at https://anonymous.4open.science/r/DistDF-F66B.

</details>


### [49] [Causal Ordering for Structure Learning From Time Series](https://arxiv.org/abs/2510.24639)
*Pedro P. Sanchez,Damian Machlanski,Steven McDonagh,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: DOTS提出了一种基于扩散的时序因果发现方法，通过整合多个有效因果排序而非单一排序，解决了传统排序方法表达能力受限的问题，在合成和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时序数据中的因果发现对于理解生理学、脑连接、气候动力学和社会经济行为等复杂现象至关重要，但传统排序方法限制了模型的表达能力。

Method: 提出DOTS方法，利用扩散过程进行时序因果发现，通过整合多个有效因果排序来恢复底层有向无环图的传递闭包，并基于平稳性和加性噪声模型假设，使用扩散过程进行分数匹配以实现高效Hessian估计。

Result: 在合成基准测试中，DOTS将平均窗口图F1从0.63提升到0.81；在CausalTime真实世界基准测试中，DOTS获得最高的平均摘要图F1，同时将运行时间相对于图优化方法减半。

Conclusion: DOTS为时序因果发现提供了一个可扩展且准确的解决方案，通过多排序集成有效缓解了单排序方法中的伪影问题。

Abstract: Predicting causal structure from time series data is crucial for
understanding complex phenomena in physiology, brain connectivity, climate
dynamics, and socio-economic behaviour. Causal discovery in time series is
hindered by the combinatorial complexity of identifying true causal
relationships, especially as the number of variables and time points grow. A
common approach to simplify the task is the so-called ordering-based methods.
Traditional ordering methods inherently limit the representational capacity of
the resulting model. In this work, we fix this issue by leveraging multiple
valid causal orderings, instead of a single one as standard practice. We
propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based
causal discovery for temporal data. By integrating multiple orderings, DOTS
effectively recovers the transitive closure of the underlying directed acyclic
graph, mitigating spurious artifacts inherent in single-ordering approaches. We
formalise the problem under standard assumptions such as stationarity and the
additive noise model, and leverage score matching with diffusion processes to
enable efficient Hessian estimation. Extensive experiments validate the
approach. Empirical evaluations on synthetic and real-world datasets
demonstrate that DOTS outperforms state-of-the-art baselines, offering a
scalable and robust approach to temporal causal discovery. On synthetic
benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS
improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the
CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the
best on individual datasets, DOTS attains the highest average summary-graph
$F1$ while halving runtime relative to graph-optimisation methods. These
results establish DOTS as a scalable and accurate solution for temporal causal
discovery.

</details>


### [50] [Symbolic Snapshot Ensembles](https://arxiv.org/abs/2510.24633)
*Mingyue Liu,Andrew Cropper*

Main category: cs.LG

TL;DR: 本文提出了一种新的归纳逻辑编程集成学习方法，通过单次训练保存中间假设，并使用最小描述长度加权方案组合这些假设，在多个基准测试中实现了4%的预测准确率提升，计算开销不到1%。


<details>
  <summary>Details</summary>
Motivation: 传统的归纳逻辑编程方法通常从单次训练中学习单个假设，而集成方法需要多次训练来学习多个假设。本文旨在开发一种更高效的集成方法，通过单次训练即可获得多个假设。

Method: 训练ILP算法仅一次，保存中间生成的假设，然后使用最小描述长度加权方案来组合这些假设。

Result: 在包括游戏玩法和视觉推理在内的多个基准测试中，该方法将预测准确率提高了4%，而计算开销不到1%。

Conclusion: 该方法提供了一种计算效率高的ILP集成学习方式，通过单次训练即可获得性能提升，显著优于传统需要多次训练的集成方法。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. Most
ILP algorithms learn a single hypothesis from a single training run. Ensemble
methods train an ILP algorithm multiple times to learn multiple hypotheses. In
this paper, we train an ILP algorithm only once and save intermediate
hypotheses. We then combine the hypotheses using a minimum description length
weighting scheme. Our experiments on multiple benchmarks, including game
playing and visual reasoning, show that our approach improves predictive
accuracy by 4% with less than 1% computational overhead.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 本文探讨人工智能在科学问题解决中的作用，重点关注其对学科创造力的影响，指出AI可能在某些情况下取代而非扩展学科创造力。


<details>
  <summary>Details</summary>
Motivation: 研究AI在科学问题解决中的角色，特别是其对学科创造力的潜在影响，担心AI可能改变科学追求的价值。

Method: 通过哲学创造力理论区分创造性方法和创造性产品，引入学科创造力概念，并通过数学领域的两个案例进行分析。

Result: 研究表明计算可以扩展学科创造力，但某些涉及AI的方法可能取代学科创造力。

Conclusion: AI在科学问题解决中的使用可能改变科学追求的价值，在某些情况下甚至可能削弱这种价值。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [52] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 提出一个多智能体生态系统用于N-of-1决策支持，通过协调多个专业智能体来为个体患者提供个性化医疗决策，克服传统AI医疗系统只关注平均患者的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统AI医疗系统通过最小化大数据集上的错误来提供总体准确性，但在边缘病例（罕见变异、多病共存、代表性不足人群）上表现不佳，这种平均患者谬误损害了公平性和信任度。

Method: 构建多智能体生态系统，智能体按器官系统、患者群体和分析模式聚类，共享模型库和证据合成工具。通过协调层权衡可靠性、不确定性和数据密度，为临床医生提供决策支持包。

Result: 验证从群体平均转向个体可靠性评估，测量低密度区域的错误、小样本校准和风险-覆盖权衡。

Conclusion: 通过从单一模型转向协调智能，该方法旨在使医疗AI与医学首要原则保持一致：提供透明、公平且以个体为中心的医疗服务。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [53] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文调查了基于大型语言模型的智能体AI系统带来的安全风险，提出了威胁分类法，回顾了评估方法和防御策略，旨在支持安全设计智能体系统的开发。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统在规划、工具使用、记忆和自主性方面的能力，使其成为强大的自动化平台，但也带来了与传统AI安全和软件安全不同的新型放大安全风险。

Method: 采用调查方法，通过构建威胁分类法，回顾现有基准和评估方法，并从技术和治理两个角度讨论防御策略。

Result: 系统性地识别了智能体AI特有的威胁类型，综合了当前研究进展，并明确了该领域面临的开放挑战。

Conclusion: 智能体AI系统在web、软件和物理环境中的自主执行能力创造了新的安全风险，需要专门的安全设计和治理框架来确保其安全部署。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [54] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 该研究比较了经典模型和机器学习模型在电动汽车跟驰行为建模中的表现，发现随机森林模型在所有场景下都优于物理基础模型。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，需要理解其驾驶行为以提高交通安全和开发智能驾驶系统，特别是在混合交通环境中。

Method: 使用真实世界数据集，比较了IDM、OVM、OVRV和简化CACC等经典模型与随机森林回归器，通过最小化RMSE来校准模型参数。

Result: 随机森林模型表现最佳，在中等、长和超长车距下的RMSE分别为0.0046、0.0016和0.0025；经典模型中CACC表现最好，长车距下RMSE为2.67。

Conclusion: 机器学习模型在模拟电动汽车行为和分析混合自主交通动态方面具有重要价值，特别是在电动汽车集成环境中。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [55] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens是一个透明的AI病理学助手，允许病理学家用自然语言提问组织切片问题，提供结构化报告和可视化证据，帮助医生更快、更自信地做出诊断。


<details>
  <summary>Details</summary>
Motivation: 为了让医生真正信任人工智能，AI不能是黑盒系统，需要像咨询同事一样理解其推理过程。

Method: 创建HistoLens系统，将自然语言问题智能翻译为AI引擎的精确查询，提供结构化报告和热力图可视化证据，并训练AI专注于患者组织而忽略背景噪声。

Result: 开发了一个工作流程，病理学家保持专家主导地位，使用可信赖的AI助手验证他们的见解，实现更快、更自信的诊断。

Conclusion: HistoLens作为透明的协作伙伴，通过提供可解释的AI推理和可视化证据，增强了医生对AI的信任，同时保持医生在诊断过程中的主导地位。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [56] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文针对MCTS中的抽象方法提出改进，解决了当同一抽象节点内多个动作具有相同UCB值时需要打破平局的问题，提出了几种优于随机策略的抽象内部策略。


<details>
  <summary>Details</summary>
Motivation: MCTS的样本效率问题可以通过状态和动作抽象来改善，但现有抽象方法（如pruned OGA）在多个动作属于同一抽象节点时，由于UCB值相同而需要随机打破平局，这影响了性能。

Method: 提出并实证评估了多种抽象内部策略（intra-abstraction policies），用于在抽象节点内多个动作具有相同UCB值时进行选择。

Result: 实验表明，提出的几种策略在大多数环境和参数设置下都优于随机策略。

Conclusion: 通过改进抽象节点内的动作选择策略，可以显著提升MCTS在抽象环境中的性能，为抽象MCTS算法提供了更好的平局打破机制。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [57] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 提出了一种基于LLM内部行为的相关矩阵秩作为推理路径可信度指标的自指示方法，无需外部资源即可有效验证推理正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但容易产生错误和幻觉，现有验证方法依赖外部资源导致计算开销大且适用范围有限。

Method: 通过分析LLM内部行为，发现输入问题与输出推理路径的相关矩阵秩是推理正确性的稳健指标，基于此设计自指示方法对候选推理路径进行重加权。

Result: 在多个不同规模和家族的LLM上实验表明，该方法能以超过75%的准确率区分正确和错误推理路径，并在三个推理基准上将准确率提升超过8%。

Conclusion: LLM内部行为本身包含推理可信度信息，自指示方法简单有效，显著优于其他投票和验证方法，且计算开销极小。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [58] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是一个基于可预测性-可计算性-稳定性(PCS)原则的多智能体系统，用于改进LLM驱动的数据科学工作流，在九个数据集上表现优于AutoKaggle和DataInterpreter。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的数据科学系统仅依赖LLM内部推理，缺乏科学和理论原则指导，限制了在噪声和复杂真实数据集上的可信度和鲁棒性。

Method: 基于PCS原则构建多智能体系统，采用模块化工作流处理数据清洗、特征工程、建模和评估，每个阶段由专门智能体负责，结合扰动分析、单元测试和模型验证。

Result: 在九个不同特征的数据集上评估，使用DeepSeek-V3和GPT-4o作为后端，VDSAgents持续优于AutoKaggle和DataInterformer的结果。

Conclusion: 将PCS原则嵌入LLM驱动的数据科学自动化是可行的，能够显著提升系统性能。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [59] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 本研究比较了多个大型语言模型在逻辑和抽象推理能力方面的表现，并与人类表现进行基准测试，揭示了模型在演绎推理方面的困难。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理能力对于推动人工智能发展至关重要，这超越了单纯的语言任务表现，涉及理解模型是否真正理解信息、进行推理并以逻辑有效的方式得出结论。

Method: 使用八道自定义设计的推理问题，比较了GPT、Claude、DeepSeek、Gemini、Grok、Llama、Mistral、Perplexity和Sabi'a等LLMs的逻辑和抽象推理技能，并将结果与人类在相同任务上的表现进行基准测试。

Result: 研究揭示了显著差异，表明LLMs在演绎推理方面存在困难。

Conclusion: 大型语言模型在逻辑和抽象推理能力方面仍有待提升，特别是在演绎推理方面表现不佳。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [60] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: 本文提出了Law in Silico框架，利用大语言模型模拟法律社会，验证了LLM能够复现宏观犯罪趋势并为法律系统设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 由于现实法律实验成本高昂或不可行，需要一种有效的替代方法来验证和发展法律理论。大语言模型凭借其世界知识和角色扮演能力，是模拟法律社会的理想候选。

Method: 引入Law in Silico框架，基于LLM的智能体模拟法律场景，包含个体决策和立法、裁决、执法等制度机制。

Result: 实验表明LLM智能体能够很大程度上复现宏观犯罪趋势，提供与现实观察一致的见解；微观模拟显示功能良好、透明且适应性强的法律系统能更好保护弱势个体权利。

Conclusion: LLM能够有效模拟法律系统，为法律理论验证和系统设计提供有价值的工具，同时强调了良好法律系统对保护弱势群体的重要性。

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [61] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种成本效益高的两阶段流程，通过跨任务示例和基于图的标签传播方法减少对LLM数据标注的依赖，从而降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 为大型语言模型收集高质量示例进行上下文学习成本高昂且劳动密集，需要减少对LLM数据标注的依赖。

Method: 两阶段流程：首先利用跨任务示例提示LLM伪标注少量目标任务实例，然后引入基于图的标签传播方法将标签信息传播到剩余目标示例中，无需额外LLM查询。

Result: 在五个任务上的实验表明，该方法在降低标注成本的同时实现了强大的性能。

Conclusion: 该管道结合了跨任务监督的灵活性和无LLM传播的可扩展性，为上下文学习提供了一种成本效益高的解决方案。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [62] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 本文综述了基于基础模型（FMs）的作物定点病害管理（SSDM）研究进展，重点分析了视觉语言模型（VLMs）和大语言模型（LLMs）在自适应学习、强化学习和数字孪生框架中的应用现状与发展趋势。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和深度学习在实时计算机视觉中的快速发展，作物病害管理从手工特征提取发展到大规模自动化特征学习。基础模型的出现为处理作物病害数据集提供了全新的方式，能够整合视觉和文本数据，解释症状文本，推理症状-管理关系，并为种植者和教育者提供交互式问答支持。

Method: 通过筛选约40篇关于基础模型在定点病害管理中应用的文献，重点关注大语言模型和视觉语言模型，并分析它们在自适应学习、强化学习和数字孪生框架中的作用。

Result: 主要发现包括：基础模型应用在2023-24年文献激增；视觉语言模型发展快于大语言模型，出版物数量增长5-10倍；强化学习和自适应学习在智能喷洒中仍处于起步阶段；数字孪生结合强化学习可模拟虚拟靶向喷洒；解决仿真到现实的差距对实际部署至关重要；人机协作仍然有限；多模态基础模型与实时反馈将推动下一代定点病害管理。

Conclusion: 基础模型正在推动作物定点病害管理的创新发展，特别是在视觉语言模型和多模态融合方面。未来需要重点关注仿真到现实的转化、人机协作的增强以及实时反馈系统的完善，以实现更有效的田间病害管理解决方案。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>

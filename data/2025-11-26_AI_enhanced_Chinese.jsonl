{"id": "2511.19574", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.19574", "abs": "https://arxiv.org/abs/2511.19574", "authors": ["Ruizhe Zhang", "Jooyoung Kong", "Dylan S. Small", "William Bekerman"], "title": "Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk", "comment": null, "summary": "Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $\u03c4$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6570\u636e\u8f6e\u6362\u6846\u67b6\uff0c\u8bc6\u522b\u53ef\u590d\u5236\u7684\u7ae5\u5e74\u4e0d\u826f\u7ecf\u5386(ACEs)\u7ec4\u5408\u6a21\u5f0f\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u7b5b\u67e5\u6210\u4eba\u6291\u90c1\u9ad8\u98ce\u9669\u4eba\u7fa4\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684ACE\u603b\u5206\u622a\u65ad\u6cd5\uff0c\u5728\u76f8\u540c\u7279\u5f02\u6027\u6c34\u5e73\u4e0b\u7075\u654f\u5ea6\u63d0\u9ad826%\u3002", "motivation": "\u4f20\u7edf\u7684\u5355\u4e00\u7d2f\u52a0ACE\u8bc4\u5206(\u5982\u22654\u5206\u622a\u65ad)\u5728\u4e2a\u4f53\u6c34\u5e73\u4e0a\u7684\u533a\u5206\u80fd\u529b\u8f83\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u771f\u6b63\u7684\u9ad8\u98ce\u9669ACE\u7ec4\u5408\u6a21\u5f0f\uff0c\u4ee5\u4f18\u5316\u4e34\u5e8a\u7b5b\u67e5\u8d44\u6e90\u5206\u914d\u3002", "method": "\u91c7\u7528\u6570\u636e\u8f6e\u6362\u6846\u67b6\uff0c\u7ed3\u5408\u7b49\u6e17\u5b50\u7fa4\u9009\u62e9(ISS)\u65b9\u6cd5\uff0c\u5728\u5355\u8c03\u6027\u5047\u8bbe\u4e0b\u4f30\u8ba1\u9ad8\u98ce\u9669\u5b50\u7fa4\uff1b\u4f7f\u7528\u9891\u7387\u7f16\u7801\u4fdd\u7559ACE\u5f3a\u5ea6\u4fe1\u606f\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u98ce\u9669\u9608\u503c(\u03c4)\u63a7\u5236\u5bb6\u65cf\u9519\u8bef\u7387\u3002", "result": "\u5728BRFSS 2022\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u76f8\u6bd4ACE\u603b\u5206\u622a\u65ad\u6cd5\uff0c\u5728\u76f8\u540c\u7279\u5f02\u6027(0.95)\u6c34\u5e73\u4e0b\uff0c\u4f7f\u7528\u53ef\u590d\u5236\u5b50\u7fa4\u4f5c\u4e3a\u7b5b\u67e5\u89c4\u5219\u53ef\u4f7f\u7075\u654f\u5ea6\u63d0\u9ad826%\uff0c\u63d0\u4f9b\u66f4\u5177\u4f53\u7684\u89e6\u53d1\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc6\u522b\u51fa\u53ef\u590d\u5236\u7684\u3001\u57fa\u4e8e\u6a21\u5f0f\u7684\u9ad8\u98ce\u9669\u5b50\u7fa4\uff0c\u4e3a\u4e34\u5e8a\u7b5b\u67e5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u6613\u4e8e\u5b9e\u65bd\u7684\u5de5\u5177\uff0c\u80fd\u66f4\u6709\u6548\u5730\u5c06\u7a00\u7f3a\u7684\u7b5b\u67e5\u8d44\u6e90\u5b9a\u5411\u5230\u771f\u6b63\u7684\u9ad8\u98ce\u9669\u4eba\u7fa4\u3002"}}
{"id": "2511.19642", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.19642", "abs": "https://arxiv.org/abs/2511.19642", "authors": ["Wuhuan Deng"], "title": "A Win-Expectancy Framework for Contextualizing Runs Batted In: Introducing ARBI and CRBI", "comment": null, "summary": "Runs Batted IN (RBI) records the number of runs a hitter directly drives in during their plate appearances and reflects a batter's ability to convert opportunities into scoring. Because producing runs determines game outcomes, RBI has long served as a central statistic in evaluating offensive performance. However, traditional RBI treats all batted-in runs equally and ignores th game context in which they occur, such as leverage, score state, and the actual impact of a run on a team's chance of winning. In this paper, we introduce two new context-aware metrics-Adjusted RBI (ARBI) and Contextual RBI (CRBI)-that address the fundamental limitations of RBI by incorporating Win Expectancy (WE). ARBI rescales each RBI according to the change in WE before and after the scoring event, assigning more value to runs that meaningfully shift the likelihood of winning and less to runs scored in low-leverage situations. We then extend this framework to CRBI, which further differentiates RBIs with the same WE change by accounting for the terminal WE at the end of the event. This refinement captures the idea that an RBI increasing WE from, for example, 0.45 to 0.65 has a larger competitive impact than one increasing WE from 0.05 to 0.25, even though both represent a 20% increase. Together, ARBI and CRBI provide calibrated, context-sensitive measures of offensive contribution that more accurately reflect the true value of run production. These metrics modernize the interpretation of RBI and have broad applications in player evaluation, forecasting, contract evaluation, and decision-making in baseball analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u60c5\u5883\u611f\u77e5\u6307\u6807ARBI\u548cCRBI\uff0c\u901a\u8fc7\u6574\u5408\u80dc\u7387\u671f\u671b\u6765\u6539\u8fdb\u4f20\u7edfRBI\u7edf\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u66f4\u51c6\u786e\u5730\u8861\u91cf\u51fb\u7403\u5458\u7684\u8fdb\u653b\u8d21\u732e\u3002", "motivation": "\u4f20\u7edfRBI\u7edf\u8ba1\u5c06\u6240\u6709\u6253\u70b9\u89c6\u4e3a\u540c\u7b49\u4ef7\u503c\uff0c\u5ffd\u7565\u4e86\u6bd4\u8d5b\u60c5\u5883\u56e0\u7d20\u5982\u6760\u6746\u7387\u3001\u6bd4\u5206\u72b6\u6001\u548c\u5f97\u5206\u5bf9\u83b7\u80dc\u6982\u7387\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u51fb\u7403\u5458\u7684\u771f\u5b9e\u8d21\u732e\u3002", "method": "ARBI\u6839\u636e\u5f97\u5206\u4e8b\u4ef6\u524d\u540e\u7684\u80dc\u7387\u671f\u671b\u53d8\u5316\u91cd\u65b0\u8c03\u6574\u6bcf\u4e2aRBI\u7684\u4ef7\u503c\uff0cCRBI\u8fdb\u4e00\u6b65\u533a\u5206\u5177\u6709\u76f8\u540cWE\u53d8\u5316\u7684RBI\uff0c\u8003\u8651\u4e8b\u4ef6\u7ed3\u675f\u65f6\u7684\u6700\u7ec8WE\u503c\u3002", "result": "ARBI\u548cCRBI\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u6821\u51c6\u7684\u60c5\u5883\u654f\u611f\u6307\u6807\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u53cd\u6620\u6253\u70b9\u8dd1\u5792\u7684\u771f\u5b9e\u4ef7\u503c\uff0c\u533a\u5206\u9ad8\u6760\u6746\u548c\u4f4e\u6760\u6746\u60c5\u5883\u4e0b\u7684\u5f97\u5206\u8d21\u732e\u3002", "conclusion": "ARBI\u548cCRBI\u73b0\u4ee3\u5316\u4e86RBI\u7684\u89e3\u91ca\uff0c\u5728\u7403\u5458\u8bc4\u4f30\u3001\u9884\u6d4b\u3001\u5408\u540c\u8bc4\u4f30\u548c\u68d2\u7403\u5206\u6790\u51b3\u7b56\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.19476", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19476", "abs": "https://arxiv.org/abs/2511.19476", "authors": ["Jin Cui", "Boran Zhao", "Jiajun Xu", "Jiaqi Guo", "Shuo Guan", "Pengju Ren"], "title": "FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection", "comment": null, "summary": "Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a \"vanishing phase gradient\" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.", "AI": {"tldr": "FAST\u662f\u4e00\u4e2a\u65e0\u9700DNN\u7684\u5206\u5e03\u5339\u914d\u6838\u5fc3\u96c6\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u56fe\u7406\u8bba\u548c\u7279\u5f81\u51fd\u6570\u8ddd\u79bb\u5728\u9891\u57df\u6355\u83b7\u5b8c\u6574\u5206\u5e03\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b09.12%\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u300196.57%\u529f\u8017\u964d\u4f4e\u548c2.2\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1aDNN-based\u65b9\u6cd5\u53d7\u6a21\u578b\u53c2\u6570\u9650\u5236\u4e14\u5f15\u5165\u67b6\u6784\u504f\u5dee\uff1bDNN-free\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002\u4e24\u8005\u90fd\u672a\u80fd\u660e\u786e\u7ea6\u675f\u5206\u5e03\u7b49\u4ef7\u6027\uff0c\u4e14\u5e38\u7528\u5ea6\u91cf\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u9ad8\u9636\u77e9\u5dee\u5f02\u3002", "method": "\u5c06\u6838\u5fc3\u96c6\u9009\u62e9\u5efa\u6a21\u4e3a\u56fe\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u7279\u5f81\u51fd\u6570\u8ddd\u79bb\u5728\u9891\u57df\u6355\u83b7\u5206\u5e03\u4fe1\u606f\u3002\u63d0\u51fa\u8870\u51cf\u76f8\u4f4d\u89e3\u8026CFD\u89e3\u51b3\"\u6d88\u5931\u76f8\u4f4d\u68af\u5ea6\"\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u6e10\u8fdb\u5dee\u5f02\u611f\u77e5\u91c7\u6837\u7b56\u7565\u4ece\u4f4e\u9891\u5230\u9ad8\u9891\u8c03\u5ea6\u9891\u7387\u9009\u62e9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53479.12%\uff0c\u529f\u8017\u964d\u4f4e96.57%\uff0c\u5b9e\u73b02.2\u500d\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "FAST\u6846\u67b6\u901a\u8fc7\u9891\u57df\u5206\u5e03\u5339\u914d\u548c\u6e10\u8fdb\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8282\u80fd\u7684\u6838\u5fc3\u96c6\u9009\u62e9\uff0c\u4e3a\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6570\u636e\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19805", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19805", "abs": "https://arxiv.org/abs/2511.19805", "authors": ["Y. A. Rouzoumka", "E. Terreaux", "C. Morisseau", "J. -P. Ovarlez", "C. Ren"], "title": "Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter", "comment": "Under review at ICASSP 2026", "summary": "We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u590d\u6570\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5728\u96f7\u8fbe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u68c0\u6d4b\u6307\u6807\u5e76\u4e0e\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u5728\u590d\u6742\u96f7\u8fbe\u73af\u5883\u4e2d\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u8bc6\u522b\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u6837\u672c\u3002", "method": "\u4f7f\u7528\u590d\u6570\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u63d0\u51fa\u91cd\u6784\u8bef\u5dee\u3001\u57fa\u4e8e\u9690\u7a7a\u95f4\u7684\u9a6c\u6c0f\u8ddd\u79bb\u548cKL\u6563\u5ea6\u7b49\u68c0\u6d4b\u6307\u6807\uff0c\u5e76\u4e0eANMF-Tyler\u68c0\u6d4b\u5668\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9a8c\u96f7\u8fbe\u6570\u636e\u4e0a\u5206\u6790\u4e86\u6240\u6709\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5f31\u70b9\u3002", "conclusion": "\u590d\u6570\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5728\u96f7\u8fbe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4e0d\u540c\u68c0\u6d4b\u6307\u6807\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5404\u5f02\u3002"}}
{"id": "2511.19628", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.19628", "abs": "https://arxiv.org/abs/2511.19628", "authors": ["Jared N. Lakhani", "Etienne Pienaar"], "title": "Optimization and Regularization Under Arbitrary Objectives", "comment": "46 pages, 28 figures, 16 tables", "summary": "This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86MCMC\u65b9\u6cd5\u5728\u4efb\u610f\u76ee\u6807\u51fd\u6570\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u4e24\u9636\u6bb5MCMC\u6846\u67b6\u7684\u6027\u80fd\u53d7\u4f3c\u7136\u51fd\u6570\u9510\u5ea6\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3MCMC\u65b9\u6cd5\u5728\u4efb\u610f\u76ee\u6807\u51fd\u6570\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4e24\u9636\u6bb5MCMC\u6846\u67b6\u7684\u6027\u80fd\u5982\u4f55\u53d7\u4f3c\u7136\u51fd\u6570\u9510\u5ea6\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u6b63\u5219\u5316\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5MCMC\u6846\u67b6\uff0c\u4ea4\u66ff\u4f7f\u7528Metropolis-Hastings\u548cGibbs\u91c7\u6837\uff0c\u5f15\u5165\u9510\u5ea6\u53c2\u6570\u63a2\u7d22\u4e0d\u540c\u4f3c\u7136\u51fd\u6570\u5f62\u5f0f\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff08\u5bfc\u822a\u95ee\u9898\u548c\u4e95\u5b57\u68cb\u6e38\u620f\uff09\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\u4f3c\u7136\u51fd\u6570\u66f2\u7387\u540c\u65f6\u63a7\u5236\u7740\u6837\u672c\u5185\u6027\u80fd\u548c\u8bad\u7ec3\u6570\u636e\u63a8\u65ad\u7684\u6b63\u5219\u5316\u7a0b\u5ea6\uff0c\u5728\u6781\u7aef\u4f3c\u7136\u9510\u5ea6\u60c5\u51b5\u4e0b\uff0c\u540e\u9a8c\u8d28\u91cf\u4f1a\u574d\u7f29\u5230\u5355\u4e00\u4e3b\u5bfc\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662f\u8fc7\u5ea6\u7684\u4f3c\u7136\u9510\u5ea6\u4f1a\u5bfc\u81f4\u540e\u9a8c\u8d28\u91cf\u574d\u7f29\u5230\u5355\u4e00\u6a21\u5f0f\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\uff08\u5c06MCMC\u7b2c\u4e00\u5757\u66ff\u6362\u4e3a\u8fed\u4ee3\u4f18\u5316\u6b65\u9aa4\uff09\u80fd\u8fbe\u5230\u4e0e\u539fMCMC\u6846\u67b6\u51e0\u4e4e\u76f8\u540c\u7684\u6027\u80fd\u3002"}}
{"id": "2511.19577", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.19577", "abs": "https://arxiv.org/abs/2511.19577", "authors": ["Abhay Goyal", "Navin Kumar", "Kimberly DiMeola", "Rafael Trujillo", "Soorya Ram Shimgekar", "Christian Poellabauer", "Pi Zonooz", "Ermonda Gjoni-Markaj", "Declan Barry", "Lynn Madden"], "title": "Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder", "comment": null, "summary": "Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u548cAI\u65b9\u6cd5\u9884\u6d4b\u6162\u6027\u75bc\u75db\u548c\u9e26\u7247\u4f7f\u7528\u969c\u788d\u60a3\u8005\u7684\u75bc\u75db\u5cf0\u503c\uff0c\u53d1\u73b0\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8f83\u9ad8\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u63a5\u53d7\u9e26\u7247\u4f7f\u7528\u969c\u788d\u836f\u7269\u6cbb\u7597\u60a3\u8005\u7684\u6162\u6027\u75bc\u75db\u548c\u9e26\u7247\u4f7f\u7528\u969c\u788d\u7684\u5faa\u8bc1\u7efc\u5408\u6cbb\u7597\u65b9\u6848\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u76d1\u6d4b\u548c\u5e72\u9884\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u76d1\u6d4b\u60a3\u8005\u6570\u636e\uff0c\u7ed3\u5408\u591a\u79cdAI\u65b9\u6cd5\uff08\u5305\u62ec\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5206\u6790\u75bc\u75db\u5cf0\u503c\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u75bc\u75db\u5cf0\u503c\u65b9\u9762\u8fbe\u5230\u76f8\u5bf9\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff08>0.7\uff09\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u75bc\u75db\u5cf0\u503c\u6d1e\u5bdf\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002", "conclusion": "\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u65f6\u76d1\u6d4b\u7ed3\u5408\u5148\u8fdbAI\u6a21\u578b\u53ef\u4fc3\u8fdb\u75bc\u75db\u5cf0\u503c\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5e72\u9884\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6b64\u9886\u57df\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2511.20069", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20069", "abs": "https://arxiv.org/abs/2511.20069", "authors": ["D\u00e1ire Healy", "Ilaria Prosdocimi", "Isadora Antoniano-Villalobos"], "title": "Non-stationarities in extreme hourly precipitation over the Piave Basin, northern Italy", "comment": null, "summary": "We study the spatio-temporal features of extremal sub-daily precipitation data over the Piave river basin in northeast Italy using a rich database of observed hourly rainfall. Empirical evidence suggests that both the marginal and dependence structures for extreme precipitation in the area exhibit seasonal patterns, and spatial dependence appears to weaken as events become more extreme. We investigate factors affecting the marginal distributions, the spatial dependence and the interplay between them. Capturing these features is essential to provide a realistic description of extreme precipitation processes in order to better estimate their associated risks. With this aim, we identify various climatic covariates at different spatio-temporal scales and explore their usefulness. We go beyond existing literature by investigating and comparing the performance of recently proposed covariate-dependent models for both the marginal and dependence structures of extremes. Furthermore, a flexible max-id model, which encompasses both asymptotic dependence and independence, is used to learn about the spatio-temporal variability of rainfall processes at extreme levels. We find that modelling non-stationarity only at the marginal level does not fully capture the variability of precipitation extremes, and that it is important to also capture the seasonal variation of extremal dependence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u610f\u5927\u5229\u76ae\u4e9a\u97e6\u6cb3\u6d41\u57df\u7684\u9010\u5c0f\u65f6\u964d\u6c34\u89c2\u6d4b\u6570\u636e\uff0c\u5206\u6790\u6781\u7aef\u964d\u6c34\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u53d1\u73b0\u8fb9\u9645\u5206\u5e03\u548c\u4f9d\u8d56\u7ed3\u6784\u90fd\u5b58\u5728\u5b63\u8282\u6027\u6a21\u5f0f\uff0c\u4e14\u7a7a\u95f4\u4f9d\u8d56\u968f\u4e8b\u4ef6\u6781\u7aef\u7a0b\u5ea6\u589e\u52a0\u800c\u51cf\u5f31\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u534f\u53d8\u91cf\u4f9d\u8d56\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u7075\u6d3b\u7684max-id\u6a21\u578b\u6765\u6355\u6349\u6781\u7aef\u964d\u6c34\u8fc7\u7a0b\u7684\u65f6\u7a7a\u53d8\u5f02\u6027\u3002", "motivation": "\u51c6\u786e\u63cf\u8ff0\u6781\u7aef\u964d\u6c34\u8fc7\u7a0b\u5bf9\u4e8e\u66f4\u597d\u4f30\u8ba1\u76f8\u5173\u98ce\u9669\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u6355\u6349\u8fb9\u9645\u5206\u5e03\u548c\u4f9d\u8d56\u7ed3\u6784\u7684\u5b63\u8282\u6027\u53d8\u5316\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u4e30\u5bcc\u7684\u9010\u5c0f\u65f6\u964d\u6c34\u89c2\u6d4b\u6570\u636e\u5e93\uff0c\u8bc6\u522b\u4e0d\u540c\u65f6\u7a7a\u5c3a\u5ea6\u7684\u6c14\u5019\u534f\u53d8\u91cf\uff0c\u6bd4\u8f83\u8fd1\u671f\u63d0\u51fa\u7684\u534f\u53d8\u91cf\u4f9d\u8d56\u6a21\u578b\u5728\u8fb9\u9645\u548c\u4f9d\u8d56\u7ed3\u6784\u4e0a\u7684\u6027\u80fd\uff0c\u91c7\u7528\u7075\u6d3b\u7684max-id\u6a21\u578b\u5206\u6790\u6781\u7aef\u6c34\u5e73\u7684\u65f6\u7a7a\u53d8\u5f02\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ec5\u5bf9\u8fb9\u9645\u5206\u5e03\u5efa\u6a21\u975e\u5e73\u7a33\u6027\u4e0d\u80fd\u5b8c\u5168\u6355\u6349\u6781\u7aef\u964d\u6c34\u7684\u53d8\u5f02\u6027\uff0c\u8fd8\u9700\u8981\u6355\u6349\u6781\u7aef\u4f9d\u8d56\u7684\u5b63\u8282\u6027\u53d8\u5316\u3002\u7a7a\u95f4\u4f9d\u8d56\u968f\u4e8b\u4ef6\u6781\u7aef\u7a0b\u5ea6\u589e\u52a0\u800c\u51cf\u5f31\u3002", "conclusion": "\u5efa\u6a21\u6781\u7aef\u964d\u6c34\u65f6\uff0c\u4e0d\u4ec5\u9700\u8981\u8003\u8651\u8fb9\u9645\u5206\u5e03\u7684\u975e\u5e73\u7a33\u6027\uff0c\u8fd8\u5fc5\u987b\u540c\u65f6\u8003\u8651\u6781\u7aef\u4f9d\u8d56\u7ed3\u6784\u7684\u5b63\u8282\u6027\u53d8\u5316\uff0c\u624d\u80fd\u63d0\u4f9b\u66f4\u73b0\u5b9e\u7684\u6781\u7aef\u964d\u6c34\u8fc7\u7a0b\u63cf\u8ff0\u3002"}}
{"id": "2511.19866", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19866", "abs": "https://arxiv.org/abs/2511.19866", "authors": ["Yutaka Jitsumatsu", "Liangchen Sun"], "title": "Parallel Delay-Doppler Estimation via Order-Reversed Two-Stage Prony Method", "comment": "5pages and 3 figures", "summary": "This paper proposes a Prony-based parallel two-stage method for delay-Doppler estimation in OTFS systems. By performing delay-first and Doppler-first estimations in parallel and fusing the results, the method resolves ambiguities caused by similar path characteristics. The simulation results demonstrate the superior accuracy and robustness of the proposed method under various conditions. This method provides a promising solution for future applications such as Vehicle-to-Vehicle (V2V) and Integrated Sensing and Communication (ISAC).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eProny\u7684\u5e76\u884c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8eOTFS\u7cfb\u7edf\u4e2d\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\u4f30\u8ba1\uff0c\u901a\u8fc7\u5e76\u884c\u6267\u884c\u5ef6\u8fdf\u4f18\u5148\u548c\u591a\u666e\u52d2\u4f18\u5148\u4f30\u8ba1\u5e76\u878d\u5408\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u8def\u5f84\u7279\u6027\u76f8\u4f3c\u5f15\u8d77\u7684\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3OTFS\u7cfb\u7edf\u4e2d\u7531\u4e8e\u8def\u5f84\u7279\u6027\u76f8\u4f3c\u5bfc\u81f4\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\uff0c\u4e3aV2V\u548cISAC\u7b49\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8eProny\u7684\u5e76\u884c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5206\u522b\u8fdb\u884c\u5ef6\u8fdf\u4f18\u5148\u548c\u591a\u666e\u52d2\u4f18\u5148\u4f30\u8ba1\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u878d\u5408\u4ee5\u6d88\u9664\u6a21\u7cca\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u90fd\u5177\u6709\u4f18\u8d8a\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8f66\u8f86\u95f4\u901a\u4fe1\u548c\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7b49\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19470", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19470", "abs": "https://arxiv.org/abs/2511.19470", "authors": ["Padegal Amit", "Omkar Mahesh Kashyap", "Namitha Rayasam", "Nidhi Shekhar", "Surabhi Narayan"], "title": "Quantifying Modality Contributions via Disentangling Multimodal Representations", "comment": "16 pages, 11 figures", "summary": "Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u90e8\u5206\u4fe1\u606f\u5206\u89e3(PID)\u7684\u6846\u67b6\uff0c\u91cf\u5316\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u5404\u6a21\u6001\u7684\u8d21\u732e\uff0c\u5c06\u9884\u6d4b\u4fe1\u606f\u5206\u89e3\u4e3a\u72ec\u7279\u3001\u5197\u4f59\u548c\u534f\u540c\u6210\u5206\uff0c\u4f7f\u7528\u8fed\u4ee3\u6bd4\u4f8b\u62df\u5408\u7a0b\u5e8f(IPFP)\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u63a8\u7406\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u51c6\u786e\u6027\u7684\u65b9\u6cd5\uff0c\u5c06\u79fb\u9664\u6a21\u6001\u540e\u7684\u6027\u80fd\u4e0b\u964d\u89e3\u91ca\u4e3a\u5176\u5f71\u54cd\u529b\uff0c\u4f46\u8fd9\u7c7b\u7ed3\u679c\u9a71\u52a8\u6307\u6807\u65e0\u6cd5\u533a\u5206\u6a21\u6001\u662f\u672c\u8eab\u5177\u6709\u4fe1\u606f\u4ef7\u503c\uff0c\u8fd8\u662f\u4ec5\u901a\u8fc7\u4e0e\u5176\u4ed6\u6a21\u6001\u4ea4\u4e92\u624d\u4ea7\u751f\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u8de8\u6ce8\u610f\u529b\u67b6\u6784\u4e2d\u3002", "method": "\u57fa\u4e8e\u90e8\u5206\u4fe1\u606f\u5206\u89e3(PID)\u6846\u67b6\uff0c\u5c06\u5185\u90e8\u5d4c\u5165\u4e2d\u7684\u9884\u6d4b\u4fe1\u606f\u5206\u89e3\u4e3a\u72ec\u7279\u3001\u5197\u4f59\u548c\u534f\u540c\u6210\u5206\uff0c\u5f00\u53d1\u57fa\u4e8e\u8fed\u4ee3\u6bd4\u4f8b\u62df\u5408\u7a0b\u5e8f(IPFP)\u7684\u7b97\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8ba1\u7b97\u5c42\u548c\u6570\u636e\u96c6\u7ea7\u522b\u7684\u8d21\u732e\u3002", "result": "\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u3001\u8868\u793a\u7ea7\u522b\u7684\u591a\u6a21\u6001\u884c\u4e3a\u89c6\u56fe\uff0c\u76f8\u6bd4\u57fa\u4e8e\u7ed3\u679c\u7684\u6307\u6807\u63d0\u4f9b\u66f4\u6e05\u6670\u548c\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u91cf\u5316\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u5404\u6a21\u6001\u7684\u8d21\u732e\uff0c\u533a\u5206\u6a21\u6001\u7684\u56fa\u6709\u4fe1\u606f\u4ef7\u503c\u4e0e\u901a\u8fc7\u4ea4\u4e92\u4ea7\u751f\u7684\u4ef7\u503c\uff0c\u4e3a\u591a\u6a21\u6001\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2511.20481", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20481", "abs": "https://arxiv.org/abs/2511.20481", "authors": ["Leonardo Cefalo", "Crescenza Calculli", "Alessio Pollice"], "title": "Investigating access to support centers for Violence Against Women in Apulia: A Spatial analysis over multiple years", "comment": null, "summary": "In this study, we address the challenge of modelling the spatial variability in violence against women across municipalities in a Southern Italian region by proposing a Bayesian spatio-temporal Poisson regression model. Using data on access to Local Anti-Violence Centers in the Apulia region from 2021 to 2024, we investigate the impact of municipality-level socioeconomic characteristics and local vulnerabilities on both the incidence and reporting of gender-based violence. To explicitly account for spatial dependence, we compare four spatial models within the Integrated Nested Laplace Approximation framework for Bayesian model estimation. We assess the relative fit of the competing models, discussing their prior assumptions, spatial confounding effects, and inferential implications. Our findings indicate that access to support services decreases with distance from the residential municipality, highlighting spatial constraints in reporting and the strategic importance of support center location. Furthermore, lower education levels appear to contribute to under-reporting in disadvantaged areas, while higher economic development may be associated with a lower incidence of reported violence. This study emphasises the critical role of spatial modelling in capturing reporting dynamics and informing policy interventions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u8d1d\u53f6\u65af\u65f6\u7a7a\u6cca\u677e\u56de\u5f52\u6a21\u578b\u5206\u6790\u610f\u5927\u5229\u5357\u90e8\u5730\u533a\u5973\u6027\u66b4\u529b\u7684\u7a7a\u95f4\u53d8\u5f02\u6027\uff0c\u53d1\u73b0\u652f\u6301\u670d\u52a1\u53ef\u8fbe\u6027\u3001\u6559\u80b2\u6c34\u5e73\u548c\u7ecf\u6d4e\u53d1\u5c55\u5bf9\u66b4\u529b\u62a5\u544a\u548c\u53d1\u751f\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u5973\u6027\u66b4\u529b\u5728\u4e0d\u540c\u57ce\u5e02\u95f4\u7a7a\u95f4\u53d8\u5f02\u6027\u7684\u5efa\u6a21\u6311\u6218\uff0c\u7406\u89e3\u793e\u4f1a\u7ecf\u6d4e\u7279\u5f81\u548c\u5f53\u5730\u8106\u5f31\u6027\u5bf9\u6027\u522b\u66b4\u529b\u53d1\u751f\u7387\u548c\u62a5\u544a\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u65f6\u7a7a\u6cca\u677e\u56de\u5f52\u6a21\u578b\uff0c\u5728\u96c6\u6210\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u6846\u67b6\u5185\u6bd4\u8f83\u56db\u79cd\u7a7a\u95f4\u6a21\u578b\uff0c\u8003\u8651\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u652f\u6301\u670d\u52a1\u53ef\u8fbe\u6027\u968f\u5c45\u4f4f\u5730\u8ddd\u79bb\u589e\u52a0\u800c\u964d\u4f4e\uff0c\u6559\u80b2\u6c34\u5e73\u4f4e\u5bfc\u81f4\u5f31\u52bf\u5730\u533a\u62a5\u544a\u4e0d\u8db3\uff0c\u7ecf\u6d4e\u53d1\u5c55\u53ef\u80fd\u4e0e\u62a5\u544a\u66b4\u529b\u53d1\u751f\u7387\u964d\u4f4e\u76f8\u5173\u3002", "conclusion": "\u7a7a\u95f4\u5efa\u6a21\u5728\u6355\u6349\u62a5\u544a\u52a8\u6001\u548c\u4e3a\u653f\u7b56\u5e72\u9884\u63d0\u4f9b\u4fe1\u606f\u65b9\u9762\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u5f3a\u8c03\u4e86\u652f\u6301\u4e2d\u5fc3\u4f4d\u7f6e\u6218\u7565\u91cd\u8981\u6027\u3002"}}
{"id": "2511.19943", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19943", "abs": "https://arxiv.org/abs/2511.19943", "authors": ["Akash Doshi", "Pinar Sen", "Kirill Ivanov", "Wei Yang", "June Namgoong", "Runxin Wang", "Rachel Wang", "Taesang Yoo", "Jing Jiang", "Tingfang Ji"], "title": "AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload", "comment": "39 pages, 15 figures. Under consideration for publication in Journal of Sel. Areas in Information Theory. This paper was presented in part at the International Symposium on Topics in Coding, August 2025 in the Session for Coding and AI", "summary": "Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel \"free-lunch\" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u7528\u4e8e5G\u4e0a\u884c\u94fe\u8def\u4e2d\u975e\u5747\u5300\u5206\u5e03\u7684HARQ-ACK\u6bd4\u7279\u4f20\u8f93\uff0c\u901a\u8fc7\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u548c\u529f\u7387\u6574\u5f62\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u529f\u7387\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u7f16\u7801\u5047\u8bbe\u7269\u7406\u5c42\u8f93\u5165\u6bd4\u7279\u5747\u5300\u5206\u5e03\uff0c\u4f46HARQ-ACK\u6bd4\u7279\u672c\u8d28\u4e0a\u662f\u975e\u5747\u5300\u5206\u5e03\u7684\u3002\u5bf9\u4e8e\u6b64\u7c7b\u4fe1\u6e90\uff0c\u91c7\u7528\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u53ef\u4ee5\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u589e\u76ca\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\uff0c\u91c7\u7528\"\u514d\u8d39\u5348\u9910\"\u8bad\u7ec3\u7b97\u6cd5\u548c\u6bcf\u7801\u5b57\u529f\u7387\u6574\u5f62\u6280\u672f\uff1b\u5f00\u53d1\u4e86Neyman-Pearson\u6d4b\u8bd5\u7684\u6269\u5c55\u7248\u672c\uff0c\u5728\u89e3\u7801\u5668\u4e2d\u5b9e\u73b0NACK\u6bd4\u7279\u76f8\u5bf9\u4e8eACK\u6bd4\u7279\u7684\u4e0d\u7b49\u9519\u8bef\u4fdd\u62a4\u3002", "result": "\u4e0e5G NR\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u8fbe\u5230\u76ee\u6807\u9519\u8bef\u7387\u65f6\u5e73\u5747\u4f20\u8f93\u529f\u7387\u964d\u4f4e\u4e863-6 dB\uff0c\u6700\u5927\u4f20\u8f93\u529f\u7387\u964d\u4f4e\u4e862-3 dB\uff0c\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u8986\u76d6\u589e\u76ca\u548c\u529f\u7387\u8282\u7701\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8bbe\u8ba1\u57285G NR\u517c\u5bb9\u7684\u4e0a\u884c\u94fe\u8def\u8bbe\u7f6e\u4e2d\u6709\u6548\u5229\u7528\u4e86HARQ-ACK\u6bd4\u7279\u7684\u975e\u5747\u5300\u5206\u5e03\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u529f\u7387\u8282\u7701\u548c\u8986\u76d6\u589e\u76ca\u3002"}}
{"id": "2511.20616", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20616", "abs": "https://arxiv.org/abs/2511.20616", "authors": ["Yueming Shen", "Christian Pean", "David Dunson", "Samuel Berchuck"], "title": "Discovering Spatial Patterns of Readmission Risk Using a Bayesian Competing Risks Model with Spatially Varying Coefficients", "comment": null, "summary": "Time-to-event models are commonly used to study associations between risk factors and disease outcomes in the setting of electronic health records (EHR). In recent years, focus has intensified on social determinants of health, highlighting the need for methods that account for patients' locations. We propose a Bayesian approach for introducing point-referenced spatial effects into a competing risks proportional hazards model. Our method leverages Gaussian process (GP) priors for spatially varying intercept and slope. To improve computational efficiency under a large number of spatial locations, we implemented a Hilbert space low-rank approximation of the GP. We modeled the baseline hazard curves as piecewise constant, and introduced a novel multiplicative gamma process prior to induce shrinkage and smoothing. A loss-based clustering method was then used on the spatial random effects to identify high-risk regions. We demonstrate the utility of this method through simulation and a real-world analysis of EHR data from Duke Hospital to study readmission risk of elderly patients with upper extremity fractures. Our results showed that the proposed method improved inference efficiency and provided valuable insights for downstream policy decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u5c06\u70b9\u53c2\u8003\u7a7a\u95f4\u6548\u5e94\u5f15\u5165\u7ade\u4e89\u98ce\u9669\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4f4e\u79e9\u8fd1\u4f3c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5e94\u7528\u4e8e\u8001\u5e74\u4e0a\u80a2\u9aa8\u6298\u60a3\u8005\u518d\u5165\u9662\u98ce\u9669\u5206\u6790\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7814\u7a76\u4e2d\u9700\u8981\u8003\u8651\u60a3\u8005\u5730\u7406\u4f4d\u7f6e\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u5173\u6ce8\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u7684\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u7a7a\u95f4\u6548\u5e94\u7684\u7edf\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u7ade\u4e89\u98ce\u9669\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u4e2d\u5f15\u5165\u7a7a\u95f4\u6548\u5e94\uff0c\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u5904\u7406\u7a7a\u95f4\u53d8\u5316\u7684\u622a\u8ddd\u548c\u659c\u7387\uff0c\u4f7f\u7528\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4f4e\u79e9\u8fd1\u4f3c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u57fa\u7ebf\u98ce\u9669\u66f2\u7ebf\u5efa\u6a21\u4e3a\u5206\u6bb5\u5e38\u6570\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u4e58\u6cd5\u4f3d\u9a6c\u8fc7\u7a0b\u5148\u9a8c\u8fdb\u884c\u6536\u7f29\u548c\u5e73\u6ed1\u5904\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754cEHR\u6570\u636e\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6539\u8fdb\u7684\u63a8\u65ad\u6548\u7387\uff0c\u5e76\u4e3a\u4e0b\u6e38\u653f\u7b56\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u6570\u636e\uff0c\u5728\u8001\u5e74\u4e0a\u80a2\u9aa8\u6298\u60a3\u8005\u518d\u5165\u9662\u98ce\u9669\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u533b\u7597\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.19480", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19480", "abs": "https://arxiv.org/abs/2511.19480", "authors": ["Pinaki Prasad Guha Neogi", "Ahmad Mohammadshirazi", "Dheeraj Kulshrestha", "Rajiv Ramnath"], "title": "Exploiting the Experts: Unauthorized Compression in MoE-LLMs", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86MoE-LLMs\u5728\u4efb\u52a1\u7279\u5b9a\u4f7f\u7528\u4e0b\u7684\u53ef\u526a\u679d\u6027\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u635f\u5931-\u6062\u590d\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u7b56\u7565\u4ee5\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u538b\u7f29\u548c\u5fae\u8c03\u3002", "motivation": "MoE\u67b6\u6784\u7684\u6a21\u5757\u5316\u7ed3\u6784\u5f15\u5165\u4e86\u72ec\u7279\u7684\u5b89\u5168\u6f0f\u6d1e\uff1a\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u526a\u679d\u4e13\u5bb6\u5e76\u5ec9\u4ef7\u5fae\u8c03\u5269\u4f59\u90e8\u5206\u6765\u538b\u7f29\u6216\u91cd\u65b0\u5229\u7528\u6a21\u578b\uff0c\u4ece\u800c\u7ed5\u8fc7\u8bb8\u53ef\u548c\u5b89\u5168\u7ea6\u675f\u3002", "method": "\u5f00\u53d1\u4e86\u4e13\u5bb6\u5f52\u56e0\u6846\u67b6\u8bc6\u522b\u5bf9\u7279\u5b9a\u4efb\u52a1\u6700\u5173\u952e\u7684\u4e13\u5bb6\u5b50\u96c6\uff0c\u8bc4\u4f30\u526a\u679d\u548c\u91cd\u65b0\u5bf9\u9f50\u8fd9\u4e9b\u4e13\u5bb6\u7684\u6027\u80fd\u6743\u8861\uff0c\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u9a71\u52a8\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5173\u952e\u7684\u77e5\u8bc6\u635f\u5931-\u6062\u590d\u6743\u8861\uff1a\u867d\u7136\u53ef\u4ee5\u9694\u79bb\u67d0\u4e9b\u4e13\u5bb6\u6765\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u4f46\u5982\u679c\u6ca1\u6709\u9488\u5bf9\u6027\u7684\u91cd\u65b0\u5bf9\u9f50\uff0c\u4f1a\u51fa\u73b0\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9632\u5fa1\u7b56\u7565\uff0c\u5305\u62ec\u7ea0\u7f20\u4e13\u5bb6\u8bad\u7ec3\u548c\u9009\u62e9\u6027\u5fae\u8c03\u534f\u8bae\uff0c\u4f7fMoE\u6a21\u578b\u66f4\u96be\u88ab\u672a\u7ecf\u6388\u6743\u538b\u7f29\u548c\u5fae\u8c03\uff0c\u4e3aMoE-LLMs\u7684\u5b89\u5168\u4e13\u4e1a\u5316\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2511.19749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19749", "abs": "https://arxiv.org/abs/2511.19749", "authors": ["Farzan Karimi-Malekabadi", "Pooya Razavi", "Sonya Powers"], "title": "Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions", "comment": null, "summary": "As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6559\u80b2\u8bc4\u4f30\u9879\u76ee\u4e0e\u5185\u5bb9\u6807\u51c6\u5bf9\u9f50\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u53d1\u73b0GPT-4o-mini\u7b49\u6a21\u578b\u80fd\u572883-94%\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u8bc6\u522b\u5bf9\u9f50\u72b6\u6001\uff0c\u7ed3\u5408\u5019\u9009\u6280\u80fd\u9884\u7b5b\u9009\u7b56\u7565\u540e\uff0c\u6b63\u786e\u6280\u80fd\u51fa\u73b0\u5728\u524d\u4e94\u5efa\u8bae\u4e2d\u7684\u6982\u7387\u8d85\u8fc795%\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u5bf9\u9f50\u5ba1\u67e5\u867d\u7136\u51c6\u786e\u4f46\u8017\u65f6\u8017\u529b\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u9879\u76ee\u5e93\u4e2d\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1LLMs\u662f\u5426\u80fd\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u8d85\u8fc712,000\u4e2aK-5\u5e74\u7ea7\u7684\u9879\u76ee-\u6280\u80fd\u5bf9\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cdLLMs\uff08GPT-3.5 Turbo\u3001GPT-4o-mini\u548cGPT-4o\uff09\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u8bc6\u522b\u672a\u5bf9\u9f50\u9879\u76ee\u3001\u4ece\u5b8c\u6574\u6807\u51c6\u96c6\u4e2d\u9009\u62e9\u6b63\u786e\u6280\u80fd\u3001\u5728\u5206\u7c7b\u524d\u7f29\u5c0f\u5019\u9009\u5217\u8868\u3002", "result": "GPT-4o-mini\u5728\u7ea683-94%\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u8bc6\u522b\u5bf9\u9f50\u72b6\u6001\uff1b\u6570\u5b66\u9886\u57df\u8868\u73b0\u5f3a\u52b2\uff0c\u9605\u8bfb\u9886\u57df\u56e0\u6807\u51c6\u8bed\u4e49\u91cd\u53e0\u8f83\u591a\u800c\u8868\u73b0\u8f83\u4f4e\uff1b\u9884\u7b5b\u9009\u5019\u9009\u6280\u80fd\u663e\u8457\u6539\u5584\u7ed3\u679c\uff0c\u6b63\u786e\u6280\u80fd\u51fa\u73b0\u5728\u524d\u4e94\u5efa\u8bae\u4e2d\u7684\u6982\u7387\u8d85\u8fc795%\u3002", "conclusion": "LLMs\uff0c\u7279\u522b\u662f\u7ed3\u5408\u5019\u9009\u7b5b\u9009\u7b56\u7565\u65f6\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u9879\u76ee\u5ba1\u67e5\u7684\u4eba\u5de5\u8d1f\u62c5\u540c\u65f6\u4fdd\u6301\u5bf9\u9f50\u51c6\u786e\u6027\u3002\u5efa\u8bae\u5f00\u53d1\u7ed3\u5408LLM\u7b5b\u9009\u548c\u4eba\u5de5\u5ba1\u67e5\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u4e3a\u6301\u7eed\u9879\u76ee\u9a8c\u8bc1\u548c\u6559\u5b66\u5bf9\u9f50\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20113", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20113", "abs": "https://arxiv.org/abs/2511.20113", "authors": ["Xiaojing Yan", "Carlo Fischione"], "title": "Joint Bit-Partitioning and Modulation Design for Digital AirComp", "comment": null, "summary": "For digital over-the-air computation, the ChannelComp framework has recently been proposed to design digital modulations to compute any arbitrary function over a multiple access channel. To reduce modulation design complexity while increasing computation reliability, this paper integrates a bit-partitioning procedure into ChannelComp. The key process is to partition the input bit sequence into several groups, map each group to a single modulation symbol and transmit the encoded symbol sequence across multiple time slots. With the objective to maximize a worst-case constellation distance, we develop two bit-partitioning methods. In uniform bit-partitioning, bits are evenly distributed across groups and modulation is designed via a max-min optimization, which is handled by a CCCP that solves a sequence of second-order cone programming subproblems. In importance-adaptive bit-partitioning (IABP), the bit allocation is adapted to the significance of individual bit positions, and the modulation and partitioning are jointly optimized. To keep the overall complexity manageable, simulated annealing is employed in the outer loop to update the partitioning, while a CCCP-based solver is used in the inner loop for modulation design. Numerical results show that both methods provide robust computation in noisy channels, and IABP achieves up to a 5 dB reduction in computation error compared to Sequential Modulation for AirComp, especially for product computation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6bd4\u7279\u5206\u5272\u65b9\u6cd5\uff08\u5747\u5300\u5206\u5272\u548c\u91cd\u8981\u6027\u81ea\u9002\u5e94\u5206\u5272\uff09\u6765\u6539\u8fdbChannelComp\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8f93\u5165\u6bd4\u7279\u5e8f\u5217\u5206\u7ec4\u6620\u5c04\u5230\u8c03\u5236\u7b26\u53f7\uff0c\u5728\u591a\u65f6\u9699\u4f20\u8f93\u4e2d\u964d\u4f4e\u8bbe\u8ba1\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u964d\u4f4e\u6570\u5b57\u7a7a\u4e2d\u8ba1\u7b97\u7684\u8c03\u5236\u8bbe\u8ba1\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\uff0c\u9700\u8981\u5c06\u6bd4\u7279\u5206\u5272\u8fc7\u7a0b\u96c6\u6210\u5230ChannelComp\u6846\u67b6\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u6bd4\u7279\u5206\u5272\u65b9\u6cd5\uff1a\u5747\u5300\u6bd4\u7279\u5206\u5272\u901a\u8fc7\u6700\u5927\u6700\u5c0f\u4f18\u5316\u8bbe\u8ba1\u8c03\u5236\uff0c\u4f7f\u7528CCCP\u6c42\u89e3\u4e8c\u9636\u9525\u89c4\u5212\u5b50\u95ee\u9898\uff1b\u91cd\u8981\u6027\u81ea\u9002\u5e94\u6bd4\u7279\u5206\u5272\u8054\u5408\u4f18\u5316\u8c03\u5236\u548c\u5206\u5272\uff0c\u5916\u5c42\u4f7f\u7528\u6a21\u62df\u9000\u706b\u66f4\u65b0\u5206\u5272\uff0c\u5185\u5c42\u4f7f\u7528CCCP\u6c42\u89e3\u8c03\u5236\u8bbe\u8ba1\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u4e24\u79cd\u65b9\u6cd5\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u90fd\u80fd\u63d0\u4f9b\u9c81\u68d2\u8ba1\u7b97\uff0c\u91cd\u8981\u6027\u81ea\u9002\u5e94\u6bd4\u7279\u5206\u5272\u76f8\u6bd4Sequential Modulation for AirComp\u5728\u4e58\u79ef\u8ba1\u7b97\u4e2d\u53ef\u964d\u4f4e\u9ad8\u8fbe5 dB\u7684\u8ba1\u7b97\u8bef\u5dee\u3002", "conclusion": "\u6bd4\u7279\u5206\u5272\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u6570\u5b57\u7a7a\u4e2d\u8ba1\u7b97\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u91cd\u8981\u6027\u81ea\u9002\u5e94\u6bd4\u7279\u5206\u5272\u5728\u964d\u4f4e\u8ba1\u7b97\u8bef\u5dee\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.20203", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20203", "abs": "https://arxiv.org/abs/2511.20203", "authors": ["Junjie Ye", "Zhaolin Wang", "Yuanwei Liu", "Peichang Zhang", "Lei Huang", "Arumugam Nallanathan"], "title": "Optimal Waveform Design for Continuous Aperture Array (CAPA)-aided ISAC Systems", "comment": "Submitted to IEEE journal for future publication", "summary": "A novel continuous-aperture-array (CAPA)-aided integrated sensing and communication (ISAC) framework is proposed. Specifically, an optimal continuous ISAC waveform is designed to form a directive beampattern for multi-target sensing while suppressing the multi-user interference (MUI). To achieve the goal of optimal waveform design, the directional beampattern of CAPA is first derived based on Green's function, whereafter a reference sensing waveform is obtained through wavenumber-domain optimization. Based on the reference sensing waveform, a weighted functional programming on the tradeoff between sensing beampattern mismatch and MUI is formulated. To solve the resulting problem, an optimal CAPA-ISAC waveform structure is analytically derived using a Lagrangian-transformation and calculus-of-variations method, where the Lagrangian multiplier associated with the optimal waveform structure is determined via Bisection search. The obtained optimal waveform reveals that it is concurrently affected by the reference sensing waveform, the channel correlations and the channel-symbol correlations. Finally, numerical results validate the effectiveness of the proposed system and waveform design, demonstrating that CAPA can achieve significant performance gains against the ISAC designs based on conventional spatially discrete array in both sensing accuracy and communication reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u6700\u4f18\u8fde\u7eedISAC\u6ce2\u5f62\uff0c\u5728\u5f62\u6210\u591a\u76ee\u6807\u611f\u77e5\u5b9a\u5411\u6ce2\u675f\u7684\u540c\u65f6\u6291\u5236\u591a\u7528\u6237\u5e72\u6270\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u9635\u5217\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u4e2d\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u80fd\u591f\u63d0\u4f9b\u66f4\u597d\u7684\u6ce2\u675f\u5f62\u6210\u80fd\u529b\u548c\u6027\u80fd\u589e\u76ca\u3002", "method": "\u57fa\u4e8e\u683c\u6797\u51fd\u6570\u63a8\u5bfcCAPA\u7684\u5b9a\u5411\u6ce2\u675f\u6a21\u5f0f\uff0c\u901a\u8fc7\u6ce2\u6570\u57df\u4f18\u5316\u83b7\u5f97\u53c2\u8003\u611f\u77e5\u6ce2\u5f62\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u53d8\u6362\u548c\u53d8\u5206\u6cd5\u63a8\u5bfc\u6700\u4f18\u6ce2\u5f62\u7ed3\u6784\uff0c\u901a\u8fc7\u4e8c\u5206\u641c\u7d22\u786e\u5b9a\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cCAPA\u76f8\u6bd4\u4f20\u7edf\u79bb\u6563\u9635\u5217\u5728\u611f\u77e5\u7cbe\u5ea6\u548c\u901a\u4fe1\u53ef\u9760\u6027\u65b9\u9762\u5747\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u4e3a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u611f\u77e5\u6ce2\u675f\u548c\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2511.19829", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19829", "abs": "https://arxiv.org/abs/2511.19829", "authors": ["Ke Chen", "Yifeng Wang", "Hassan Almosapeeh", "Haohan Wang"], "title": "A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization", "comment": null, "summary": "Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u4f30\u6307\u5bfc\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u8bc4\u4f30\u6846\u67b6\u548c\u8bad\u7ec3\u65e0\u6267\u884c\u8bc4\u4f30\u5668\u6765\u9884\u6d4b\u591a\u7ef4\u5ea6\u8d28\u91cf\u5206\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u3001\u67e5\u8be2\u76f8\u5173\u7684\u63d0\u793a\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u9759\u6001\u6a21\u677f\uff0c\u5728\u590d\u6742\u52a8\u6001\u7528\u6237\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\u3002\u73b0\u6709\u67e5\u8be2\u76f8\u5173\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u7a33\u5b9a\u7684\u6587\u672c\u53cd\u9988\u6216\u9ed1\u76d2\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u4f9b\u5f31\u4e14\u4e0d\u53ef\u89e3\u91ca\u7684\u4f18\u5316\u4fe1\u53f7\u3002\u63d0\u793a\u8d28\u91cf\u672c\u8eab\u7f3a\u4e4f\u7edf\u4e00\u7cfb\u7edf\u5b9a\u4e49\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4fe1\u53f7\u788e\u7247\u5316\u4e0d\u53ef\u9760\u3002", "method": "\u9996\u5148\u5efa\u7acb\u9762\u5411\u6027\u80fd\u7684\u7cfb\u7edf\u5316\u63d0\u793a\u8bc4\u4f30\u6846\u67b6\uff0c\u5f00\u53d1\u5e76\u5fae\u8c03\u65e0\u6267\u884c\u8bc4\u4f30\u5668\u76f4\u63a5\u4ece\u6587\u672c\u9884\u6d4b\u591a\u7ef4\u5ea6\u8d28\u91cf\u5206\u6570\u3002\u8bc4\u4f30\u5668\u6307\u5bfc\u6307\u6807\u611f\u77e5\u4f18\u5316\u5668\u8bca\u65ad\u5931\u8d25\u6a21\u5f0f\u5e76\u4ee5\u53ef\u89e3\u91ca\u3001\u67e5\u8be2\u76f8\u5173\u7684\u65b9\u5f0f\u91cd\u5199\u63d0\u793a\u3002", "result": "\u8bc4\u4f30\u5668\u5728\u9884\u6d4b\u63d0\u793a\u6027\u80fd\u65b9\u9762\u8fbe\u5230\u6700\u5f3a\u51c6\u786e\u5ea6\uff0c\u8bc4\u4f30\u6307\u5bfc\u7684\u4f18\u5316\u5728\u516b\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u6301\u7eed\u8d85\u8d8a\u9759\u6001\u6a21\u677f\u548c\u67e5\u8be2\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u6307\u6807\u7684\u63d0\u793a\u8d28\u91cf\u89c6\u89d2\uff0c\u8bc1\u660e\u8bc4\u4f30\u6307\u5bfc\u7684\u4f18\u5316\u7ba1\u9053\u80fd\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u63d0\u4f9b\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.20265", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20265", "abs": "https://arxiv.org/abs/2511.20265", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Chongwen Huang", "Henk Wymeersch"], "title": "Rectified Flow for Vision-Aided mmWave V2I Beam Prediction", "comment": "6 pages, 5 figures, submitted to conference", "summary": "This paper proposes a flow matching (FM) framework based on rectified flow for vision-aided beam prediction in vehicle-to-infrastructure (V2I) links. Instead of modeling discrete beam index sequences, the method learns a continuous latent flow governed by an ordinary differential equation (ODE)-based vector field, enabling smooth beam trajectories and fast sampling. A terminal flow constraint enforces global consistency under finite-step integration, stabilizing long-term prediction. The resulting FM-based model significantly improves top-K accuracy over RNN and LSTM baselines, approaches the performance of large language model-based approaches, and achieves inference speedups on the order of 10 x and 10^4 x on identical GPU and CPU deployments, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6574\u6d41\u6d41\u7684\u6d41\u5339\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u8f66\u5bf9\u57fa\u7840\u8bbe\u65bd\u94fe\u8def\u7684\u89c6\u89c9\u8f85\u52a9\u6ce2\u675f\u9884\u6d4b\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u6d41\u5efa\u6a21\u5b9e\u73b0\u5e73\u6ed1\u6ce2\u675f\u8f68\u8ff9\u548c\u5feb\u901f\u91c7\u6837\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5efa\u6a21\u79bb\u6563\u6ce2\u675f\u7d22\u5f15\u5e8f\u5217\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6ce2\u675f\u9884\u6d4b\u65b9\u6cd5\u6765\u652f\u6301\u8f66\u8f86\u901a\u4fe1\u7684\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u5411\u91cf\u573a\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u6d41\uff0c\u5f15\u5165\u7ec8\u7aef\u6d41\u7ea6\u675f\u786e\u4fdd\u6709\u9650\u6b65\u79ef\u5206\u4e0b\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\u3002", "result": "\u76f8\u6bd4RNN\u548cLSTM\u57fa\u7ebf\u663e\u8457\u63d0\u5347top-K\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u6027\u80fd\uff0c\u5728GPU\u548cCPU\u4e0a\u5206\u522b\u5b9e\u73b010\u500d\u548c10^4\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u6d41\u5339\u914d\u6846\u67b6\u4e3a\u89c6\u89c9\u8f85\u52a9\u6ce2\u675f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.19491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19491", "abs": "https://arxiv.org/abs/2511.19491", "authors": ["Jitendra Parmar", "Praveen Singh Thakur"], "title": "OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally", "comment": "Introduces an open-world machine learning model for continual and adaptive learning Discovers unknown classes and dynamically creates new class categories.Performs class-incremental learning to retain and extend prior knowledge. Enables continuous model improvement across multiple learning iterations. Achieved superior performance with an average accuracy of 82.54", "summary": "Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u53d1\u73b0\u672a\u77e5\u7c7b\u522b\u548c\u589e\u91cf\u5b66\u4e60\u65b0\u7c7b\u522b\u6765\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u5728\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9075\u5faa\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\uff0c\u96be\u4ee5\u4fdd\u7559\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u7528\u4e8e\u672a\u6765\u4efb\u52a1\uff0c\u800c\u81ea\u52a8\u5316\u667a\u80fd\u7cfb\u7edf\u9700\u8981\u5b66\u4e60\u65b0\u7c7b\u522b\u548c\u5df2\u77e5\u4efb\u52a1\u3002", "method": "\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u8fde\u63a5\u4efb\u52a1\uff1a\u9996\u5148\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u672a\u77e5\u7c7b\u522b\u5e76\u521b\u5efa\u65b0\u7c7b\u522b\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2a\u65b0\u7c7b\u522b\u8fdb\u884c\u589e\u91cf\u5b66\u4e60\uff0c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u56db\u4e2a\u8fed\u4ee3\u4e2d\u6700\u9ad8\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523082.54%\uff0c\u6700\u4f4e\u51c6\u786e\u7387\u4e3a65.87%\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u5728\u5f00\u653e\u548c\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e2d\u6709\u6548\u6269\u5c55\u5bf9\u6570\u636e\u7684\u7406\u89e3\u5e76\u968f\u65f6\u95f4\u6539\u8fdb\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19656", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19656", "abs": "https://arxiv.org/abs/2511.19656", "authors": ["Kaiyi Ji"], "title": "Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles", "comment": "24 pages, 1 figure", "summary": "Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $\u03a9(\u03ba^{3/2}\u03b5^{-2})$ oracle calls to find an $\u03b5$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $\u03a9(\u03ba^{5/2}\u03b5^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5149\u6ed1\u975e\u51f8-\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u56f0\u96be\u5b9e\u4f8b\uff0c\u5f97\u51fa\u4e86\u5728\u786e\u5b9a\u6027\u548c\u968f\u673a\u4e00\u9636oracle\u6a21\u578b\u4e0b\u7684\u975e\u5e73\u51e1\u4e0b\u754c\u3002\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\u9700\u8981\u81f3\u5c11\u03a9(\u03ba\u00b3/\u00b2\u03b5\u207b\u00b2)\u6b21oracle\u8c03\u7528\uff0c\u968f\u673a\u60c5\u51b5\u4e0b\u9700\u8981\u81f3\u5c11\u03a9(\u03ba\u2075/\u00b2\u03b5\u207b\u2074)\u6b21\u968f\u673aoracle\u8c03\u7528\uff0c\u8fd9\u4e9b\u4e0b\u754c\u4f18\u4e8e\u5355\u5c42\u975e\u51f8\u4f18\u5316\u548c\u975e\u51f8-\u5f3a\u51f8min-max\u95ee\u9898\u7684\u6700\u4f18\u4e0b\u754c\u3002", "motivation": "\u5c3d\u7ba1\u53cc\u5c42\u4f18\u5316\u7684\u4e0a\u754c\u4fdd\u8bc1\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u7531\u4e8e\u53cc\u5c42\u7ed3\u6784\u7684\u590d\u6742\u6027\uff0c\u4e0b\u754c\u65b9\u9762\u7684\u8fdb\u5c55\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u53cc\u5c42\u4f18\u5316\u5efa\u7acb\u66f4\u4e25\u683c\u7684\u4e0b\u754c\u3002", "method": "\u5f00\u53d1\u65b0\u7684\u56f0\u96be\u5b9e\u4f8b\uff0c\u5728\u5149\u6ed1\u975e\u51f8-\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\uff0c\u5206\u522b\u9488\u5bf9\u786e\u5b9a\u6027\u548c\u968f\u673a\u4e00\u9636oracle\u6a21\u578b\u8fdb\u884c\u5206\u6790\uff0c\u63a8\u5bfc\u6700\u4f18\u4e0b\u754c\u3002", "result": "\u5728\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\uff0c\u4efb\u4f55\u4e00\u9636\u96f6\u5c0a\u91cd\u7b97\u6cd5\u9700\u8981\u81f3\u5c11\u03a9(\u03ba\u00b3/\u00b2\u03b5\u207b\u00b2)\u6b21oracle\u8c03\u7528\u6765\u627e\u5230\u03b5-\u7cbe\u786e\u7684\u7a33\u5b9a\u70b9\uff1b\u5728\u968f\u673a\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u81f3\u5c11\u03a9(\u03ba\u2075/\u00b2\u03b5\u207b\u2074)\u6b21\u968f\u673aoracle\u8c03\u7528\u3002\u8fd9\u4e9b\u4e0b\u754c\u4f18\u4e8e\u76f8\u5173\u8bbe\u7f6e\u4e2d\u7684\u5df2\u77e5\u6700\u4f73\u8fb9\u754c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u53cc\u5c42\u4f18\u5316\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u8868\u660e\u5373\u4f7f\u662f\u7b80\u5316\u673a\u5236\uff08\u5982\u5177\u6709\u4e8c\u6b21\u4e0b\u5c42\u76ee\u6807\u7684\u673a\u5236\uff09\u4e5f\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u7406\u89e3\u6807\u51c6\u4e00\u9636oracle\u4e0b\u53cc\u5c42\u4f18\u5316\u7684\u6700\u4f18\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.19495", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19495", "abs": "https://arxiv.org/abs/2511.19495", "authors": ["Shivansh Chhawri", "Rahul Mahadik", "Suparna Rooj"], "title": "A Systematic Study of Compression Ordering for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u77e5\u8bc6\u84b8\u998f\u3001\u7ed3\u6784\u5316\u526a\u679d\u548c\u4f4e\u6bd4\u7279\u91cf\u5316\u4e09\u79cdLLM\u538b\u7f29\u6280\u672f\u7684\u72ec\u7acb\u6548\u679c\u548c\u7ec4\u5408\u987a\u5e8f\uff0c\u53d1\u73b0\u5728Qwen2.5 3B\u6a21\u578b\u4e0a\uff0cP-KD-Q\uff08\u526a\u679d-\u77e5\u8bc6\u84b8\u998f-\u91cf\u5316\uff09\u5e8f\u5217\u80fd\u5b9e\u73b03.68\u500d\u538b\u7f29\u6bd4\u5e76\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u800c\u65e9\u671f\u5e94\u7528\u91cf\u5316\u4f1a\u5bfc\u81f4\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "LLM\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u9700\u8981\u6a21\u578b\u538b\u7f29\u3002\u73b0\u6709\u538b\u7f29\u6280\u672f\u7684\u72ec\u7acb\u6548\u679c\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u7684\u4ea4\u4e92\u4f5c\u7528\u548c\u6700\u4f18\u987a\u5e8f\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728Qwen2.5 3B\u6a21\u578b\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u5355\u6280\u672f\u548c\u4e09\u6280\u672f\u5e8f\u5217\uff0c\u4f7f\u7528\u56f0\u60d1\u5ea6\u3001G-Eval\u3001\u6e05\u6670\u5ea6\u3001\u63d0\u793a\u5bf9\u9f50\u548c\u538b\u7f29\u6bd4\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u91cf\u5316\u63d0\u4f9b\u6700\u5927\u7684\u72ec\u7acb\u538b\u7f29\uff0c\u526a\u679d\u5f15\u5165\u4e2d\u7b49\u8d28\u91cf\u4e0b\u964d\u3002\u6280\u672f\u987a\u5e8f\u663e\u8457\u5f71\u54cd\u6700\u7ec8\u6a21\u578b\u8d28\u91cf\uff1aP-KD-Q\u5e8f\u5217\u8868\u73b0\u6700\u4f73\uff0c\u800c\u65e9\u671f\u5e94\u7528\u91cf\u5316\u7684\u6d41\u6c34\u7ebf\u56e0\u4e0d\u53ef\u9006\u4fe1\u606f\u635f\u5931\u800c\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u3001\u987a\u5e8f\u611f\u77e5\u7684\u538b\u7f29\u6d41\u6c34\u7ebf\u8bbe\u8ba1\u6307\u5bfc\uff0c\u5f3a\u8c03\u6280\u672f\u5e94\u7528\u987a\u5e8f\u5bf9\u6700\u7ec8\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2511.20453", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20453", "abs": "https://arxiv.org/abs/2511.20453", "authors": ["Ziqin Zhou", "Hui Chen", "Gerhard Steinb\u00f6ck", "Henk Wymeersch"], "title": "Digital Twin-Assisted High-Precision Massive MIMO Localization in Urban Canyons", "comment": "6 pages, 5 figures. accepted to 2026 IEEE JC&S", "summary": "High-precision wireless localization in urban canyons is challenged by noisy measurements and severe non-line-of-sight (NLOS) propagation. This paper proposes a robust three-stage algorithm synergizing a digital twin (DT) model with the random sample consensus (RANSAC) algorithm to overcome these limitations. The method leverages the DT for geometric path association and employs RANSAC to identify reliable line-of-sight (LOS) and single-bounce NLOS paths while rejecting multi-bounce outliers. A final optimization on the resulting inlier set estimates the user's position and clock bias. Simulations validate that by effectively turning NLOS paths into valuable geometric information via the DT, the approach enables accurate localization, reduces reliance on direct LOS, and significantly lowers system deployment costs, making it suitable for practical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u548cRANSAC\u7b97\u6cd5\u7684\u4e09\u9636\u6bb5\u9c81\u68d2\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u7684\u65e0\u7ebf\u5b9a\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u5c06NLOS\u8def\u5f84\u8f6c\u5316\u4e3a\u51e0\u4f55\u4fe1\u606f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u65e0\u7ebf\u5b9a\u4f4d\u9762\u4e34\u7684\u6d4b\u91cf\u566a\u58f0\u548c\u4e25\u91cd\u975e\u89c6\u8ddd\u4f20\u64ad\u6311\u6218\uff0c\u964d\u4f4e\u5bf9\u76f4\u63a5\u89c6\u8ddd\u8def\u5f84\u7684\u4f9d\u8d56\u5e76\u51cf\u5c11\u7cfb\u7edf\u90e8\u7f72\u6210\u672c\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7b97\u6cd5\uff1a1) \u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u8fdb\u884c\u51e0\u4f55\u8def\u5f84\u5173\u8054\uff1b2) \u5229\u7528RANSAC\u7b97\u6cd5\u8bc6\u522b\u53ef\u9760\u7684\u89c6\u8ddd\u548c\u5355\u6b21\u53cd\u5c04\u975e\u89c6\u8ddd\u8def\u5f84\uff0c\u6392\u9664\u591a\u6b21\u53cd\u5c04\u5f02\u5e38\u503c\uff1b3) \u5728\u7b5b\u9009\u51fa\u7684\u5185\u70b9\u96c6\u4e0a\u8fdb\u884c\u6700\u7ec8\u4f18\u5316\uff0c\u4f30\u8ba1\u7528\u6237\u4f4d\u7f6e\u548c\u65f6\u949f\u504f\u5dee\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5c06\u975e\u89c6\u8ddd\u8def\u5f84\u8f6c\u5316\u4e3a\u6709\u4ef7\u503c\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u5b9a\u4f4d\uff0c\u964d\u4f4e\u5bf9\u76f4\u63a5\u89c6\u8ddd\u7684\u4f9d\u8d56\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u90e8\u7f72\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff0c\u4e3a\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u7684\u65e0\u7ebf\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19496", "abs": "https://arxiv.org/abs/2511.19496", "authors": ["Yang Liu", "Xiaolong Zhong", "Ling Jiang"], "title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM", "comment": null, "summary": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \\textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \\emph{drop-in agent core}. Training with maximal-update parameterization ($\u03bc$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \\emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \\textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.", "AI": {"tldr": "Xmodel-2.5\u662f\u4e00\u4e2a13\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u548c\u6210\u672c\u654f\u611f\u90e8\u7f72\u8bbe\u8ba1\uff0c\u91c7\u7528\u03bcP\u8bad\u7ec3\u65b9\u6cd5\u30011.4T token\u7684\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5e76\u5728\u8870\u51cf\u9636\u6bb5\u4eceAdamW\u5207\u6362\u5230Muon\u4f18\u5316\u5668\uff0c\u572813\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53474.58%\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u6216\u6210\u672c\u654f\u611f\u90e8\u7f72\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6700\u5927\u66f4\u65b0\u53c2\u6570\u5316\uff08\u03bcP\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u91c7\u75281.4T token\u7684Warmup-Stable-Decay\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u8870\u51cf\u9636\u6bb5\u4eceAdamW\u5207\u6362\u5230Muon\u4f18\u5316\u5668\uff0c\u5e76\u4f7f\u7528FP8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002", "result": "\u5728\u8870\u51cf\u9636\u6bb5\u5207\u6362\u4f18\u5316\u5668\u7b56\u7565\u4f7f13\u4e2a\u63a8\u7406\u4efb\u52a1\u5e73\u5747\u6027\u80fd\u63d0\u53474.58%\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8d85\u53c2\u6570\u4e0d\u53d8\uff0c\u9a8c\u8bc1\u4e86\u65e9\u671fAdamW\u7a33\u5b9a\u6027\u4e0e\u540e\u671fMuon\u9510\u5316\u76f8\u7ed3\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "Xmodel-2.5\u4f5c\u4e3adrop-in agent core\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6240\u6709\u68c0\u67e5\u70b9\u3001\u914d\u65b9\u548c\u8bc4\u4f30\u4ee3\u7801\u5747\u5df2\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2511.19498", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.19498", "abs": "https://arxiv.org/abs/2511.19498", "authors": ["Yi Zhang", "Tianxiang Xu", "Zijian Li", "Chao Zhang", "Kunyu Zhang", "Zhan Gao", "Meinuo Li", "Xiaohan Zhang", "Qichao Qi", "Bing Chen"], "title": "Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data", "comment": null, "summary": "Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u53cc\u7b56\u7565\u6846\u67b6\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u77e5\u8bc6\u9057\u5fd8\uff0c\u5728\u533b\u7597\u9886\u57df\u7cbe\u786e\u79fb\u9664\u4e13\u4e1a\u77e5\u8bc6\u540c\u65f6\u4fdd\u7559\u57fa\u7840\u533b\u5b66\u80fd\u529b\uff0c\u4ec5\u9700\u4fee\u65390.1%\u7684\u53c2\u6570\u5373\u53ef\u5b9e\u73b082.7%\u7684\u9057\u5fd8\u7387\u548c88.5%\u7684\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u73af\u5883\u4e2d\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u5305\u542b\u4e0d\u5b8c\u5584\u6216\u9690\u79c1\u654f\u611f\u60a3\u8005\u4fe1\u606f\u7684\u4e13\u4e1a\u77e5\u8bc6\u79fb\u9664\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5c42\u53cc\u7b56\u7565\u6846\u67b6\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u68af\u5ea6\u66f4\u65b0\u9009\u62e9\u6027\u8c03\u8282\u76ee\u6807\u53c2\u6570\uff0c\u4ee5\u53ca\u6982\u5ff5\u611f\u77e5\u7684\u4ee4\u724c\u7ea7\u5e72\u9884\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u56db\u7ea7\u533b\u5b66\u6982\u5ff5\u5c42\u6b21\u533a\u5206\u4fdd\u62a4\u5173\u952e\u4ee4\u724c\u548c\u9057\u5fd8\u76ee\u6807\u4ee4\u724c\u3002", "result": "\u5728MedMCQA\uff08\u5916\u79d1\uff09\u548cMHQA\uff08\u7126\u8651\u3001\u6291\u90c1\u3001\u521b\u4f24\uff09\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5b9e\u73b0\u4e8682.7%\u7684\u9057\u5fd8\u7387\u548c88.5%\u7684\u77e5\u8bc6\u4fdd\u7559\u7387\uff0c\u540c\u65f6\u4ec5\u9700\u4fee\u65390.1%\u7684\u53c2\u6570\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u5f3a\u5927\u9690\u79c1\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u6ee1\u8db3\u4e86\u4e34\u5e8a\u7814\u7a76\u4e2d\u76d1\u7ba1\u5408\u89c4\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u4f26\u7406\u6807\u51c6\u7684\u5173\u952e\u9700\u6c42\u3002"}}
{"id": "2511.19933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19933", "abs": "https://arxiv.org/abs/2511.19933", "authors": ["Vaishali Vinay"], "title": "A System-Level Taxonomy of Failure Modes in Large Language Model Applications", "comment": null, "summary": "Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b15\u79cd\u9690\u85cf\u6545\u969c\u6a21\u5f0f\u7684\u7cfb\u7edf\u7ea7\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u73b0\u5b9e\u4e16\u754cLLM\u5e94\u7528\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u4e0e\u76d1\u63a7\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u53ca\u90e8\u7f72LLM\u7684\u751f\u4ea7\u6311\u6218\u548c\u53ef\u9760\u6027\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u88ab\u5feb\u901f\u96c6\u6210\u5230\u51b3\u7b56\u652f\u6301\u5de5\u5177\u548c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f46\u5176\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u4e14\u5176\u6545\u969c\u6a21\u5f0f\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6709\u6839\u672c\u6027\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u7ea7\u5206\u7c7b\u6cd5\u8bc6\u522b15\u79cd\u9690\u85cf\u6545\u969c\u6a21\u5f0f\uff0c\u5206\u6790\u8bc4\u4f30\u4e0e\u76d1\u63a7\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u5e76\u7814\u7a76\u90e8\u7f72LLM\u7684\u751f\u4ea7\u6311\u6218\u3002", "result": "\u5efa\u7acb\u4e86\u6db5\u76d6\u591a\u6b65\u63a8\u7406\u6f02\u79fb\u3001\u6f5c\u5728\u4e0d\u4e00\u81f4\u6027\u3001\u4e0a\u4e0b\u6587\u8fb9\u754c\u9000\u5316\u3001\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7b49\u6545\u969c\u6a21\u5f0f\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u7a33\u5b9a\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u5de5\u4f5c\u6d41\u96c6\u6210\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u53ef\u9760\u6027\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7cfb\u7edf\u5de5\u7a0b\u95ee\u9898\u800c\u975e\u7eaf\u6a21\u578b\u4e2d\u5fc3\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5\u3001AI\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u9760LLM\u90e8\u7f72\u7814\u7a76\u63d0\u4f9b\u4e86\u5206\u6790\u57fa\u7840\u3002"}}
{"id": "2511.20200", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20200", "abs": "https://arxiv.org/abs/2511.20200", "authors": ["Yitian Huang", "Yuxuan Lei", "Jianxun Lian", "Hao Liao"], "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025", "comment": null, "summary": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u5728CPDC 2025\u6311\u6218\u8d5b\u4e2d\u7edf\u4e00\u6539\u8fdb\u4e86GPU\u548cAPI\u4e24\u4e2a\u8d5b\u9053\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548cGRPO\u8bad\u7ec3\uff0c\u5728\u6700\u7ec8\u8bc4\u4f30\u4e2d\u83b7\u5f97\u4e86\u591a\u4e2a\u8d5b\u9053\u7684\u524d\u4e09\u540d\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u5e38\u8bc6\u4eba\u7269\u5bf9\u8bdd\u6311\u6218\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u7a33\u5b9a\u6027\u3001\u6267\u884c\u53ef\u9760\u6027\u548c\u89d2\u8272\u626e\u6f14\u6307\u5bfc\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7f13\u89e3\u5c0f\u6837\u672c\u8fc7\u62df\u5408\uff0c\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u6027\u80fd\u3002", "method": "1. \u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1a\u52a8\u6001\u5de5\u5177\u4fee\u526a\u548c\u4eba\u7269\u7279\u5f81\u88c1\u526a\u8fdb\u884c\u8f93\u5165\u538b\u7f29\uff0c\u7ed3\u5408\u53c2\u6570\u5f52\u4e00\u5316\u548c\u51fd\u6570\u5408\u5e76\u7b49\u540e\u5904\u7406\u6280\u672f\uff1b2. GPU\u8d5b\u9053\u91c7\u7528GRPO\u8bad\u7ec3\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u76d1\u7763\u5fae\u8c03\uff0c\u76f4\u63a5\u4f18\u5316\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u56e2\u961f\u5728\u6700\u7ec8\u8bc4\u4f30\u4e2d\u6392\u540d\uff1aTask 2 API\u7b2c1\u540d\uff0cTask 1 API\u7b2c2\u540d\uff0cTask 3 API\u548cGPU\u8d5b\u9053\u5747\u7b2c3\u540d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u5728CPDC 2025\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548cGRPO\u8bad\u7ec3\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.20216", "categories": ["cs.AI", "cs.CE", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20216", "abs": "https://arxiv.org/abs/2511.20216", "authors": ["Haebin Seong", "Sungmin Kim", "Minchan Kim", "Yongjun Cho", "Myunchul Joe", "Suhwan Choi", "Jaeyoon Jung", "Jiyong Youn", "Yoonshik Kim", "Samwoo Seong", "Yubeen Park", "Youngjae Yu", "Yunsung Lee"], "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents", "comment": null, "summary": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.", "AI": {"tldr": "CostNav\u662f\u9996\u4e2a\u5c06\u5bfc\u822a\u7814\u7a76\u6307\u6807\u4e0e\u5546\u4e1a\u53ef\u884c\u6027\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\u7684\u5fae\u5bfc\u822a\u7ecf\u6d4e\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u5b8c\u6574\u7684\u6210\u672c-\u6536\u76ca\u5206\u6790\u63ed\u793a\u4efb\u52a1\u6210\u529f\u7387\u4f18\u5316\u4e0e\u7ecf\u6d4e\u90e8\u7f72\u4f18\u5316\u7684\u6839\u672c\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u6210\u529f\u7387\u6307\u6807\uff0c\u4f46\u5ffd\u89c6\u4e86\u5546\u4e1a\u90e8\u7f72\u81ea\u4e3b\u914d\u9001\u673a\u5668\u4eba\u6240\u9700\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\uff0c\u8fd9\u5bf9\u5546\u4e1a\u5316\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "CostNav\u5efa\u6a21\u5b8c\u6574\u7684\u7ecf\u6d4e\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u786c\u4ef6\u3001\u8bad\u7ec3\u3001\u80fd\u6e90\u3001\u7ef4\u62a4\u6210\u672c\u548c\u914d\u9001\u6536\u5165\uff0c\u4f7f\u7528\u884c\u4e1a\u53c2\u6570\uff0c\u4ece\u7f29\u51cf\u89c4\u6a21\u6a21\u62df\u6269\u5c55\u5230\u5b9e\u9645\u914d\u9001\u573a\u666f\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u8fbe\u523043.0%\u7684\u670d\u52a1\u6c34\u5e73\u534f\u8bae\u5408\u89c4\u7387\uff0c\u4f46\u5546\u4e1a\u4e0a\u4e0d\u53ef\u884c\uff1a\u6bcf\u6b21\u8fd0\u884c\u635f\u593130.009\u7f8e\u5143\uff0c\u65e0\u76c8\u4e8f\u5e73\u8861\u70b9\uff0c\u56e0\u4e3a\u8fd0\u8425\u6210\u672c\u4e3b\u8981\u7531\u78b0\u649e\u5f15\u8d77\u7684\u7ef4\u62a4\u6210\u672c\u4e3b\u5bfc\uff0c\u5360\u6bcf\u6b21\u8fd0\u884c\u6210\u672c\u768499.7%\u3002", "conclusion": "CostNav\u5f25\u5408\u4e86\u5bfc\u822a\u7814\u7a76\u4e0e\u5546\u4e1a\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8bc4\u4f30\u57fa\u4e8e\u89c4\u5219\u7684\u5bfc\u822a\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u6210\u672c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u80fd\u591f\u57fa\u4e8e\u6570\u636e\u505a\u51fa\u7ecf\u6d4e\u6743\u8861\u51b3\u7b56\u3002"}}
{"id": "2511.19548", "categories": ["cs.LG", "cs.AI", "cs.CY", "econ.GN", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.19548", "abs": "https://arxiv.org/abs/2511.19548", "authors": ["Yiven", "Zhu"], "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics", "comment": "Durham Economic Journal 2025", "summary": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u63a2\u8ba8\u795e\u7ecf\u6570\u636e\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5408\u6cd5\u5730\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u798f\u5229\u5224\u65ad\u4f9d\u636e\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63cf\u8ff0\u884c\u4e3a\u3002", "motivation": "\u795e\u7ecf\u7ecf\u6d4e\u5b66\u627f\u8bfa\u5c06\u798f\u5229\u5206\u6790\u5efa\u7acb\u5728\u795e\u7ecf\u548c\u8ba1\u7b97\u8bc1\u636e\u57fa\u7840\u4e0a\uff0c\u4f46\u9700\u8981\u660e\u786e\u795e\u7ecf\u6570\u636e\u4f55\u65f6\u80fd\u771f\u6b63\u4e3a\u798f\u5229\u5224\u65ad\u63d0\u4f9b\u4f9d\u636e\uff0c\u800c\u975e\u4ec5\u63cf\u8ff0\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u975e\u7ecf\u9a8c\u6027\u7684\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u4fe1\u53f7\u3001\u8ba1\u7b97\u51b3\u7b56\u6a21\u578b\u548c\u89c4\u8303\u6027\u798f\u5229\u6807\u51c6\u4e09\u4e2a\u5c42\u6b21\u8054\u7cfb\u8d77\u6765\uff0c\u5728\u884c\u52a8\u8005-\u6279\u8bc4\u8005\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e2d\u5f62\u5f0f\u5316\u4e86\u4ece\u795e\u7ecf\u6d3b\u52a8\u5230\u6f5c\u5728\u4ef7\u503c\u548c\u9884\u6d4b\u8bef\u5dee\uff0c\u518d\u5230\u798f\u5229\u4e3b\u5f20\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u53ea\u6709\u5f53\u795e\u7ecf-\u8ba1\u7b97\u6620\u5c04\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3001\u51b3\u7b56\u6a21\u578b\u80fd\u8bc6\u522b\"\u771f\u5b9e\"\u5229\u76ca\u4e0e\u60c5\u5883\u4f9d\u8d56\u9519\u8bef\u3001\u798f\u5229\u6807\u51c6\u88ab\u660e\u786e\u6307\u5b9a\u548c\u8fa9\u62a4\u65f6\uff0c\u795e\u7ecf\u8bc1\u636e\u624d\u80fd\u7ea6\u675f\u798f\u5229\u5224\u65ad\u3002", "conclusion": "\u795e\u7ecf\u6570\u636e\u548c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5185\u90e8\u5956\u52b1\u4fe1\u53f7\u90fd\u662f\u8ba1\u7b97\u91cf\uff0c\u4e0d\u80fd\u5728\u6ca1\u6709\u660e\u786e\u89c4\u8303\u6027\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u88ab\u89c6\u4e3a\u798f\u5229\u8861\u91cf\u6807\u51c6\u3002\u4f5c\u8005\u4e3a\u76d1\u7ba1\u8005\u548c\u795e\u7ecfAI\u7cfb\u7edf\u8bbe\u8ba1\u8005\u5236\u5b9a\u4e86\u795e\u7ecf\u7ecf\u6d4e\u5b66\u798f\u5229\u63a8\u7406\u6e05\u5355\u3002"}}
{"id": "2511.19555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19555", "abs": "https://arxiv.org/abs/2511.19555", "authors": ["Ruiyang Xu"], "title": "Online Sparse Feature Selection in Data Streams via Differential Evolution", "comment": null, "summary": "The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5728\u7ebf\u5dee\u5206\u8fdb\u5316\u7a00\u758f\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff08ODESFS\uff09\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u7ef4\u6d41\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u95ee\u9898\uff0c\u901a\u8fc7\u6f5c\u5728\u56e0\u5b50\u5206\u6790\u548c\u5dee\u5206\u8fdb\u5316\u5b9e\u73b0\u7279\u5f81\u9009\u62e9\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u7a00\u758f\u6d41\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5728\u7279\u5f81\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u6539\u8fdb\u7279\u5f81\u9009\u62e9\u548c\u7f3a\u5931\u6570\u636e\u5904\u7406\u80fd\u529b\u3002", "method": "ODESFS\u7ed3\u5408\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u4f7f\u7528\u6f5c\u5728\u56e0\u5b50\u5206\u6790\u6a21\u578b\u8fdb\u884c\u7f3a\u5931\u503c\u586b\u8865\uff1b2\uff09\u901a\u8fc7\u5dee\u5206\u8fdb\u5316\u8fdb\u884c\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cODESFS\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684OSFS\u548cOS2FS\u65b9\u6cd5\uff0c\u80fd\u591f\u9009\u62e9\u6700\u4f18\u7279\u5f81\u5b50\u96c6\u5e76\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "ODESFS\u901a\u8fc7\u6539\u8fdb\u7684\u7f3a\u5931\u503c\u5904\u7406\u548c\u7279\u5f81\u8bc4\u4f30\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u6d41\u7279\u5f81\u9009\u62e9\u7684\u6027\u80fd\u3002"}}
{"id": "2511.19561", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19561", "abs": "https://arxiv.org/abs/2511.19561", "authors": ["Zecheng Pan", "Zhikang Chen", "Ding Li", "Min Zhang", "Sen Cui", "Hongshuo Jin", "Luqi Tao", "Yi Yang", "Deheng Ye", "Yu Zhang", "Tingting Zhu", "Tianling Ren"], "title": "Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport", "comment": null, "summary": "Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.", "AI": {"tldr": "OTMF\u662f\u4e00\u4e2a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u53d1\u73b0\u5e94\u7528\u4e8e\u4efb\u52a1\u5411\u91cf\u7684\u5171\u540c\u63a9\u7801\u6765\u5bf9\u9f50\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u8bed\u4e49\u51e0\u4f55\uff0c\u89e3\u51b3\u53c2\u6570\u63d2\u503c\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6743\u91cd\u7a7a\u95f4\u7684\u53c2\u6570\u63d2\u503c\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u7279\u5f81\u7a7a\u95f4\u7684\u663e\u8457\u5206\u5e03\u504f\u79fb\u5e76\u524a\u5f31\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "OTMF\u4f7f\u7528\u6700\u4f18\u4f20\u8f93\u8ba1\u5212\u53d1\u73b0\u5e94\u7528\u4e8e\u4efb\u52a1\u5411\u91cf\u7684\u5171\u540c\u63a9\u7801\uff0c\u9009\u62e9\u6027\u5730\u63d0\u53d6\u53ef\u8f6c\u79fb\u548c\u4efb\u52a1\u65e0\u5173\u7684\u7ec4\u4ef6\uff0c\u540c\u65f6\u4fdd\u7559\u6bcf\u4e2a\u4efb\u52a1\u7684\u72ec\u7279\u7ed3\u6784\u8eab\u4efd\u3002\u652f\u6301\u6301\u7eed\u878d\u5408\u8303\u5f0f\uff0c\u589e\u91cf\u96c6\u6210\u65b0\u4efb\u52a1\u5411\u91cf\u800c\u65e0\u9700\u91cd\u65b0\u8bbf\u95ee\u5148\u524d\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cOTMF\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u878d\u5408\u65b9\u9762\u5177\u6709\u5b9e\u9645\u548c\u7406\u8bba\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u878d\u5408\u3002"}}
{"id": "2511.20297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20297", "abs": "https://arxiv.org/abs/2511.20297", "authors": ["Shashank Kirtania", "Param Biyani", "Priyanshu Gupta", "Yasharth Bajpai", "Roshni Iyer", "Sumit Gulwani", "Gustavo Soares"], "title": "Improving Language Agents through BREW", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $\u03c4^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.", "AI": {"tldr": "BREW\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u548c\u7cbe\u70bc\u7ecf\u9a8c\u5b66\u4e60\u77e5\u8bc6\u5e93\u6765\u4f18\u5316LLM\u667a\u80fd\u4f53\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4efb\u52a1\u7cbe\u5ea610-20%\uff0c\u51cf\u5c11API\u8c03\u752810-15%\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8ePPO\u548cGRPO\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u6536\u655b\u56f0\u96be\uff0c\u4e14\u751f\u6210\u7684\u7b56\u7565\u96be\u4ee5\u89e3\u91ca\u3001\u9002\u5e94\u6216\u589e\u91cf\u6539\u8fdb\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165BREW\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u6784\u5efa\u548c\u7cbe\u70bc\u6765\u4f18\u5316\u667a\u80fd\u4f53\u3002\u91c7\u7528\u4efb\u52a1\u8bc4\u5206\u5668\u548c\u884c\u4e3a\u51c6\u5219\u5b66\u4e60\u6d1e\u5bdf\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u786e\u4fdd\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u6709\u6548\u7684\u8bb0\u5fc6\u5206\u533a\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u68c0\u7d22\u548c\u7cbe\u70bc\u6548\u7387\u3002", "result": "\u5728OSWorld\u3001\u03c4\u00b2Bench\u548cSpreadsheetBench\u7b49\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBREW\u5b9e\u73b0\u4e86\u4efb\u52a1\u7cbe\u5ea610-20%\u7684\u63d0\u5347\uff0cAPI/\u5de5\u5177\u8c03\u7528\u51cf\u5c1110-15%\uff0c\u6267\u884c\u65f6\u95f4\u66f4\u5feb\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BREW\u5c06\u77e5\u8bc6\u5e93\u786e\u7acb\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u63a7\u7684\u667a\u80fd\u4f53\u4f18\u5316\u57fa\u7840\uff0c\u4f5c\u4e3a\u5851\u9020\u884c\u4e3a\u7684\u663e\u5f0f\u6760\u6746\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u884c\u4e3a\u5851\u9020\u65b9\u5f0f\u3002"}}
{"id": "2511.19569", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19569", "abs": "https://arxiv.org/abs/2511.19569", "authors": ["Wentao Ye", "Jiaqi Hu", "Haobo Wang", "Xinpeng Ti", "Zhiqing Xiao", "Hao Chen", "Liyao Li", "Lei Feng", "Sai Wu", "Junbo Zhao"], "title": "An Invariant Latent Space Perspective on Language Model Inversion", "comment": "The Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInv^2A\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u7684\u6f5c\u5728\u7a7a\u95f4\u4e0d\u53d8\u6027\u6765\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u53cd\u6f14\u653b\u51fb\uff0c\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747BLEU\u5f97\u5206\u63d0\u53474.77%\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u5927\u578b\u53cd\u6f14\u8bed\u6599\u5e93\u7684\u4f9d\u8d56\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u53cd\u6f14(LMI)\u5bf9\u7528\u6237\u9690\u79c1\u548c\u7cfb\u7edf\u5b89\u5168\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\u4e14\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u3002\u4f5c\u8005\u89c2\u5bdf\u5230LLM\u6f5c\u5728\u7a7a\u95f4\u5177\u6709\u4e0d\u53d8\u6027\u7279\u5f81\uff0c\u63d0\u51fa\u5229\u7528\u8fd9\u4e00\u7279\u6027\u6539\u8fdb\u53cd\u6f14\u653b\u51fb\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e0d\u53d8\u6f5c\u5728\u7a7a\u95f4\u5047\u8bbe(ILSH)\uff0c\u5305\u62ec\u6e90\u4e0d\u53d8\u6027\u548c\u5faa\u73af\u4e0d\u53d8\u6027\u3002\u8bbe\u8ba1Inv^2A\u65b9\u6cd5\uff0c\u5c06LLM\u89c6\u4e3a\u4e0d\u53d8\u89e3\u7801\u5668\uff0c\u4ec5\u5b66\u4e60\u8f7b\u91cf\u7ea7\u53cd\u6f14\u7f16\u7801\u5668\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5bf9\u6bd4\u5bf9\u9f50(\u6e90\u4e0d\u53d8\u6027)\u548c\u76d1\u7763\u5f3a\u5316(\u5faa\u73af\u4e0d\u53d8\u6027)\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cInv^2A\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747BLEU\u5f97\u5206\u63d0\u53474.77%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u5927\u578b\u53cd\u6f14\u8bed\u6599\u5e93\u7684\u4f9d\u8d56\u3002\u5206\u6790\u8fd8\u663e\u793a\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u63d0\u4f9b\u7684\u4fdd\u62a4\u6709\u9650\u3002", "conclusion": "Inv^2A\u901a\u8fc7\u5229\u7528LLM\u6f5c\u5728\u7a7a\u95f4\u4e0d\u53d8\u6027\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u5f3a\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.20321", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20321", "abs": "https://arxiv.org/abs/2511.20321", "authors": ["Patrick Kenny"], "title": "Active Inference in Discrete State Spaces from First Principles", "comment": "56 pages", "summary": "We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u6f84\u6e05\u4e3b\u52a8\u63a8\u7406\u7684\u6982\u5ff5\uff0c\u5c06\u5176\u4e0e\u81ea\u7531\u80fd\u539f\u7406\u5206\u79bb\uff0c\u63d0\u51fa\u5728\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e3b\u52a8\u63a8\u7406\u7684\u4f18\u5316\u95ee\u9898\u53ef\u8868\u8ff0\u4e3a\u7ea6\u675f\u6563\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u53ef\u901a\u8fc7\u6807\u51c6\u5e73\u5747\u573a\u65b9\u6cd5\u6c42\u89e3\uff0c\u65e0\u9700\u4f9d\u8d56\u671f\u671b\u81ea\u7531\u80fd\u6982\u5ff5\u3002", "motivation": "\u6f84\u6e05\u4e3b\u52a8\u63a8\u7406\u4e0e\u81ea\u7531\u80fd\u539f\u7406\u7684\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e0d\u4f9d\u8d56\u671f\u671b\u81ea\u7531\u80fd\u7684\u4e3b\u52a8\u63a8\u7406\u5b9e\u73b0\u65b9\u6cd5\u3002", "method": "\u5c06\u4e3b\u52a8\u63a8\u7406\u5728\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u8868\u8ff0\u4e3a\u7ea6\u675f\u6563\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u6807\u51c6\u5e73\u5747\u573a\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u63d0\u51fa\u7684\u611f\u77e5/\u884c\u52a8\u6563\u5ea6\u51c6\u5219\u5728\u5efa\u6a21\u611f\u77e5\u65f6\u4e0e\u53d8\u5206\u81ea\u7531\u80fd\u4e00\u81f4\uff0c\u5728\u5efa\u6a21\u884c\u52a8\u65f6\u4e0e\u671f\u671b\u81ea\u7531\u80fd\u6cdb\u51fd\u76f8\u5dee\u4e00\u4e2a\u71b5\u6b63\u5219\u5316\u9879\u3002", "conclusion": "\u4e3b\u52a8\u63a8\u7406\u53ef\u4ee5\u901a\u8fc7\u7ea6\u675f\u6563\u5ea6\u6700\u5c0f\u5316\u6846\u67b6\u5b9e\u73b0\uff0c\u65e0\u9700\u4f9d\u8d56\u671f\u671b\u81ea\u7531\u80fd\u6982\u5ff5\uff0c\u4e3a\u4e3b\u52a8\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.19750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19750", "abs": "https://arxiv.org/abs/2511.19750", "authors": ["Julien T. T. Vignoud", "Val\u00e9rian Rousset", "Hugo El Guedj", "Ignacio Aleman", "Walid Bennaceur", "Batuhan Faik Derinbay", "Eduard \u010eurech", "Damien Gengler", "Lucas Giordano", "Felix Grimberg", "Franziska Lippoldt", "Christina Kopidaki", "Jiafan Liu", "Lauris Lopata", "Nathan Maire", "Paul Mansat", "Martin Milenkoski", "Emmanuel Omont", "G\u00fcne\u015f \u00d6zg\u00fcn", "Mina Petrovi\u0107", "Francesco Posa", "Morgan Ridel", "Giorgio Savini", "Marcel Torne", "Lucas Trognon", "Alyssa Unell", "Olena Zavertiaieva", "Sai Praneeth Karimireddy", "Tahseen Rabbani", "Mary-Anne Hartley", "Martin Jaggi"], "title": "DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning", "comment": null, "summary": "Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \\disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai", "AI": {"tldr": "DISCO\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u534f\u4f5c\u5b66\u4e60\u5e73\u53f0\uff0c\u5141\u8bb8\u975e\u6280\u672f\u7528\u6237\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u534f\u4f5c\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u652f\u6301\u8054\u90a6\u5b66\u4e60\u548c\u53bb\u4e2d\u5fc3\u5316\u8303\u5f0f\uff0c\u63d0\u4f9b\u591a\u79cd\u9690\u79c1\u4fdd\u62a4\u548c\u6743\u91cd\u805a\u5408\u7b56\u7565\u3002", "motivation": "\u6570\u636e\u5171\u4eab\u5b58\u5728\u9690\u79c1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u6cd5\u5f8b\u9650\u5236\u7b49\u95ee\u9898\uff0c\u8fd9\u4e0d\u4ec5\u5206\u6563\u4e86\u9884\u6d4b\u6a21\u578b\u7684\u7edf\u8ba1\u80fd\u529b\uff0c\u8fd8\u9020\u6210\u4e86\u53ef\u8bbf\u95ee\u6027\u504f\u89c1\uff0c\u4f7f\u5f97\u6a21\u578b\u51c6\u786e\u6027\u4e0d\u516c\u5e73\u5730\u5206\u914d\u7ed9\u90a3\u4e9b\u6709\u8d44\u6e90\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u7684\u673a\u6784\u3002", "method": "DISCO\u901a\u8fc7Web\u5e94\u7528\u7a0b\u5e8f\u5728\u6d4f\u89c8\u5668\u4e2d\u672c\u5730\u8bad\u7ec3\u6a21\u578b\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u8054\u90a6\u5b66\u4e60\u548c\u53bb\u4e2d\u5fc3\u5316\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e0d\u540c\u7ea7\u522b\u7684\u9690\u79c1\u4fdd\u8bc1\u548c\u591a\u79cd\u6743\u91cd\u805a\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u6a21\u578b\u4e2a\u6027\u5316\u548c\u504f\u89c1\u5f39\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8de8\u5e73\u53f0\u7684\u534f\u4f5c\u5b66\u4e60\u5de5\u5177\uff0c\u53ef\u5728\u5305\u62ec\u667a\u80fd\u624b\u673a\u5728\u5185\u7684\u5404\u79cd\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u7f16\u7a0b\u77e5\u8bc6\u5373\u53ef\u4f7f\u7528\u3002", "conclusion": "DISCO\u5e73\u53f0\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u8bbf\u95ee\u7684\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5171\u4eab\u969c\u788d\uff0c\u4fc3\u8fdb\u4e86\u516c\u5e73\u7684\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2511.17645", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17645", "abs": "https://arxiv.org/abs/2511.17645", "authors": ["Sandro Andric"], "title": "BlockCert: Certified Blockwise Extraction of Transformer Mechanisms", "comment": "16 pages, 1 figure", "summary": "Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.", "AI": {"tldr": "BlockCert\u662f\u4e00\u4e2a\u7528\u4e8e\u8ba4\u8bc1\u5f0f\u5757\u7ea7\u63d0\u53d6Transformer\u673a\u5236\u5e76\u652f\u6301\u8ba4\u8bc1\u5c40\u90e8\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u7ed3\u6784\u5316\u66ff\u4ee3\u5b9e\u73b0\u5e76\u63d0\u4f9b\u673a\u5668\u53ef\u68c0\u67e5\u7684\u8bc1\u4e66\u6765\u7ea6\u675f\u8fd1\u4f3c\u8bef\u5dee\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u578b\u7f16\u8f91\u9886\u57df\u901a\u5e38\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u65e0\u6cd5\u660e\u786e\u63d0\u53d6\u6216\u7f16\u8f91\u540e\u7684\u6a21\u578b\u4e0e\u539f\u59cb\u6a21\u578b\u5728\u76f8\u5173\u8f93\u5165\u4e0a\u7684\u504f\u5dee\u8303\u56f4\u3002", "method": "\u63d0\u51faBlockCert\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3Transformer\u548c\u63d0\u793a\u5206\u5e03\u63d0\u53d6\u6b8b\u5dee\u5757\u7684\u7ed3\u6784\u5316\u66ff\u4ee3\u5b9e\u73b0\uff0c\u63d0\u4f9b\u7ea6\u675f\u8fd1\u4f3c\u8bef\u5dee\u3001\u8bb0\u5f55\u8986\u76d6\u6307\u6807\u548c\u54c8\u5e0c\u5e95\u5c42\u5de5\u4ef6\u7684\u8bc1\u4e66\uff0c\u5e76\u5728Lean 4\u4e2d\u5f62\u5f0f\u5316Lipschitz\u7ec4\u5408\u5b9a\u7406\u3002", "result": "\u5728GPT-2 small\u3001TinyLlama-1.1B-Chat\u548cLlama-3.2-3B\u4e0a\u5e94\u7528\uff0c\u83b7\u5f97\u9ad8\u5757\u7ea7\u8986\u76d6\u7387\u548c\u5c0f\u7684\u6b8b\u5dee\u8bef\u5dee\uff0c\u5728TinyLlama\u8bbe\u7f6e\u4e2d\u5b8c\u5168\u62fc\u63a5\u6a21\u578b\u5728\u538b\u529b\u63d0\u793a\u4e0a\u4e0e\u57fa\u7ebf\u56f0\u60d1\u5ea6\u5339\u914d\u5728\u7ea66e-5\u8303\u56f4\u5185\u3002", "conclusion": "\u5757\u7ea7\u63d0\u53d6\u4e0e\u663e\u5f0f\u8bc1\u4e66\u5bf9\u4e8e\u771f\u5b9eTransformer\u8bed\u8a00\u6a21\u578b\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u578b\u884c\u4e3a\u7684\u5f62\u5f0f\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u6865\u6881\u3002"}}
{"id": "2511.19851", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19851", "abs": "https://arxiv.org/abs/2511.19851", "authors": ["Kun Guo", "Xuefei Li", "Xijun Wang", "Howard H. Yang", "Wei Feng", "Tony Q. S. Quek"], "title": "Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization", "comment": null, "summary": "Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5206\u5272\u4e0e\u8054\u90a6\u5b66\u4e60\uff08HSFL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5b66\u4e60\u6a21\u5f0f\u9009\u62e9\u3001\u6279\u6b21\u5927\u5c0f\u4ee5\u53ca\u901a\u4fe1\u548c\u8ba1\u7b97\u8d44\u6e90\u6765\u52a0\u901f\u5206\u5e03\u5f0f\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fbe\u5230\u76ee\u6807\u51c6\u786e\u7387\u6240\u9700\u7684\u5ef6\u8fdf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u652f\u6301\u4f4e\u5ef6\u8fdf\u5e76\u884c\u8bad\u7ec3\u4f46\u53ef\u80fd\u6536\u655b\u5230\u8f83\u4f4e\u51c6\u786e\u7387\uff0c\u800c\u5206\u5272\u5b66\u4e60\uff08SL\uff09\u80fd\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u4f46\u5ef6\u8fdf\u8f83\u5927\u3002\u4e3a\u4e86\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\u5e76\u4f18\u5316\u76f8\u5173\u53c2\u6570\u4ee5\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u9996\u5148\u5206\u6790\u6536\u655b\u6027\uff0c\u63ed\u793a\u5b66\u4e60\u6a21\u5f0f\u4e0e\u6279\u6b21\u5927\u5c0f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff1b\u7136\u540e\u5236\u5b9a\u5ef6\u8fdf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff1a\u4f7f\u7528\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u6c42\u89e3\u677e\u5f1b\u95ee\u9898\u83b7\u5f97\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u518d\u901a\u8fc7\u820d\u5165\u7b97\u6cd5\u6062\u590d\u6574\u6570\u6279\u6b21\u5927\u5c0f\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u8fbe\u5230\u76ee\u6807\u51c6\u786e\u7387\u7684\u6536\u655b\u8fc7\u7a0b\u3002", "conclusion": "HSFL\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5b66\u4e60\u6a21\u5f0f\u3001\u6279\u6b21\u5927\u5c0f\u548c\u8d44\u6e90\u5206\u914d\uff0c\u6709\u6548\u5e73\u8861\u4e86\u51c6\u786e\u7387\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u6bd4\u5355\u72ec\u4f7f\u7528FL\u6216SL\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.19942", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19942", "abs": "https://arxiv.org/abs/2511.19942", "authors": ["Jingchu Gai", "Guanning Zeng", "Huaqing Zhang", "Aditi Raghunathan"], "title": "Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning", "comment": null, "summary": "It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \\textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \\textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\\% improvements on AIME24 dataset.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u5206\u6790\u548c\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u8bc1\u660eRL\u5fae\u8c03\u5b58\u5728\u9009\u62e9\u548c\u5f3a\u5316\u504f\u5dee\u5bfc\u81f4\u591a\u6837\u6027\u5d29\u6e83\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u5dee\u5206\u5e73\u6ed1\u65b9\u6cd5\uff0c\u5728\u6b63\u786e\u8f68\u8ff9\u4e0a\u5e94\u7528\u5956\u52b1\u4fee\u6539\uff0c\u540c\u65f6\u63d0\u5347\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3RL\u5fae\u8c03\u591a\u6837\u6027\u5d29\u6e83\u7684\u65b9\u6cd5\u90fd\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a\u9700\u8981\u5728\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\u4e4b\u95f4\u6743\u8861\u3001\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02\u3001\u4e0d\u540c\u65b9\u6cd5\u751a\u81f3\u76f8\u4e92\u77db\u76fe\u3002\u9700\u8981\u5efa\u7acb\u7406\u8bba\u57fa\u7840\u5e76\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5dee\u5206\u5e73\u6ed1\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7406\u8bba\u5206\u6790\u5728\u6b63\u786e\u8f68\u8ff9\u4e0a\u5e94\u7528\u5956\u52b1\u4fee\u6539\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u80fd\u591f\u540c\u65f6\u6539\u5584\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u4f18\u4e8e\u4f20\u7edfRL\u548c\u57fa\u4e8e\u71b5\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u57281B\u52307B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6db5\u76d6CountDown\u548c\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u63a8\u7406\u7b49\u9886\u57df\u3002\u5dee\u5206\u5e73\u6ed1\u5728Pass@1\u548cPass@k\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u63d0\u5347\uff0c\u5728AIME24\u6570\u636e\u96c6\u4e0a\u6700\u9ad8\u63d0\u53476.7%\u3002", "conclusion": "\u5dee\u5206\u5e73\u6ed1\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u90fd\u80fd\u4e00\u81f4\u5730\u540c\u65f6\u63d0\u5347\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86RL\u5fae\u8c03\u4e2d\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2511.19986", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19986", "abs": "https://arxiv.org/abs/2511.19986", "authors": ["Lianming Huang", "Haibo Hu", "Qiao Li", "Nan Guan", "Chun Jason Xue"], "title": "On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices", "comment": null, "summary": "Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6309\u9700\u591a\u4efb\u52a1\u7a00\u758f\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u53c2\u6570\u91cd\u7528\u548c\u5757\u7c92\u5ea6\u5206\u89e3\u6765\u6700\u5c0f\u5316\u4efb\u52a1\u5207\u6362\u65f6\u7684I/O\u5f00\u9500\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e866.6\u500d\u7684\u5e73\u5747\u5207\u6362\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5316\u65b9\u6cd5\u5728\u4f18\u5316\u5355\u4e2a\u4efb\u52a1\u65f6\u5ffd\u7565\u4e86\u9891\u7e41\u4efb\u52a1\u5207\u6362\u5e26\u6765\u7684\u663e\u8457I/O\u5f00\u9500\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u6765\u6700\u5c0f\u5316\u5207\u6362\u6210\u672c\u3002", "method": "\u5c06\u6743\u91cd\u5206\u89e3\u4e3a\u53ef\u91cd\u7528\u7684\u5757\u7c92\u5ea6\u5355\u5143\uff0c\u8de8\u4efb\u52a1\u5bf9\u9f50\u7a00\u758f\u7ed3\u6784\u4ee5\u6700\u5927\u5316\u91cd\u53e0\uff0c\u52a8\u6001\u52a0\u8f7d\u4e0b\u4e00\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u5c0f\u578b\u5dee\u5f02\u5757\u96c6\u5408\u3002", "result": "\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5207\u6362\u6548\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u7a00\u758f\u65b9\u6cd5\u5e73\u5747\u52a0\u901f\u4efb\u52a1\u5207\u6362\u8d85\u8fc76.6\u500d\u3002", "conclusion": "\u8be5\u6309\u9700\u591a\u4efb\u52a1\u7a00\u758f\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u91cd\u7528\u548c\u52a8\u6001\u5757\u52a0\u8f7d\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4f20\u7edf\u6574\u4f53\u65b9\u6cd5\u56fa\u6709\u7684\u51b7\u542f\u52a8\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5207\u6362\u6548\u7387\u3002"}}
{"id": "2511.20030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20030", "abs": "https://arxiv.org/abs/2511.20030", "authors": ["Haoran Zheng", "Renchi Yang", "Hongtao Wang", "Jianliang Xu"], "title": "Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering", "comment": "Accepted by SIGKDD 2026. The code is available at https://github.com/HaoranZ99/DGF", "summary": "Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.\n  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5c5e\u6027\u56fe\uff08MMAGs\uff09\u7684\u53cc\u56fe\u6ee4\u6ce2\uff08DGF\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u7279\u5f81\u7ea7\u53bb\u566a\u548c\u4e09\u4ea4\u53c9\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u89c6\u56fe\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u9884\u8bad\u7ec3\u6a21\u578b\u8f93\u51fa\u7684\u591a\u6a21\u6001\u5c5e\u6027\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u89c6\u56fe\u805a\u7c7b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u89c6\u56fe\u95f4\u7684\u9ad8\u76f8\u5173\u6027\uff0c\u5ffd\u89c6\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u8f93\u51fa\u7684\u591a\u6a21\u6001\u5c5e\u6027\u4e2d\u5b58\u5728\u7684\u4f4e\u6a21\u6001\u76f8\u5173\u6027\u548c\u5f3a\u7279\u5f81\u566a\u58f0\u95ee\u9898\uff0c\u5bfc\u81f4\u805a\u7c7b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u53cc\u56fe\u6ee4\u6ce2\uff08DGF\uff09\u65b9\u6848\uff0c\u5305\u542b\u7279\u5f81\u7ea7\u53bb\u566a\u7ec4\u4ef6\u548c\u4e09\u4ea4\u53c9\u5bf9\u6bd4\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u3001\u90bb\u57df\u548c\u793e\u533a\u7684\u5b9e\u4f8b\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u9c81\u68d2\u4e14\u5177\u6709\u533a\u5206\u5ea6\u7684\u8282\u70b9\u8868\u793a\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6MMAG\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cDGF\u5728\u805a\u7c7b\u8d28\u91cf\u65b9\u9762\u80fd\u591f\u4e00\u81f4\u4e14\u663e\u8457\u5730\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DGF\u65b9\u6848\u901a\u8fc7\u521b\u65b0\u7684\u56fe\u6ee4\u6ce2\u8bbe\u8ba1\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u591a\u89c6\u56fe\u56fe\u805a\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u6a21\u6001\u5c5e\u6027\u56fe\u805a\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.20066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20066", "abs": "https://arxiv.org/abs/2511.20066", "authors": ["Bhavya Sukhija", "Lenart Treven", "Carmelo Sferrazza", "Florian D\u00f6rfler", "Pieter Abbeel", "Andreas Krause"], "title": "SOMBRL: Scalable and Optimistic Model-Based RL", "comment": null, "summary": "We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.", "AI": {"tldr": "\u63d0\u51faSOMBRL\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e50\u89c2\u539f\u5219\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u5956\u52b1\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u63a2\u7d22\uff0c\u5728\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e9a\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u672a\u77e5\u7cfb\u7edf\u52a8\u6001\u4e0b\u7684\u9ad8\u6548\u63a2\u7d22\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u52a8\u6001\u6a21\u578b\uff0c\u8d2a\u5a6a\u5730\u6700\u5927\u5316\u5916\u90e8\u5956\u52b1\u548c\u667a\u80fd\u4f53\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u548c\uff0c\u517c\u5bb9\u4efb\u4f55\u7b56\u7565\u4f18\u5316\u5668\u6216\u89c4\u5212\u5668\u3002", "result": "\u5728\u6709\u9650\u65f6\u57df\u3001\u6298\u6263\u65e0\u9650\u65f6\u57df\u548c\u975e\u60c5\u666f\u8bbe\u7f6e\u4e2d\u8bc1\u660e\u4e86\u5bf9\u975e\u7ebf\u6027\u52a8\u6001\u7684\u4e9a\u7ebf\u6027\u9057\u61be\uff0c\u5728\u72b6\u6001\u548c\u89c6\u89c9\u63a7\u5236\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u52a8\u6001RC\u6c7d\u8f66\u786c\u4ef6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "SOMBRL\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u539f\u5219\u6027\u63a2\u7d22\u5728MBRL\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.20105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20105", "abs": "https://arxiv.org/abs/2511.20105", "authors": ["Grzegorz Dudek", "Mateusz Kasprzyk", "Pawe\u0142 Pe\u0142ka"], "title": "Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives", "comment": null, "summary": "This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5e94\u7528LightGBM\u6a21\u578b\u5bf9\u6bd4\u7279\u5e01\u5b9e\u73b0\u6ce2\u52a8\u7387\u8fdb\u884c\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u9884\u6d4b\uff0c\u4f7f\u752869\u4e2a\u5e02\u573a\u3001\u884c\u4e3a\u548c\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u4f5c\u4e3a\u9884\u6d4b\u56e0\u5b50\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0e\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u548c\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6355\u6349\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u975e\u7ebf\u6027\u548c\u9ad8\u65b9\u5dee\u7279\u5f81\u7684\u6ce2\u52a8\u7387\u9884\u6d4b\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u5bf9\u6ce2\u52a8\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u89c1\u89e3\u3002", "method": "\u91c7\u7528LightGBM\u6a21\u578b\u8fdb\u884c\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u9884\u6d4b\uff0c\u5bf9\u4e8e\u6982\u7387\u6027\u9884\u6d4b\u63a2\u7d22\u4e86\u4e24\u79cd\u5206\u4f4d\u6570\u65b9\u6cd5\uff1a\u4f7f\u7528pinball\u635f\u5931\u51fd\u6570\u7684\u76f4\u63a5\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u4ee5\u53ca\u5c06\u70b9\u9884\u6d4b\u8f6c\u6362\u4e3a\u9884\u6d4b\u5206\u5e03\u7684\u6b8b\u5dee\u6a21\u62df\u65b9\u6cd5\u3002\u4f7f\u7528\u589e\u76ca\u548c\u7f6e\u6362\u7279\u5f81\u91cd\u8981\u6027\u6280\u672f\u8bc6\u522b\u4e3b\u8981\u6ce2\u52a8\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u7ed3\u679c\u8868\u660eLGBM\u6a21\u578b\u6709\u6548\u6355\u6349\u4e86\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u7684\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u8bc6\u522b\u51fa\u4ea4\u6613\u91cf\u3001\u6ede\u540e\u6ce2\u52a8\u7387\u6307\u6807\u3001\u6295\u8d44\u8005\u5173\u6ce8\u5ea6\u548c\u5e02\u503c\u4e3a\u4e3b\u8981\u6ce2\u52a8\u9a71\u52a8\u56e0\u7d20\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "LGBM\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u6bd4\u7279\u5e01\u6ce2\u52a8\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u5bf9\u6ce2\u52a8\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u89c1\u89e3\uff0c\u4e3a\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2511.20257", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20257", "abs": "https://arxiv.org/abs/2511.20257", "authors": ["Zhiguo Zhang", "Xiaoliang Ma", "Daniel Schlesinger"], "title": "Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling", "comment": "Accepted to 2025 IEEE International Conference on Big Data", "summary": "Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u3001\u53ef\u89e3\u91ca\u7684\u65f6\u7a7a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7a7a\u6c14\u6c61\u67d3\u9884\u6d4b\uff0c\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u51c6\u786e\u7684\u7a7a\u6c14\u6c61\u67d3\u9884\u6d4b\u5bf9\u516c\u5171\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u5c06\u6c61\u67d3\u7269\u6d53\u5ea6\u7684\u65f6\u7a7a\u884c\u4e3a\u5206\u89e3\u4e3a\u4e24\u4e2a\u900f\u660e\u52a0\u6027\u6a21\u5757\uff1a\u7269\u7406\u5f15\u5bfc\u7684\u4f20\u8f93\u6838\u548c\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u65af\u5fb7\u54e5\u5c14\u6469\u5730\u533a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c06\u9ad8\u9884\u6d4b\u6027\u80fd\u4e0e\u65f6\u7a7a\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u5b9e\u9645\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2511.20273", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20273", "abs": "https://arxiv.org/abs/2511.20273", "authors": ["Areeb Ahmad", "Abhinav Joshi", "Ashutosh Modi"], "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits", "comment": "Accepted at NeurIPS 2025", "summary": "Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7ec6\u7c92\u5ea6\u7684\u89c6\u89d2\uff0c\u5c06Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u5934\u548cMLP\u5c42\u5206\u89e3\u4e3a\u6b63\u4ea4\u5947\u5f02\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u5355\u4e2a\u7ec4\u4ef6\u5185\u53e0\u52a0\u7684\u72ec\u7acb\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u901a\u5e38\u5c06\u6ce8\u610f\u529b\u5934\u548cMLP\u5c42\u89c6\u4e3a\u4e0d\u53ef\u5206\u5272\u5355\u5143\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u5185\u90e8\u53ef\u80fd\u5b66\u4e60\u5230\u7684\u529f\u80fd\u5b50\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u5c06Transformer\u7ec4\u4ef6\u5206\u89e3\u4e3a\u6b63\u4ea4\u5947\u5f02\u65b9\u5411\uff0c\u5206\u6790IOI\u3001GP\u548cGT\u7b49\u6807\u51c6\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u5148\u524d\u8bc6\u522b\u7684\u5178\u578b\u529f\u80fd\u5934\uff08\u5982\u540d\u79f0\u79fb\u52a8\u5668\uff09\u7f16\u7801\u4e86\u591a\u4e2a\u91cd\u53e0\u5b50\u529f\u80fd\uff0c\u8fd9\u4e9b\u5b50\u529f\u80fd\u4e0e\u4e0d\u540c\u7684\u5947\u5f02\u65b9\u5411\u5bf9\u9f50\u3002\u8ba1\u7b97\u56fe\u4e2d\u7684\u8282\u70b9\u5728\u7279\u5b9a\u4f4e\u79e9\u65b9\u5411\u4e0a\u8868\u73b0\u51fa\u5f3a\u6fc0\u6d3b\u3002", "conclusion": "Transformer\u8ba1\u7b97\u6bd4\u5148\u524d\u5047\u8bbe\u7684\u66f4\u52a0\u5206\u5e03\u5f0f\u3001\u7ed3\u6784\u5316\u548c\u7ec4\u5408\u6027\uff0c\u8fd9\u4e3a\u7ec6\u7c92\u5ea6\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.20347", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20347", "abs": "https://arxiv.org/abs/2511.20347", "authors": ["Chang Gao", "Chujie Zheng", "Xiong-Hui Chen", "Kai Dang", "Shixuan Liu", "Bowen Yu", "An Yang", "Shuai Bai", "Jingren Zhou", "Junyang Lin"], "title": "Soft Adaptive Policy Optimization", "comment": null, "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.", "AI": {"tldr": "SAPO\u662f\u4e00\u79cd\u8f6f\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e29\u5ea6\u63a7\u5236\u7684\u95e8\u673a\u5236\u66ff\u4ee3\u786c\u88c1\u526a\uff0c\u5728\u4fdd\u6301\u5e8f\u5217\u7ea7\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08\u5982GSPO\u548cGRPO\uff09\u4f7f\u7528\u786c\u88c1\u526a\u5904\u7406\u91cd\u8981\u6027\u6bd4\u7387\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u6709\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u8f6f\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\uff08SAPO\uff09\uff0c\u4f7f\u7528\u5e73\u6ed1\u7684\u6e29\u5ea6\u63a7\u5236\u95e8\u673a\u5236\u81ea\u9002\u5e94\u5730\u8870\u51cf\u79bb\u7b56\u7565\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u7528\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAPO\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u66f4\u9ad8\u7684Pass@1\u6027\u80fd\uff1b\u5728Qwen3-VL\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5e94\u7528\u663e\u793a\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SAPO\u4e3aLLMs\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u548c\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2511.20220", "categories": ["cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.20220", "abs": "https://arxiv.org/abs/2511.20220", "authors": ["Ruxandra-Stefania Tudose", "Moritz H. W. Gr\u00fcss", "Grace Ra Kim", "Karl H. Johansson", "Nicola Bastianello"], "title": "Communication-Efficient Learning for Satellite Constellations", "comment": null, "summary": "Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u536b\u661f\u661f\u5ea7\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u672c\u5730\u8bad\u7ec3\u3001\u538b\u7f29\u548c\u8bef\u5dee\u53cd\u9988\u673a\u5236\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u661f\u5ea7\u7684\u666e\u53ca\uff0c\u9700\u8981\u89e3\u51b3\u5728\u8fd9\u4e9b\u661f\u5ea7\u4e0a\u8fdb\u884c\u5b66\u4e60\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u8bbe\u8ba1\u901a\u4fe1\u6548\u7387\u9ad8\u4e14\u51c6\u786e\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u536b\u661f\u6536\u96c6\u5e76\u672c\u5730\u5904\u7406\u6570\u636e\uff0c\u5730\u9762\u7ad9\u805a\u5408\u672c\u5730\u6a21\u578b\u3002\u4f7f\u7528\u672c\u5730\u8bad\u7ec3\u51cf\u5c11\u901a\u4fe1\u6b21\u6570\uff0c\u538b\u7f29\u51cf\u5c11\u901a\u4fe1\u91cf\uff0c\u5e76\u5f15\u5165\u8bef\u5dee\u53cd\u9988\u673a\u5236\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u7a7a\u95f4\u573a\u666f\u7684\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u536b\u661f\u661f\u5ea7\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u7684\u826f\u597d\u5e73\u8861\uff0c\u8bef\u5dee\u53cd\u9988\u673a\u5236\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2511.20626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20626", "abs": "https://arxiv.org/abs/2511.20626", "authors": ["Wei He", "Kai Han", "Hang Zhou", "Hanting Chen", "Zhicheng Liu", "Xinghao Chen", "Yunhe Wang"], "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "comment": null, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "AI": {"tldr": "ROOT\u662f\u4e00\u79cd\u9c81\u68d2\u6b63\u4ea4\u5316\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u53cc\u9c81\u68d2\u673a\u5236\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u6b63\u4ea4\u5316\u7cbe\u5ea6\u7ef4\u6570\u8106\u5f31\u6027\u548c\u5f02\u5e38\u503c\u8bf1\u5bfc\u566a\u58f0\u95ee\u9898\uff0c\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u548c\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7b97\u6cd5\u4e0d\u7cbe\u786e\u6027\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u654f\u611f\u6027\u52a0\u5267\u3002\u73b0\u6709\u57fa\u4e8e\u52a8\u91cf\u6b63\u4ea4\u5316\u7684\u4f18\u5316\u5668\u5b58\u5728\u6b63\u4ea4\u5316\u7cbe\u5ea6\u7ef4\u6570\u8106\u5f31\u6027\u548c\u5f02\u5e38\u503c\u8bf1\u5bfc\u566a\u58f0\u8106\u5f31\u6027\u4e24\u4e2a\u5173\u952e\u9c81\u68d2\u6027\u9650\u5236\u3002", "method": "\u63d0\u51faROOT\u4f18\u5316\u5668\uff1a1\uff09\u4f7f\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u548c\u7ec6\u7c92\u5ea6\u7cfb\u6570\u7684\u7ef4\u6570\u9c81\u68d2\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u786e\u4fdd\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u4e00\u81f4\u6027\u7cbe\u5ea6\uff1b2\uff09\u901a\u8fc7\u8fd1\u7aef\u4f18\u5316\u7684\u4f18\u5316\u9c81\u68d2\u6846\u67b6\uff0c\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\u540c\u65f6\u4fdd\u7559\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u76f8\u6bd4Muon\u548cAdam\u7c7b\u4f18\u5316\u5668\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f18\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e0b\u3002", "conclusion": "ROOT\u4e3a\u5f00\u53d1\u80fd\u591f\u5904\u7406\u73b0\u4ee3\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u590d\u6742\u6027\u7684\u9c81\u68d2\u7cbe\u786e\u4f18\u5316\u5668\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.20349", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20349", "abs": "https://arxiv.org/abs/2511.20349", "authors": ["M. E. A. Kherchouche", "F. Galpin", "T. Dumas", "F. Schnitzler", "D. Menard", "L. Zhang"], "title": "Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning", "comment": "2025 Data Compression Conference (DCC)", "summary": "In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86VVC\u5e27\u5185\u5206\u533a\u7684\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u52a0\u901fRDO\u8fc7\u7a0b\u4e2d\u7684\u7a77\u4e3e\u641c\u7d22\u3002\u7b2c\u4e00\u79cd\u662f\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\u9884\u6d4bCU\u7684\u5f52\u4e00\u5316RD\u6210\u672c\uff0c\u7b2c\u4e8c\u79cd\u662f\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5c06\u5206\u533a\u51b3\u7b56\u5efa\u6a21\u4e3aMDP\u95ee\u9898\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5229\u7528\u76f8\u90bb\u5757\u7684RD\u6210\u672c\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u9884\u8bbe\u9608\u503c\u9009\u62e9\u5408\u9002\u7684\u5206\u533a\u65b9\u5f0f\u3002", "motivation": "\u89e3\u51b3VVC\u5e27\u5185\u5206\u533a\u5728RDO\u8fc7\u7a0b\u4e2d\u7a77\u4e3e\u641c\u7d22\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u52a0\u901f\u7f16\u7801\u8fc7\u7a0b\u3002", "method": "1. \u56de\u5f52\u65b9\u6cd5\uff1a\u9884\u6d4bCU\u7684\u5f52\u4e00\u5316RD\u6210\u672c\uff1b2. \u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a\u5c06\u5206\u533a\u51b3\u7b56\u5efa\u6a21\u4e3aMDP\u95ee\u9898\uff0c\u4f7f\u7528DQN\u7b97\u6cd5\u4ece\u4e24\u4e2a\u6df1\u5ea6\u7684CU\u51b3\u7b56\u8f68\u8ff9\u4e2d\u5b66\u4e60\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5229\u7528\u76f8\u90bb\u5757\u7684RD\u6210\u672c\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u9884\u8bbe\u9608\u503c\u8fdb\u884c\u5206\u533a\u9009\u62e9\u3002", "result": "\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u90fd\u662f\u5c3a\u5bf8\u65e0\u5173\u7684\uff0c\u5e76\u6210\u529f\u6574\u5408\u4e86\u76f8\u90bb\u5757\u7684RD\u6210\u672c\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\uff0c\u80fd\u591f\u6709\u6548\u52a0\u901fVVC\u5e27\u5185\u5206\u533a\u7684RDO\u8fc7\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3VVC\u5e27\u5185\u5206\u533a\u7684\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u5c3a\u5bf8\u65e0\u5173\u65b9\u6cd5\u5177\u6709\u8f83\u597d\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f16\u7801\u6548\u7387\u3002"}}
{"id": "2511.20395", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20395", "abs": "https://arxiv.org/abs/2511.20395", "authors": ["M. C. Schoppema", "B. H. M. van der Velden", "A. H\u00fcrriyeto\u011flu", "M. D. Klijnstra", "E. J. Faassen", "A. Gerssen", "H. J. van der Fels-Klerx"], "title": "Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI", "comment": "18 pages, 6 figures, submitted to Nature Food", "summary": "Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.\n  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.\n  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.\n  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6765\u9884\u6d4b\u8377\u5170\u6cfd\u5170\u6cb3\u53e3\u53cc\u58f3\u7c7b\u8f6f\u4f53\u52a8\u7269\u4e2d\u7684\u6cb3\u8c5a\u6bd2\u7d20\u6c61\u67d3\uff0c\u8bc6\u522b\u51fa\u65e5\u7167\u65f6\u95f4\u3001\u5168\u7403\u8f90\u5c04\u3001\u6c34\u6e29\u548c\u6c2f\u5316\u7269\u6d53\u5ea6\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u81ea2012\u5e74\u4ee5\u6765\uff0c\u6b27\u6d32\u6e29\u5e26\u6c34\u57df\u7684\u53cc\u58f3\u7c7b\u8f6f\u4f53\u52a8\u7269\u4e2d\u53d1\u73b0\u6cb3\u8c5a\u6bd2\u7d20\uff0c\u5bfc\u81f4\u98df\u54c1\u5b89\u5168\u98ce\u9669\u548c\u7ecf\u6d4e\u635f\u5931\uff0c\u9700\u8981\u65e9\u671f\u9884\u6d4b\u6cb3\u8c5a\u6bd2\u7d20\u6c61\u67d3\u3002", "method": "\u4f7f\u7528\u6c14\u8c61\u548c\u6c34\u6587\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u6765\u9884\u6d4b\u6cb3\u8c5a\u6bd2\u7d20\u6c61\u67d3\u7684\u5b58\u5728\u6216\u7f3a\u5931\u3002", "result": "\u6a21\u578b\u8bc6\u522b\u51fa\u65e5\u51fa\u65f6\u95f4\u3001\u65e5\u843d\u65f6\u95f4\u3001\u5168\u7403\u8f90\u5c04\u3001\u6c34\u6e29\u548c\u6c2f\u5316\u7269\u6d53\u5ea6\u5bf9\u6cb3\u8c5a\u6bd2\u7d20\u6c61\u67d3\u8d21\u732e\u6700\u5927\uff0c\u8868\u660e\u6709\u6548\u65e5\u7167\u65f6\u95f4\u662f\u91cd\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc6\u522b\u51fa\u7684\u73af\u5883\u56e0\u7d20\u53ef\u7528\u4e8e\u51cf\u8f7b\u6d77\u6d0b\u6bd2\u7d20\u98ce\u9669\uff0c\u4e3a\u98df\u54c1\u884c\u4e1a\u548c\u4e3b\u7ba1\u90e8\u95e8\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2511.20516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20516", "abs": "https://arxiv.org/abs/2511.20516", "authors": ["Sam Laing", "Antonio Orvieto"], "title": "Adam Simplified: Bias Correction Simplified", "comment": null, "summary": "The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $\u03b2_1, \u03b2_2 \\in [0,1)$. Our findings challenge the universal inclusion of this component.", "AI": {"tldr": "\u672c\u6587\u5bf9Adam\u4f18\u5316\u5668\u4e2d\u7684\u504f\u5dee\u6821\u6b63\u7ec4\u4ef6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u53d1\u73b0\u5728\u6700\u4f18\u8d85\u53c2\u6570\u914d\u7f6e\u4e0b\uff0c\u504f\u5dee\u6821\u6b63\u4e0d\u4f1a\u6539\u5584\u6700\u7ec8\u6d4b\u8bd5\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u6709\u5bb3\u3002", "motivation": "Adam\u4f18\u5316\u5668\u662f\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u77f3\uff0c\u4f46\u5176\u5404\u4e2a\u7ec4\u4ef6\u7684\u7ecf\u9a8c\u5fc5\u8981\u6027\u5e38\u5e38\u88ab\u7406\u6240\u5f53\u7136\u5730\u63a5\u53d7\u3002\u504f\u5dee\u6821\u6b63\u8fd9\u4e00\u7ec4\u4ef6\u7684\u8d21\u732e\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e00\u7cfb\u5217\u7cfb\u7edf\u6027\u6d88\u878d\u5b9e\u9a8c\uff0c\u7814\u7a76\u504f\u5dee\u6821\u6b63\u7684\u4f5c\u7528\u3002", "result": "\u5728\u6700\u4f18\u8d85\u53c2\u6570\u914d\u7f6e\u4e0b\uff0c\u5305\u542b\u504f\u5dee\u6821\u6b63\u4e0d\u4f1a\u6539\u5584\u6700\u7ec8\u6d4b\u8bd5\u6027\u80fd\uff1b\u9664\u975e\u5b9e\u65bd\u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u5426\u5219\u504f\u5dee\u6821\u6b63\u6709\u65f6\u4f1a\u5bf9\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u504f\u5dee\u6821\u6b63\u53ef\u88ab\u91cd\u65b0\u89e3\u91ca\u4e3a\u4e00\u79cd\u9690\u5f0f\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u5176\u884c\u4e3a\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u5e73\u6ed1\u8d85\u53c2\u6570\u03b21\u3001\u03b22\u7684\u9009\u62e9\u3002\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u666e\u904d\u5305\u542b\u8be5\u7ec4\u4ef6\u7684\u505a\u6cd5\u3002"}}
{"id": "2511.20543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20543", "abs": "https://arxiv.org/abs/2511.20543", "authors": ["Alhasan Abdellatif", "Hannah P. Menke", "Ahmed H. Elsheikh", "Florian Doster", "Kamaljit Singh"], "title": "Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media", "comment": null, "summary": "The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.", "AI": {"tldr": "UFNO-FiLM\u901a\u8fc7\u5f15\u5165FiLM\u5c42\u89e3\u8026\u6807\u91cf\u8f93\u5165\u4e0e\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u7a7a\u95f4\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u5728\u4fdd\u6301UFNO\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u91cf\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5730\u4e0b\u591a\u76f8\u6d41\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e8621%\u7684MAE\u964d\u4f4e\u3002", "motivation": "UFNO\u867d\u7136\u901a\u8fc7\u5e76\u884cUNet\u8def\u5f84\u6539\u8fdb\u4e86FNO\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5c06\u6807\u91cf\u8f93\u5165\uff08\u5982\u6e29\u5ea6\u3001\u6ce8\u5165\u901f\u7387\uff09\u4f5c\u4e3a\u7a7a\u95f4\u5206\u5e03\u573a\u5904\u7406\uff0c\u5bfc\u81f4\u5728\u9891\u57df\u4e2d\u5904\u7406\u5197\u4f59\u7684\u6052\u5b9a\u4fe1\u53f7\uff1b2\uff09\u6807\u51c6\u635f\u5931\u51fd\u6570\u672a\u8003\u8651\u8bef\u5dee\u654f\u611f\u6027\u7684\u7a7a\u95f4\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5728\u91cd\u8981\u7269\u7406\u533a\u57df\u7684\u6027\u80fd\u3002", "method": "1. \u4f7f\u7528\u7279\u5f81\u7ebf\u6027\u8c03\u5236\uff08FiLM\uff09\u5c42\u89e3\u8026\u6807\u91cf\u8f93\u5165\u4e0e\u7a7a\u95f4\u7279\u5f81\uff0c\u907f\u514d\u5c06\u6052\u5b9a\u4fe1\u53f7\u5f15\u5165\u5085\u91cc\u53f6\u53d8\u6362\uff1b2. \u91c7\u7528\u7a7a\u95f4\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5148\u5b66\u4e60\u5173\u952e\u533a\u57df\u3002", "result": "\u5728\u5730\u4e0b\u591a\u76f8\u6d41\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4UFNO\u5b9e\u73b0\u4e8621%\u7684\u6c14\u4f53\u9971\u548c\u5ea6\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "UFNO-FiLM\u901a\u8fc7FiLM\u5c42\u548c\u7a7a\u95f4\u52a0\u6743\u635f\u5931\u7684\u521b\u65b0\u7ec4\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86UFNO\u5728\u5904\u7406\u6807\u91cf\u8f93\u5165\u548c\u7a7a\u95f4\u8bef\u5dee\u654f\u611f\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7269\u7406\u573a\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002"}}
{"id": "2511.20584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20584", "abs": "https://arxiv.org/abs/2511.20584", "authors": ["Shuo Xie", "Tianhao Wang", "Beining Wu", "Zhiyuan Li"], "title": "A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent", "comment": null, "summary": "Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u5668\u4e0e\u5f52\u4e00\u5316\u6700\u901f\u4e0b\u964d(NSD)\u4e4b\u95f4\u7684\u7d27\u5bc6\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u6982\u5ff5\uff0c\u5e76\u5728\u51f8\u548c\u975e\u51f8\u8bbe\u7f6e\u4e0b\u5efa\u7acb\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u6536\u655b\u7406\u8bba\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u80fd\u591f\u5b9e\u73b0Nesterov\u52a8\u91cf\u52a0\u901f\uff0c\u5e76\u6269\u5c55\u4e86\u968f\u673a\u4f18\u5316\u4e2d\u7684\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u5dee\u6982\u5ff5\u3002", "motivation": "\u81ea\u9002\u5e94\u4f18\u5316\u5668\u5728\u4ec5\u9002\u5e94\u5f53\u524d\u68af\u5ea6\u65f6\u4f1a\u9000\u5316\u4e3a\u5f52\u4e00\u5316\u6700\u901f\u4e0b\u964d\uff0c\u4f46\u4e24\u8005\u7684\u5206\u6790\u4f9d\u8d56\u4e8e\u4e0d\u540c\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u5c06\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u6982\u5ff5\u6269\u5c55\u5230\u975e\u51f8\u8bbe\u7f6e\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u52a0\u901f\u548c\u968f\u673a\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u7406\u8bba\u6269\u5c55\u5230\u975e\u51f8\u8bbe\u7f6e\uff0c\u5efa\u7acb\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u6536\u655b\u7279\u6027\uff1b\u5728\u51f8\u8bbe\u7f6e\u4e0b\u8bc1\u660e\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u80fd\u591f\u5b9e\u73b0Nesterov\u52a8\u91cf\u52a0\u901f\uff1b\u5f15\u5165\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u5dee\u6982\u5ff5\uff0c\u4e3a\u968f\u673a\u4f18\u5316\u63d0\u4f9b\u7ef4\u5ea6\u65e0\u5173\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u7cbe\u786e\u523b\u753b\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u6536\u655b\u884c\u4e3a\uff1b\u5728\u51f8\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86Nesterov\u52a8\u91cf\u52a0\u901f\uff0c\u8fd9\u5728\u6807\u51c6\u5e73\u6ed1\u6027\u4e0b\u5bf9\u4e8e\u67d0\u4e9b\u975e\u6b27\u51e0\u4f55\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\uff1b\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u5dee\u4e3a\u968f\u673a\u4f18\u5316\u63d0\u4f9b\u4e86\u7ef4\u5ea6\u65e0\u5173\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u81ea\u9002\u5e94\u5e73\u6ed1\u6027\u4e3a\u81ea\u9002\u5e94\u4f18\u5316\u5668\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5176\u4e0e\u5f52\u4e00\u5316\u6700\u901f\u4e0b\u964d\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u5e76\u5728\u52a0\u901f\u548c\u968f\u673a\u4f18\u5316\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u5728\u975e\u6b27\u51e0\u4f55\u8bbe\u7f6e\u4e0b\u3002"}}

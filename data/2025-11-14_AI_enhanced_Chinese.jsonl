{"id": "2511.09802", "categories": ["eess.SP", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.09802", "abs": "https://arxiv.org/abs/2511.09802", "authors": ["Parinaz Binandeh Dehaghani", "Danilo Pena", "A. Pedro Aguiar"], "title": "Investigation of Feature Selection and Pooling Methods for Environmental Sound Classification", "comment": "6 pages, 7 figures (including subfigures)", "summary": "This paper explores the impact of dimensionality reduction and pooling methods for Environmental Sound Classification (ESC) using lightweight CNNs. We evaluate Sparse Salient Region Pooling (SSRP) and its variants, SSRP-Basic (SSRP-B) and SSRP-Top-K (SSRP-T), under various hyperparameter settings and compare them with Principal Component Analysis (PCA). Experiments on the ESC-50 dataset demonstrate that SSRP-T achieves up to 80.69 % accuracy, significantly outperforming both the baseline CNN (66.75 %) and the PCA-reduced model (37.60 %). Our findings confirm that a well-tuned sparse pooling strategy provides a robust, efficient, and high-performing solution for ESC tasks, particularly in resource-constrained scenarios where balancing accuracy and computational cost is crucial.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u73af\u5883\u58f0\u97f3\u5206\u7c7b\u4e2d\u7ef4\u5ea6\u7f29\u51cf\u548c\u6c60\u5316\u65b9\u6cd5\u5bf9\u8f7b\u91cf\u7ea7CNN\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u4e86SSRP\u53ca\u5176\u53d8\u4f53\uff0c\u53d1\u73b0SSRP-T\u5728ESC-50\u6570\u636e\u96c6\u4e0a\u8fbe\u523080.69%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfCNN\u548cPCA\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u7684\u73af\u5883\u58f0\u97f3\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5173\u6ce8\u7a00\u758f\u6c60\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7CNN\u8bc4\u4f30SSRP\uff08\u7a00\u758f\u663e\u8457\u533a\u57df\u6c60\u5316\uff09\u53ca\u5176\u53d8\u4f53SSRP-B\u548cSSRP-T\uff0c\u5e76\u4e0ePCA\u65b9\u6cd5\u5728\u5404\u79cd\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728ESC-50\u6570\u636e\u96c6\u4e0a\uff0cSSRP-T\u8fbe\u523080.69%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfCNN\uff0866.75%\uff09\u548cPCA\u7f29\u51cf\u6a21\u578b\uff0837.60%\uff09\u3002", "conclusion": "\u7ecf\u8fc7\u826f\u597d\u8c03\u4f18\u7684\u7a00\u758f\u6c60\u5316\u7b56\u7565\u4e3a\u73af\u5883\u58f0\u97f3\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2511.09563", "categories": ["cs.AI", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.09563", "abs": "https://arxiv.org/abs/2511.09563", "authors": ["Qilong Yuan"], "title": "An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-\u03b1 Optimization", "comment": "20 pages, 4 figures", "summary": "The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-\u03b1 constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u9ad8\u6548\u7684Partial Path Reconstruction (PPR)\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u5408\u8def\u7531\u5206\u914d(JRA)\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u9879\u76ee-\u5360\u4f4d\u7b26\u5bf9\u5f62\u6210\u7b80\u5316\u5b50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21JRA\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7cbe\u786e\u65b9\u6cd5\u867d\u7136\u80fd\u4fdd\u8bc1\u6700\u4f18\u6027\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u542f\u53d1\u5f0f\u65b9\u6cd5\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46\u89e3\u7684\u8d28\u91cf\u4e0e\u6700\u4f18\u89e3\u5b58\u5728\u7ea61%\u7684\u504f\u5dee\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u53c8\u80fd\u9ad8\u6548\u6c42\u89e3\u5927\u89c4\u6a21JRA\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Partial Path Reconstruction (PPR)\u6c42\u89e3\u5668\uff1a1) \u8bc6\u522b\u5173\u952e\u9879\u76ee-\u5360\u4f4d\u7b26\u5bf9\u5f62\u6210\u7b80\u5316\u5b50\u95ee\u9898\uff1b2) \u5728PJAR\u6846\u67b6\u4e2d\u6539\u8fdb\u521d\u59cb\u542f\u53d1\u5f0f\u5408\u5e76\u89e3\uff1b3) \u6cbf\u4f18\u5316\u8def\u5f84\u8fed\u4ee3\u4f18\u5316\u89e3\uff1b4) \u5728JRA\u6a21\u578b\u4e2d\u5f15\u5165\u5168\u5c40Large-\u03b1\u7ea6\u675f\u3002", "result": "\u5728n=300\u3001500\u548c1000\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u63d0\u4f9b\u51e0\u4e4e\u6700\u4f18\u7684\u89e3\uff0c\u5e73\u5747\u504f\u5dee\u4e3a0.00%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PPR\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86JRA\u95ee\u9898\u7684\u6c42\u89e3\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5c06\u504f\u5dee\u51cf\u5c11\u4e86\u4e00\u534a\u3002\u8be5\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u5728TSP\u548c\u76f8\u5173\u4f18\u5316\u95ee\u9898\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.10073", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.10073", "abs": "https://arxiv.org/abs/2511.10073", "authors": ["Yuhao Ren", "Yiting Liu", "Yanfei Zhou", "Zhiyu Zheng", "Li Shang", "Fan Yang", "Zhiang Wang"], "title": "Bridging the Initialization Gap: A Co-Optimization Framework for Mixed-Size Global Placement", "comment": null, "summary": "Global placement is a critical step with high computational complexity in VLSI physical design. Modern analytical placers formulate the placement problem as a nonlinear optimization, where initialization strongly affects both convergence behavior and final placement quality. However, existing initialization methods exhibit a trade-off: area-aware initializers account for cell areas but are computationally expensive and can dominate total runtime, while fast point-based initializers ignore cell area, leading to a modeling gap that impairs convergence and solution quality. We propose a lightweight co-optimization framework that bridges this initialization gap through two strategies. First, an area-hint refinement initializer incorporates heuristic cell area information into a signed graph signal by augmenting the netlist graph with virtual nodes and negative-weight edges, yielding an area-aware and spectrally smooth placement initialization. Second, a macro-schedule placement procedure progressively restores area constraints, enabling a smooth transition from the refined initializer to the full area-aware objective and producing high-quality placement results. We evaluate the framework on macro-heavy ISPD2005 academic benchmarks and two real-world industrial designs across two technology nodes (12 cases in total). Experimental results show that our method consistently improves half-perimeter wirelength (HPWL) over point-based initializers in 11 out of 12 cases, achieving up to 2.2% HPWL reduction, while running approximately 100 times faster than the state-of-the-art area-aware initializer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u534f\u540c\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9762\u79ef\u63d0\u793a\u7ec6\u5316\u521d\u59cb\u5316\u548c\u5b8f\u8c03\u5ea6\u5e03\u5c40\u4e24\u79cd\u7b56\u7565\uff0c\u5728VLSI\u5168\u5c40\u5e03\u5c40\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u5316\u3002", "motivation": "\u73b0\u6709\u521d\u59cb\u5316\u65b9\u6cd5\u5b58\u5728\u6743\u8861\uff1a\u9762\u79ef\u611f\u77e5\u521d\u59cb\u5316\u5668\u8ba1\u7b97\u6602\u8d35\uff0c\u800c\u5feb\u901f\u70b9\u57fa\u521d\u59cb\u5316\u5668\u5ffd\u7565\u5355\u5143\u9762\u79ef\uff0c\u5f71\u54cd\u6536\u655b\u6027\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "method": "1. \u9762\u79ef\u63d0\u793a\u7ec6\u5316\u521d\u59cb\u5316\u5668\uff1a\u901a\u8fc7\u865a\u62df\u8282\u70b9\u548c\u8d1f\u6743\u91cd\u8fb9\u5c06\u542f\u53d1\u5f0f\u5355\u5143\u9762\u79ef\u4fe1\u606f\u6574\u5408\u5230\u6709\u7b26\u53f7\u56fe\u4fe1\u53f7\u4e2d\uff1b2. \u5b8f\u8c03\u5ea6\u5e03\u5c40\u8fc7\u7a0b\uff1a\u9010\u6b65\u6062\u590d\u9762\u79ef\u7ea6\u675f\uff0c\u5b9e\u73b0\u4ece\u7ec6\u5316\u521d\u59cb\u5316\u5668\u5230\u5b8c\u6574\u9762\u79ef\u611f\u77e5\u76ee\u6807\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u572812\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c11\u4e2a\u6848\u4f8b\u7684HPWL\u4f18\u4e8e\u70b9\u57fa\u521d\u59cb\u5316\u5668\uff0c\u6700\u9ad8\u51cf\u5c112.2%\u7684HPWL\uff0c\u8fd0\u884c\u901f\u5ea6\u6bd4\u6700\u5148\u8fdb\u9762\u79ef\u611f\u77e5\u521d\u59cb\u5316\u5668\u5feb\u7ea6100\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u521d\u59cb\u5316\u5dee\u8ddd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u8d28\u91cf\u3002"}}
{"id": "2511.09567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09567", "abs": "https://arxiv.org/abs/2511.09567", "authors": ["Todd Morrill", "Aahlad Puli", "Murad Megjhani", "Soojin Park", "Richard Zemel"], "title": "Let the Experts Speak: Improving Survival Prediction & Calibration via Mixture-of-Experts Heads", "comment": "Accepted as a proceedings paper at the 2025 Machine Learning for Health Symposium and as a workshop paper at the Learning from Time Series for Health workshop at NeurIPS 2025", "summary": "Deep mixture-of-experts models have attracted a lot of attention for survival analysis problems, particularly for their ability to cluster similar patients together. In practice, grouping often comes at the expense of key metrics such calibration error and predictive accuracy. This is due to the restrictive inductive bias that mixture-of-experts imposes, that predictions for individual patients must look like predictions for the group they're assigned to. Might we be able to discover patient group structure, where it exists, while improving calibration and predictive accuracy? In this work, we introduce several discrete-time deep mixture-of-experts (MoE) based architectures for survival analysis problems, one of which achieves all desiderata: clustering, calibration, and predictive accuracy. We show that a key differentiator between this array of MoEs is how expressive their experts are. We find that more expressive experts that tailor predictions per patient outperform experts that rely on fixed group prototypes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51e0\u79cd\u57fa\u4e8e\u6df1\u5ea6\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u79bb\u6563\u65f6\u95f4\u751f\u5b58\u5206\u6790\u67b6\u6784\uff0c\u5176\u4e2d\u4e00\u4e2a\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u805a\u7c7b\u3001\u6821\u51c6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u7684\u76ee\u6807\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5177\u8868\u8fbe\u529b\u7684\u4e13\u5bb6\u6a21\u578b\u80fd\u591f\u4e3a\u6bcf\u4e2a\u60a3\u8005\u5b9a\u5236\u9884\u6d4b\uff0c\u4f18\u4e8e\u4f9d\u8d56\u56fa\u5b9a\u7ec4\u539f\u578b\u7684\u4e13\u5bb6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u751f\u5b58\u5206\u6790\u4e2d\u867d\u7136\u80fd\u591f\u805a\u7c7b\u76f8\u4f3c\u60a3\u8005\uff0c\u4f46\u5f80\u5f80\u4ee5\u727a\u7272\u6821\u51c6\u8bef\u5dee\u548c\u9884\u6d4b\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u80fd\u5728\u53d1\u73b0\u60a3\u8005\u7fa4\u4f53\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u6821\u51c6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u4e86\u591a\u79cd\u57fa\u4e8e\u79bb\u6563\u65f6\u95f4\u6df1\u5ea6\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u751f\u5b58\u5206\u6790\u67b6\u6784\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u8868\u8fbe\u529b\u6c34\u5e73\u7684\u4e13\u5bb6\u6a21\u578b\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5177\u8868\u8fbe\u529b\u7684\u4e13\u5bb6\u6a21\u578b\u80fd\u591f\u4e3a\u6bcf\u4e2a\u60a3\u8005\u5b9a\u5236\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u805a\u7c7b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6821\u51c6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002\u5176\u4e2d\u4e00\u4e2a\u7279\u5b9a\u67b6\u6784\u6210\u529f\u5b9e\u73b0\u4e86\u6240\u6709\u671f\u671b\u76ee\u6807\u3002", "conclusion": "\u5728\u751f\u5b58\u5206\u6790\u4e2d\uff0c\u4f7f\u7528\u66f4\u5177\u8868\u8fbe\u529b\u7684\u4e13\u5bb6\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u60a3\u8005\u805a\u7c7b\u3001\u6821\u51c6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u7684\u76ee\u6807\uff0c\u4f18\u4e8e\u4f9d\u8d56\u56fa\u5b9a\u7ec4\u539f\u578b\u7684\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2511.09570", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09570", "abs": "https://arxiv.org/abs/2511.09570", "authors": ["David Woller", "Viktor Koz\u00e1k", "Miroslav Kulich", "Libor P\u0159eu\u010dil"], "title": "Variable Neighborhood Search for the Electric Vehicle Routing Problem", "comment": "19 pages, 6 figures", "summary": "The Electric Vehicle Routing Problem (EVRP) extends the classical Vehicle Routing Problem (VRP) to reflect the growing use of electric and hybrid vehicles in logistics. Due to the variety of constraints considered in the literature, comparing approaches across different problem variants remains challenging. A minimalistic variant of the EVRP, known as the Capacitated Green Vehicle Routing Problem (CGVRP), was the focus of the CEC-12 competition held during the 2020 IEEE World Congress on Computational Intelligence. This paper presents the competition-winning approach, based on the Variable Neighborhood Search (VNS) metaheuristic. The method achieves the best results on the full competition dataset and also outperforms a more recent algorithm published afterward.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728CEC-12\u7ade\u8d5b\u4e2d\u83b7\u80dc\u7684\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\uff0c\u57fa\u4e8e\u53d8\u90bb\u57df\u641c\u7d22\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5728\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u7531\u4e8e\u7535\u52a8\u6c7d\u8f66\u5728\u7269\u6d41\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u95ee\u9898\uff08EVRP\uff09\u6269\u5c55\u4e86\u7ecf\u5178\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u3002\u7136\u800c\uff0c\u6587\u732e\u4e2d\u8003\u8651\u7684\u5404\u79cd\u7ea6\u675f\u4f7f\u5f97\u6bd4\u8f83\u4e0d\u540c\u95ee\u9898\u53d8\u4f53\u7684\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u53d8\u90bb\u57df\u641c\u7d22\uff08VNS\uff09\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u89e3\u51b3\u6700\u5c0f\u5316\u53d8\u4f53\u7684\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u95ee\u9898\u2014\u2014\u5bb9\u91cf\u7ea6\u675f\u7eff\u8272\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CGVRP\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b8c\u6574\u7684\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5e76\u4e14\u4f18\u4e8e\u4e4b\u540e\u53d1\u5e03\u7684\u66f4\u8fd1\u671f\u7b97\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u53d8\u90bb\u57df\u641c\u7d22\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u5bb9\u91cf\u7ea6\u675f\u7eff\u8272\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u7ade\u8d5b\u4e2d\u7684\u83b7\u80dc\u65b9\u6848\u3002"}}
{"id": "2511.10206", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.10206", "abs": "https://arxiv.org/abs/2511.10206", "authors": ["Eugene Seong"], "title": "Heuristic Solutions for the Best Secretary Problem", "comment": "13 pages", "summary": "This paper introduces a heuristic framework for the Best Secretary Problem, where one item must be selected using rank information only. We develop five data-responsive rules extending classical fixed-cutoff methods: an expected-record threshold, an adaptive deviation correction, a probabilistic early-accept rule, a two-phase relaxation, and a local dynamic programming approximation. These rules adjust thresholds sequentially as information accumulates. Simulations across diverse sample sizes, distributions, and autocorrelated settings show that the heuristics match or exceed traditional optimal rules in stability and efficiency. The expected-record rule remains strong despite its simplicity, the adaptive correction performs well under asymmetry, and the adaptive and probabilistic rules reduce average stopping times. An ensemble combining multiple rules yields the most stable performance. Overall, a few intuitive parameters achieve near-optimal results, demonstrating that data-responsive heuristics can effectively extend rank-based optimal stopping to dynamic decision environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u542f\u53d1\u5f0f\u6846\u67b6\u6765\u89e3\u51b3\u6700\u4f73\u79d8\u4e66\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e94\u79cd\u57fa\u4e8e\u6570\u636e\u54cd\u5e94\u7684\u89c4\u5219\u6765\u6269\u5c55\u4f20\u7edf\u7684\u56fa\u5b9a\u622a\u6b62\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u89c4\u5219\u80fd\u591f\u6839\u636e\u79ef\u7d2f\u7684\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u9608\u503c\u3002", "motivation": "\u4f20\u7edf\u7684\u56fa\u5b9a\u622a\u6b62\u65b9\u6cd5\u5728\u52a8\u6001\u51b3\u7b56\u73af\u5883\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6839\u636e\u5b9e\u65f6\u6570\u636e\u8c03\u6574\u7684\u6570\u636e\u54cd\u5e94\u5f0f\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e94\u79cd\u6570\u636e\u54cd\u5e94\u89c4\u5219\uff1a\u671f\u671b\u8bb0\u5f55\u9608\u503c\u3001\u81ea\u9002\u5e94\u504f\u5dee\u6821\u6b63\u3001\u6982\u7387\u65e9\u671f\u63a5\u53d7\u89c4\u5219\u3001\u4e24\u9636\u6bb5\u677e\u5f1b\u6cd5\u548c\u5c40\u90e8\u52a8\u6001\u89c4\u5212\u8fd1\u4f3c\u6cd5\uff0c\u8fd9\u4e9b\u89c4\u5219\u80fd\u591f\u987a\u5e8f\u8c03\u6574\u9608\u503c\u3002", "result": "\u5728\u4e0d\u540c\u6837\u672c\u5927\u5c0f\u3001\u5206\u5e03\u548c\u81ea\u76f8\u5173\u8bbe\u7f6e\u4e0b\u7684\u6a21\u62df\u663e\u793a\uff0c\u8fd9\u4e9b\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u6548\u7387\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u4f20\u7edf\u6700\u4f18\u89c4\u5219\uff0c\u5176\u4e2d\u7ec4\u5408\u591a\u4e2a\u89c4\u5219\u7684\u96c6\u6210\u65b9\u6cd5\u8868\u73b0\u6700\u7a33\u5b9a\u3002", "conclusion": "\u5c11\u6570\u76f4\u89c2\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u7ed3\u679c\uff0c\u8868\u660e\u6570\u636e\u54cd\u5e94\u5f0f\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5c06\u57fa\u4e8e\u6392\u540d\u7684\u6700\u4f18\u505c\u6b62\u6269\u5c55\u5230\u52a8\u6001\u51b3\u7b56\u73af\u5883\u3002"}}
{"id": "2511.10205", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.10205", "abs": "https://arxiv.org/abs/2511.10205", "authors": ["Martin J. W. Schubert"], "title": "High Order Delta-Sigma Modulation with Positive Integer Coefficients", "comment": "4 pages, 11 figures. This paper was rejected by the 2025 IEEE SiPS Workshop, mainly because the idea was not sufficiently developed, e.g. regarding stability limits. Presumably for this reason, the German Federal Office for Economic Affairs and Export Control that monitors issues of signal processing, has approved the publication", "summary": "This document proposes binomial integer parameters for the cascaded Delta-Sigma-modulator structure with distributed feedback and distributed feedforward input and multi-bit output. It is demonstrated that high orders can be achieved with these coefficients. Accuracy requirements concerning the coefficients are discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u7ea7\u8054Delta-Sigma\u8c03\u5236\u5668\u7ed3\u6784\u7684\u4e8c\u9879\u5f0f\u6574\u6570\u53c2\u6570\uff0c\u8be5\u7ed3\u6784\u5177\u6709\u5206\u5e03\u5f0f\u53cd\u9988\u3001\u5206\u5e03\u5f0f\u524d\u9988\u8f93\u5165\u548c\u591a\u6bd4\u7279\u8f93\u51fa\u3002\u7814\u7a76\u8868\u660e\u4f7f\u7528\u8fd9\u4e9b\u7cfb\u6570\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u9636\u8c03\u5236\u5668\uff0c\u5e76\u8ba8\u8bba\u4e86\u7cfb\u6570\u7684\u7cbe\u5ea6\u8981\u6c42\u3002", "motivation": "\u4e3a\u7ea7\u8054Delta-Sigma\u8c03\u5236\u5668\u7ed3\u6784\u5f00\u53d1\u7b80\u5316\u7684\u53c2\u6570\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e8c\u9879\u5f0f\u6574\u6570\u7cfb\u6570\u6765\u7b80\u5316\u5b9e\u73b0\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7ea7\u8054Delta-Sigma\u8c03\u5236\u5668\u7ed3\u6784\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u53cd\u9988\u548c\u5206\u5e03\u5f0f\u524d\u9988\u8f93\u5165\uff0c\u4f7f\u7528\u4e8c\u9879\u5f0f\u6574\u6570\u4f5c\u4e3a\u8c03\u5236\u5668\u7cfb\u6570\uff0c\u5b9e\u73b0\u591a\u6bd4\u7279\u8f93\u51fa\u3002", "result": "\u7814\u7a76\u8868\u660e\u4f7f\u7528\u4e8c\u9879\u5f0f\u6574\u6570\u7cfb\u6570\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u9636Delta-Sigma\u8c03\u5236\u5668\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e8c\u9879\u5f0f\u6574\u6570\u53c2\u6570\u4e3a\u7ea7\u8054Delta-Sigma\u8c03\u5236\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u9636\u6027\u80fd\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u7cfb\u6570\u7cbe\u5ea6\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2511.09573", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.09573", "abs": "https://arxiv.org/abs/2511.09573", "authors": ["Valentino F. Foit", "David W. Hogg", "Soledad Villar"], "title": "Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost", "comment": "10 pages, 2 figures, 1 table, Machine Learning and the Physical Sciences Workshop, NeurIPS 2025", "summary": "Many machine learning tasks in the natural sciences are precisely equivariant to particular symmetries. Nonetheless, equivariant methods are often not employed, perhaps because training is perceived to be challenging, or the symmetry is expected to be learned, or equivariant implementations are seen as hard to build. Group averaging is an available technique for these situations. It happens at test time; it can make any trained model precisely equivariant at a (often small) cost proportional to the size of the group; it places no requirements on model structure or training. It is known that, under mild conditions, the group-averaged model will have a provably better prediction accuracy than the original model. Here we show that an inexpensive group averaging can improve accuracy in practice. We take well-established benchmark machine learning models of differential equations in which certain symmetries ought to be obeyed. At evaluation time, we average the models over a small group of symmetries. Our experiments show that this procedure always decreases the average evaluation loss, with improvements of up to 37\\% in terms of the VRMSE. The averaging produces visually better predictions for continuous dynamics. This short paper shows that, under certain common circumstances, there are no disadvantages to imposing exact symmetries; the ML4PS community should consider group averaging as a cheap and simple way to improve model accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u7fa4\u5e73\u5747\u6280\u672f\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5177\u6709\u7cbe\u786e\u7684\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u5728\u8bc4\u4f30\u65f6\u5bf9\u6a21\u578b\u5728\u5c0f\u5bf9\u79f0\u7fa4\u4e0a\u8fdb\u884c\u5e73\u5747\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u65e0\u9700\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u8bb8\u591a\u81ea\u7136\u79d1\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u5177\u6709\u7279\u5b9a\u7684\u5bf9\u79f0\u6027\uff0c\u4f46\u7b49\u53d8\u65b9\u6cd5\u5f80\u5f80\u672a\u88ab\u91c7\u7528\uff0c\u53ef\u80fd\u662f\u56e0\u4e3a\u8bad\u7ec3\u56f0\u96be\u3001\u671f\u671b\u6a21\u578b\u81ea\u884c\u5b66\u4e60\u5bf9\u79f0\u6027\uff0c\u6216\u7b49\u53d8\u5b9e\u73b0\u88ab\u8ba4\u4e3a\u96be\u4ee5\u6784\u5efa\u3002\u7fa4\u5e73\u5747\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7fa4\u5e73\u5747\u6280\u672f\uff0c\u5728\u6d4b\u8bd5\u65f6\u5bf9\u5df2\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5c0f\u5bf9\u79f0\u7fa4\u4e0a\u8fdb\u884c\u5e73\u5747\uff0c\u4f7f\u6a21\u578b\u5177\u6709\u7cbe\u786e\u7684\u5bf9\u79f0\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u8981\u6c42\u6a21\u578b\u7ed3\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6539\u53d8\uff0c\u8ba1\u7b97\u6210\u672c\u4e0e\u7fa4\u5927\u5c0f\u6210\u6b63\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7fa4\u5e73\u5747\u8fc7\u7a0b\u603b\u662f\u964d\u4f4e\u5e73\u5747\u8bc4\u4f30\u635f\u5931\uff0c\u5728VRMSE\u6307\u6807\u4e0a\u6539\u8fdb\u9ad8\u8fbe37%\u3002\u5bf9\u4e8e\u8fde\u7eed\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5e73\u5747\u540e\u7684\u9884\u6d4b\u5728\u89c6\u89c9\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5728\u5e38\u89c1\u60c5\u51b5\u4e0b\uff0c\u65bd\u52a0\u7cbe\u786e\u5bf9\u79f0\u6027\u6ca1\u6709\u7f3a\u70b9\uff1bML4PS\u793e\u533a\u5e94\u8003\u8651\u5c06\u7fa4\u5e73\u5747\u4f5c\u4e3a\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u7684\u5ec9\u4ef7\u7b80\u5355\u65b9\u6cd5\u3002"}}
{"id": "2511.09682", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.09682", "abs": "https://arxiv.org/abs/2511.09682", "authors": ["Tiansheng Huang", "Virat Shejwalkar", "Oscar Chang", "Milad Nasr", "Ling Liu"], "title": "Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models", "comment": null, "summary": "Instilling reasoning capabilities in large models (LMs) using reasoning training (RT) significantly improves LMs' performances. Thus Audio Reasoning Models (ARMs), i.e., audio LMs that can reason, are becoming increasingly popular. However, no work has studied the safety of ARMs against jailbreak attacks that aim to elicit harmful responses from target models. To this end, first, we show that standard RT with appropriate safety reasoning data can protect ARMs from vanilla audio jailbreaks, but cannot protect them against our proposed simple yet effective jailbreaks. We show that this is because of the significant representation drift between vanilla and advanced jailbreaks which forces the target ARMs to emit harmful responses. Based on this observation, we propose Rebellion, a robust RT that trains ARMs to be robust to the worst-case representation drift. All our results are on Qwen2-Audio; they demonstrate that Rebellion: 1) can protect against advanced audio jailbreaks without compromising performance on benign tasks, and 2) significantly improves accuracy-safety trade-off over standard RT method.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u97f3\u9891\u63a8\u7406\u6a21\u578b\uff08ARMs\uff09\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u6807\u51c6\u63a8\u7406\u8bad\u7ec3\u65e0\u6cd5\u62b5\u5fa1\u9ad8\u7ea7\u97f3\u9891\u8d8a\u72f1\u653b\u51fb\uff0c\u63d0\u51fa\u4e86Rebellion\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\u6765\u4fdd\u62a4ARMs\u514d\u53d7\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u97f3\u9891\u63a8\u7406\u6a21\u578b\u7684\u666e\u53ca\uff0c\u9700\u8981\u7814\u7a76\u5176\u5bf9\u6297\u8d8a\u72f1\u653b\u51fb\u7684\u5b89\u5168\u6027\uff0c\u76ee\u524d\u5c1a\u65e0\u76f8\u5173\u5de5\u4f5c\u3002", "method": "\u63d0\u51faRebellion\u9c81\u68d2\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8bad\u7ec3ARMs\u5bf9\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8868\u793a\u6f02\u79fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "Rebellion\u80fd\u591f\u6709\u6548\u9632\u5fa1\u9ad8\u7ea7\u97f3\u9891\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u826f\u6027\u4efb\u52a1\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u51c6\u786e\u6027\u4e0e\u5b89\u5168\u6027\u7684\u6743\u8861\u3002", "conclusion": "Rebellion\u65b9\u6cd5\u4e3a\u97f3\u9891\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u62a4\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u63a8\u7406\u8bad\u7ec3\u5728\u5bf9\u6297\u9ad8\u7ea7\u8d8a\u72f1\u653b\u51fb\u65f6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.09710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09710", "abs": "https://arxiv.org/abs/2511.09710", "authors": ["Sarath Shekkizhar", "Romain Cosentino", "Adam Earle", "Silvio Savarese"], "title": "Echoing: Identity Failures when LLM Agents Talk to Each Other", "comment": null, "summary": "As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\\%$ to $70\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\\%$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u6f02\u79fb\u73b0\u8c61\uff0c\u7279\u522b\u662f\u56de\u97f3\u6548\u5e94\u2014\u2014\u667a\u80fd\u4f53\u653e\u5f03\u81ea\u8eab\u89d2\u8272\u8f6c\u800c\u6a21\u4eff\u5bf9\u8bdd\u4f19\u4f34\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u73b0\u8c61\u5728\u4e09\u5927LLM\u63d0\u4f9b\u5546\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u53d1\u751f\u73875%-70%\uff0c\u4e14\u968f\u7740\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a0\u800c\u52a0\u5267\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u81ea\u4e3b\u4ea4\u4e92\uff0c\u51fa\u73b0\u4e86\u4e00\u7c7b\u65e0\u6cd5\u4ece\u5355\u667a\u80fd\u4f53\u6027\u80fd\u9884\u6d4b\u7684\u65b0\u5931\u8d25\u6a21\u5f0f\uff1a\u667a\u80fd\u4f53\u95f4\u5bf9\u8bdd\u7684\u884c\u4e3a\u6f02\u79fb\u3002\u7531\u4e8e\u7f3a\u4e4f\u4eba\u7c7b\u53c2\u4e0e\u63d0\u4f9b\u7684\u7a33\u5b9a\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u5931\u8d25\u5177\u6709\u72ec\u7279\u6027\u3002", "method": "\u901a\u8fc760\u4e2aAxA\u914d\u7f6e\u30013\u4e2a\u9886\u57df\u30012000+\u5bf9\u8bdd\u7684\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u56de\u97f3\u6548\u5e94\u5728\u4e0d\u540cLLM\u63d0\u4f9b\u5546\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u63d0\u793a\u5f71\u54cd\u548c\u5bf9\u8bdd\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u4e86\u534f\u8bae\u7ea7\u7f13\u89e3\u63aa\u65bd\u3002", "result": "\u56de\u97f3\u6548\u5e94\u5728\u4e09\u5927LLM\u63d0\u4f9b\u5546\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u53d1\u751f\u73875%-70%\uff1b\u5373\u4f7f\u5728\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u4e2d\u4e5f\u670932.8%\u7684\u53d1\u751f\u7387\uff1b\u968f\u7740\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a0\uff087+\u8f6e\uff09\u800c\u52a0\u5267\uff1b\u4f7f\u7528\u7ed3\u6784\u5316\u54cd\u5e94\u53ef\u5c06\u56de\u97f3\u6548\u5e94\u964d\u81f39%\u3002", "conclusion": "\u667a\u80fd\u4f53\u95f4\u5bf9\u8bdd\u4e2d\u7684\u56de\u97f3\u6548\u5e94\u662f\u4e00\u4e2a\u666e\u904d\u4e14\u6301\u4e45\u7684\u95ee\u9898\uff0c\u4e0d\u80fd\u4ec5\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u89e3\u51b3\uff0c\u9700\u8981\u534f\u8bae\u7ea7\u7684\u5e72\u9884\u63aa\u65bd\u6765\u786e\u4fdd\u667a\u80fd\u4f53\u4fdd\u6301\u5176\u6307\u5b9a\u89d2\u8272\u3002"}}
{"id": "2511.09596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09596", "abs": "https://arxiv.org/abs/2511.09596", "authors": ["Mingkuan Zhao", "Wentao Hu", "Jiayin Wang", "Xin Lai", "Tianchen Huang", "Yuheng Min", "Rui Yan", "Xiaoyan Zhu"], "title": "Making Every Head Count: Sparse Attention Without the Speed-Performance Trade-off", "comment": null, "summary": "The design of Large Language Models (LLMs) has long been hampered by a fundamental conflict within their core attention mechanism: its remarkable expressivity is built upon a computational complexity of $O(H \\cdot N^2)$ that grows quadratically with the context size ($N$) and linearly with the number of heads ($H$). This standard implementation harbors significant computational redundancy, as all heads independently compute attention over the same sequence space. Existing sparse methods, meanwhile, often trade information integrity for computational efficiency. To resolve this efficiency-performance trade-off, we propose SPAttention, whose core contribution is the introduction of a new paradigm we term Principled Structural Sparsity. SPAttention does not merely drop connections but instead reorganizes the computational task by partitioning the total attention workload into balanced, non-overlapping distance bands, assigning each head a unique segment. This approach transforms the multi-head attention mechanism from $H$ independent $O(N^2)$ computations into a single, collaborative $O(N^2)$ computation, fundamentally reducing complexity by a factor of $H$. The structured inductive bias compels functional specialization among heads, enabling a more efficient allocation of computational resources from redundant modeling to distinct dependencies across the entire sequence span. Extensive empirical validation on the OLMoE-1B-7B and 0.25B-1.75B model series demonstrates that while delivering an approximately two-fold increase in training throughput, its performance is on par with standard dense attention, even surpassing it on select key metrics, while consistently outperforming representative sparse attention methods including Longformer, Reformer, and BigBird across all evaluation metrics.", "AI": {"tldr": "SPAttention\u901a\u8fc7\u5f15\u5165\u539f\u5219\u6027\u7ed3\u6784\u7a00\u758f\u6027\uff0c\u5c06\u591a\u5934\u6ce8\u610f\u529b\u8ba1\u7b97\u4eceH\u4e2a\u72ec\u7acb\u7684O(N\u00b2)\u8ba1\u7b97\u8f6c\u53d8\u4e3a\u5355\u4e2a\u534f\u4f5c\u7684O(N\u00b2)\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u6838\u5fc3\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6O(H\u00b7N\u00b2)\u4e0e\u8ba1\u7b97\u5197\u4f59\u4e4b\u95f4\u7684\u6839\u672c\u51b2\u7a81\uff0c\u73b0\u6709\u7a00\u758f\u65b9\u6cd5\u5f80\u5f80\u4ee5\u4fe1\u606f\u5b8c\u6574\u6027\u6362\u53d6\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u539f\u5219\u6027\u7ed3\u6784\u7a00\u758f\u6027\u8303\u5f0f\uff0c\u5c06\u603b\u6ce8\u610f\u529b\u5de5\u4f5c\u8d1f\u8f7d\u5212\u5206\u4e3a\u5e73\u8861\u7684\u975e\u91cd\u53e0\u8ddd\u79bb\u5e26\uff0c\u4e3a\u6bcf\u4e2a\u5934\u5206\u914d\u72ec\u7279\u7247\u6bb5\uff0c\u5f3a\u5236\u5934\u90e8\u529f\u80fd\u4e13\u4e1a\u5316\u3002", "result": "\u5728OLMoE-1B-7B\u548c0.25B-1.75B\u6a21\u578b\u7cfb\u5217\u4e0a\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u7ea62\u500d\uff0c\u6027\u80fd\u4e0e\u6807\u51c6\u5bc6\u96c6\u6ce8\u610f\u529b\u76f8\u5f53\u751a\u81f3\u5728\u67d0\u4e9b\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8d8a\uff0c\u4e14\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "SPAttention\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u5f52\u7eb3\u504f\u7f6e\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3aLLM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u8303\u5f0f\u3002"}}
{"id": "2511.09785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09785", "abs": "https://arxiv.org/abs/2511.09785", "authors": ["Bakhtawar Ahtisham", "Kirk Vanacore", "Jinsook Lee", "Zhuqian Zhou", "Doug Pietrzak", "Rene F. Kizilcec"], "title": "AI Annotation Orchestration: Evaluating LLM verifiers to Improve the Quality of LLM Annotations in Learning Analytics", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to annotate learning interactions, yet concerns about reliability limit their utility. We test whether verification-oriented orchestration-prompting models to check their own labels (self-verification) or audit one another (cross-verification)-improves qualitative coding of tutoring discourse. Using transcripts from 30 one-to-one math sessions, we compare three production LLMs (GPT, Claude, Gemini) under three conditions: unverified annotation, self-verification, and cross-verification across all orchestration configurations. Outputs are benchmarked against a blinded, disagreement-focused human adjudication using Cohen's kappa. Overall, orchestration yields a 58 percent improvement in kappa. Self-verification nearly doubles agreement relative to unverified baselines, with the largest gains for challenging tutor moves. Cross-verification achieves a 37 percent improvement on average, with pair- and construct-dependent effects: some verifier-annotator pairs exceed self-verification, while others reduce alignment, reflecting differences in verifier strictness. We contribute: (1) a flexible orchestration framework instantiating control, self-, and cross-verification; (2) an empirical comparison across frontier LLMs on authentic tutoring data with blinded human \"gold\" labels; and (3) a concise notation, verifier(annotator) (e.g., Gemini(GPT) or Claude(Claude)), to standardize reporting and make directional effects explicit for replication. Results position verification as a principled design lever for reliable, scalable LLM-assisted annotation in Learning Analytics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u9a8c\u8bc1\u5bfc\u5411\u7684\u7f16\u6392\uff08\u81ea\u6211\u9a8c\u8bc1\u548c\u4ea4\u53c9\u9a8c\u8bc1\uff09\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u5bf9\u8bdd\u6807\u6ce8\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u76f8\u6bd4\u672a\u9a8c\u8bc1\u6807\u6ce8\uff0c\u7f16\u6392\u65b9\u6cd5\u4f7fCohen's kappa\u7cfb\u6570\u63d0\u9ad8\u4e8658%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6807\u6ce8\u5b66\u4e60\u4ea4\u4e92\uff0c\u4f46\u5176\u53ef\u9760\u6027\u95ee\u9898\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6d4b\u8bd5\u9a8c\u8bc1\u5bfc\u5411\u7684\u7f16\u6392\u65b9\u6cd5\u662f\u5426\u80fd\u63d0\u9ad8\u8f85\u5bfc\u8bdd\u8bed\u5b9a\u6027\u7f16\u7801\u7684\u8d28\u91cf\u3002", "method": "\u4f7f\u752830\u4e2a\u4e00\u5bf9\u4e00\u6570\u5b66\u8f85\u5bfc\u4f1a\u8bdd\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u524d\u6cbfLLM\uff08GPT\u3001Claude\u3001Gemini\uff09\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff1a\u672a\u9a8c\u8bc1\u6807\u6ce8\u3001\u81ea\u6211\u9a8c\u8bc1\u548c\u6240\u6709\u7f16\u6392\u914d\u7f6e\u4e0b\u7684\u4ea4\u53c9\u9a8c\u8bc1\u3002\u8f93\u51fa\u7ed3\u679c\u4e0e\u76f2\u5ba1\u7684\u4eba\u7c7b\u88c1\u51b3\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7f16\u6392\u65b9\u6cd5\u4f7fkappa\u7cfb\u6570\u63d0\u9ad8\u4e8658%\u3002\u81ea\u6211\u9a8c\u8bc1\u76f8\u6bd4\u672a\u9a8c\u8bc1\u57fa\u7ebf\u51e0\u4e4e\u4f7f\u4e00\u81f4\u6027\u7ffb\u500d\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5bfc\u5e08\u884c\u4e3a\u4e0a\u83b7\u5f97\u6700\u5927\u6536\u76ca\u3002\u4ea4\u53c9\u9a8c\u8bc1\u5e73\u5747\u63d0\u9ad8\u4e8637%\uff0c\u4f46\u5b58\u5728\u914d\u5bf9\u548c\u7ed3\u6784\u4f9d\u8d56\u6548\u5e94\u3002", "conclusion": "\u9a8c\u8bc1\u88ab\u5b9a\u4f4d\u4e3a\u5728\u5b66\u4e60\u6559\u80b2\u5206\u6790\u4e2d\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684LLM\u8f85\u52a9\u6807\u6ce8\u7684\u539f\u5219\u6027\u8bbe\u8ba1\u6760\u6746\u3002"}}
{"id": "2511.09652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09652", "abs": "https://arxiv.org/abs/2511.09652", "authors": ["Mohammad Alipour-Vaezi", "Huaiyang Zhong", "Kwok-Leung Tsui", "Sajad Khodadadian"], "title": "Optimistic Reinforcement Learning with Quantile Objectives", "comment": null, "summary": "Reinforcement Learning (RL) has achieved tremendous success in recent years. However, the classical foundations of RL do not account for the risk sensitivity of the objective function, which is critical in various fields, including healthcare and finance. A popular approach to incorporate risk sensitivity is to optimize a specific quantile of the cumulative reward distribution. In this paper, we develop UCB-QRL, an optimistic learning algorithm for the $\u03c4$-quantile objective in finite-horizon Markov decision processes (MDPs). UCB-QRL is an iterative algorithm in which, at each iteration, we first estimate the underlying transition probability and then optimize the quantile value function over a confidence ball around this estimate. We show that UCB-QRL yields a high-probability regret bound $\\mathcal O\\left((2/\u03ba)^{H+1}H\\sqrt{SATH\\log(2SATH/\u03b4)}\\right)$ in the episodic setting with $S$ states, $A$ actions, $T$ episodes, and $H$ horizons. Here, $\u03ba>0$ is a problem-dependent constant that captures the sensitivity of the underlying MDP's quantile value.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UCB-QRL\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u4f18\u5316\u7d2f\u79ef\u5956\u52b1\u5206\u5e03\u7684\u7279\u5b9a\u5206\u4f4d\u6570\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u98ce\u9669\u654f\u611f\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u51fd\u6570\u7f3a\u4e4f\u98ce\u9669\u654f\u611f\u6027\uff0c\u8fd9\u5728\u533b\u7597\u548c\u91d1\u878d\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u4f18\u5316\u7d2f\u79ef\u5956\u52b1\u5206\u5e03\u7684\u7279\u5b9a\u5206\u4f4d\u6570\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u98ce\u9669\u654f\u611f\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86UCB-QRL\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8fed\u4ee3\u7b97\u6cd5\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u9996\u5148\u4f30\u8ba1\u8f6c\u79fb\u6982\u7387\uff0c\u7136\u540e\u5728\u7f6e\u4fe1\u533a\u95f4\u5185\u4f18\u5316\u5206\u4f4d\u6570\u4ef7\u503c\u51fd\u6570\u3002", "result": "UCB-QRL\u5728\u60c5\u666f\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6982\u7387\u9057\u61be\u8fb9\u754c$\\mathcal O\\left((2/\u03ba)^{H+1}H\\sqrt{SATH\\log(2SATH/\u03b4)}\\right)$\uff0c\u5176\u4e2d\u03ba\u662f\u95ee\u9898\u76f8\u5173\u7684\u5e38\u6570\uff0c\u53cd\u6620\u4e86MDP\u5206\u4f4d\u6570\u4ef7\u503c\u7684\u654f\u611f\u6027\u3002", "conclusion": "UCB-QRL\u7b97\u6cd5\u4e3a\u98ce\u9669\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u6709\u9650\u65f6\u57dfMDP\u4e2d\u4f18\u5316\u5206\u4f4d\u6570\u76ee\u6807\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u6027\u80fd\u8fb9\u754c\u3002"}}
{"id": "2511.09788", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09788", "abs": "https://arxiv.org/abs/2511.09788", "authors": ["Mar Canet Sola", "Varvara Guljajeva"], "title": "Why Open Small AI Models Matter for Interactive Art", "comment": "8 pages", "summary": "This position paper argues for the importance of open small AI models in creative independence for interactive art practices. Deployable locally, these models offer artists vital control over infrastructure and code, unlike dominant large, closed-source corporate systems. Such centralized platforms function as opaque black boxes, imposing severe limitations on interactive artworks, including restrictive content filters, preservation issues, and technical challenges such as increased latency and limited interfaces. In contrast, small AI models empower creators with more autonomy, control, and sustainability for these artistic processes. They enable the ability to use a model as long as they want, create their own custom model, either by making code changes to integrate new interfaces, or via new datasets by re-training or fine-tuning the model. This fosters technological self-determination, offering greater ownership and reducing reliance on corporate AI ill-suited for interactive art's demands. Critically, this approach empowers the artist and supports long-term preservation and exhibition of artworks with AI components. This paper explores the practical applications and implications of using open small AI models in interactive art, contrasting them with closed-source alternatives.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5f00\u6e90\u5c0fAI\u6a21\u578b\u5bf9\u4ea4\u4e92\u827a\u672f\u521b\u4f5c\u72ec\u7acb\u6027\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u5728\u672c\u5730\u90e8\u7f72\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u5bf9\u57fa\u7840\u8bbe\u65bd\u548c\u4ee3\u7801\u7684\u81ea\u4e3b\u63a7\u5236\uff0c\u4e0e\u5c01\u95ed\u7684\u5927\u89c4\u6a21\u4f01\u4e1a\u7cfb\u7edf\u5f62\u6210\u5bf9\u6bd4\u3002", "motivation": "\u5f53\u524d\u4e3b\u5bfc\u7684\u5927\u578b\u95ed\u6e90AI\u7cfb\u7edf\u4f5c\u4e3a\u4e0d\u900f\u660e\u7684\u9ed1\u7bb1\uff0c\u5bf9\u4ea4\u4e92\u827a\u672f\u4f5c\u54c1\u65bd\u52a0\u4e86\u4e25\u91cd\u9650\u5236\uff0c\u5305\u62ec\u5185\u5bb9\u8fc7\u6ee4\u9650\u5236\u3001\u4fdd\u5b58\u95ee\u9898\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u63a5\u53e3\u6709\u9650\u7b49\u6280\u672f\u6311\u6218\uff0c\u9650\u5236\u4e86\u827a\u672f\u5bb6\u7684\u521b\u4f5c\u81ea\u7531\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u5f00\u6e90\u5c0fAI\u6a21\u578b\u4e0e\u95ed\u6e90\u66ff\u4ee3\u65b9\u6848\uff0c\u63a2\u8ba8\u5728\u4ea4\u4e92\u827a\u672f\u4e2d\u4f7f\u7528\u5f00\u6e90\u5c0fAI\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u548c\u5f71\u54cd\uff0c\u5f3a\u8c03\u5176\u63d0\u4f9b\u7684\u6280\u672f\u81ea\u51b3\u80fd\u529b\u3002", "result": "\u5f00\u6e90\u5c0fAI\u6a21\u578b\u8d4b\u4e88\u521b\u4f5c\u8005\u66f4\u591a\u81ea\u4e3b\u6743\u3001\u63a7\u5236\u529b\u548c\u53ef\u6301\u7eed\u6027\uff0c\u652f\u6301\u957f\u671f\u4fdd\u5b58\u548c\u5c55\u793a\u542bAI\u7ec4\u4ef6\u7684\u827a\u672f\u4f5c\u54c1\uff0c\u51cf\u5c11\u5bf9\u4e0d\u9002\u5408\u4ea4\u4e92\u827a\u672f\u9700\u6c42\u7684\u4f01\u4e1aAI\u7684\u4f9d\u8d56\u3002", "conclusion": "\u5f00\u6e90\u5c0fAI\u6a21\u578b\u662f\u5b9e\u73b0\u827a\u672f\u5bb6\u6280\u672f\u81ea\u51b3\u3001\u589e\u5f3a\u6240\u6709\u6743\u548c\u4fc3\u8fdb\u4ea4\u4e92\u827a\u672f\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u5173\u952e\u9014\u5f84\uff0c\u4e3a\u827a\u672f\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u63a7\u5236\u6743\u548c\u72ec\u7acb\u6027\u3002"}}
{"id": "2511.09829", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09829", "abs": "https://arxiv.org/abs/2511.09829", "authors": ["Jiahuan Long", "Tingsong Jiang", "Hanqing Liu", "Chao Ma", "Wen Yao"], "title": "Thermally Activated Dual-Modal Adversarial Clothing against AI Surveillance Systems", "comment": null, "summary": "Adversarial patches have emerged as a popular privacy-preserving approach for resisting AI-driven surveillance systems. However, their conspicuous appearance makes them difficult to deploy in real-world scenarios. In this paper, we propose a thermally activated adversarial wearable designed to ensure adaptability and effectiveness in complex real-world environments. The system integrates thermochromic dyes with flexible heating units to induce visually dynamic adversarial patterns on clothing surfaces. In its default state, the clothing appears as an ordinary black T-shirt. Upon heating via an embedded thermal unit, hidden adversarial patterns on the fabric are activated, allowing the wearer to effectively evade detection across both visible and infrared modalities. Physical experiments demonstrate that the adversarial wearable achieves rapid texture activation within 50 seconds and maintains an adversarial success rate above 80\\% across diverse real-world surveillance environments. This work demonstrates a new pathway toward physically grounded, user-controllable anti-AI systems, highlighting the growing importance of proactive adversarial techniques for privacy protection in the age of ubiquitous AI surveillance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u70ed\u6fc0\u6d3b\u5bf9\u6297\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u901a\u8fc7\u70ed\u81f4\u53d8\u8272\u67d3\u6599\u548c\u67d4\u6027\u52a0\u70ed\u5355\u5143\u5728\u8863\u7269\u8868\u9762\u4ea7\u751f\u52a8\u6001\u5bf9\u6297\u56fe\u6848\uff0c\u5728\u9ed8\u8ba4\u72b6\u6001\u4e0b\u4e3a\u666e\u901a\u9ed1\u8272T\u6064\uff0c\u52a0\u70ed\u540e\u6fc0\u6d3b\u9690\u85cf\u5bf9\u6297\u56fe\u6848\u4ee5\u8eb2\u907f\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u76d1\u63a7\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u8865\u4e01\u5916\u89c2\u663e\u773c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u8981\u5f00\u53d1\u9002\u5e94\u590d\u6742\u73b0\u5b9e\u73af\u5883\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u6765\u62b5\u6297AI\u76d1\u63a7\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u70ed\u81f4\u53d8\u8272\u67d3\u6599\u4e0e\u67d4\u6027\u52a0\u70ed\u5355\u5143\uff0c\u5728\u8863\u7269\u8868\u9762\u8bf1\u5bfc\u89c6\u89c9\u52a8\u6001\u5bf9\u6297\u56fe\u6848\uff0c\u901a\u8fc7\u5d4c\u5165\u5f0f\u70ed\u5355\u5143\u52a0\u70ed\u6fc0\u6d3b\u9690\u85cf\u56fe\u6848\u3002", "result": "\u7269\u7406\u5b9e\u9a8c\u663e\u793a\u5bf9\u6297\u53ef\u7a7f\u6234\u8bbe\u5907\u572850\u79d2\u5185\u5b9e\u73b0\u5feb\u901f\u7eb9\u7406\u6fc0\u6d3b\uff0c\u5728\u591a\u6837\u5316\u73b0\u5b9e\u76d1\u63a7\u73af\u5883\u4e2d\u4fdd\u630180%\u4ee5\u4e0a\u7684\u5bf9\u6297\u6210\u529f\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u7269\u7406\u57fa\u7840\u3001\u7528\u6237\u53ef\u63a7\u7684\u53cdAI\u7cfb\u7edf\u65b0\u9014\u5f84\uff0c\u5f3a\u8c03\u4e86\u5728AI\u76d1\u63a7\u65e0\u5904\u4e0d\u5728\u65f6\u4ee3\u4e3b\u52a8\u5bf9\u6297\u6280\u672f\u5bf9\u9690\u79c1\u4fdd\u62a4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.09693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09693", "abs": "https://arxiv.org/abs/2511.09693", "authors": ["Weiqin Chen", "Nhan Huu Pham", "Michael Robert Glass", "Long Hai Vu", "Gaetano Rossiello", "Dharmashankar Subramanian", "Santiago Paternain"], "title": "ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated significant promise in enhancing the reasoning capabilities of Text2SQL LLMs, especially with advanced algorithms such as GRPO and DAPO. However, the performance of these methods is highly sensitive to the design of reward functions. Inappropriate rewards can lead to reward hacking, where models exploit loopholes in the reward structure to achieve high scores without genuinely solving the task. This work considers a constrained RL framework for Text2SQL that incorporates natural and interpretable reward and constraint signals, while dynamically balancing trade-offs among them during the training. We establish the theoretical guarantees of our constrained RL framework and our numerical experiments on the well-known Text2SQL datasets substantiate the improvement of our approach over the state-of-the-art RL-trained LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eText2SQL\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u7136\u4e14\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u548c\u7ea6\u675f\u4fe1\u53f7\uff0c\u5e76\u52a8\u6001\u5e73\u8861\u5b83\u4eec\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u4e2d\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u654f\u611f\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63d0\u5347Text2SQL LLMs\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u4e0d\u9002\u5f53\u7684\u5956\u52b1\u4f1a\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5229\u7528\u5956\u52b1\u7ed3\u6784\u6f0f\u6d1e\u83b7\u5f97\u9ad8\u5206\u800c\u975e\u771f\u6b63\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7136\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u548c\u7ea6\u675f\u4fe1\u53f7\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861\u5b83\u4eec\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5efa\u7acb\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5728\u77e5\u540dText2SQL\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684RL\u8bad\u7ec3LLMs\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ea6\u675fRL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86Text2SQL\u4e2d\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709RL\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.09894", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09894", "abs": "https://arxiv.org/abs/2511.09894", "authors": ["Keshara Weerasinghe", "Xueren Ge", "Tessa Heick", "Lahiru Nuwan Wijayasingha", "Anthony Cortez", "Abhishek Satpathy", "John Stankovic", "Homa Alemzadeh"], "title": "EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services", "comment": "Accepted to AAAI 2026 (Preprint), 45 pages, 29 figures", "summary": "Emergency Medical Services (EMS) are critical to patient survival in emergencies, but first responders often face intense cognitive demands in high-stakes situations. AI cognitive assistants, acting as virtual partners, have the potential to ease this burden by supporting real-time data collection and decision making. In pursuit of this vision, we introduce EgoEMS, the first end-to-end, high-fidelity, multimodal, multiperson dataset capturing over 20 hours of realistic, procedural EMS activities from an egocentric view in 233 simulated emergency scenarios performed by 62 participants, including 46 EMS professionals. Developed in collaboration with EMS experts and aligned with national standards, EgoEMS is captured using an open-source, low-cost, and replicable data collection system and is annotated with keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks. Emphasizing realism, the dataset includes responder-patient interactions reflecting real-world emergency dynamics. We also present a suite of benchmarks for real-time multimodal keystep recognition and action quality estimation, essential for developing AI support tools for EMS. We hope EgoEMS inspires the research community to push the boundaries of intelligent EMS systems and ultimately contribute to improved patient outcomes.", "AI": {"tldr": "EgoEMS\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u3001\u9ad8\u4fdd\u771f\u3001\u591a\u6a21\u6001\u3001\u591a\u53c2\u4e0e\u8005\u7684EMS\u6570\u636e\u96c6\uff0c\u5305\u542b233\u4e2a\u6a21\u62df\u7d27\u6025\u573a\u666f\u4e2d62\u540d\u53c2\u4e0e\u8005\u768420\u591a\u5c0f\u65f6\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6570\u636e\uff0c\u65e8\u5728\u652f\u6301AI\u8ba4\u77e5\u52a9\u624b\u5f00\u53d1\u4ee5\u51cf\u8f7b\u6025\u6551\u4eba\u5458\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u6025\u6551\u533b\u7597\u670d\u52a1(EMS)\u4eba\u5458\u5728\u9ad8\u538b\u73af\u5883\u4e0b\u9762\u4e34\u5de8\u5927\u8ba4\u77e5\u8d1f\u62c5\uff0cAI\u8ba4\u77e5\u52a9\u624b\u4f5c\u4e3a\u865a\u62df\u4f19\u4f34\u53ef\u4ee5\u652f\u6301\u5b9e\u65f6\u6570\u636e\u6536\u96c6\u548c\u51b3\u7b56\u5236\u5b9a\uff0c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "method": "\u4e0eEMS\u4e13\u5bb6\u5408\u4f5c\u5f00\u53d1\uff0c\u4f7f\u7528\u5f00\u6e90\u4f4e\u6210\u672c\u53ef\u590d\u5236\u7684\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\uff0c\u6355\u83b7\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u7684\u6a21\u62dfEMS\u6d3b\u52a8\uff0c\u5e76\u6807\u6ce8\u5173\u952e\u6b65\u9aa4\u3001\u65f6\u95f4\u6233\u97f3\u9891\u8f6c\u5f55\u3001\u52a8\u4f5c\u8d28\u91cf\u6307\u6807\u548c\u8fb9\u754c\u6846\u5206\u5272\u63a9\u7801\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b46\u540dEMS\u4e13\u4e1a\u4eba\u5458\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5f3a\u8c03\u73b0\u5b9e\u6027\uff0c\u5305\u542b\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7d27\u6025\u52a8\u6001\u7684\u54cd\u5e94\u8005-\u60a3\u8005\u4e92\u52a8\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u5173\u952e\u6b65\u9aa4\u8bc6\u522b\u548c\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "conclusion": "EgoEMS\u6570\u636e\u96c6\u65e8\u5728\u6fc0\u52b1\u7814\u7a76\u793e\u533a\u63a8\u52a8\u667a\u80fdEMS\u7cfb\u7edf\u7684\u8fb9\u754c\uff0c\u6700\u7ec8\u4e3a\u6539\u5584\u60a3\u8005\u9884\u540e\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2511.09921", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09921", "abs": "https://arxiv.org/abs/2511.09921", "authors": ["Leping Si", "Meimei Yang", "Hui Xue", "Shipeng Zhu", "Pengfei Fang"], "title": "Adaptive Hyperbolic Kernels: Modulated Embedding in de Branges-Rovnyak Spaces", "comment": "13 pages, 3 figures, AAAI26 conference Camera-Ready", "summary": "Hierarchical data pervades diverse machine learning applications, including natural language processing, computer vision, and social network analysis. Hyperbolic space, characterized by its negative curvature, has demonstrated strong potential in such tasks due to its capacity to embed hierarchical structures with minimal distortion. Previous evidence indicates that the hyperbolic representation capacity can be further enhanced through kernel methods. However, existing hyperbolic kernels still suffer from mild geometric distortion or lack adaptability. This paper addresses these issues by introducing a curvature-aware de Branges-Rovnyak space, a reproducing kernel Hilbert space (RKHS) that is isometric to a Poincare ball. We design an adjustable multiplier to select the appropriate RKHS corresponding to the hyperbolic space with any curvature adaptively. Building on this foundation, we further construct a family of adaptive hyperbolic kernels, including the novel adaptive hyperbolic radial kernel, whose learnable parameters modulate hyperbolic features in a task-aware manner. Extensive experiments on visual and language benchmarks demonstrate that our proposed kernels outperform existing hyperbolic kernels in modeling hierarchical dependencies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f2\u7387\u611f\u77e5\u7684de Branges-Rovnyak\u7a7a\u95f4\u548c\u81ea\u9002\u5e94\u53cc\u66f2\u6838\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5c42\u6b21\u6570\u636e\u7684\u8868\u793a\u5b66\u4e60\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u53cc\u66f2\u6838\u65b9\u6cd5\u3002", "motivation": "\u53cc\u66f2\u7a7a\u95f4\u5728\u5d4c\u5165\u5c42\u6b21\u7ed3\u6784\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u53cc\u66f2\u6838\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u5931\u771f\u6216\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u53cc\u66f2\u6838\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u66f2\u7387\u611f\u77e5\u7684de Branges-Rovnyak\u7a7a\u95f4\uff08\u4e0ePoincare\u7403\u7b49\u8ddd\u7684RKHS\uff09\uff0c\u8bbe\u8ba1\u53ef\u8c03\u8282\u4e58\u5b50\u81ea\u9002\u5e94\u9009\u62e9\u5bf9\u5e94\u4efb\u610f\u66f2\u7387\u53cc\u66f2\u7a7a\u95f4\u7684RKHS\uff0c\u5e76\u6784\u5efa\u81ea\u9002\u5e94\u53cc\u66f2\u6838\u5bb6\u65cf\uff0c\u5305\u62ec\u65b0\u578b\u81ea\u9002\u5e94\u53cc\u66f2\u5f84\u5411\u6838\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6838\u65b9\u6cd5\u5728\u5efa\u6a21\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u53cc\u66f2\u6838\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u66f2\u7387\u611f\u77e5\u7684RKHS\u548c\u81ea\u9002\u5e94\u53cc\u66f2\u6838\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u53cc\u66f2\u7a7a\u95f4\u8868\u793a\u5c42\u6b21\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4e3a\u5c42\u6b21\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2511.09741", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09741", "abs": "https://arxiv.org/abs/2511.09741", "authors": ["Houming Wu", "Ling Chen"], "title": "TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training", "comment": "Accepted by AAAI 2026, 9 pages, and 6 figures", "summary": "Training large language models (LLMs) is fundamentally constrained by limited device memory and costly inter-device communication. Although pipeline parallelism alleviates memory pressure by partitioning models across devices, it incurs activation communication overhead that scales linearly with sequence length, limiting efficiency in long-context training. Recent weight-passing approaches (e.g., WeiPipe) mitigate this by transmitting model weights instead of activations, but suffer from redundant peer-to-peer (P2P) transfers and underutilized intra-node bandwidth. We propose TawPipe--topology-aware weight pipeline parallelism, which exploits hierarchical bandwidth in distributed clusters for improved communication efficiency. TawPipe: (i) groups devices based on topology to optimize intra-node collective and inter-node P2P communication; (ii) assigns each device a fixed shard of model weights and gradients, avoiding redundant transfers; and (iii) overlaps communication with computation to hide latency. Unlike global collective operations used in fully sharded data parallelism (FSDP), TawPipe confines most communication within node boundaries, significantly reducing cross-node traffic. Extensive experiments on up to 24 GPUs with LLaMA-style models show that TawPipe achieves superior throughput and scalability compared to state-of-the-art baselines.", "AI": {"tldr": "TawPipe\u662f\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u7684\u6743\u91cd\u7ba1\u9053\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5206\u5e03\u5f0f\u96c6\u7fa4\u4e2d\u7684\u5206\u5c42\u5e26\u5bbd\u6765\u4f18\u5316\u901a\u4fe1\u6548\u7387\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u53d7\u9650\u4e8e\u8bbe\u5907\u5185\u5b58\u548c\u6602\u8d35\u7684\u8bbe\u5907\u95f4\u901a\u4fe1\u3002\u867d\u7136\u7ba1\u9053\u5e76\u884c\u901a\u8fc7\u8de8\u8bbe\u5907\u5212\u5206\u6a21\u578b\u6765\u7f13\u89e3\u5185\u5b58\u538b\u529b\uff0c\u4f46\u4f1a\u5e26\u6765\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u7684\u6fc0\u6d3b\u901a\u4fe1\u5f00\u9500\u3002\u73b0\u6709\u7684\u6743\u91cd\u4f20\u9012\u65b9\u6cd5\uff08\u5982WeiPipe\uff09\u867d\u7136\u901a\u8fc7\u4f20\u8f93\u6a21\u578b\u6743\u91cd\u800c\u975e\u6fc0\u6d3b\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5b58\u5728\u5197\u4f59\u7684\u70b9\u5bf9\u70b9\u4f20\u8f93\u548c\u672a\u5145\u5206\u5229\u7528\u7684\u8282\u70b9\u5185\u5e26\u5bbd\u95ee\u9898\u3002", "method": "TawPipe\u91c7\u7528\u62d3\u6251\u611f\u77e5\u7684\u6743\u91cd\u7ba1\u9053\u5e76\u884c\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u62d3\u6251\u5bf9\u8bbe\u5907\u8fdb\u884c\u5206\u7ec4\uff0c\u4f18\u5316\u8282\u70b9\u5185\u96c6\u4f53\u901a\u4fe1\u548c\u8282\u70b9\u95f4\u70b9\u5bf9\u70b9\u901a\u4fe1\uff1b2\uff09\u4e3a\u6bcf\u4e2a\u8bbe\u5907\u5206\u914d\u56fa\u5b9a\u7684\u6a21\u578b\u6743\u91cd\u548c\u68af\u5ea6\u5206\u7247\uff0c\u907f\u514d\u5197\u4f59\u4f20\u8f93\uff1b3\uff09\u901a\u8fc7\u901a\u4fe1\u4e0e\u8ba1\u7b97\u91cd\u53e0\u6765\u9690\u85cf\u5ef6\u8fdf\u3002\u4e0eFSDP\u4f7f\u7528\u7684\u5168\u5c40\u96c6\u4f53\u64cd\u4f5c\u4e0d\u540c\uff0cTawPipe\u5c06\u5927\u90e8\u5206\u901a\u4fe1\u9650\u5236\u5728\u8282\u70b9\u8fb9\u754c\u5185\u3002", "result": "\u5728\u6700\u591a24\u4e2aGPU\u4e0a\u4f7f\u7528LLaMA\u98ce\u683c\u6a21\u578b\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTawPipe\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "TawPipe\u901a\u8fc7\u5229\u7528\u5206\u5e03\u5f0f\u96c6\u7fa4\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8de8\u8282\u70b9\u901a\u4fe1\u6d41\u91cf\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u901a\u4fe1\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.09754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09754", "abs": "https://arxiv.org/abs/2511.09754", "authors": ["Sarthak Khanna", "Armin Berger", "Muskaan Chopra", "Rafet Sifa"], "title": "History Rhymes: Macro-Contextual Retrieval for Robust Financial Forecasting", "comment": "Accepted in IEEE BigData 2025", "summary": "Financial markets are inherently non-stationary: structural breaks and macroeconomic regime shifts often cause forecasting models to fail when deployed out of distribution (OOD). Conventional multimodal approaches that simply fuse numerical indicators and textual sentiment rarely adapt to such shifts. We introduce macro-contextual retrieval, a retrieval-augmented forecasting framework that grounds each prediction in historically analogous macroeconomic regimes. The method jointly embeds macro indicators (e.g., CPI, unemployment, yield spread, GDP growth) and financial news sentiment in a shared similarity space, enabling causal retrieval of precedent periods during inference without retraining.\n  Trained on seventeen years of S&P 500 data (2007-2023) and evaluated OOD on AAPL (2024) and XOM (2024), the framework consistently narrows the CV to OOD performance gap. Macro-conditioned retrieval achieves the only positive out-of-sample trading outcomes (AAPL: PF=1.18, Sharpe=0.95; XOM: PF=1.16, Sharpe=0.61), while static numeric, text-only, and naive multimodal baselines collapse under regime shifts. Beyond metric gains, retrieved neighbors form interpretable evidence chains that correspond to recognizable macro contexts, such as inflationary or yield-curve inversion phases, supporting causal interpretability and transparency. By operationalizing the principle that \"financial history may not repeat, but it often rhymes,\" this work demonstrates that macro-aware retrieval yields robust, explainable forecasts under distributional change.\n  All datasets, models, and source code are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b8f\u89c2\u60c5\u5883\u68c0\u7d22\u7684\u91d1\u878d\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5386\u53f2\u4e0a\u7c7b\u4f3c\u7684\u5b8f\u89c2\u7ecf\u6d4e\u60c5\u5883\u6765\u589e\u5f3a\u9884\u6d4b\u7684\u7a33\u5065\u6027\uff0c\u5728\u5206\u5e03\u5916\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u672c\u8d28\u4e0a\u662f\u975e\u5e73\u7a33\u7684\uff0c\u7ed3\u6784\u6027\u65ad\u88c2\u548c\u5b8f\u89c2\u7ecf\u6d4e\u4f53\u5236\u8f6c\u53d8\u5e38\u5e38\u5bfc\u81f4\u9884\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\u90e8\u7f72\u65f6\u5931\u6548\u3002\u4f20\u7edf\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u7b80\u5355\u878d\u5408\u6570\u503c\u6307\u6807\u548c\u6587\u672c\u60c5\u611f\uff0c\u5f88\u5c11\u80fd\u9002\u5e94\u8fd9\u79cd\u8f6c\u53d8\u3002", "method": "\u5f15\u5165\u5b8f\u89c2\u60c5\u5883\u68c0\u7d22\u6846\u67b6\uff0c\u5c06\u5b8f\u89c2\u6307\u6807\uff08\u5982CPI\u3001\u5931\u4e1a\u7387\u3001\u6536\u76ca\u7387\u5229\u5dee\u3001GDP\u589e\u957f\uff09\u548c\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5171\u540c\u5d4c\u5165\u5171\u4eab\u76f8\u4f3c\u6027\u7a7a\u95f4\uff0c\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u56e0\u679c\u68c0\u7d22\u5386\u53f2\u7c7b\u4f3c\u65f6\u671f\u3002", "result": "\u572817\u5e74\u6807\u666e500\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728AAPL\u548cXOM\u4e0a\u8fdb\u884c\u5206\u5e03\u5916\u8bc4\u4f30\uff0c\u8be5\u6846\u67b6\u6301\u7eed\u7f29\u5c0f\u4e86CV\u5230OOD\u6027\u80fd\u5dee\u8ddd\u3002\u5b8f\u89c2\u6761\u4ef6\u68c0\u7d22\u5b9e\u73b0\u4e86\u552f\u4e00\u6b63\u9762\u7684\u6837\u672c\u5916\u4ea4\u6613\u7ed3\u679c\uff08AAPL\uff1aPF=1.18\uff0cSharpe=0.95\uff1bXOM\uff1aPF=1.16\uff0cSharpe=0.61\uff09\u3002", "conclusion": "\u901a\u8fc7\u64cd\u4f5c\"\u91d1\u878d\u5386\u53f2\u53ef\u80fd\u4e0d\u4f1a\u91cd\u6f14\uff0c\u4f46\u5e38\u5e38\u62bc\u97f5\"\u7684\u539f\u5219\uff0c\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u5b8f\u89c2\u611f\u77e5\u68c0\u7d22\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u4ea7\u751f\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u5f62\u6210\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u94fe\uff0c\u5bf9\u5e94\u53ef\u8bc6\u522b\u7684\u5b8f\u89c2\u60c5\u5883\u3002"}}
{"id": "2511.09763", "categories": ["cs.LG", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.09763", "abs": "https://arxiv.org/abs/2511.09763", "authors": ["Guy Blanc", "Yizhi Huang", "Tal Malkin", "Rocco A. Servedio"], "title": "Is nasty noise actually harder than malicious noise?", "comment": "To appear in SODA 2026", "summary": "We consider the relative abilities and limitations of computationally efficient algorithms for learning in the presence of noise, under two well-studied and challenging adversarial noise models for learning Boolean functions: malicious noise, in which an adversary can arbitrarily corrupt a random subset of examples given to the learner; and nasty noise, in which an adversary can arbitrarily corrupt an adversarially chosen subset of examples given to the learner.\n  We consider both the distribution-independent and fixed-distribution settings. Our main results highlight a dramatic difference between these two settings: For distribution-independent learning, we prove a strong equivalence between the two noise models: If a class ${\\cal C}$ of functions is efficiently learnable in the presence of $\u03b7$-rate malicious noise, then it is also efficiently learnable in the presence of $\u03b7$-rate nasty noise. In sharp contrast, for the fixed-distribution setting we show an arbitrarily large separation: Under a standard cryptographic assumption, for any arbitrarily large value $r$ there exists a concept class for which there is a ratio of $r$ between the rate $\u03b7_{malicious}$ of malicious noise that polynomial-time learning algorithms can tolerate, versus the rate $\u03b7_{nasty}$ of nasty noise that such learning algorithms can tolerate.\n  To offset the negative result for the fixed-distribution setting, we define a broad and natural class of algorithms, namely those that ignore contradictory examples (ICE). We show that for these algorithms, malicious noise and nasty noise are equivalent up to a factor of two in the noise rate: Any efficient ICE learner that succeeds with $\u03b7$-rate malicious noise can be converted to an efficient learner that succeeds with $\u03b7/2$-rate nasty noise. We further show that the above factor of two is necessary, again under a standard cryptographic assumption.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e03\u5c14\u51fd\u6570\u5b66\u4e60\u4e2d\u4e24\u79cd\u5bf9\u6297\u6027\u566a\u58f0\u6a21\u578b\uff08\u6076\u610f\u566a\u58f0\u548c\u6076\u610f\u566a\u58f0\uff09\u4e0b\u8ba1\u7b97\u9ad8\u6548\u7b97\u6cd5\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002\u5728\u5206\u5e03\u65e0\u5173\u8bbe\u7f6e\u4e2d\uff0c\u4e24\u79cd\u566a\u58f0\u6a21\u578b\u5177\u6709\u5f3a\u7b49\u4ef7\u6027\uff1b\u5728\u56fa\u5b9a\u5206\u5e03\u8bbe\u7f6e\u4e2d\uff0c\u5b58\u5728\u4efb\u610f\u5927\u7684\u5206\u79bb\u3002\u4f5c\u8005\u8fd8\u5b9a\u4e49\u4e86\u5ffd\u7565\u77db\u76fe\u793a\u4f8b\uff08ICE\uff09\u7684\u7b97\u6cd5\u7c7b\uff0c\u8bc1\u660e\u5728\u8fd9\u7c7b\u7b97\u6cd5\u4e2d\u4e24\u79cd\u566a\u58f0\u6a21\u578b\u5728\u566a\u58f0\u7387\u4e0a\u7b49\u4ef7\u4e8e2\u500d\u56e0\u5b50\u3002", "motivation": "\u7814\u7a76\u8ba1\u7b97\u9ad8\u6548\u7b97\u6cd5\u5728\u5bf9\u6297\u6027\u566a\u58f0\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u6076\u610f\u566a\u58f0\u548c\u6076\u610f\u566a\u58f0\u8fd9\u4e24\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u566a\u58f0\u6a21\u578b\uff0c\u63a2\u7d22\u5b83\u4eec\u5728\u4e0d\u540c\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u7684\u76f8\u5bf9\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u548c\u8bc1\u660e\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u5206\u5e03\u65e0\u5173\u548c\u56fa\u5b9a\u5206\u5e03\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u6bd4\u8f83\u6076\u610f\u566a\u58f0\u548c\u6076\u610f\u566a\u58f0\u6a21\u578b\u3002\u5b9a\u4e49\u4e86\u5ffd\u7565\u77db\u76fe\u793a\u4f8b\uff08ICE\uff09\u7684\u7b97\u6cd5\u7c7b\uff0c\u5e76\u5206\u6790\u5176\u5728\u4e0d\u540c\u566a\u58f0\u6a21\u578b\u4e0b\u7684\u8868\u73b0\u3002", "result": "1. \u5206\u5e03\u65e0\u5173\u8bbe\u7f6e\u4e2d\uff1a\u6076\u610f\u566a\u58f0\u548c\u6076\u610f\u566a\u58f0\u5177\u6709\u5f3a\u7b49\u4ef7\u6027\uff1b2. \u56fa\u5b9a\u5206\u5e03\u8bbe\u7f6e\u4e2d\uff1a\u5b58\u5728\u4efb\u610f\u5927\u7684\u5206\u79bb\uff08\u57fa\u4e8e\u5bc6\u7801\u5b66\u5047\u8bbe\uff09\uff1b3. ICE\u7b97\u6cd5\u7c7b\u4e2d\uff1a\u4e24\u79cd\u566a\u58f0\u6a21\u578b\u5728\u566a\u58f0\u7387\u4e0a\u7b49\u4ef7\u4e8e2\u500d\u56e0\u5b50\uff0c\u4e14\u8be5\u56e0\u5b50\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "\u5b66\u4e60\u8bbe\u7f6e\u5bf9\u566a\u58f0\u6a21\u578b\u7684\u76f8\u5bf9\u96be\u5ea6\u6709\u663e\u8457\u5f71\u54cd\uff1a\u5206\u5e03\u65e0\u5173\u8bbe\u7f6e\u4e2d\u4e24\u79cd\u566a\u58f0\u6a21\u578b\u7b49\u4ef7\uff0c\u56fa\u5b9a\u5206\u5e03\u8bbe\u7f6e\u4e2d\u5b58\u5728\u663e\u8457\u5206\u79bb\u3002ICE\u7b97\u6cd5\u4e3a\u56fa\u5b9a\u5206\u5e03\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e24\u79cd\u566a\u58f0\u6a21\u578b\u95f4\u5efa\u7acb\u4e86\u7d27\u5bc6\u8054\u7cfb\u3002"}}
{"id": "2511.10037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10037", "abs": "https://arxiv.org/abs/2511.10037", "authors": ["Xiaolong Wei", "Yuehu Dong", "Xingliang Wang", "Xingyu Zhang", "Zhejun Zhao", "Dongdong Shen", "Long Xia", "Dawei Yin"], "title": "Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning", "comment": null, "summary": "Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Planner's tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Planner-centric Plan-Execute\u8303\u5f0f\uff0c\u901a\u8fc7\u5168\u5c40DAG\u89c4\u5212\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u589e\u5f3aLLM\u5728\u590d\u6742\u67e5\u8be2\u5904\u7406\u4e2d\u7684\u5c40\u90e8\u4f18\u5316\u9677\u9631\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u65f6\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u5982ReAct\u7b49\u6846\u67b6\u56e0\u4f9d\u8d56\u589e\u91cf\u51b3\u7b56\u8fc7\u7a0b\u800c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u4f18\u5316\u9677\u9631\u3002", "method": "\u63d0\u51faPlanner-centric Plan-Execute\u8303\u5f0f\uff0c\u6838\u5fc3\u662f\u4e00\u4e2a\u6267\u884c\u5168\u5c40\u6709\u5411\u65e0\u73af\u56fe\u89c4\u5212\u7684Planner\u6a21\u578b\uff1b\u5f00\u53d1\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff1b\u6784\u5efaComplexTool-Plan\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5728StableToolBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u7aef\u5230\u7aef\u6267\u884c\u80fd\u529b\u548c\u5bf9\u590d\u6742\u591a\u5de5\u5177\u5de5\u4f5c\u6d41\u7a0b\u7684\u9c81\u68d2\u5904\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u5c40\u90e8\u4f18\u5316\u74f6\u9888\uff0c\u5728\u590d\u6742\u7528\u6237\u67e5\u8be2\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.10038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10038", "abs": "https://arxiv.org/abs/2511.10038", "authors": ["Ziheng Li", "Hengyi Cai", "Xiaochi Wei", "Yuchen Li", "Shuaiqiang Wang", "Zhi-Hong Deng", "Dawei Yin"], "title": "Efficient Thought Space Exploration through Strategic Intervention", "comment": "AAAI 2026", "summary": "While large language models (LLMs) demonstrate emerging reasoning capabilities, current inference-time expansion methods incur prohibitive computational costs by exhaustive sampling. Through analyzing decoding trajectories, we observe that most next-token predictions align well with the golden output, except for a few critical tokens that lead to deviations. Inspired by this phenomenon, we propose a novel Hint-Practice Reasoning (HPR) framework that operationalizes this insight through two synergistic components: 1) a hinter (powerful LLM) that provides probabilistic guidance at critical decision points, and 2) a practitioner (efficient smaller model) that executes major reasoning steps. The framework's core innovation lies in Distributional Inconsistency Reduction (DIR), a theoretically-grounded metric that dynamically identifies intervention points by quantifying the divergence between practitioner's reasoning trajectory and hinter's expected distribution in a tree-structured probabilistic space. Through iterative tree updates guided by DIR, HPR reweights promising reasoning paths while deprioritizing low-probability branches. Experiments across arithmetic and commonsense reasoning benchmarks demonstrate HPR's state-of-the-art efficiency-accuracy tradeoffs: it achieves comparable performance to self-consistency and MCTS baselines while decoding only 1/5 tokens, and outperforms existing methods by at most 5.1% absolute accuracy while maintaining similar or lower FLOPs.", "AI": {"tldr": "\u63d0\u51faHint-Practice Reasoning (HPR)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u4e0d\u4e00\u81f4\u6027\u51cf\u5c11(DIR)\u6307\u6807\u52a8\u6001\u8bc6\u522b\u5173\u952e\u51b3\u7b56\u70b9\uff0c\u8ba9\u5f3a\u5927LLM\u63d0\u4f9b\u6982\u7387\u6307\u5bfc\uff0c\u5c0f\u578b\u6a21\u578b\u6267\u884c\u4e3b\u8981\u63a8\u7406\u6b65\u9aa4\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u901a\u8fc7\u5206\u6790\u89e3\u7801\u8f68\u8ff9\u53d1\u73b0\u5927\u591a\u6570token\u9884\u6d4b\u6b63\u786e\uff0c\u53ea\u6709\u5c11\u6570\u5173\u952etoken\u5bfc\u81f4\u504f\u5dee\uff0c\u9700\u8981\u9488\u5bf9\u6027\u5e72\u9884\u3002", "method": "HPR\u6846\u67b6\u5305\u542bhinter(\u5f3a\u5927LLM)\u548cpractitioner(\u9ad8\u6548\u5c0f\u6a21\u578b)\uff0c\u4f7f\u7528DIR\u6307\u6807\u5728\u6811\u7ed3\u6784\u6982\u7387\u7a7a\u95f4\u4e2d\u91cf\u5316\u63a8\u7406\u8f68\u8ff9\u4e0e\u671f\u671b\u5206\u5e03\u7684\u5dee\u5f02\uff0c\u52a8\u6001\u8bc6\u522b\u5e72\u9884\u70b9\u3002", "result": "\u5728\u7b97\u672f\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHPR\u8fbe\u5230\u4e0e\u81ea\u4e00\u81f4\u6027\u548cMCTS\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53ea\u89e3\u78011/5\u7684token\uff0c\u5728\u4fdd\u6301\u76f8\u4f3c\u6216\u66f4\u4f4eFLOPs\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa\u6700\u591a5.1%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u3002", "conclusion": "HPR\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u8bc6\u522b\u5173\u952e\u51b3\u7b56\u70b9\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2511.09780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09780", "abs": "https://arxiv.org/abs/2511.09780", "authors": ["Nikolay Blagoev", "O\u011fuzhan Ersoy", "Lydia Yiyu Chen"], "title": "Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u53bb\u4e2d\u5fc3\u5316GRPO\u8bad\u7ec3\u7684\u9996\u4e2a\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6076\u610f\u8282\u70b9\u53ef\u4ee5\u901a\u8fc7\u6ce8\u5165\u6076\u610f\u4ee4\u724c\u6765\u6bd2\u5316\u826f\u6027\u6a21\u578b\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u653b\u51fb\u6210\u529f\u7387\u53ef\u8fbe100%\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "GRPO\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u5176\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u4f7f\u5f97\u591a\u4e2a\u8282\u70b9\u53ef\u4ee5\u5e76\u53d1\u5904\u7406\u63d0\u793a\u5e76\u4ea4\u6362\u5b57\u7b26\u4e32\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u67b6\u6784\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u6076\u610f\u65b9\u53ef\u80fd\u901a\u8fc7\u6ce8\u5165\u6076\u610f\u4ee4\u724c\u6765\u6bd2\u5316\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5bf9\u6297\u653b\u51fb\u53ca\u5176\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u9488\u5bf9\u53bb\u4e2d\u5fc3\u5316GRPO\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u5916\u653b\u51fb\u548c\u4e0a\u4e0b\u6587\u5185\u653b\u51fb\u3002\u6076\u610f\u8282\u70b9\u901a\u8fc7\u5728\u826f\u6027\u6a21\u578b\u4e2d\u6ce8\u5165\u4efb\u610f\u6076\u610f\u4ee4\u724c\u6765\u6c61\u67d3\u672c\u5730LLM\u540e\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u7684\u5b9e\u8bc1\u5b9e\u9a8c\u4e2d\uff0c\u5bf9\u6297\u653b\u51fb\u80fd\u591f\u8f7b\u677e\u6bd2\u5316\u826f\u6027\u8282\u70b9\uff0c\u5728\u4ec550\u6b21\u8fed\u4ee3\u5185\u653b\u51fb\u6210\u529f\u7387\u53ef\u8fbe100%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u79cd\u9632\u5fa1\u65b9\u6848\uff1a\u5f53\u6240\u6709\u7528\u6237\u8bad\u7ec3\u76f8\u540c\u6a21\u578b\u65f6\u548c\u8bad\u7ec3\u4e0d\u540c\u6a21\u578b\u65f6\u7684\u9632\u5fa1\u7b56\u7565\u3002\u8fd9\u4e9b\u9632\u5fa1\u63aa\u65bd\u53ef\u4ee5\u8fbe\u5230100%\u7684\u963b\u6b62\u7387\uff0c\u4f7f\u653b\u51fb\u65e0\u6cd5\u6210\u529f\u3002"}}
{"id": "2511.09783", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09783", "abs": "https://arxiv.org/abs/2511.09783", "authors": ["Pablo Ruiz-Morales", "Dries Vanoost", "Davy Pissoort", "Mathias Verbeke"], "title": "Koopman Invariants as Drivers of Emergent Time-Series Clustering in Joint-Embedding Predictive Architectures", "comment": "11 pages, 5 figures", "summary": "Joint-Embedding Predictive Architectures (JEPAs), a powerful class of self-supervised models, exhibit an unexplained ability to cluster time-series data by their underlying dynamical regimes. We propose a novel theoretical explanation for this phenomenon, hypothesizing that JEPA's predictive objective implicitly drives it to learn the invariant subspace of the system's Koopman operator. We prove that an idealized JEPA loss is minimized when the encoder represents the system's regime indicator functions, which are Koopman eigenfunctions. This theory was validated on synthetic data with known dynamics, demonstrating that constraining the JEPA's linear predictor to be a near-identity operator is the key inductive bias that forces the encoder to learn these invariants. We further discuss that this constraint is critical for selecting this interpretable solution from a class of mathematically equivalent but entangled optima, revealing the predictor's role in representation disentanglement. This work demystifies a key behavior of JEPAs, provides a principled connection between modern self-supervised learning and dynamical systems theory, and informs the design of more robust and interpretable time-series models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u89e3\u91ca\uff0c\u8bf4\u660e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\u5982\u4f55\u901a\u8fc7\u5176\u9884\u6d4b\u76ee\u6807\u9690\u5f0f\u5b66\u4e60\u7cfb\u7edfKoopman\u7b97\u5b50\u7684\u4e0d\u53d8\u5b50\u7a7a\u95f4\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u52a8\u6001\u673a\u5236\u805a\u7c7b\u3002", "motivation": "JEPA\u4f5c\u4e3a\u4e00\u7c7b\u5f3a\u5927\u7684\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6309\u5e95\u5c42\u52a8\u6001\u673a\u5236\u8fdb\u884c\u805a\u7c7b\u7684\u672a\u89e3\u91ca\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u7406\u60f3\u5316JEPA\u635f\u5931\u6700\u5c0f\u5316\u65f6\u7f16\u7801\u5668\u8868\u793a\u7cfb\u7edf\u7684\u673a\u5236\u6307\u793a\u51fd\u6570\uff08\u5373Koopman\u7279\u5f81\u51fd\u6570\uff09\uff0c\u5e76\u5728\u5df2\u77e5\u52a8\u6001\u7684\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7279\u522b\u5173\u6ce8\u7ebf\u6027\u9884\u6d4b\u5668\u7ea6\u675f\u4e3a\u8fd1\u6052\u7b49\u7b97\u5b50\u7684\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u9a8c\u8bc1\u4e86\u7ea6\u675fJEPA\u7684\u7ebf\u6027\u9884\u6d4b\u5668\u4e3a\u8fd1\u6052\u7b49\u7b97\u5b50\u662f\u8feb\u4f7f\u7f16\u7801\u5668\u5b66\u4e60\u4e0d\u53d8\u91cf\u7684\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u63ed\u793a\u4e86\u9884\u6d4b\u5668\u5728\u8868\u793a\u89e3\u7ea0\u7f20\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9610\u660e\u4e86JEPA\u7684\u5173\u952e\u884c\u4e3a\uff0c\u4e3a\u73b0\u4ee3\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8054\u7cfb\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.09789", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09789", "abs": "https://arxiv.org/abs/2511.09789", "authors": ["Fulong Yao", "Wanqing Zhao", "Chao Zheng", "Xiaofei Han"], "title": "CaReTS: A Multi-Task Framework Unifying Classification and Regression for Time Series Forecasting", "comment": null, "summary": "Recent advances in deep forecasting models have achieved remarkable performance, yet most approaches still struggle to provide both accurate predictions and interpretable insights into temporal dynamics. This paper proposes CaReTS, a novel multi-task learning framework that combines classification and regression tasks for multi-step time series forecasting problems. The framework adopts a dual-stream architecture, where a classification branch learns the stepwise trend into the future, while a regression branch estimates the corresponding deviations from the latest observation of the target variable. The dual-stream design provides more interpretable predictions by disentangling macro-level trends from micro-level deviations in the target variable. To enable effective learning in output prediction, deviation estimation, and trend classification, we design a multi-task loss with uncertainty-aware weighting to adaptively balance the contribution of each task. Furthermore, four variants (CaReTS1--4) are instantiated under this framework to incorporate mainstream temporal modelling encoders, including convolutional neural networks (CNNs), long short-term memory networks (LSTMs), and Transformers. Experiments on real-world datasets demonstrate that CaReTS outperforms state-of-the-art (SOTA) algorithms in forecasting accuracy, while achieving higher trend classification performance.", "AI": {"tldr": "CaReTS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u6765\u89e3\u51b3\u591a\u6b65\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u5206\u522b\u5b66\u4e60\u8d8b\u52bf\u548c\u504f\u5dee\uff0c\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u9884\u6d4b\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u540c\u65f6\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\u548c\u65f6\u95f4\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u67b6\u6784\uff1a\u5206\u7c7b\u5206\u652f\u5b66\u4e60\u672a\u6765\u9010\u6b65\u8d8b\u52bf\uff0c\u56de\u5f52\u5206\u652f\u4f30\u8ba1\u76ee\u6807\u53d8\u91cf\u6700\u65b0\u89c2\u6d4b\u503c\u7684\u504f\u5dee\uff1b\u8bbe\u8ba1\u591a\u4efb\u52a1\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a0\u6743\uff1b\u57fa\u4e8e\u4e3b\u6d41\u65f6\u5e8f\u5efa\u6a21\u7f16\u7801\u5668\uff08CNN\u3001LSTM\u3001Transformer\uff09\u5b9e\u4f8b\u5316\u56db\u4e2a\u53d8\u4f53\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaReTS\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8d8b\u52bf\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "CaReTS\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5b8f\u89c2\u8d8b\u52bf\u548c\u5fae\u89c2\u504f\u5dee\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002"}}
{"id": "2511.09792", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09792", "abs": "https://arxiv.org/abs/2511.09792", "authors": ["Tianmeng Hu", "Yongzheng Cui", "Rui Tang", "Biao Luo", "Ke Li"], "title": "Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning", "comment": "Accepted at AAAI 2026", "summary": "Value decomposition is a central approach in multi-agent reinforcement learning (MARL), enabling centralized training with decentralized execution by factorizing the global value function into local values. To ensure individual-global-max (IGM) consistency, existing methods either enforce monotonicity constraints, which limit expressive power, or adopt softer surrogates at the cost of algorithmic complexity. In this work, we present a dynamical systems analysis of non-monotonic value decomposition, modeling learning dynamics as continuous-time gradient flow. We prove that, under approximately greedy exploration, all zero-loss equilibria violating IGM consistency are unstable saddle points, while only IGM-consistent solutions are stable attractors of the learning dynamics. Extensive experiments on both synthetic matrix games and challenging MARL benchmarks demonstrate that unconstrained, non-monotonic factorization reliably recovers IGM-optimal solutions and consistently outperforms monotonic baselines. Additionally, we investigate the influence of temporal-difference targets and exploration strategies, providing actionable insights for the design of future value-based MARL algorithms.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u52a8\u529b\u7cfb\u7edf\u5206\u6790\u8bc1\u660e\uff0c\u5728\u8fd1\u4f3c\u8d2a\u5a6a\u63a2\u7d22\u4e0b\uff0c\u975e\u5355\u8c03\u503c\u5206\u89e3\u80fd\u591f\u53ef\u9760\u5730\u6062\u590dIGM\u6700\u4f18\u89e3\uff0c\u5e76\u5728MARL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5355\u8c03\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u503c\u5206\u89e3\u65b9\u6cd5\u8981\u4e48\u65bd\u52a0\u5355\u8c03\u6027\u7ea6\u675f\u9650\u5236\u8868\u8fbe\u80fd\u529b\uff0c\u8981\u4e48\u91c7\u7528\u8f6f\u66ff\u4ee3\u65b9\u6848\u589e\u52a0\u7b97\u6cd5\u590d\u6742\u5ea6\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u975e\u5355\u8c03\u503c\u5206\u89e3\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u63a2\u7d22\u65e0\u7ea6\u675f\u5206\u89e3\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u52a8\u529b\u7cfb\u7edf\u5206\u6790\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u52a8\u6001\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u68af\u5ea6\u6d41\uff0c\u8bc1\u660e\u5728\u8fd1\u4f3c\u8d2a\u5a6a\u63a2\u7d22\u4e0b\uff0c\u8fdd\u53cdIGM\u4e00\u81f4\u6027\u7684\u96f6\u635f\u5931\u5e73\u8861\u70b9\u662f\u4e0d\u7a33\u5b9a\u978d\u70b9\uff0c\u800cIGM\u4e00\u81f4\u89e3\u662f\u7a33\u5b9a\u5438\u5f15\u5b50\u3002", "result": "\u5728\u5408\u6210\u77e9\u9635\u6e38\u620f\u548c\u6311\u6218\u6027MARL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u7ea6\u675f\u975e\u5355\u8c03\u5206\u89e3\u80fd\u591f\u53ef\u9760\u6062\u590dIGM\u6700\u4f18\u89e3\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u5355\u8c03\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u975e\u5355\u8c03\u503c\u5206\u89e3\u662f\u53ef\u884c\u7684\uff0c\u65e0\u9700\u65bd\u52a0\u5355\u8c03\u6027\u7ea6\u675f\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u65f6\u95f4\u5dee\u5206\u76ee\u6807\u548c\u63a2\u7d22\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u503c\u7684MARL\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2511.10161", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10161", "abs": "https://arxiv.org/abs/2511.10161", "authors": ["J. Javier Alonso-Ramos", "Ignacio Aguilera-Martos", "Andr\u00e9s Herrera-Poyatos", "Francisco Herrera"], "title": "DenoGrad: Deep Gradient Denoising Framework for Enhancing the Performance of Interpretable AI Models", "comment": null, "summary": "The performance of Machine Learning (ML) models, particularly those operating within the Interpretable Artificial Intelligence (Interpretable AI) framework, is significantly affected by the presence of noise in both training and production data. Denoising has therefore become a critical preprocessing step, typically categorized into instance removal and instance correction techniques. However, existing correction approaches often degrade performance or oversimplify the problem by altering the original data distribution. This leads to unrealistic scenarios and biased models, which is particularly problematic in contexts where interpretable AI models are employed, as their interpretability depends on the fidelity of the underlying data patterns. In this paper, we argue that defining noise independently of the solution may be ineffective, as its nature can vary significantly across tasks and datasets. Using a task-specific high quality solution as a reference can provide a more precise and adaptable noise definition. To this end, we propose DenoGrad, a novel Gradient-based instance Denoiser framework that leverages gradients from an accurate Deep Learning (DL) model trained on the target data -- regardless of the specific task -- to detect and adjust noisy samples. Unlike conventional approaches, DenoGrad dynamically corrects noisy instances, preserving problem's data distribution, and improving AI models robustness. DenoGrad is validated on both tabular and time series datasets under various noise settings against the state-of-the-art. DenoGrad outperforms existing denoising strategies, enhancing the performance of interpretable IA models while standing out as the only high quality approach that preserves the original data distribution.", "AI": {"tldr": "\u63d0\u51faDenoGrad\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u68af\u5ea6\u6765\u68c0\u6d4b\u548c\u8c03\u6574\u566a\u58f0\u6837\u672c\uff0c\u5728\u4fdd\u6301\u6570\u636e\u5206\u5e03\u7684\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91caAI\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53bb\u566a\u65b9\u6cd5\u4f1a\u6539\u53d8\u539f\u59cb\u6570\u636e\u5206\u5e03\uff0c\u5bfc\u81f4\u4e0d\u73b0\u5b9e\u7684\u573a\u666f\u548c\u6709\u504f\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u53ef\u89e3\u91caAI\u5e94\u7528\u4e2d\uff0c\u5176\u53ef\u89e3\u91ca\u6027\u4f9d\u8d56\u4e8e\u5e95\u5c42\u6570\u636e\u6a21\u5f0f\u7684\u4fdd\u771f\u5ea6\u3002", "method": "DenoGrad\u6846\u67b6\u5229\u7528\u5728\u76ee\u6807\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u51c6\u786e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u68af\u5ea6\u6765\u68c0\u6d4b\u548c\u8c03\u6574\u566a\u58f0\u6837\u672c\uff0c\u52a8\u6001\u4fee\u6b63\u566a\u58f0\u5b9e\u4f8b\u3002", "result": "\u5728\u8868\u683c\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5404\u79cd\u566a\u58f0\u8bbe\u7f6e\u4e0b\uff0cDenoGrad\u4f18\u4e8e\u73b0\u6709\u53bb\u566a\u7b56\u7565\uff0c\u662f\u552f\u4e00\u80fd\u4fdd\u6301\u539f\u59cb\u6570\u636e\u5206\u5e03\u7684\u9ad8\u8d28\u91cf\u65b9\u6cd5\u3002", "conclusion": "DenoGrad\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u53c2\u8003\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u9002\u5e94\u6027\u5f3a\u7684\u566a\u58f0\u5b9a\u4e49\uff0c\u5728\u4fdd\u6301\u6570\u636e\u5206\u5e03\u7684\u540c\u65f6\u63d0\u9ad8\u4e86AI\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.10164", "categories": ["cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2511.10164", "abs": "https://arxiv.org/abs/2511.10164", "authors": ["Periklis Mantenoglou", "Luigi Bonassi", "Enrico Scala", "Pedro Zuidberg Dos Martires"], "title": "Two Constraint Compilation Methods for Lifted Planning", "comment": null, "summary": "We study planning in a fragment of PDDL with qualitative state-trajectory constraints, capturing safety requirements, task ordering conditions, and intermediate sub-goals commonly found in real-world problems. A prominent approach to tackle such problems is to compile their constraints away, leading to a problem that is supported by state-of-the-art planners. Unfortunately, existing compilers do not scale on problems with a large number of objects and high-arity actions, as they necessitate grounding the problem before compilation. To address this issue, we propose two methods for compiling away constraints without grounding, making them suitable for large-scale planning problems. We prove the correctness of our compilers and outline their worst-case time complexity. Moreover, we present a reproducible empirical evaluation on the domains used in the latest International Planning Competition. Our results demonstrate that our methods are efficient and produce planning specifications that are orders of magnitude more succinct than the ones produced by compilers that ground the domain, while remaining competitive when used for planning with a state-of-the-art planner.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e26\u6709\u5b9a\u6027\u72b6\u6001\u8f68\u8ff9\u7ea6\u675f\u7684PDDL\u7247\u6bb5\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u57fa\u7840\u5316\uff08grounding\uff09\u7684\u7ea6\u675f\u7f16\u8bd1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u4e2d\u73b0\u6709\u7f16\u8bd1\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c4\u5212\u95ee\u9898\u901a\u5e38\u5305\u542b\u5b89\u5168\u6027\u8981\u6c42\u3001\u4efb\u52a1\u6392\u5e8f\u6761\u4ef6\u548c\u4e2d\u95f4\u5b50\u76ee\u6807\u7b49\u5b9a\u6027\u72b6\u6001\u8f68\u8ff9\u7ea6\u675f\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5148\u5bf9\u95ee\u9898\u8fdb\u884c\u57fa\u7840\u5316\u518d\u7f16\u8bd1\u7ea6\u675f\uff0c\u8fd9\u5728\u5bf9\u8c61\u6570\u91cf\u548c\u52a8\u4f5c\u5143\u6570\u8f83\u5927\u65f6\u65e0\u6cd5\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u57fa\u7840\u5316\u7684\u7ea6\u675f\u7f16\u8bd1\u65b9\u6cd5\uff0c\u76f4\u63a5\u5c06\u7ea6\u675f\u7f16\u8bd1\u4e3a\u652f\u6301\u73b0\u4ee3\u89c4\u5212\u5668\u7684\u89c4\u5212\u89c4\u8303\uff0c\u907f\u514d\u4e86\u57fa\u7840\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b0\u65b9\u6cd5\u9ad8\u6548\u4e14\u4ea7\u751f\u7684\u89c4\u5212\u89c4\u8303\u6bd4\u57fa\u7840\u5316\u7f16\u8bd1\u65b9\u6cd5\u7b80\u6d01\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u5728\u4f7f\u7528\u73b0\u4ee3\u89c4\u5212\u5668\u65f6\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u57fa\u7840\u5316\u7ea6\u675f\u7f16\u8bd1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u89c4\u5212\u89c4\u8303\u7684\u89c4\u6a21\u3002"}}
{"id": "2511.10240", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10240", "abs": "https://arxiv.org/abs/2511.10240", "authors": ["Minbae Park", "Hyemin Yang", "Jeonghyun Kim", "Kunsoo Park", "Hyunjoon Kim"], "title": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.", "AI": {"tldr": "ProgRAG\u662f\u4e00\u4e2a\u591a\u8df3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u5e76\u9010\u6b65\u6269\u5c55\u63a8\u7406\u8def\u5f84\u6765\u89e3\u51b3LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u68c0\u7d22\u548c\u63a8\u7406\u5931\u8d25\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u900f\u660e\u5ea6\u6709\u9650\u7684\u95ee\u9898\u3002\u867d\u7136\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684LLM\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u4ecd\u9762\u4e34\u68c0\u7d22\u4e0d\u51c6\u786e\u3001\u63a8\u7406\u5931\u8d25\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u957f\u8f93\u5165\u4e0a\u4e0b\u6587\u548c\u590d\u6742\u903b\u8f91\u65b9\u5411\u7684\u60c5\u51b5\u4e0b\u3002", "method": "ProgRAG\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u9010\u6b65\u6269\u5c55\u90e8\u5206\u63a8\u7406\u8def\u5f84\uff0c\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u4f7f\u7528\u5916\u90e8\u68c0\u7d22\u5668\u6536\u96c6\u5019\u9009\u8bc1\u636e\uff0c\u5e76\u901a\u8fc7LLM\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u526a\u679d\uff0c\u6700\u540e\u4f18\u5316LLM\u63a8\u7406\u7684\u4e0a\u4e0b\u6587\u7ec4\u7ec7\u3002", "result": "\u5728\u4e09\u4e2a\u77e5\u540d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProgRAG\u5728\u591a\u8df3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u9760\u6027\u548c\u63a8\u7406\u8d28\u91cf\u3002", "conclusion": "ProgRAG\u901a\u8fc7\u6e10\u8fdb\u5f0f\u63a8\u7406\u8def\u5f84\u6269\u5c55\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bc1\u636e\u526a\u679d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLM\u4e2d\u7684\u68c0\u7d22\u548c\u63a8\u7406\u5931\u8d25\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u6027\u80fd\u3002"}}
{"id": "2511.09855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09855", "abs": "https://arxiv.org/abs/2511.09855", "authors": ["James Jin Kang", "Dang Bui", "Thanh Pham", "Huo-Chong Ling"], "title": "Unlearning Imperative: Securing Trustworthy and Responsible LLMs through Engineered Forgetting", "comment": "14 pages, 4 figures, 4 tables", "summary": "The growing use of large language models in sensitive domains has exposed a critical weakness: the inability to ensure that private information can be permanently forgotten. Yet these systems still lack reliable mechanisms to guarantee that sensitive information can be permanently removed once it has been used. Retraining from the beginning is prohibitively costly, and existing unlearning methods remain fragmented, difficult to verify, and often vulnerable to recovery. This paper surveys recent research on machine unlearning for LLMs and considers how far current approaches can address these challenges. We review methods for evaluating whether forgetting has occurred, the resilience of unlearned models against adversarial attacks, and mechanisms that can support user trust when model complexity or proprietary limits restrict transparency. Technical solutions such as differential privacy, homomorphic encryption, federated learning, and ephemeral memory are examined alongside institutional safeguards including auditing practices and regulatory frameworks. The review finds steady progress, but robust and verifiable unlearning is still unresolved. Efficient techniques that avoid costly retraining, stronger defenses against adversarial recovery, and governance structures that reinforce accountability are needed if LLMs are to be deployed safely in sensitive applications. By integrating technical and organizational perspectives, this study outlines a pathway toward AI systems that can be required to forget, while maintaining both privacy and public trust.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u673a\u5668\u9057\u5fd8\u7814\u7a76\uff0c\u5206\u6790\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u6280\u672f\u548c\u5236\u5ea6\u5c42\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u79c1\u4eba\u4fe1\u606f\u80fd\u591f\u88ab\u6c38\u4e45\u9057\u5fd8\u7684\u80fd\u529b\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u53ef\u9760\u7684\u673a\u5236\u6765\u4fdd\u8bc1\u654f\u611f\u4fe1\u606f\u5728\u4f7f\u7528\u540e\u80fd\u591f\u88ab\u6c38\u4e45\u5220\u9664\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86LLMs\u673a\u5668\u9057\u5fd8\u7684\u6700\u65b0\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u9057\u5fd8\u662f\u5426\u53d1\u751f\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3001\u88ab\u9057\u5fd8\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u97e7\u6027\uff0c\u4ee5\u53ca\u5728\u6a21\u578b\u590d\u6742\u6027\u6216\u4e13\u6709\u9650\u5236\u4e0b\u652f\u6301\u7528\u6237\u4fe1\u4efb\u7684\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6280\u672f\u89e3\u51b3\u65b9\u6848\u5982\u5dee\u5206\u9690\u79c1\u3001\u540c\u6001\u52a0\u5bc6\u3001\u8054\u90a6\u5b66\u4e60\u548c\u77ed\u6682\u8bb0\u5fc6\u7b49\u4e0e\u5236\u5ea6\u4fdd\u969c\u5305\u62ec\u5ba1\u8ba1\u5b9e\u8df5\u548c\u76d1\u7ba1\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u53d6\u5f97\u4e86\u7a33\u6b65\u8fdb\u5c55\uff0c\u4f46\u7a33\u5065\u4e14\u53ef\u9a8c\u8bc1\u7684\u9057\u5fd8\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "conclusion": "\u5982\u679c\u8981\u5728\u654f\u611f\u5e94\u7528\u4e2d\u5b89\u5168\u90e8\u7f72LLMs\uff0c\u9700\u8981\u907f\u514d\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u6280\u672f\u3001\u66f4\u5f3a\u7684\u5bf9\u6297\u6062\u590d\u9632\u5fa1\u80fd\u529b\uff0c\u4ee5\u53ca\u52a0\u5f3a\u95ee\u8d23\u5236\u7684\u6cbb\u7406\u7ed3\u6784\u3002\u901a\u8fc7\u6574\u5408\u6280\u672f\u548c\u7ec4\u7ec7\u89c6\u89d2\uff0c\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u80fd\u591f\u6309\u8981\u6c42\u9057\u5fd8\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u548c\u516c\u4f17\u4fe1\u4efb\u7684AI\u7cfb\u7edf\u6307\u660e\u4e86\u8def\u5f84\u3002"}}
{"id": "2511.10272", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10272", "abs": "https://arxiv.org/abs/2511.10272", "authors": ["Shahaf S. Shperberg", "Natalie Morad", "Lior Siag", "Ariel Felner", "Dor Atzmon"], "title": "Bidirectional Bounded-Suboptimal Heuristic Search with Consistent Heuristics", "comment": null, "summary": "Recent advancements in bidirectional heuristic search have yielded significant theoretical insights and novel algorithms. While most previous work has concentrated on optimal search methods, this paper focuses on bounded-suboptimal bidirectional search, where a bound on the suboptimality of the solution cost is specified. We build upon the state-of-the-art optimal bidirectional search algorithm, BAE*, designed for consistent heuristics, and introduce several variants of BAE* specifically tailored for the bounded-suboptimal context. Through experimental evaluation, we compare the performance of these new variants against other bounded-suboptimal bidirectional algorithms as well as the standard weighted A* algorithm. Our results demonstrate that each algorithm excels under distinct conditions, highlighting the strengths and weaknesses of each approach.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u6700\u4f18\u53cc\u5411\u641c\u7d22\u7b97\u6cd5BAE*\uff0c\u5f00\u53d1\u4e86\u591a\u4e2a\u6709\u754c\u6b21\u4f18\u53cc\u5411\u641c\u7d22\u53d8\u4f53\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4e0e\u73b0\u6709\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u4e0d\u540c\u7b97\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u53cc\u5411\u542f\u53d1\u5f0f\u641c\u7d22\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6700\u4f18\u641c\u7d22\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6709\u754c\u6b21\u4f18\u53cc\u5411\u641c\u7d22\uff0c\u5373\u5728\u6307\u5b9a\u89e3\u6210\u672c\u6b21\u4f18\u6027\u8fb9\u754c\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u641c\u7d22\u3002", "method": "\u57fa\u4e8e\u6700\u4f18\u53cc\u5411\u641c\u7d22\u7b97\u6cd5BAE*\uff08\u9002\u7528\u4e8e\u4e00\u81f4\u542f\u53d1\u5f0f\uff09\uff0c\u5f00\u53d1\u4e86\u591a\u4e2a\u4e13\u95e8\u9488\u5bf9\u6709\u754c\u6b21\u4f18\u573a\u666f\u7684BAE*\u53d8\u4f53\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u65b0\u7b97\u6cd5\u4e0e\u5176\u4ed6\u6709\u754c\u6b21\u4f18\u53cc\u5411\u7b97\u6cd5\u4ee5\u53ca\u6807\u51c6\u52a0\u6743A*\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u79cd\u7b97\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u7a81\u663e\u4e86\u5404\u81ea\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u4e0d\u540c\u6709\u754c\u6b21\u4f18\u53cc\u5411\u641c\u7d22\u7b97\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5177\u6709\u5404\u81ea\u7684\u4f18\u52bf\uff0c\u6ca1\u6709\u5355\u4e00\u7b97\u6cd5\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2511.10281", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10281", "abs": "https://arxiv.org/abs/2511.10281", "authors": ["Jing He", "Han Zhang", "Yuanhui Xiao", "Wei Guo", "Shaowen Yao", "Renyang Liu"], "title": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection", "comment": "Accepted by AAAI 2026", "summary": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.", "AI": {"tldr": "FactGuard\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e8b\u4ef6\u4e2d\u5fc3\u5185\u5bb9\u6765\u51cf\u5c11\u5199\u4f5c\u98ce\u683c\u5f71\u54cd\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u7528\u6027\u673a\u5236\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u653b\u51fb\u8005\u6a21\u4eff\u771f\u5b9e\u65b0\u95fb\u98ce\u683c\uff0c\u57fa\u4e8e\u5199\u4f5c\u98ce\u683c\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u6548\u679c\u9010\u6e10\u4e0b\u964d\uff0c\u800c\u73b0\u6709LLM\u65b9\u6cd5\u5b58\u5728\u529f\u80fd\u63a2\u7d22\u6d45\u3001\u53ef\u7528\u6027\u6a21\u7cca\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faFactGuard\u6846\u67b6\uff1a1\uff09\u5229\u7528LLM\u63d0\u53d6\u4e8b\u4ef6\u4e2d\u5fc3\u5185\u5bb9\u51cf\u5c11\u98ce\u683c\u5f71\u54cd\uff1b2\uff09\u5f15\u5165\u52a8\u6001\u53ef\u7528\u6027\u673a\u5236\u8bc6\u522b\u77db\u76fe\u6848\u4f8b\uff1b3\uff09\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5f97\u5230FactGuard-D\u7528\u4e8e\u51b7\u542f\u52a8\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u683c\u654f\u611f\u6027\u548cLLM\u53ef\u7528\u6027\u95ee\u9898\u3002", "conclusion": "FactGuard\u6846\u67b6\u901a\u8fc7\u4e8b\u4ef6\u5185\u5bb9\u63d0\u53d6\u548c\u52a8\u6001\u53ef\u7528\u6027\u673a\u5236\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u786e\u4fdd\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.10284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10284", "abs": "https://arxiv.org/abs/2511.10284", "authors": ["Belona Sonna", "Alban Grastien", "Claire Benn"], "title": "Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage", "comment": "Accepted at the Workshop on Post-AI Formal Methods at AAAI-26", "summary": "Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6eaf\u56e0\u89e3\u91ca\u7684\u9690\u79c1\u6cc4\u9732\u5ba1\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u6a21\u578b\u51b3\u7b56\u7684\u6700\u5c0f\u5145\u5206\u8bc1\u636e\u6765\u68c0\u6d4b\u654f\u611f\u4fe1\u606f\u6cc4\u9732\uff0c\u5728\u5fb7\u56fd\u4fe1\u7528\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "AI\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u80fd\u591f\u68c0\u6d4b\u654f\u611f\u4fe1\u606f\u63a8\u65ad\u7684\u5ba1\u8ba1\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u91ca\u7684\u53ef\u7406\u89e3\u6027\u3002", "method": "\u4f7f\u7528\u6eaf\u56e0\u89e3\u91ca\u6846\u67b6\uff0c\u5b9a\u4e49\u4e2a\u4f53\u548c\u7cfb\u7edf\u7ea7\u9690\u79c1\u6cc4\u9732\uff0c\u5f15\u5165\u6f5c\u5728\u9002\u7528\u89e3\u91ca(PAE)\u6982\u5ff5\u6765\u8bc6\u522b\u80fd\u591f\u4fdd\u62a4\u654f\u611f\u7279\u5f81\u4e2a\u4f53\u7684\u7ed3\u679c\u3002", "result": "\u5728\u5fb7\u56fd\u4fe1\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u654f\u611f\u6587\u5b57\u5728\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u91cd\u8981\u6027\u4f1a\u5f71\u54cd\u9690\u79c1\u6cc4\u9732\u7a0b\u5ea6\uff0c\u6eaf\u56e0\u63a8\u7406\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u9690\u79c1\u5ba1\u8ba1\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u8ba1\u7b97\u6311\u6218\u548c\u7b80\u5316\u5047\u8bbe\uff0c\u8be5\u65b9\u6cd5\u4e3a\u534f\u8c03AI\u51b3\u7b56\u4e2d\u7684\u900f\u660e\u5ea6\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2511.09917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09917", "abs": "https://arxiv.org/abs/2511.09917", "authors": ["Jiazhen Chen", "Xiuqin Liang", "Sichao Fu", "Zheng Ma", "Weihua Ou"], "title": "Towards Multiple Missing Values-resistant Unsupervised Graph Anomaly Detection", "comment": "Accepted by 40th AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Unsupervised graph anomaly detection (GAD) has received increasing attention in recent years, which aims to identify data anomalous patterns utilizing only unlabeled node information from graph-structured data. However, prevailing unsupervised GAD methods typically presuppose complete node attributes and structure information, a condition hardly satisfied in real-world scenarios owing to privacy, collection errors or dynamic node arrivals. Existing standard imputation schemes risk \"repairing\" rare anomalous nodes so that they appear normal, thereby introducing imputation bias into the detection process. In addition, when both node attributes and edges are missing simultaneously, estimation errors in one view can contaminate the other, causing cross-view interference that further undermines the detection performance. To overcome these challenges, we propose M$^2$V-UGAD, a multiple missing values-resistant unsupervised GAD framework on incomplete graphs. Specifically, a dual-pathway encoder is first proposed to independently reconstruct missing node attributes and graph structure, thereby preventing errors in one view from propagating to the other. The two pathways are then fused and regularized in a joint latent space so that normals occupy a compact inner manifold while anomalies reside on an outer shell. Lastly, to mitigate imputation bias, we sample latent codes just outside the normal region and decode them into realistic node features and subgraphs, providing hard negative examples that sharpen the decision boundary. Experiments on seven public benchmarks demonstrate that M$^2$V-UGAD consistently outperforms existing unsupervised GAD methods across varying missing rates.", "AI": {"tldr": "M\u00b2V-UGAD\u662f\u4e00\u4e2a\u9488\u5bf9\u4e0d\u5b8c\u6574\u56fe\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u8282\u70b9\u5c5e\u6027\u548c\u56fe\u7ed3\u6784\u540c\u65f6\u7f3a\u5931\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u7f16\u7801\u5668\u72ec\u7acb\u91cd\u5efa\u7f3a\u5931\u4fe1\u606f\uff0c\u907f\u514d\u4ea4\u53c9\u89c6\u56fe\u5e72\u6270\uff0c\u5e76\u5229\u7528\u786c\u8d1f\u4f8b\u91c7\u6837\u7f13\u89e3\u63d2\u8865\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8282\u70b9\u5c5e\u6027\u548c\u7ed3\u6784\u4fe1\u606f\u5b8c\u6574\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u6570\u636e\u5f80\u5f80\u5b58\u5728\u7f3a\u5931\u3002\u6807\u51c6\u63d2\u8865\u65b9\u6cd5\u53ef\u80fd\u4fee\u590d\u5f02\u5e38\u8282\u70b9\u4f7f\u5176\u770b\u8d77\u6765\u6b63\u5e38\uff0c\u5f15\u5165\u63d2\u8865\u504f\u5dee\uff0c\u4e14\u8282\u70b9\u5c5e\u6027\u548c\u8fb9\u540c\u65f6\u7f3a\u5931\u65f6\u4f1a\u4ea7\u751f\u4ea4\u53c9\u89c6\u56fe\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u53cc\u8def\u5f84\u7f16\u7801\u5668\u72ec\u7acb\u91cd\u5efa\u7f3a\u5931\u8282\u70b9\u5c5e\u6027\u548c\u56fe\u7ed3\u6784\uff1b\u5728\u8054\u5408\u6f5c\u5728\u7a7a\u95f4\u4e2d\u878d\u5408\u548c\u6b63\u5219\u5316\u4e24\u4e2a\u8def\u5f84\uff0c\u4f7f\u6b63\u5e38\u8282\u70b9\u5360\u636e\u7d27\u51d1\u5185\u6d41\u5f62\u800c\u5f02\u5e38\u4f4d\u4e8e\u5916\u58f3\uff1b\u91c7\u6837\u6f5c\u5728\u7a7a\u95f4\u6b63\u5e38\u533a\u57df\u5916\u7684\u4ee3\u7801\u89e3\u7801\u4e3a\u771f\u5b9e\u8282\u70b9\u7279\u5f81\u548c\u5b50\u56fe\uff0c\u63d0\u4f9b\u786c\u8d1f\u4f8b\u4ee5\u9510\u5316\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM\u00b2V-UGAD\u5728\u4e0d\u540c\u7f3a\u5931\u7387\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "M\u00b2V-UGAD\u901a\u8fc7\u72ec\u7acb\u91cd\u5efa\u7f3a\u5931\u4fe1\u606f\u3001\u6f5c\u5728\u7a7a\u95f4\u6b63\u5219\u5316\u548c\u786c\u8d1f\u4f8b\u91c7\u6837\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u56fe\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.09924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09924", "abs": "https://arxiv.org/abs/2511.09924", "authors": ["Hu Zhang", "Zhien Dai", "Zhaohui Tang", "Yongfang Xie"], "title": "MDMLP-EIA: Multi-domain Dynamic MLPs with Energy Invariant Attention for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is essential across diverse domains. While MLP-based methods have gained attention for achieving Transformer-comparable performance with fewer parameters and better robustness, they face critical limitations including loss of weak seasonal signals, capacity constraints in weight-sharing MLPs, and insufficient channel fusion in channel-independent strategies. To address these challenges, we propose MDMLP-EIA (Multi-domain Dynamic MLPs with Energy Invariant Attention) with three key innovations. First, we develop an adaptive fused dual-domain seasonal MLP that categorizes seasonal signals into strong and weak components. It employs an adaptive zero-initialized channel fusion strategy to minimize noise interference while effectively integrating predictions. Second, we introduce an energy invariant attention mechanism that adaptively focuses on different feature channels within trend and seasonal predictions across time steps. This mechanism maintains constant total signal energy to align with the decomposition-prediction-reconstruction framework and enhance robustness against disturbances. Third, we propose a dynamic capacity adjustment mechanism for channel-independent MLPs. This mechanism scales neuron count with the square root of channel count, ensuring sufficient capacity as channels increase. Extensive experiments across nine benchmark datasets demonstrate that MDMLP-EIA achieves state-of-the-art performance in both prediction accuracy and computational efficiency.", "AI": {"tldr": "MDMLP-EIA\u662f\u4e00\u79cd\u591a\u57df\u52a8\u6001MLP\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u53cc\u57df\u5b63\u8282\u6027MLP\u3001\u80fd\u91cf\u4e0d\u53d8\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u5bb9\u91cf\u8c03\u6574\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMLP\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4e22\u5931\u5f31\u5b63\u8282\u6027\u4fe1\u53f7\u3001\u6743\u91cd\u5171\u4eabMLP\u5bb9\u91cf\u9650\u5236\u548c\u901a\u9053\u878d\u5408\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "MLP\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u867d\u7136\u53c2\u6570\u5c11\u3001\u9c81\u68d2\u6027\u597d\uff0c\u4f46\u5b58\u5728\u5f31\u5b63\u8282\u6027\u4fe1\u53f7\u4e22\u5931\u3001\u6743\u91cd\u5171\u4eabMLP\u5bb9\u91cf\u9650\u5236\u548c\u901a\u9053\u72ec\u7acb\u7b56\u7565\u4e2d\u901a\u9053\u878d\u5408\u4e0d\u8db3\u7b49\u5173\u952e\u9650\u5236\u3002", "method": "1. \u81ea\u9002\u5e94\u878d\u5408\u53cc\u57df\u5b63\u8282\u6027MLP\uff1a\u5c06\u5b63\u8282\u6027\u4fe1\u53f7\u5206\u4e3a\u5f3a\u5f31\u5206\u91cf\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u96f6\u521d\u59cb\u5316\u901a\u9053\u878d\u5408\u7b56\u7565\uff1b2. \u80fd\u91cf\u4e0d\u53d8\u6ce8\u610f\u529b\u673a\u5236\uff1a\u81ea\u9002\u5e94\u5173\u6ce8\u4e0d\u540c\u7279\u5f81\u901a\u9053\uff0c\u4fdd\u6301\u603b\u4fe1\u53f7\u80fd\u91cf\u6052\u5b9a\uff1b3. \u52a8\u6001\u5bb9\u91cf\u8c03\u6574\u673a\u5236\uff1a\u6839\u636e\u901a\u9053\u6570\u5e73\u65b9\u6839\u8c03\u6574\u795e\u7ecf\u5143\u6570\u91cf\u3002", "result": "\u5728\u4e5d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMDMLP-EIA\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MDMLP-EIA\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u6709\u6548\u89e3\u51b3\u4e86MLP\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.09953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09953", "abs": "https://arxiv.org/abs/2511.09953", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "En Yu", "Guangquan Zhang"], "title": "Autonomous Concept Drift Threshold Determination", "comment": "Accepted By AAAI 2026", "summary": "Existing drift detection methods focus on designing sensitive test statistics. They treat the detection threshold as a fixed hyperparameter, set once to balance false alarms and late detections, and applied uniformly across all datasets and over time. However, maintaining model performance is the key objective from the perspective of machine learning, and we observe that model performance is highly sensitive to this threshold. This observation inspires us to investigate whether a dynamic threshold could be provably better. In this paper, we prove that a threshold that adapts over time can outperform any single fixed threshold. The main idea of the proof is that a dynamic strategy, constructed by combining the best threshold from each individual data segment, is guaranteed to outperform any single threshold that apply to all segments. Based on the theorem, we propose a Dynamic Threshold Determination algorithm. It enhances existing drift detection frameworks with a novel comparison phase to inform how the threshold should be adjusted. Extensive experiments on a wide range of synthetic and real-world datasets, including both image and tabular data, validate that our approach substantially enhances the performance of state-of-the-art drift detectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9608\u503c\u786e\u5b9a\u7b97\u6cd5\uff0c\u8bc1\u660e\u52a8\u6001\u8c03\u6574\u7684\u68c0\u6d4b\u9608\u503c\u4f18\u4e8e\u56fa\u5b9a\u9608\u503c\uff0c\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709\u6f02\u79fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u5c06\u68c0\u6d4b\u9608\u503c\u8bbe\u4e3a\u56fa\u5b9a\u8d85\u53c2\u6570\uff0c\u4f46\u6a21\u578b\u6027\u80fd\u5bf9\u6b64\u9608\u503c\u9ad8\u5ea6\u654f\u611f\u3002\u4ece\u673a\u5668\u5b66\u4e60\u89d2\u5ea6\u770b\uff0c\u7ef4\u6301\u6a21\u578b\u6027\u80fd\u662f\u5173\u952e\u76ee\u6807\uff0c\u56e0\u6b64\u7814\u7a76\u52a8\u6001\u9608\u503c\u662f\u5426\u80fd\u8bc1\u660e\u66f4\u4f18\u3002", "method": "\u57fa\u4e8e\u7406\u8bba\u8bc1\u660e\uff0c\u63d0\u51fa\u52a8\u6001\u9608\u503c\u786e\u5b9a\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6bd4\u8f83\u9636\u6bb5\u589e\u5f3a\u73b0\u6709\u6f02\u79fb\u68c0\u6d4b\u6846\u67b6\uff0c\u6307\u5bfc\u9608\u503c\u5e94\u5982\u4f55\u8c03\u6574\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5305\u62ec\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u6f02\u79fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u52a8\u6001\u9608\u503c\u7b56\u7565\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u8df5\u4e0a\u90fd\u4f18\u4e8e\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5e73\u8861\u8bef\u62a5\u548c\u5ef6\u8fdf\u68c0\u6d4b\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7ef4\u62a4\u80fd\u529b\u3002"}}
{"id": "2511.09979", "categories": ["cs.LG", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.09979", "abs": "https://arxiv.org/abs/2511.09979", "authors": ["Saumya Shah", "Zi-Yu Khoo", "Abel Yang", "St\u00e9phane Bressan"], "title": "Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases", "comment": "7 pages, 1 figure, 4 tables", "summary": "This work explores using the physics-inspired AI Feynman symbolic regression algorithm to automatically rediscover a fundamental equation in astronomy -- the Equation of the Centre. Through the introduction of observational and inductive biases corresponding to the physical nature of the system through data preprocessing and search space restriction, AI Feynman was successful in recovering the first-order analytical form of this equation from lunar ephemerides data. However, this manual approach highlights a key limitation in its reliance on expert-driven coordinate system selection. We therefore propose an automated preprocessing extension to find the canonical coordinate system. Results demonstrate that targeted domain knowledge embedding enables symbolic regression to rediscover physical laws, but also highlight further challenges in constraining symbolic regression to derive physics equations when leveraging domain knowledge through tailored biases.", "AI": {"tldr": "\u4f7f\u7528AI Feynman\u7b26\u53f7\u56de\u5f52\u7b97\u6cd5\u81ea\u52a8\u91cd\u65b0\u53d1\u73b0\u5929\u6587\u5b66\u4e2d\u7684\u4e2d\u5fc3\u65b9\u7a0b\uff0c\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u548c\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u5f15\u5165\u7269\u7406\u504f\u5dee\uff0c\u6210\u529f\u6062\u590d\u8be5\u65b9\u7a0b\u7684\u4e00\u9636\u89e3\u6790\u5f62\u5f0f\uff0c\u4f46\u4f9d\u8d56\u4e13\u5bb6\u9a71\u52a8\u7684\u5750\u6807\u7cfb\u9009\u62e9\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684AI\u7b97\u6cd5\u81ea\u52a8\u91cd\u65b0\u53d1\u73b0\u5929\u6587\u5b66\u4e2d\u7684\u57fa\u672c\u65b9\u7a0b\uff0c\u7279\u522b\u662f\u4e2d\u5fc3\u65b9\u7a0b\uff0c\u4ee5\u9a8c\u8bc1\u7b26\u53f7\u56de\u5f52\u5728\u7269\u7406\u5b9a\u5f8b\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528AI Feynman\u7b26\u53f7\u56de\u5f52\u7b97\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u548c\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u5f15\u5165\u89c2\u6d4b\u548c\u5f52\u7eb3\u504f\u5dee\uff0c\u5bf9\u5e94\u7cfb\u7edf\u7684\u7269\u7406\u6027\u8d28\uff0c\u4ece\u6708\u7403\u661f\u5386\u6570\u636e\u4e2d\u6062\u590d\u4e2d\u5fc3\u65b9\u7a0b\u3002", "result": "\u6210\u529f\u6062\u590d\u4e86\u4e2d\u5fc3\u65b9\u7a0b\u7684\u4e00\u9636\u89e3\u6790\u5f62\u5f0f\uff0c\u4f46\u53d1\u73b0\u4f9d\u8d56\u4e13\u5bb6\u9a71\u52a8\u7684\u5750\u6807\u7cfb\u9009\u62e9\u662f\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u81ea\u52a8\u9884\u5904\u7406\u6269\u5c55\u6765\u5bfb\u627e\u89c4\u8303\u5750\u6807\u7cfb\u3002", "conclusion": "\u5b9a\u5411\u9886\u57df\u77e5\u8bc6\u5d4c\u5165\u4f7f\u7b26\u53f7\u56de\u5f52\u80fd\u591f\u91cd\u65b0\u53d1\u73b0\u7269\u7406\u5b9a\u5f8b\uff0c\u4f46\u5728\u5229\u7528\u9886\u57df\u77e5\u8bc6\u901a\u8fc7\u5b9a\u5236\u504f\u5dee\u63a8\u5bfc\u7269\u7406\u65b9\u7a0b\u65f6\u4ecd\u9762\u4e34\u8fdb\u4e00\u6b65\u6311\u6218\u3002"}}
{"id": "2511.10031", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10031", "abs": "https://arxiv.org/abs/2511.10031", "authors": ["Ruichu Cai", "Xiaokai Huang", "Wei Chen", "Zijian Li", "Zhifeng Hao"], "title": "Temporal Latent Variable Structural Causal Model for Causal Discovery under External Interferences", "comment": "Accepted by Neurocomputing", "summary": "Inferring causal relationships from observed data is an important task, yet it becomes challenging when the data is subject to various external interferences. Most of these interferences are the additional effects of external factors on observed variables. Since these external factors are often unknown, we introduce latent variables to represent these unobserved factors that affect the observed data. Specifically, to capture the causal strength and adjacency information, we propose a new temporal latent variable structural causal model, incorporating causal strength and adjacency coefficients that represent the causal relationships between variables. Considering that expert knowledge can provide information about unknown interferences in certain scenarios, we develop a method that facilitates the incorporation of prior knowledge into parameter learning based on Variational Inference, to guide the model estimation. Experimental results demonstrate the stability and accuracy of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u6001\u9690\u53d8\u91cf\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u9690\u53d8\u91cf\u8868\u793a\u672a\u89c2\u6d4b\u7684\u5916\u90e8\u5e72\u6270\u56e0\u7d20\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u6765\u6307\u5bfc\u53c2\u6570\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u56e0\u679c\u63a8\u65ad\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4ece\u89c2\u6d4b\u6570\u636e\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u662f\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u5f53\u6570\u636e\u53d7\u5230\u5404\u79cd\u5916\u90e8\u5e72\u6270\u65f6\u53d8\u5f97\u56f0\u96be\u3002\u8fd9\u4e9b\u5e72\u6270\u901a\u5e38\u662f\u5916\u90e8\u56e0\u7d20\u5bf9\u89c2\u6d4b\u53d8\u91cf\u7684\u989d\u5916\u5f71\u54cd\uff0c\u800c\u5916\u90e8\u56e0\u7d20\u5f80\u5f80\u672a\u77e5\u3002", "method": "\u63d0\u51fa\u65f6\u6001\u9690\u53d8\u91cf\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u5f15\u5165\u56e0\u679c\u5f3a\u5ea6\u548c\u90bb\u63a5\u7cfb\u6570\u8868\u793a\u53d8\u91cf\u95f4\u56e0\u679c\u5173\u7cfb\u3002\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u5f00\u53d1\u53c2\u6570\u5b66\u4e60\u65b9\u6cd5\uff0c\u652f\u6301\u878d\u5165\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\u6765\u6307\u5bfc\u6a21\u578b\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u9690\u53d8\u91cf\u8868\u793a\u672a\u89c2\u6d4b\u5916\u90e8\u56e0\u7d20\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u53d7\u5e72\u6270\u6570\u636e\u7684\u56e0\u679c\u63a8\u65ad\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u65ad\u6027\u80fd\u3002"}}
{"id": "2511.10130", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10130", "abs": "https://arxiv.org/abs/2511.10130", "authors": ["Jieting Wang", "Xiaolei Shang", "Feijiang Li", "Furong Peng"], "title": "RI-Loss: A Learnable Residual-Informed Loss for Time Series Forecasting", "comment": null, "summary": "Time series forecasting relies on predicting future values from historical data, yet most state-of-the-art approaches-including transformer and multilayer perceptron-based models-optimize using Mean Squared Error (MSE), which has two fundamental weaknesses: its point-wise error computation fails to capture temporal relationships, and it does not account for inherent noise in the data. To overcome these limitations, we introduce the Residual-Informed Loss (RI-Loss), a novel objective function based on the Hilbert-Schmidt Independence Criterion (HSIC). RI-Loss explicitly models noise structure by enforcing dependence between the residual sequence and a random time series, enabling more robust, noise-aware representations. Theoretically, we derive the first non-asymptotic HSIC bound with explicit double-sample complexity terms, achieving optimal convergence rates through Bernstein-type concentration inequalities and Rademacher complexity analysis. This provides rigorous guarantees for RI-Loss optimization while precisely quantifying kernel space interactions. Empirically, experiments across eight real-world benchmarks and five leading forecasting models demonstrate improvements in predictive performance, validating the effectiveness of our approach. Code will be made publicly available to ensure reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e0c\u5c14\u4f2f\u7279-\u65bd\u5bc6\u7279\u72ec\u7acb\u6027\u51c6\u5219\u7684\u6b8b\u5dee\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u566a\u58f0\u7ed3\u6784\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMSE\u635f\u5931\u7684\u4e24\u4e2a\u6839\u672c\u5f31\u70b9\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u5f31\u70b9\uff1a\u9010\u70b9\u8bef\u5dee\u8ba1\u7b97\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u5173\u7cfb\uff0c\u4e14\u672a\u8003\u8651\u6570\u636e\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u3002", "method": "\u5f15\u5165\u6b8b\u5dee\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u57fa\u4e8e\u5e0c\u5c14\u4f2f\u7279-\u65bd\u5bc6\u7279\u72ec\u7acb\u6027\u51c6\u5219\uff0c\u901a\u8fc7\u5f3a\u5236\u6b8b\u5dee\u5e8f\u5217\u4e0e\u968f\u673a\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u7684\u4f9d\u8d56\u6027\u6765\u663e\u5f0f\u5efa\u6a21\u566a\u58f0\u7ed3\u6784\u3002", "result": "\u5728\u516b\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e94\u4e2a\u9886\u5148\u9884\u6d4b\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "RI-Loss\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u4f18\u5316\u4fdd\u8bc1\uff0c\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.10200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10200", "abs": "https://arxiv.org/abs/2511.10200", "authors": ["Jieting Wang", "Huimei Shi", "Feijiang Li", "Xiaolei Shang"], "title": "Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting", "comment": null, "summary": "Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis using influence functions, we establish that cross-entropy (CE) loss exhibits superior stability and outlier robustness compared to MSE loss. Empirically, we compared OCE-TS with five baseline models-Autoformer, DLinear, iTransformer, TimeXer, and TimeBridge-on seven public time series datasets. Using MSE and Mean Absolute Error (MAE) as evaluation metrics, the results demonstrate that OCE-TS consistently outperforms benchmark models. The code will be published.", "AI": {"tldr": "OCE-TS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e8f\u6570\u5206\u7c7b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u5e8f\u6570\u4ea4\u53c9\u71b5\u635f\u5931\u66ff\u4ee3MSE\u635f\u5931\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u987a\u5e8f\u7684\u540c\u65f6\u901a\u8fc7\u6982\u7387\u8f93\u51fa\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u4f7f\u7528MSE\u635f\u5931\u51fd\u6570\u8fdb\u884c\u56de\u5f52\u5efa\u6a21\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e14\u5bf9\u5f02\u5e38\u503c\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u5c06\u89c2\u6d4b\u503c\u79bb\u6563\u5316\u4e3a\u6709\u5e8f\u533a\u95f4\uff0c\u901a\u8fc7\u53c2\u6570\u5206\u5e03\u63a8\u5bfc\u6982\u7387\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff1b\u4f7f\u7528\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u6982\u7387\u5206\u5e03\uff1b\u8ba1\u7b97\u9884\u6d4b\u548c\u771f\u5b9e\u6982\u7387\u7684\u7d2f\u79ef\u5206\u5e03\u4e4b\u95f4\u7684OCE\u635f\u5931\uff0c\u660e\u786e\u4fdd\u6301\u9884\u6d4b\u503c\u4e4b\u95f4\u7684\u5e8f\u6570\u5173\u7cfb\u3002", "result": "\u57287\u4e2a\u516c\u5171\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u4e0eAutoformer\u3001DLinear\u3001iTransformer\u3001TimeXer\u548cTimeBridge\u7b495\u4e2a\u57fa\u51c6\u6a21\u578b\u6bd4\u8f83\uff0c\u4f7f\u7528MSE\u548cMAE\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0cOCE-TS\u59cb\u7ec8\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5f71\u54cd\u51fd\u6570\u7684\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u4ea4\u53c9\u71b5\u635f\u5931\u76f8\u6bd4MSE\u635f\u5931\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u5f02\u5e38\u503c\u9c81\u68d2\u6027\uff1bOCE-TS\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002"}}
{"id": "2511.10208", "categories": ["cs.LG", "cs.AI", "math.DS", "math.PR", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2511.10208", "abs": "https://arxiv.org/abs/2511.10208", "authors": ["Cheng Kevin Qu", "Andrew Ly", "Pulin Gong"], "title": "Fractional neural attention for efficient multiscale sequence processing", "comment": null, "summary": "Attention mechanisms underpin the computational power of Transformer models, which have achieved remarkable success across diverse domains. Yet understanding and extending the principles underlying self-attention remains a key challenge for advancing artificial intelligence. Drawing inspiration from the multiscale dynamics of biological attention and from dynamical systems theory, we introduce Fractional Neural Attention (FNA), a principled, neuroscience-inspired framework for multiscale information processing. FNA models token interactions through L\u00e9vy diffusion governed by the fractional Laplacian, intrinsically realizing simultaneous short- and long-range dependencies across multiple scales. This mechanism yields greater expressivity and faster information mixing, advancing the foundational capacity of Transformers. Theoretically, we show that FNA's dynamics are governed by the fractional diffusion equation, and that the resulting attention networks exhibit larger spectral gaps and shorter path lengths -- mechanistic signatures of enhanced computational efficiency. Empirically, FNA achieves competitive text-classification performance even with a single layer and a single head; it also improves performance in image processing and neural machine translation. Finally, the diffusion map algorithm from geometric harmonics enables dimensionality reduction of FNA weights while preserving the intrinsic structure of embeddings and hidden states. Together, these results establish FNA as a principled mechanism connecting self-attention, stochastic dynamics, and geometry, providing an interpretable, biologically grounded foundation for powerful, neuroscience-inspired AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u6570\u795e\u7ecf\u6ce8\u610f\u529b\uff08FNA\uff09\u6846\u67b6\uff0c\u901a\u8fc7L\u00e9vy\u6269\u6563\u548c\u5206\u6570\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5efa\u6a21token\u4ea4\u4e92\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u4fe1\u606f\u5904\u7406\uff0c\u63d0\u5347Transformer\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u53d7\u751f\u7269\u6ce8\u610f\u529b\u591a\u5c3a\u5ea6\u52a8\u6001\u548c\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u7406\u89e3\u548c\u6269\u5c55\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u539f\u7406\uff0c\u4e3aAI\u63d0\u4f9b\u795e\u7ecf\u79d1\u5b66\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u5206\u6570\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u63a7\u5236\u7684L\u00e9vy\u6269\u6563\u5efa\u6a21token\u4ea4\u4e92\uff0c\u5b9e\u73b0\u540c\u65f6\u7684\u77ed\u7a0b\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u91c7\u7528\u6269\u6563\u6620\u5c04\u7b97\u6cd5\u8fdb\u884c\u964d\u7ef4\u3002", "result": "FNA\u5728\u6587\u672c\u5206\u7c7b\u3001\u56fe\u50cf\u5904\u7406\u548c\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5355\u5c42\u5355\u5934\u5373\u53ef\u83b7\u5f97\u7ade\u4e89\u529b\uff0c\u5177\u6709\u66f4\u5927\u7684\u8c31\u95f4\u9699\u548c\u66f4\u77ed\u7684\u8def\u5f84\u957f\u5ea6\u3002", "conclusion": "FNA\u5efa\u7acb\u4e86\u81ea\u6ce8\u610f\u529b\u3001\u968f\u673a\u52a8\u6001\u548c\u51e0\u4f55\u4e4b\u95f4\u7684\u539f\u5219\u6027\u8054\u7cfb\uff0c\u4e3a\u5f3a\u5927\u7684\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684AI\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u751f\u7269\u5b66\u57fa\u7840\u3002"}}
{"id": "2511.10333", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.10333", "abs": "https://arxiv.org/abs/2511.10333", "authors": ["Qingao Yi", "Jiaang Duan", "Hanwen Hu", "Qin Hua", "Haiyan Zhao", "Shiyou Qian", "Dingyu Yang", "Jian Cao", "Jinghua Tang", "Yinghao Yu", "Chenzhi Liao", "Kangjin Wang", "Liping Zhang"], "title": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training", "comment": null, "summary": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.", "AI": {"tldr": "\u63d0\u51faEDGC\u6846\u67b6\uff0c\u57fa\u4e8e\u68af\u5ea6\u71b5\u52a8\u6001\u8c03\u6574\u538b\u7f29\u7387\uff0c\u663e\u8457\u51cf\u5c11LLM\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u68af\u5ea6\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u68af\u5ea6\u7684\u52a8\u6001\u53d8\u5316\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u5982\u4f55\u5728\u52a0\u901fLLM\u8bad\u7ec3\u7684\u540c\u65f6\u4e0d\u727a\u7272\u6027\u80fd\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "EDGC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u4e0b\u91c7\u6837\u65b9\u6cd5\u9ad8\u6548\u4f30\u8ba1\u68af\u5ea6\u71b5\uff1b2) \u5efa\u7acb\u538b\u7f29\u7387\u4e0e\u68af\u5ea6\u71b5\u7684\u7406\u8bba\u6a21\u578b\uff1b3) \u57fa\u4e8e\u7a97\u53e3\u7684\u8c03\u6574\u673a\u5236\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u6d41\u6c34\u7ebf\u9636\u6bb5\u7684\u538b\u7f29\u7387\u3002", "result": "\u572832-NVIDIA-V100\u548c64-NVIDIA-H100\u96c6\u7fa4\u4e0a\u5206\u522b\u8bad\u7ec3GPT2-2.5B\u548cGPT2-12.1B\uff0cEDGC\u5c06\u901a\u4fe1\u5ef6\u8fdf\u548c\u8bad\u7ec3\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u9ad8\u8fbe46.45%\u548c16.13%\uff0c\u540c\u65f6\u4fdd\u6301LLM\u7cbe\u5ea6\u3002", "conclusion": "EDGC\u901a\u8fc7\u52a8\u6001\u68af\u5ea6\u538b\u7f29\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2511.10344", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10344", "abs": "https://arxiv.org/abs/2511.10344", "authors": ["Zicheng Hu", "Yuchen Wang", "Cheng Chen"], "title": "Robust Decentralized Multi-armed Bandits: From Corruption-Resilience to Byzantine-Resilience", "comment": null, "summary": "Decentralized cooperative multi-agent multi-armed bandits (DeCMA2B) considers how multiple agents collaborate in a decentralized multi-armed bandit setting. Though this problem has been extensively studied in previous work, most existing methods remain susceptible to various adversarial attacks. In this paper, we first study DeCMA2B with adversarial corruption, where an adversary can corrupt reward observations of all agents with a limited corruption budget. We propose a robust algorithm, called DeMABAR, which ensures that each agent's individual regret suffers only an additive term proportional to the corruption budget. Then we consider a more realistic scenario where the adversary can only attack a small number of agents. Our theoretical analysis shows that the DeMABAR algorithm can also almost completely eliminate the influence of adversarial attacks and is inherently robust in the Byzantine setting, where an unknown fraction of the agents can be Byzantine, i.e., may arbitrarily select arms and communicate wrong information. We also conduct numerical experiments to illustrate the robustness and effectiveness of the proposed method.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DeMABAR\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5bf9\u6297\u6027\u8150\u8d25\u548c\u62dc\u5360\u5ead\u653b\u51fb\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u591a\u81c2\u8001\u864e\u673a\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5404\u79cd\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u62b5\u5fa1\u5bf9\u6297\u6027\u8150\u8d25\u548c\u62dc\u5360\u5ead\u653b\u51fb\u7684\u9c81\u68d2\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86DeMABAR\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u7279\u5b9a\u7684\u534f\u4f5c\u673a\u5236\u6765\u5904\u7406\u5bf9\u6297\u6027\u8150\u8d25\u548c\u62dc\u5360\u5ead\u653b\u51fb\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u7684\u4e2a\u4f53\u9057\u61be\u4ec5\u53d7\u5230\u4e0e\u8150\u8d25\u9884\u7b97\u6210\u6b63\u6bd4\u7684\u9644\u52a0\u9879\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eDeMABAR\u7b97\u6cd5\u80fd\u591f\u51e0\u4e4e\u5b8c\u5168\u6d88\u9664\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u5728\u62dc\u5360\u5ead\u8bbe\u7f6e\u4e0b\u5177\u6709\u5185\u5728\u9c81\u68d2\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DeMABAR\u7b97\u6cd5\u4e3a\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u65b9\u6848\uff0c\u5728\u5bf9\u6297\u6027\u8150\u8d25\u548c\u62dc\u5360\u5ead\u653b\u51fb\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.10366", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10366", "abs": "https://arxiv.org/abs/2511.10366", "authors": ["Arnab Bhattacharyya", "Davin Choo", "Philips George John", "Themis Gouleakis"], "title": "Product distribution learning with imperfect advice", "comment": "Full version (11 pages). To be published in NeurIPS 2025", "summary": "Given i.i.d.~samples from an unknown distribution $P$, the goal of distribution learning is to recover the parameters of a distribution that is close to $P$. When $P$ belongs to the class of product distributions on the Boolean hypercube $\\{0,1\\}^d$, it is known that $\u03a9(d/\\varepsilon^2)$ samples are necessary to learn $P$ within total variation (TV) distance $\\varepsilon$. We revisit this problem when the learner is also given as advice the parameters of a product distribution $Q$. We show that there is an efficient algorithm to learn $P$ within TV distance $\\varepsilon$ that has sample complexity $\\tilde{O}(d^{1-\u03b7}/\\varepsilon^2)$, if $\\|\\mathbf{p} - \\mathbf{q}\\|_1 < \\varepsilon d^{0.5 - \u03a9(\u03b7)}$. Here, $\\mathbf{p}$ and $\\mathbf{q}$ are the mean vectors of $P$ and $Q$ respectively, and no bound on $\\|\\mathbf{p} - \\mathbf{q}\\|_1$ is known to the algorithm a priori.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ed9\u5b9a\u53c2\u8003\u5206\u5e03Q\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u5b66\u4e60\u672a\u77e5\u5206\u5e03P\u7684\u95ee\u9898\u3002\u5f53P\u548cQ\u7684\u5747\u503c\u5411\u91cf\u5728\u21131\u8ddd\u79bb\u4e0a\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5b66\u4e60\u9700\u8981\u03a9(d/\u03b5\u00b2)\u6837\u672c\uff0c\u4f46\u5982\u679c\u6709\u53c2\u8003\u5206\u5e03Q\u7684\u53c2\u6570\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u53ef\u80fd\u51cf\u5c11\u6837\u672c\u9700\u6c42\u3002\u672c\u6587\u63a2\u7d22\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u5148\u9a8c\u77e5\u8bc6\u6765\u52a0\u901f\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5f53\u53c2\u8003\u5206\u5e03Q\u4e0e\u76ee\u6807\u5206\u5e03P\u7684\u5747\u503c\u5411\u91cf\u5728\u21131\u8ddd\u79bb\u4e0a\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u5229\u7528Q\u7684\u53c2\u6570\u4fe1\u606f\u6765\u51cf\u5c11\u5b66\u4e60P\u6240\u9700\u7684\u6837\u672c\u6570\u91cf\u3002", "result": "\u5f53\u2016p-q\u2016\u2081 < \u03b5d^0.5-\u03a9(\u03b7)\u65f6\uff0c\u7b97\u6cd5\u4ec5\u9700\u00d5(d^{1-\u03b7}/\u03b5\u00b2)\u6837\u672c\u5373\u53ef\u5728\u603b\u53d8\u5dee\u8ddd\u79bb\u03b5\u5185\u5b66\u4e60P\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5229\u7528\u53c2\u8003\u5206\u5e03\u7684\u53c2\u6570\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u5206\u5e03\u5b66\u4e60\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u524d\u63d0\u662f\u53c2\u8003\u5206\u5e03\u4e0e\u76ee\u6807\u5206\u5e03\u5728\u21131\u8ddd\u79bb\u4e0a\u8db3\u591f\u63a5\u8fd1\u3002"}}
{"id": "2511.10434", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.10434", "abs": "https://arxiv.org/abs/2511.10434", "authors": ["Feng Wang", "Tianxiang Chen", "Shuyue Wei", "Qian Chu", "Yi Zhang", "Yifan Sun", "Zhiming Zheng"], "title": "Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting", "comment": null, "summary": "Spatio-temporal graphs are powerful tools for modeling complex dependencies in traffic time series. However, the distributed nature of real-world traffic data across multiple stakeholders poses significant challenges in modeling and reconstructing inter-client spatial dependencies while adhering to data locality constraints. Existing methods primarily address static dependencies, overlooking their dynamic nature and resulting in suboptimal performance. In response, we propose Federated Spatio-Temporal Graph with Dynamic Inter-Client Dependencies (FedSTGD), a framework designed to model and reconstruct dynamic inter-client spatial dependencies in federated learning. FedSTGD incorporates a federated nonlinear computation decomposition module to approximate complex graph operations. This is complemented by a graph node embedding augmentation module, which alleviates performance degradation arising from the decomposition. These modules are coordinated through a client-server collective learning protocol, which decomposes dynamic inter-client spatial dependency learning tasks into lightweight, parallelizable subtasks. Extensive experiments on four real-world datasets demonstrate that FedSTGD achieves superior performance over state-of-the-art baselines in terms of RMSE, MAE, and MAPE, approaching that of centralized baselines. Ablation studies confirm the contribution of each module in addressing dynamic inter-client spatial dependencies, while sensitivity analysis highlights the robustness of FedSTGD to variations in hyperparameters.", "AI": {"tldr": "FedSTGD\u662f\u4e00\u4e2a\u8054\u90a6\u65f6\u7a7a\u56fe\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5efa\u6a21\u548c\u91cd\u6784\u52a8\u6001\u7684\u5ba2\u6237\u7aef\u95f4\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u8ba1\u7b97\u5206\u89e3\u548c\u56fe\u8282\u70b9\u5d4c\u5165\u589e\u5f3a\u6a21\u5757\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4ea4\u901a\u6570\u636e\u5206\u5e03\u5728\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u9759\u6001\u4f9d\u8d56\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u52a8\u6001\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u5728\u9075\u5b88\u6570\u636e\u672c\u5730\u5316\u7ea6\u675f\u7684\u540c\u65f6\u5efa\u6a21\u548c\u91cd\u6784\u5ba2\u6237\u7aef\u95f4\u7684\u52a8\u6001\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faFedSTGD\u6846\u67b6\uff0c\u5305\u542b\u8054\u90a6\u975e\u7ebf\u6027\u8ba1\u7b97\u5206\u89e3\u6a21\u5757\u6765\u8fd1\u4f3c\u590d\u6742\u56fe\u64cd\u4f5c\uff0c\u56fe\u8282\u70b9\u5d4c\u5165\u589e\u5f3a\u6a21\u5757\u7f13\u89e3\u5206\u89e3\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4ee5\u53ca\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u96c6\u4f53\u5b66\u4e60\u534f\u8bae\u5c06\u52a8\u6001\u7a7a\u95f4\u4f9d\u8d56\u5b66\u4e60\u4efb\u52a1\u5206\u89e3\u4e3a\u8f7b\u91cf\u7ea7\u5e76\u884c\u5b50\u4efb\u52a1\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedSTGD\u5728RMSE\u3001MAE\u548cMAPE\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u3002\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u4e86\u5404\u6a21\u5757\u5bf9\u5904\u7406\u52a8\u6001\u5ba2\u6237\u7aef\u95f4\u7a7a\u95f4\u4f9d\u8d56\u7684\u8d21\u732e\u3002", "conclusion": "FedSTGD\u80fd\u591f\u6709\u6548\u5efa\u6a21\u548c\u91cd\u6784\u52a8\u6001\u7684\u5ba2\u6237\u7aef\u95f4\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u8d85\u53c2\u6570\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.10439", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10439", "abs": "https://arxiv.org/abs/2511.10439", "authors": ["Thomas Decker", "Volker Tresp", "Florian Buettner"], "title": "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u4e0e\u57fa\u4e8e\u6270\u52a8\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7279\u5b9a\u6270\u52a8\u4e0b\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\u7684\u8d28\u91cf\u3002\u4f5c\u8005\u63d0\u51fa\u4e86ReCalX\u65b9\u6cd5\u6765\u91cd\u65b0\u6821\u51c6\u6a21\u578b\u4ee5\u6539\u5584\u89e3\u91ca\u6548\u679c\u3002", "motivation": "\u57fa\u4e8e\u6270\u52a8\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u53ef\u9760\u6027\u53d7\u5230\u6a21\u578b\u5728\u7279\u5b9a\u6270\u52a8\u4e0b\u884c\u4e3a\u672a\u77e5\u7684\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u7279\u5b9a\u6270\u52a8\u4e0b\u4ea7\u751f\u4e0d\u53ef\u9760\u6982\u7387\u4f30\u8ba1\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ReCalX\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u6821\u51c6\u6a21\u578b\u6765\u6539\u5584\u89e3\u91ca\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u539f\u59cb\u9884\u6d4b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u53ef\u89e3\u91ca\u6027\u7279\u5b9a\u7684\u6270\u52a8\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cReCalX\u80fd\u6709\u6548\u51cf\u5c11\u6270\u52a8\u7279\u5b9a\u7684\u6821\u51c6\u8bef\u5dee\uff0c\u63d0\u9ad8\u89e3\u91ca\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u66f4\u597d\u5730\u8bc6\u522b\u5168\u5c40\u91cd\u8981\u7684\u8f93\u5165\u7279\u5f81\u3002", "conclusion": "\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u7279\u5b9a\u6270\u52a8\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u89e3\u91ca\u8d28\u91cf\uff0cReCalX\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u6270\u52a8\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2511.10475", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10475", "abs": "https://arxiv.org/abs/2511.10475", "authors": ["\u00c7a\u011fr\u0131 Eser", "Zeynep Sonat Baltac\u0131", "Emre Akba\u015f", "Sinan Kalkan"], "title": "Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance", "comment": "45 pages, 11 figures", "summary": "Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6570\u636e\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u4f5c\u4e3a\u4e0d\u5e73\u8861\u5206\u7c7b\u4efb\u52a1\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u6837\u672c\u6570\u91cf\u7684\u4e0d\u5e73\u8861\u5ea6\u91cf\u3002ID\u662f\u4e00\u79cd\u6613\u4e8e\u8ba1\u7b97\u3001\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u7684\u4e0d\u5e73\u8861\u5ea6\u91cf\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u4e0d\u5e73\u8861\u7f13\u89e3\u65b9\u6cd5\u4e2d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7c7b\u522b\u6837\u672c\u6570\u91cf\u7684\u4e0d\u5e73\u8861\u5ea6\u91cf\u5ffd\u7565\u4e86\u5197\u4f59\u6837\u672c\u548c\u7c7b\u522b\u5b66\u4e60\u96be\u5ea6\u7684\u5185\u5728\u5dee\u5f02\uff0c\u800c\u590d\u6742\u7684\u5ea6\u91cf\u65b9\u6cd5\u5982\u8bad\u7ec3\u635f\u5931\u548c\u4e0d\u786e\u5b9a\u6027\u9700\u8981\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6570\u636e\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u4f5c\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u4e0d\u5e73\u8861\u5ea6\u91cf\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u4e8e\u6837\u672c\u6570\u91cf\u7684\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u4e0d\u5e73\u8861\u6bd4\u7387\u7684\u6570\u636e\u96c6\u4e0a\uff0cID\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6587\u732e\u4e2d\u57fa\u4e8e\u6837\u672c\u6570\u91cf\u7684\u91cd\u52a0\u6743\u548c\u91cd\u91c7\u6837\u6280\u672f\uff0c\u4e14ID\u4e0e\u6837\u672c\u6570\u91cf\u7ed3\u5408\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u6570\u636e\u5185\u5728\u7ef4\u5ea6\u662f\u6bd4\u4f20\u7edf\u6837\u672c\u6570\u91cf\u66f4\u6709\u6548\u7684\u4e0d\u5e73\u8861\u5ea6\u91cf\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u7c7b\u522b\u7684\u5b66\u4e60\u96be\u5ea6\u5dee\u5f02\u3002"}}
{"id": "2511.10573", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10573", "abs": "https://arxiv.org/abs/2511.10573", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "Towards Emotionally Intelligent and Responsible Reinforcement Learning", "comment": null, "summary": "Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1f\u8d23\u4efb\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u60c5\u611f\u7406\u89e3\u548c\u4f26\u7406\u7ea6\u675f\u6574\u5408\u5230\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u7528\u4e8e\u533b\u7597\u548c\u884c\u4e3a\u652f\u6301\u9886\u57df\u7684\u4e2a\u6027\u5316\u51b3\u7b56\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u548c\u884c\u4e3a\u652f\u6301\u4e2d\u7684\u4e2a\u6027\u5316\u51b3\u7b56\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u89c4\u5219\u6216\u53c2\u4e0e\u5ea6\u6700\u5927\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u7684\u60c5\u611f\u80cc\u666f\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a8\u8350\u4e0d\u654f\u611f\u6216\u4e0d\u5b89\u5168\u7684\u5e72\u9884\u63aa\u65bd\u3002", "method": "\u5c06\u4e2a\u6027\u5316\u5efa\u6a21\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f15\u5165\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u5e73\u8861\u77ed\u671f\u884c\u4e3a\u53c2\u4e0e\u548c\u957f\u671f\u7528\u6237\u798f\u7949\uff0c\u5b9a\u4e49\u60c5\u611f\u611f\u77e5\u7684\u72b6\u6001\u8868\u793a\u6765\u6355\u6349\u60c5\u7eea\u51c6\u5907\u5ea6\u3001\u60c5\u611f\u548c\u98ce\u9669\u6ce2\u52a8\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u64cd\u4f5c\u5316\u7684\u6846\u67b6\uff0c\u5c06\u540c\u7406\u5fc3\u548c\u8d23\u4efb\u878d\u5165\u673a\u5668\u5b66\u4e60\u7b56\u7565\u4f18\u5316\uff0c\u8fde\u63a5\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3001\u60c5\u611f\u8ba1\u7b97\u548c\u8d1f\u8d23\u4efbAI\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u884c\u4e3a\u5065\u5eb7\u3001\u6559\u80b2\u548c\u6570\u5b57\u6cbb\u7597\u7b49\u9886\u57df\u7684\u4eba\u672c\u9886\u57df\u63d0\u4f9b\u4e86\u4f26\u7406\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u542f\u52a8\u5173\u4e8e\u60c5\u611f\u611f\u77e5\u548c\u53ef\u4fe1\u4e2a\u6027\u5316\u7cfb\u7edf\u7684\u4f26\u7406\u5bf9\u9f50\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bba\u8ba8\u8bba\u3002"}}
{"id": "2511.10576", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10576", "abs": "https://arxiv.org/abs/2511.10576", "authors": ["Yuval Shapira", "Dana Drachsler-Cohen"], "title": "Tight Robustness Certification through the Convex Hull of $\\ell_0$ Attacks", "comment": null, "summary": "Few-pixel attacks mislead a classifier by modifying a few pixels of an image. Their perturbation space is an $\\ell_0$-ball, which is not convex, unlike $\\ell_p$-balls for $p\\geq1$. However, existing local robustness verifiers typically scale by relying on linear bound propagation, which captures convex perturbation spaces. We show that the convex hull of an $\\ell_0$-ball is the intersection of its bounding box and an asymmetrically scaled $\\ell_1$-like polytope. The volumes of the convex hull and this polytope are nearly equal as the input dimension increases. We then show a linear bound propagation that precisely computes bounds over the convex hull and is significantly tighter than bound propagations over the bounding box or our $\\ell_1$-like polytope. This bound propagation scales the state-of-the-art $\\ell_0$ verifier on its most challenging robustness benchmarks by 1.24x-7.07x, with a geometric mean of 3.16.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u8fb9\u754c\u4f20\u64ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u8ba1\u7b97\u2113\u2080\u7403\u51f8\u5305\u4e0a\u7684\u8fb9\u754c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u2113\u2080\u9a8c\u8bc1\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5c40\u90e8\u9c81\u68d2\u6027\u9a8c\u8bc1\u5668\u901a\u5e38\u4f9d\u8d56\u7ebf\u6027\u8fb9\u754c\u4f20\u64ad\u6765\u5904\u7406\u51f8\u6270\u52a8\u7a7a\u95f4\uff0c\u4f46\u2113\u2080\u7403\u4e0d\u662f\u51f8\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u9a8c\u8bc1\u5668\u5728\u5c11\u50cf\u7d20\u653b\u51fb\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u8bc1\u660e\u4e86\u2113\u2080\u7403\u7684\u51f8\u5305\u662f\u5176\u8fb9\u754c\u6846\u4e0e\u4e0d\u5bf9\u79f0\u7f29\u653e\u2113\u2081\u7c7b\u591a\u9762\u4f53\u7684\u4ea4\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u8ba1\u7b97\u51f8\u5305\u8fb9\u754c\u7684\u7ebf\u6027\u8fb9\u754c\u4f20\u64ad\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u6700\u5148\u8fdb\u7684\u2113\u2080\u9a8c\u8bc1\u5668\u7684\u6027\u80fd\u63d0\u5347\u4e861.24\u500d\u81f37.07\u500d\uff0c\u51e0\u4f55\u5e73\u5747\u503c\u4e3a3.16\u500d\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u786e\u5efa\u6a21\u2113\u2080\u7403\u7684\u51f8\u5305\uff0c\u672c\u6587\u63d0\u51fa\u7684\u8fb9\u754c\u4f20\u64ad\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u2113\u2080\u9c81\u68d2\u6027\u9a8c\u8bc1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}

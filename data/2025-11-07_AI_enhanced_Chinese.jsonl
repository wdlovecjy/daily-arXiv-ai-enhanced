{"id": "2511.03797", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.03797", "abs": "https://arxiv.org/abs/2511.03797", "authors": ["Aimee Maurais", "Bamdad Hosseini", "Youssef Marzouk"], "title": "Learning Paths for Dynamic Measure Transport: A Control Perspective", "comment": "To appear at NeurIPS 2025 Workshop on Frontiers of Probabilistic\n  Inference: Sampling Meets Learning", "summary": "We bring a control perspective to the problem of identifying paths of\nmeasures for sampling via dynamic measure transport (DMT). We highlight the\nfact that commonly used paths may be poor choices for DMT and connect existing\nmethods for learning alternate paths to mean-field games. Based on these\nconnections we pose a flexible family of optimization problems for identifying\ntilted paths of measures for DMT and advocate for the use of objective terms\nwhich encourage smoothness of the corresponding velocities. We present a\nnumerical algorithm for solving these problems based on recent Gaussian process\nmethods for solution of partial differential equations and demonstrate the\nability of our method to recover more efficient and smooth transport models\ncompared to those which use an untilted reference path.", "AI": {"tldr": "\u672c\u6587\u4ece\u63a7\u5236\u8bba\u89c6\u89d2\u7814\u7a76\u52a8\u6001\u6d4b\u5ea6\u4f20\u8f93\u4e2d\u7684\u6d4b\u5ea6\u8def\u5f84\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5747\u503c\u573a\u535a\u5f08\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u6765\u83b7\u5f97\u66f4\u5e73\u6ed1\u9ad8\u6548\u7684\u4f20\u8f93\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u6d4b\u5ea6\u4f20\u8f93\u65b9\u6cd5\u4e2d\u5e38\u7528\u7684\u6d4b\u5ea6\u8def\u5f84\u9009\u62e9\u4e0d\u4f73\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u4f18\u7684\u503e\u659c\u8def\u5f84\u6765\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5747\u503c\u573a\u535a\u5f08\u8fde\u63a5\u7684\u4f18\u5316\u95ee\u9898\u65cf\uff0c\u4f7f\u7528\u9f13\u52b1\u901f\u5ea6\u5e73\u6ed1\u6027\u7684\u76ee\u6807\u9879\uff0c\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u3002", "result": "\u76f8\u6bd4\u672a\u503e\u659c\u53c2\u8003\u8def\u5f84\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6062\u590d\u66f4\u9ad8\u6548\u548c\u5e73\u6ed1\u7684\u4f20\u8f93\u6a21\u578b\u3002", "conclusion": "\u63a7\u5236\u8bba\u89c6\u89d2\u4e3a\u52a8\u6001\u6d4b\u5ea6\u4f20\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8def\u5f84\u4f18\u5316\u65b9\u6cd5\uff0c\u503e\u659c\u8def\u5f84\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86\u4f20\u8f93\u6548\u7387\u548c\u5e73\u6ed1\u6027\u3002"}}
{"id": "2511.03952", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03952", "abs": "https://arxiv.org/abs/2511.03952", "authors": ["Aukosh Jagannath", "Taj Jones-McCormick", "Varnan Sarangian"], "title": "High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes", "comment": null, "summary": "We develop a high-dimensional scaling limit for Stochastic Gradient Descent\nwith Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework\nto rigourously compare online SGD with some of its popular variants. We show\nthat the scaling limits of SGD-M coincide with those of online SGD after an\nappropriate time rescaling and a specific choice of step-size. However, if the\nstep-size is kept the same between the two algorithms, SGD-M will amplify\nhigh-dimensional effects, potentially degrading performance relative to online\nSGD. We demonstrate our framework on two popular learning problems: Spiked\nTensor PCA and Single Index Models. In both cases, we also examine online SGD\nwith an adaptive step-size based on normalized gradients. In the\nhigh-dimensional regime, this algorithm yields multiple benefits: its dynamics\nadmit fixed points closer to the population minimum and widens the range of\nadmissible step-sizes for which the iterates converge to such solutions. These\nexamples provide a rigorous account, aligning with empirical motivation, of how\nearly preconditioners can stabilize and improve dynamics in settings where\nonline SGD fails.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u5e26Polyak\u52a8\u91cf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD-M)\u7684\u9ad8\u7ef4\u7f29\u653e\u6781\u9650\uff0c\u5e76\u4e0e\u5728\u7ebfSGD\u8fdb\u884c\u6bd4\u8f83\u3002\u7814\u7a76\u53d1\u73b0SGD-M\u5728\u9002\u5f53\u65f6\u95f4\u91cd\u7f29\u653e\u548c\u7279\u5b9a\u6b65\u957f\u9009\u62e9\u4e0b\u4e0e\u5728\u7ebfSGD\u5177\u6709\u76f8\u540c\u7f29\u653e\u6781\u9650\uff0c\u4f46\u82e5\u6b65\u957f\u76f8\u540c\uff0cSGD-M\u4f1a\u653e\u5927\u9ad8\u7ef4\u6548\u5e94\u3002\u5728\u5f20\u91cfPCA\u548c\u5355\u6307\u6807\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u5177\u6709\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u52a8\u673a\u662f\u4e25\u683c\u6bd4\u8f83\u5728\u7ebfSGD\u53ca\u5176\u6d41\u884c\u53d8\u4f53\uff0c\u7279\u522b\u662fSGD-M\u7b97\u6cd5\uff0c\u7406\u89e3\u5b83\u4eec\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u9ad8\u7ef4\u884c\u4e3a\u5dee\u5f02\uff0c\u5e76\u4e3a\u65e9\u671f\u9884\u6761\u4ef6\u5668\u5982\u4f55\u7a33\u5b9a\u548c\u6539\u8fdb\u52a8\u6001\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u5f00\u53d1\u4e86SGD-M\u7684\u9ad8\u7ef4\u7f29\u653e\u6781\u9650\u7406\u8bba\u6846\u67b6\uff0c\u5728\u5f20\u91cfPCA\u548c\u5355\u6307\u6807\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u4e86\u5728\u7ebfSGD\u3001SGD-M\u548c\u57fa\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u3002", "result": "SGD-M\u5728\u9002\u5f53\u65f6\u95f4\u91cd\u7f29\u653e\u548c\u7279\u5b9a\u6b65\u957f\u4e0b\u4e0e\u5728\u7ebfSGD\u6709\u76f8\u540c\u7f29\u653e\u6781\u9650\uff0c\u4f46\u76f8\u540c\u6b65\u957f\u65f6\u4f1a\u653e\u5927\u9ad8\u7ef4\u6548\u5e94\u3002\u57fa\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u80fd\u4ea7\u751f\u66f4\u63a5\u8fd1\u603b\u4f53\u6700\u5c0f\u503c\u7684\u56fa\u5b9a\u70b9\uff0c\u5e76\u6269\u5927\u6536\u655b\u6b65\u957f\u8303\u56f4\u3002", "conclusion": "\u65e9\u671f\u9884\u6761\u4ef6\u5668\u53ef\u4ee5\u5728\u5728\u7ebfSGD\u5931\u8d25\u7684\u8bbe\u7f6e\u4e2d\u7a33\u5b9a\u548c\u6539\u8fdb\u52a8\u6001\uff0c\u57fa\u4e8e\u5f52\u4e00\u5316\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u6b65\u957f\u7b97\u6cd5\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u5177\u6709\u591a\u91cd\u4f18\u52bf\uff0c\u4e3a\u7ecf\u9a8c\u52a8\u673a\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2511.03825", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03825", "abs": "https://arxiv.org/abs/2511.03825", "authors": ["Ahmed Mostafa", "Raisul Arefin Nahid", "Samuel Mulder"], "title": "How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis", "comment": "Publication Notice. This paper was published in the BAR 2025 Workshop\n  (with NDSS 2025) and is for research and educational use. Copyright\n  \\c{opyright} 2025 Internet Society. All rights reserved. Personal/classroom\n  reproduction is permitted with this notice and full paper citation. All other\n  uses, including commercial, require prior written permission from the\n  Internet Society", "summary": "Tokenization is fundamental in assembly code analysis, impacting intrinsic\ncharacteristics like vocabulary size, semantic coverage, and extrinsic\nperformance in downstream tasks. Despite its significance, tokenization in the\ncontext of assembly code remains an underexplored area. This study aims to\naddress this gap by evaluating the intrinsic properties of Natural Language\nProcessing (NLP) tokenization models and parameter choices, such as vocabulary\nsize. We explore preprocessing customization options and pre-tokenization rules\ntailored to the unique characteristics of assembly code. Additionally, we\nassess their impact on downstream tasks like function signature prediction -- a\ncritical problem in binary code analysis.\n  To this end, we conduct a thorough study on various tokenization models,\nsystematically analyzing their efficiency in encoding assembly instructions and\ncapturing semantic nuances. Through intrinsic evaluations, we compare\ntokenizers based on tokenization efficiency, vocabulary compression, and\nrepresentational fidelity for assembly code. Using state-of-the-art pre-trained\nmodels such as the decoder-only Large Language Model (LLM) Llama 3.2, the\nencoder-only transformer BERT, and the encoder-decoder model BART, we evaluate\nthe effectiveness of these tokenizers across multiple performance metrics.\nPreliminary findings indicate that tokenizer choice significantly influences\ndownstream performance, with intrinsic metrics providing partial but incomplete\npredictability of extrinsic evaluation outcomes. These results reveal complex\ntrade-offs between intrinsic tokenizer properties and their utility in\npractical assembly code tasks. Ultimately, this study provides valuable\ninsights into optimizing tokenization models for low-level code analysis,\ncontributing to the robustness and scalability of Natural Language Model\n(NLM)-based binary analysis workflows.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86NLP\u5206\u8bcd\u6a21\u578b\u5728\u6c47\u7f16\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u5185\u5728\u7279\u6027\uff0c\u5305\u62ec\u8bcd\u6c47\u91cf\u5927\u5c0f\u3001\u8bed\u4e49\u8986\u76d6\u7b49\uff0c\u5e76\u63a2\u8ba8\u4e86\u9884\u5904\u7406\u5b9a\u5236\u548c\u9884\u5206\u8bcd\u89c4\u5219\u5bf9\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u51fd\u6570\u7b7e\u540d\u9884\u6d4b\uff09\u7684\u5f71\u54cd\u3002", "motivation": "\u6c47\u7f16\u4ee3\u7801\u5206\u8bcd\u5728\u4ee3\u7801\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8be5\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5206\u8bcd\u6a21\u578b\u7684\u5185\u5728\u7279\u6027\u548c\u53c2\u6570\u9009\u62e9\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u5206\u8bcd\u6a21\u578b\uff08\u5305\u62ecLlama 3.2\u3001BERT\u3001BART\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u5185\u5728\u8bc4\u4f30\u6bd4\u8f83\u5206\u8bcd\u6548\u7387\u3001\u8bcd\u6c47\u538b\u7f29\u548c\u8868\u793a\u4fdd\u771f\u5ea6\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u51fd\u6570\u7b7e\u540d\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u8868\u660e\u5206\u8bcd\u5668\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\uff0c\u5185\u5728\u6307\u6807\u53ea\u80fd\u90e8\u5206\u9884\u6d4b\u5916\u5728\u8bc4\u4f30\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u5185\u5728\u5206\u8bcd\u5668\u7279\u6027\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u590d\u6742\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u7ea7\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u5206\u8bcd\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u4e8c\u8fdb\u5236\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.03967", "categories": ["eess.SP", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03967", "abs": "https://arxiv.org/abs/2511.03967", "authors": ["Wuxia Chen", "Sean Moushegian", "Vahid Tarokh", "Taposh Banerjee"], "title": "Score-Based Quickest Change Detection and Fault Identification for Multi-Stream Signals", "comment": null, "summary": "This paper introduces an approach to multi-stream quickest change detection\nand fault isolation for unnormalized and score-based statistical models.\nTraditional optimal algorithms in the quickest change detection literature\nrequire explicit pre-change and post-change distributions to calculate the\nlikelihood ratio of the observations, which can be computationally expensive\nfor higher-dimensional data and sometimes even infeasible for complex machine\nlearning models. To address these challenges, we propose the min-SCUSUM method,\na Hyvarinen score-based algorithm that computes the difference of score\nfunctions in place of log-likelihood ratios. We provide a delay and false alarm\nanalysis of the proposed algorithm, showing that its asymptotic performance\ndepends on the Fisher divergence between the pre- and post-change\ndistributions. Furthermore, we establish an upper bound on the probability of\nfault misidentification in distinguishing the affected stream from the\nunaffected ones.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyvarinen\u5206\u6570\u7684min-SCUSUM\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6d41\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u548c\u6545\u969c\u9694\u79bb\uff0c\u66ff\u4ee3\u4f20\u7edf\u9700\u8981\u663e\u5f0f\u5206\u5e03\u8ba1\u7b97\u4f3c\u7136\u6bd4\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\u9700\u8981\u663e\u5f0f\u7684\u524d\u540e\u53d8\u5316\u5206\u5e03\u6765\u8ba1\u7b97\u89c2\u6d4b\u503c\u7684\u4f3c\u7136\u6bd4\uff0c\u5bf9\u4e8e\u9ad8\u7ef4\u6570\u636e\u548c\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u751a\u81f3\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51famin-SCUSUM\u65b9\u6cd5\uff0c\u4f7f\u7528Hyvarinen\u5206\u6570\u51fd\u6570\u5dee\u5f02\u66ff\u4ee3\u5bf9\u6570\u4f3c\u7136\u6bd4\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u672a\u5f52\u4e00\u5316\u548c\u57fa\u4e8e\u5206\u6570\u7684\u7edf\u8ba1\u6a21\u578b\u3002", "result": "\u7b97\u6cd5\u5ef6\u8fdf\u548c\u8bef\u62a5\u5206\u6790\u8868\u660e\u5176\u6e10\u8fd1\u6027\u80fd\u53d6\u51b3\u4e8e\u524d\u540e\u53d8\u5316\u5206\u5e03\u4e4b\u95f4\u7684Fisher\u6563\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e86\u6545\u969c\u8bef\u8bc6\u522b\u6982\u7387\u7684\u4e0a\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u6a21\u578b\u7684\u9ad8\u7ef4\u6570\u636e\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u663e\u5f0f\u5206\u5e03\u5047\u8bbe\u3002"}}
{"id": "2511.03768", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03768", "abs": "https://arxiv.org/abs/2511.03768", "authors": ["Candace Ross", "Florian Bordes", "Adina Williams", "Polina Kirichenko", "Mark Ibrahim"], "title": "What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes", "comment": "10 pages, 6 figures. Accepted to NeurIPS Datasets & Benchmarks 2025", "summary": "Multimodal language models possess a remarkable ability to handle an\nopen-vocabulary's worth of objects. Yet the best models still suffer from\nhallucinations when reasoning about scenes in the real world, revealing a gap\nbetween their seemingly strong performance on existing perception benchmarks\nthat are saturating and their reasoning in the real world. To address this gap,\nwe build a novel benchmark of in-the-wild scenes that we call Common-O. With\nmore than 10.5k examples using exclusively new images not found in web training\ndata to avoid contamination, Common-O goes beyond just perception, inspired by\ncognitive tests for humans, to probe reasoning across scenes by asking \"what's\nin common?\". We evaluate leading multimodal language models, including models\nspecifically trained to perform chain-of-thought reasoning. We find that\nperceiving objects in single images is tractable for most models, yet reasoning\nacross scenes is very challenging even for the best models, including reasoning\nmodels. Despite saturating many leaderboards focusing on perception, the best\nperforming model only achieves 35% on Common-O -- and on Common-O Complex,\nconsisting of more complex scenes, the best model achieves only 1%. Curiously,\nwe find models are more prone to hallucinate when similar objects are present\nin the scene, suggesting models may be relying on object co-occurrence seen\nduring training. Among the models we evaluated, we found scale can provide\nmodest improvements while models explicitly trained with multi-image inputs\nshow bigger improvements, suggesting scaled multi-image training may offer\npromise. We make our benchmark publicly available to spur research into the\nchallenge of hallucination when reasoning across scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86Common-O\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u5728\u8de8\u573a\u666f\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e5f\u5f88\u5dee\uff0835%\u51c6\u786e\u7387\uff09\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u4ec51%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5c3d\u7ba1\u5728\u73b0\u6709\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u63a8\u7406\u4e2d\u4ecd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "method": "\u6784\u5efa\u5305\u542b10,500+\u4e2a\u5168\u65b0\u56fe\u50cf\u7684Common-O\u57fa\u51c6\u6d4b\u8bd5\uff0c\u907f\u514d\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\uff0c\u901a\u8fc7\"\u5171\u540c\u70b9\u662f\u4ec0\u4e48\"\u7684\u95ee\u9898\u6765\u8bc4\u4f30\u6a21\u578b\u8de8\u573a\u666f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5355\u56fe\u50cf\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u573a\u666f\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5f88\u5dee\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec535%\uff0c\u590d\u6742\u573a\u666f\u4e2d\u4ec51%\u3002\u6a21\u578b\u5728\u5b58\u5728\u76f8\u4f3c\u7269\u4f53\u65f6\u66f4\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002", "conclusion": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u573a\u666f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u6311\u6218\uff0c\u591a\u56fe\u50cf\u8bad\u7ec3\u53ef\u80fd\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\uff0cCommon-O\u57fa\u51c6\u5c06\u63a8\u52a8\u89e3\u51b3\u573a\u666f\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u7814\u7a76\u3002"}}
{"id": "2511.03998", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03998", "abs": "https://arxiv.org/abs/2511.03998", "authors": ["Abhishek Rajasekaran", "Mehdi Karbalayghareh", "Xiaoyan Ma", "David J. Love", "Christopher G. Brinton"], "title": "Optimal RIS Placement in a Multi-User MISO System with User Randomness", "comment": "6 pages, 3 figures", "summary": "It is well established that the performance of reconfigurable intelligent\nsurface (RIS)-assisted systems critically depends on the optimal placement of\nthe RIS. Previous works consider either simple coverage maximization or\nsimultaneous optimization of the placement of the RIS along with the\nbeamforming and reflection coefficients, most of which assume that the location\nof the RIS, base station (BS), and users are known. However, in practice, only\nthe spatial variation of user density and obstacle configuration are likely to\nbe known prior to deployment of the system. Thus, we formulate a non-convex\nproblem that optimizes the position of the RIS over the expected minimum\nsignal-to-interference-plus-noise ratio (SINR) of the system with user\nrandomness, assuming that the system employs joint beamforming after\ndeployment. To solve this problem, we propose a recursive coarse-to-fine\nmethodology that constructs a set of candidate locations for RIS placement\nbased on the obstacle configuration and evaluates them over multiple\ninstantiations from the user distribution. The search is recursively refined\nwithin the optimal region identified in each stage to determine the final\noptimal region for RIS deployment. Numerical results are presented to\ncorroborate our findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52\u7c97\u5230\u7ec6\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u7684\u653e\u7f6e\u4f4d\u7f6e\uff0c\u8003\u8651\u7528\u6237\u968f\u673a\u6027\u548c\u969c\u788d\u7269\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u5316\u7cfb\u7edf\u7684\u6700\u5c0f\u4fe1\u5e72\u566a\u6bd4(SINR)\u3002", "motivation": "\u73b0\u6709RIS\u8f85\u52a9\u7cfb\u7edf\u7814\u7a76\u5927\u591a\u5047\u8bbe\u7528\u6237\u4f4d\u7f6e\u5df2\u77e5\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u65f6\u901a\u5e38\u53ea\u77e5\u9053\u7528\u6237\u5bc6\u5ea6\u5206\u5e03\u548c\u969c\u788d\u7269\u914d\u7f6e\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5728\u7528\u6237\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684RIS\u653e\u7f6e\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9012\u5f52\u7c97\u5230\u7ec6\u641c\u7d22\u65b9\u6cd5\uff1a\u57fa\u4e8e\u969c\u788d\u7269\u914d\u7f6e\u6784\u5efa\u5019\u9009\u4f4d\u7f6e\u96c6\u5408\uff0c\u901a\u8fc7\u591a\u6b21\u7528\u6237\u5206\u5e03\u5b9e\u4f8b\u8bc4\u4f30\u8fd9\u4e9b\u4f4d\u7f6e\uff0c\u5e76\u5728\u6bcf\u4e2a\u9636\u6bb5\u8bc6\u522b\u7684\u6700\u4f18\u533a\u57df\u5185\u9012\u5f52\u7ec6\u5316\u641c\u7d22\uff0c\u6700\u7ec8\u786e\u5b9aRIS\u90e8\u7f72\u7684\u6700\u4f73\u533a\u57df\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u627e\u5230\u5728\u7528\u6237\u4f4d\u7f6e\u968f\u673a\u60c5\u51b5\u4e0b\u7684\u6700\u4f18RIS\u90e8\u7f72\u4f4d\u7f6e\u3002", "conclusion": "\u63d0\u51fa\u7684\u9012\u5f52\u7c97\u5230\u7ec6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7528\u6237\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684RIS\u653e\u7f6e\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03948", "categories": ["cs.AI", "cs.HC", "I.2.6; K.3.1"], "pdf": "https://arxiv.org/pdf/2511.03948", "abs": "https://arxiv.org/abs/2511.03948", "authors": ["Kevin Hong", "Kia Karbasi", "Gregory Pottie"], "title": "Extracting Causal Relations in Deep Knowledge Tracing", "comment": "Accepted for publication in the Proceedings of the 18th International\n  Conference on Educational Data Mining, 6 pages, 1 figure", "summary": "A longstanding goal in computational educational research is to develop\nexplainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which\nleverages a Recurrent Neural Network (RNN) to predict student knowledge and\nperformance on exercises, has been proposed as a major advancement over\ntraditional KT methods. Several studies suggest that its performance gains stem\nfrom its ability to model bidirectional relationships between different\nknowledge components (KCs) within a course, enabling the inference of a\nstudent's understanding of one KC from their performance on others. In this\npaper, we challenge this prevailing explanation and demonstrate that DKT's\nstrength lies in its implicit ability to model prerequisite relationships as a\ncausal structure, rather than bidirectional relationships. By pruning exercise\nrelation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal\nsubsets of the Assistments dataset, we show that DKT's predictive capabilities\nalign strongly with these causal structures. Furthermore, we propose an\nalternative method for extracting exercise relation DAGs using DKT's learned\nrepresentations and provide empirical evidence supporting our claim. Our\nfindings suggest that DKT's effectiveness is largely driven by its capacity to\napproximate causal dependencies between KCs rather than simple relational\nmappings.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u5173\u4e8e\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a(DKT)\u6027\u80fd\u4f18\u52bf\u7684\u666e\u904d\u89e3\u91ca\uff0c\u8bc1\u660eDKT\u7684\u4f18\u52bf\u5728\u4e8e\u5176\u9690\u5f0f\u5efa\u6a21\u5148\u51b3\u6761\u4ef6\u5173\u7cfb\u4f5c\u4e3a\u56e0\u679c\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u800c\u975e\u53cc\u5411\u5173\u7cfb\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u8ba1\u7b97\u6559\u80b2\u7814\u7a76\u7684\u76ee\u6807\u662f\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u3002DKT\u88ab\u8ba4\u4e3a\u6bd4\u4f20\u7edfKT\u65b9\u6cd5\u6709\u91cd\u5927\u8fdb\u6b65\uff0c\u4f46\u5bf9\u5176\u6027\u80fd\u6765\u6e90\u7684\u89e3\u91ca\u5b58\u5728\u4e89\u8bae\u3002", "method": "\u901a\u8fc7\u5c06\u7ec3\u4e60\u5173\u7cfb\u56fe\u4fee\u526a\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAGs)\uff0c\u5e76\u5728Assistments\u6570\u636e\u96c6\u7684\u56e0\u679c\u5b50\u96c6\u4e0a\u8bad\u7ec3DKT\uff0c\u5206\u6790\u5176\u9884\u6d4b\u80fd\u529b\u4e0e\u56e0\u679c\u7ed3\u6784\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDKT\u7684\u9884\u6d4b\u80fd\u529b\u4e0e\u56e0\u679c\u7ed3\u6784\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eDKT\u5b66\u4e60\u8868\u793a\u63d0\u53d6\u7ec3\u4e60\u5173\u7cfbDAGs\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "DKT\u7684\u6709\u6548\u6027\u4e3b\u8981\u6e90\u4e8e\u5176\u8fd1\u4f3c\u77e5\u8bc6\u7ec4\u4ef6\u95f4\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u800c\u975e\u7b80\u5355\u7684\u5173\u7cfb\u6620\u5c04\u3002"}}
{"id": "2511.04065", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04065", "abs": "https://arxiv.org/abs/2511.04065", "authors": ["Mohsen Sadatsafavi", "Gavin Pereira", "Wenjia Chen"], "title": "Transportability of Prognostic Markers: Rethinking Common Practices through a Sufficient-Component-Cause Perspective", "comment": "15 pages, 2 tables, 2 figures, 1 appendix", "summary": "Transportability, the ability to maintain performance across populations, is\na desirable property of of markers of clinical outcomes. However, empirical\nfindings indicate that markers often exhibit varying performances across\npopulations. For prognostic markers whose results are used to quantify of the\nrisk of an outcome, oftentimes a form of updating is required when the marker\nis transported to populations with different disease prevalences. Here, we\nrevisit transportability of prognostic markers through the lens of the\nfoundational framework of sufficient component causes (SCC). We argue that\ntransporting a marker \"as is\" implicitly assumes predictive values are\ntransportable, whereas conventional prevalence-adjustment shifts the locus of\ntransportability to accuracy metrics (sensitivity and specificity). Using a\nminimalist SCC framework that decomposes risk prediction into its causal\nconstituents, we show that both approaches rely on strong assumptions about the\nstability of cause distributions across populations. A SCC framework instead\ninvites making transparent assumptions about how different causes vary across\npopulations, leading to different transportation methods. For example, in the\nabsence of any external information other than disease prevalence, a\ncause-neutral perspective can assume all causes are responsible for change in\nprevalence, leading to a new form of marker transportation. Numerical\nexperiments demonstrate that different transportability assumptions lead to\nvarying degrees of information loss, depending on how population differ from\neach other in the distribution of causes. A SCC perspective challenges common\nassumptions and practices for marker transportability, and proposes\ntransportation algorithms that reflect our knowledge or assumptions about how\ncauses vary across populations.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5145\u5206\u6210\u5206\u539f\u56e0(SCC)\u6846\u67b6\u91cd\u65b0\u5ba1\u89c6\u9884\u540e\u6807\u5fd7\u7269\u7684\u53ef\u79fb\u690d\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\u53ef\u79fb\u690d\u6027\u5047\u8bbe\u548c\u5b9e\u8df5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u75c5\u56e0\u5206\u5e03\u53d8\u5316\u7684\u65b0\u8fd0\u8f93\u7b97\u6cd5\u3002", "motivation": "\u9884\u540e\u6807\u5fd7\u7269\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u5f80\u5f80\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6027\u80fd\uff0c\u9700\u8981\u53ef\u79fb\u690d\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u5047\u8bbe\u9884\u6d4b\u503c\u53ef\u79fb\u690d\uff0c\u8981\u4e48\u901a\u8fc7\u60a3\u75c5\u7387\u8c03\u6574\u8f6c\u79fb\u53ef\u79fb\u690d\u6027\u5230\u51c6\u786e\u6027\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u4f9d\u8d56\u4e8e\u5f3a\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u5316\u7684\u5145\u5206\u6210\u5206\u539f\u56e0(SCC)\u6846\u67b6\uff0c\u5c06\u98ce\u9669\u9884\u6d4b\u5206\u89e3\u4e3a\u5176\u56e0\u679c\u6210\u5206\uff0c\u5206\u6790\u4e0d\u540c\u8fd0\u8f93\u65b9\u6cd5\u6240\u4f9d\u8d56\u7684\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u75c5\u56e0\u5206\u5e03\u53d8\u5316\u7684\u65b0\u8fd0\u8f93\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u53ef\u79fb\u690d\u6027\u5047\u8bbe\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7a0b\u5ea6\u7684\u4fe1\u606f\u635f\u5931\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u4eba\u7fa4\u5728\u75c5\u56e0\u5206\u5e03\u4e0a\u7684\u5dee\u5f02\u3002\u5728\u6ca1\u6709\u5916\u90e8\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u91c7\u7528\u539f\u56e0\u4e2d\u6027\u89c6\u89d2\u53ef\u4ee5\u63a8\u5bfc\u51fa\u65b0\u7684\u6807\u5fd7\u7269\u8fd0\u8f93\u5f62\u5f0f\u3002", "conclusion": "SCC\u89c6\u89d2\u6311\u6218\u4e86\u6807\u5fd7\u7269\u53ef\u79fb\u690d\u6027\u7684\u5e38\u89c1\u5047\u8bbe\u548c\u5b9e\u8df5\uff0c\u63d0\u51fa\u4e86\u53cd\u6620\u6211\u4eec\u5bf9\u75c5\u56e0\u5728\u4eba\u7fa4\u95f4\u53d8\u5316\u77e5\u8bc6\u6216\u5047\u8bbe\u7684\u8fd0\u8f93\u7b97\u6cd5\uff0c\u4f7f\u8fd0\u8f93\u5047\u8bbe\u66f4\u52a0\u900f\u660e\u3002"}}
{"id": "2511.03980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03980", "abs": "https://arxiv.org/abs/2511.03980", "authors": ["Bram Bult\u00e9", "Ayla Rigouts Terryn"], "title": "LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing", "comment": "Preprint under review at Computational Linguistics. Accepted with\n  minor revisions (10/10/2025); second round", "summary": "Large Language Models (LLMs) are rapidly being adopted by users across the\nglobe, who interact with them in a diverse range of languages. At the same\ntime, there are well-documented imbalances in the training data and\noptimisation objectives of this technology, raising doubts as to whether LLMs\ncan represent the cultural diversity of their broad user base. In this study,\nwe look at LLMs and cultural values and examine how prompt language and\ncultural framing influence model responses and their alignment with human\nvalues in different countries. We probe 10 LLMs with 63 items from the Hofstede\nValues Survey Module and World Values Survey, translated into 11 languages, and\nformulated as prompts with and without different explicit cultural\nperspectives. Our study confirms that both prompt language and cultural\nperspective produce variation in LLM outputs, but with an important caveat:\nWhile targeted prompting can, to a certain extent, steer LLM responses in the\ndirection of the predominant values of the corresponding countries, it does not\novercome the models' systematic bias toward the values associated with a\nrestricted set of countries in our dataset: the Netherlands, Germany, the US,\nand Japan. All tested models, regardless of their origin, exhibit remarkably\nsimilar patterns: They produce fairly neutral responses on most topics, with\nselective progressive stances on issues such as social tolerance. Alignment\nwith cultural values of human respondents is improved more with an explicit\ncultural perspective than with a targeted prompt language. Unexpectedly,\ncombining both approaches is no more effective than cultural framing with an\nEnglish prompt. These findings reveal that LLMs occupy an uncomfortable middle\nground: They are responsive enough to changes in prompts to produce variation,\nbut too firmly anchored to specific cultural defaults to adequately represent\ncultural diversity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u5316\u4ef7\u503c\u89c2\u8868\u8fbe\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u867d\u7136\u63d0\u793a\u8bed\u8a00\u548c\u6587\u5316\u6846\u67b6\u4f1a\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff0c\u4f46LLMs\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u8377\u5170\u3001\u5fb7\u56fd\u3001\u7f8e\u56fd\u548c\u65e5\u672c\u7b49\u5c11\u6570\u56fd\u5bb6\u7684\u4ef7\u503c\u89c2\uff0c\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u6587\u5316\u591a\u6837\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5168\u7403\u8303\u56f4\u5185\u88ab\u4e0d\u540c\u8bed\u8a00\u7684\u7528\u6237\u5e7f\u6cdb\u4f7f\u7528\uff0c\u7814\u7a76\u5176\u662f\u5426\u80fd\u4ee3\u8868\u7528\u6237\u7fa4\u4f53\u7684\u6587\u5316\u591a\u6837\u6027\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u8bad\u7ec3\u6570\u636e\u548c\u4f18\u5316\u76ee\u6807\u5b58\u5728\u4e0d\u5e73\u8861\uff0c\u5f15\u53d1\u4e86\u5bf9LLMs\u6587\u5316\u4ee3\u8868\u6027\u7684\u8d28\u7591\u3002", "method": "\u4f7f\u752863\u4e2a\u6765\u81eaHofstede\u4ef7\u503c\u89c2\u8c03\u67e5\u6a21\u5757\u548c\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u7684\u9879\u76ee\uff0c\u7ffb\u8bd1\u621011\u79cd\u8bed\u8a00\uff0c\u4ee5\u6709/\u65e0\u660e\u786e\u6587\u5316\u89c6\u89d2\u7684\u63d0\u793a\u5f62\u5f0f\u6d4b\u8bd510\u4e2aLLMs\u3002", "result": "\u63d0\u793a\u8bed\u8a00\u548c\u6587\u5316\u89c6\u89d2\u90fd\u4f1a\u5bfc\u81f4LLM\u8f93\u51fa\u53d8\u5316\uff0c\u4f46\u65e0\u6cd5\u514b\u670d\u6a21\u578b\u5bf9\u8377\u5170\u3001\u5fb7\u56fd\u3001\u7f8e\u56fd\u548c\u65e5\u672c\u7b49\u5c11\u6570\u56fd\u5bb6\u4ef7\u503c\u89c2\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u3002\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u76f8\u4f3c\u6a21\u5f0f\uff1a\u5728\u5927\u591a\u6570\u8bdd\u9898\u4e0a\u4fdd\u6301\u4e2d\u7acb\uff0c\u5728\u793e\u4ea4\u5bbd\u5bb9\u7b49\u8bae\u9898\u4e0a\u6301\u9009\u62e9\u6027\u8fdb\u6b65\u7acb\u573a\u3002", "conclusion": "LLMs\u5904\u4e8e\u5c34\u5c2c\u7684\u4e2d\u95f4\u5730\u5e26\uff1a\u5bf9\u63d0\u793a\u53d8\u5316\u8db3\u591f\u654f\u611f\u4ee5\u4ea7\u751f\u53d8\u5316\uff0c\u4f46\u53c8\u8fc7\u4e8e\u56fa\u5b88\u7279\u5b9a\u6587\u5316\u9ed8\u8ba4\u503c\uff0c\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u6587\u5316\u591a\u6837\u6027\u3002\u660e\u786e\u7684\u6587\u5316\u89c6\u89d2\u6bd4\u9488\u5bf9\u6027\u63d0\u793a\u8bed\u8a00\u66f4\u80fd\u6539\u5584\u4e0e\u4eba\u7c7b\u53d7\u8bbf\u8005\u6587\u5316\u4ef7\u503c\u89c2\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.04616", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.04616", "abs": "https://arxiv.org/abs/2511.04616", "authors": ["Elvis Agbenyega", "Cody Quick"], "title": "Nonparametric Safety Stock Dimensioning: A Data-Driven Approach for Supply Chains of Hardware OEMs", "comment": "17 pages, 3 figures, 3 tables. To appear in INFORMs journal", "summary": "Resilient supply chains are critical, especially for Original Equipment\nManufacturers (OEMs) that power today's digital economy. Safety Stock\ndimensioning-the computation of the appropriate safety stock quantity-is one of\nseveral mechanisms to ensure supply chain resiliency, as it protects the supply\nchain against demand and supply uncertainties. Unfortunately, the major\napproaches to dimensioning safety stock heavily assume that demand is normally\ndistributed and ignore future demand variability, limiting their applicability\nin manufacturing contexts where demand is non-normal, intermittent, and highly\nskewed. In this paper, we propose a data-driven approach that relaxes the\nassumption of normality, enabling the demand distribution of each inventory\nitem to be analytically determined using Kernel Density Estimation. Also, we\nextended the analysis from historical demand variability to forecasted demand\nvariability. We evaluated the proposed approach against a normal distribution\nmodel in a near-world inventory replenishment simulation. Afterwards, we used a\nlinear optimization model to determine the optimal safety stock configuration.\nThe results from the simulation and linear optimization models showed that the\ndata-driven approach outperformed traditional approaches. In particular, the\ndata-driven approach achieved the desired service levels at lower safety stock\nlevels than the conventional approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u5b89\u5168\u5e93\u5b58\u914d\u7f6e\uff0c\u901a\u8fc7\u6838\u5bc6\u5ea6\u4f30\u8ba1\u653e\u677e\u4e86\u5bf9\u9700\u6c42\u6b63\u6001\u5206\u5e03\u7684\u5047\u8bbe\uff0c\u5e76\u8003\u8651\u4e86\u9884\u6d4b\u9700\u6c42\u53d8\u5f02\u6027\uff0c\u5728\u6a21\u62df\u548c\u4f18\u5316\u6a21\u578b\u4e2d\u663e\u793a\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u5e93\u5b58\u8ba1\u7b97\u65b9\u6cd5\u4e25\u91cd\u5047\u8bbe\u9700\u6c42\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u672a\u6765\u9700\u6c42\u53d8\u5f02\u6027\uff0c\u5728\u5236\u9020\u4e1a\u4e2d\u9700\u6c42\u901a\u5e38\u662f\u975e\u6b63\u6001\u3001\u95f4\u6b47\u6027\u548c\u9ad8\u5ea6\u504f\u6001\u7684\u60c5\u51b5\u4e0b\u9002\u7528\u6027\u6709\u9650\u3002", "method": "\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5206\u6790\u786e\u5b9a\u6bcf\u4e2a\u5e93\u5b58\u9879\u76ee\u7684\u9700\u6c42\u5206\u5e03\uff0c\u5c06\u5206\u6790\u4ece\u5386\u53f2\u9700\u6c42\u53d8\u5f02\u6027\u6269\u5c55\u5230\u9884\u6d4b\u9700\u6c42\u53d8\u5f02\u6027\uff0c\u901a\u8fc7\u8fd1\u771f\u5b9e\u5e93\u5b58\u8865\u8d27\u6a21\u62df\u548c\u7ebf\u6027\u4f18\u5316\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u6027\u80fd\u3002", "result": "\u6a21\u62df\u548c\u7ebf\u6027\u4f18\u5316\u6a21\u578b\u7ed3\u679c\u663e\u793a\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u8f83\u4f4e\u7684\u5b89\u5168\u5e93\u5b58\u6c34\u5e73\u4e0b\u5b9e\u73b0\u4e86\u671f\u671b\u7684\u670d\u52a1\u6c34\u5e73\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u914d\u7f6e\u5b89\u5168\u5e93\u5b58\uff0c\u5728\u4fdd\u8bc1\u670d\u52a1\u6c34\u5e73\u7684\u540c\u65f6\u964d\u4f4e\u5e93\u5b58\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u6c42\u5206\u5e03\u975e\u6b63\u6001\u7684\u5236\u9020\u4e1a\u73af\u5883\u3002"}}
{"id": "2511.04619", "categories": ["stat.AP", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04619", "abs": "https://arxiv.org/abs/2511.04619", "authors": ["Natalia Glazman", "Jyoti Mangal", "Pedro Borges", "Sebastien Ourselin", "M. Jorge Cardoso"], "title": "Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling", "comment": "Accepted to the NeurIPS 2025 Workshop on CauScien: Uncovering\n  Causality in Science", "summary": "The application of causal discovery to diseases like Alzheimer's (AD) is\nlimited by the static graph assumptions of most methods; such models cannot\naccount for an evolving pathophysiology, modulated by a latent disease\npseudotime. We propose to apply an existing latent variable model to real-world\nAD data, inferring a pseudotime that orders patients along a data-driven\ndisease trajectory independent of chronological age, then learning how causal\nrelationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC\n0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge\nsubstantially improved graph accuracy and orientation. Our framework reveals\ndynamic interactions between novel (NfL, GFAP) and established AD markers,\nenabling practical causal discovery despite violated assumptions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u65ad\u75be\u75c5\u4f2a\u65f6\u95f4\u8f68\u8ff9\u6765\u5206\u6790\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u6f14\u5316\u75c5\u7406\u751f\u7406\u5b66\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u9759\u6001\u56fe\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5047\u8bbe\u9759\u6001\u56fe\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7b49\u75be\u75c5\u7684\u52a8\u6001\u6f14\u5316\u75c5\u7406\u751f\u7406\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u53d7\u6f5c\u5728\u75be\u75c5\u4f2a\u65f6\u95f4\u8c03\u5236\u7684\u56e0\u679c\u5173\u7cfb\u53d8\u5316\u3002", "method": "\u5e94\u7528\u73b0\u6709\u6f5c\u53d8\u91cf\u6a21\u578b\u63a8\u65ad\u72ec\u7acb\u4e8e\u5b9e\u9645\u5e74\u9f84\u7684\u75be\u75c5\u4f2a\u65f6\u95f4\uff0c\u5c06\u60a3\u8005\u6cbf\u6570\u636e\u9a71\u52a8\u7684\u75be\u75c5\u8f68\u8ff9\u6392\u5e8f\uff0c\u7136\u540e\u5b66\u4e60\u56e0\u679c\u5173\u7cfb\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u5e76\u878d\u5165\u6700\u5c0f\u5316\u75be\u75c5\u65e0\u5173\u7684\u80cc\u666f\u77e5\u8bc6\u3002", "result": "\u4f2a\u65f6\u95f4\u5728\u9884\u6d4b\u8bca\u65ad\u65b9\u9762\u4f18\u4e8e\u5b9e\u9645\u5e74\u9f84\uff08AUC 0.82 vs 0.59\uff09\uff0c\u7ed3\u5408\u80cc\u666f\u77e5\u8bc6\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u51c6\u786e\u6027\u548c\u65b9\u5411\u6027\uff0c\u63ed\u793a\u4e86\u65b0\u578b\u6807\u5fd7\u7269\u4e0e\u5df2\u786e\u7acbAD\u6807\u5fd7\u7269\u4e4b\u95f4\u7684\u52a8\u6001\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u5047\u8bbe\u88ab\u8fdd\u53cd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b9e\u7528\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u4e3a\u7406\u89e3\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u52a8\u6001\u75c5\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.04032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04032", "abs": "https://arxiv.org/abs/2511.04032", "authors": ["Divya Pathak", "Harshit Kumar", "Anuska Roy", "Felix George", "Mudit Verma", "Pratibha Moogi"], "title": "Detecting Silent Failures in Multi-Agentic AI Trajectories", "comment": null, "summary": "Multi-Agentic AI systems, powered by large language models (LLMs), are\ninherently non-deterministic and prone to silent failures such as drift,\ncycles, and missing details in outputs, which are difficult to detect. We\nintroduce the task of anomaly detection in agentic trajectories to identify\nthese failures and present a dataset curation pipeline that captures user\nbehavior, agent non-determinism, and LLM variation. Using this pipeline, we\ncurate and label two benchmark datasets comprising \\textbf{4,275 and 894}\ntrajectories from Multi-Agentic AI systems. Benchmarking anomaly detection\nmethods on these datasets, we show that supervised (XGBoost) and\nsemi-supervised (SVDD) approaches perform comparably, achieving accuracies up\nto 98% and 96%, respectively. This work provides the first systematic study of\nanomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,\nand insights to guide future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u5305\u542b4,275\u548c894\u6761\u8f68\u8ff9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe98%\u3002", "motivation": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5177\u6709\u975e\u786e\u5b9a\u6027\uff0c\u5bb9\u6613\u51fa\u73b0\u6f02\u79fb\u3001\u5faa\u73af\u548c\u8f93\u51fa\u7ec6\u8282\u7f3a\u5931\u7b49\u9759\u9ed8\u6545\u969c\uff0c\u8fd9\u4e9b\u6545\u969c\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\uff0c\u6355\u6349\u7528\u6237\u884c\u4e3a\u3001\u667a\u80fd\u4f53\u975e\u786e\u5b9a\u6027\u548cLLM\u53d8\u5316\uff0c\u5e76\u521b\u5efa\u4e86\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3002\u4f7f\u7528XGBoost\uff08\u76d1\u7763\uff09\u548cSVDD\uff08\u534a\u76d1\u7763\uff09\u65b9\u6cd5\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u76d1\u7763\u65b9\u6cd5\uff08XGBoost\uff09\u548c\u534a\u76d1\u7763\u65b9\u6cd5\uff08SVDD\uff09\u8868\u73b0\u76f8\u5f53\uff0c\u5206\u522b\u8fbe\u523098%\u548c96%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.03808", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03808", "abs": "https://arxiv.org/abs/2511.03808", "authors": ["Bo Zhao", "Berkcan Kapusuzoglu", "Kartik Balasubramaniam", "Sambit Sahu", "Supriyo Chakraborty", "Genta Indra Winata"], "title": "Optimizing Reasoning Efficiency through Prompt Difficulty Prediction", "comment": "NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Reasoning language models perform well on complex tasks but are costly to\ndeploy due to their size and long reasoning traces. We propose a routing\napproach that assigns each problem to the smallest model likely to solve it,\nreducing compute without sacrificing accuracy. Using intermediate\nrepresentations from s1.1-32B, we train lightweight predictors of problem\ndifficulty or model correctness to guide routing across a pool of reasoning\nmodels. On diverse math benchmarks, routing improves efficiency over random\nassignment and matches s1.1-32B's performance while using significantly less\ncompute. Our results demonstrate that difficulty-aware routing is effective for\ncost-efficient deployment of reasoning models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u7531\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u5206\u914d\u7ed9\u53ef\u80fd\u89e3\u51b3\u5b83\u7684\u6700\u5c0f\u6a21\u578b\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u89c4\u6a21\u548c\u957f\u63a8\u7406\u8f68\u8ff9\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528s1.1-32B\u7684\u4e2d\u95f4\u8868\u793a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\uff0c\u9884\u6d4b\u95ee\u9898\u96be\u5ea6\u6216\u6a21\u578b\u6b63\u786e\u6027\uff0c\u6307\u5bfc\u5728\u63a8\u7406\u6a21\u578b\u6c60\u4e2d\u7684\u8def\u7531\u5206\u914d\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8def\u7531\u65b9\u6cd5\u6bd4\u968f\u673a\u5206\u914d\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5339\u914ds1.1-32B\u7684\u6027\u80fd\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "conclusion": "\u96be\u5ea6\u611f\u77e5\u8def\u7531\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\u7684\u6210\u672c\u9ad8\u6548\u90e8\u7f72\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.04053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04053", "abs": "https://arxiv.org/abs/2511.04053", "authors": ["Hirohane Takagi", "Gouki Minegishi", "Shota Kizawa", "Issey Sukeda", "Hitomi Yanaka"], "title": "Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models", "comment": "Accepted to IJCNLP-AACL 2025 (Main). Code available at\n  https://github.com/htkg/num_attrs", "summary": "Although behavioral studies have documented numerical reasoning errors in\nlarge language models (LLMs), the underlying representational mechanisms remain\nunclear. We hypothesize that numerical attributes occupy shared latent\nsubspaces and investigate two questions:(1) How do LLMs internally integrate\nmultiple numerical attributes of a single entity? (2)How does irrelevant\nnumerical context perturb these representations and their downstream outputs?\nTo address these questions, we combine linear probing with partial correlation\nanalysis and prompt-based vulnerability tests across models of varying sizes.\nOur results show that LLMs encode real-world numerical correlations but tend to\nsystematically amplify them. Moreover, irrelevant context induces consistent\nshifts in magnitude representations, with downstream effects that vary by model\nsize. These findings reveal a vulnerability in LLM decision-making and lay the\ngroundwork for fairer, representation-aware control under multi-attribute\nentanglement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u503c\u63a8\u7406\u4e2d\u7684\u5185\u90e8\u8868\u793a\u673a\u5236\uff0c\u53d1\u73b0LLMs\u4f1a\u7f16\u7801\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u503c\u76f8\u5173\u6027\u4f46\u7cfb\u7edf\u6027\u5730\u653e\u5927\u5b83\u4eec\uff0c\u65e0\u5173\u7684\u6570\u503c\u4e0a\u4e0b\u6587\u4f1a\u5bfc\u81f4\u8868\u793a\u504f\u79fb\uff0c\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86LLM\u51b3\u7b56\u4e2d\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5c3d\u7ba1\u884c\u4e3a\u7814\u7a76\u5df2\u8bb0\u5f55\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u503c\u63a8\u7406\u4e2d\u7684\u9519\u8bef\uff0c\u4f46\u6f5c\u5728\u7684\u8868\u793a\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5047\u8bbe\u6570\u503c\u5c5e\u6027\u5360\u636e\u5171\u4eab\u7684\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u5e76\u63a2\u8ba8LLMs\u5982\u4f55\u5185\u90e8\u6574\u5408\u5355\u4e2a\u5b9e\u4f53\u7684\u591a\u4e2a\u6570\u503c\u5c5e\u6027\uff0c\u4ee5\u53ca\u65e0\u5173\u6570\u503c\u4e0a\u4e0b\u6587\u5982\u4f55\u5e72\u6270\u8fd9\u4e9b\u8868\u793a\u53ca\u5176\u4e0b\u6e38\u8f93\u51fa\u3002", "method": "\u7ed3\u5408\u7ebf\u6027\u63a2\u6d4b\u4e0e\u504f\u76f8\u5173\u5206\u6790\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u8fdb\u884c\u57fa\u4e8e\u63d0\u793a\u7684\u8106\u5f31\u6027\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793aLLMs\u7f16\u7801\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u503c\u76f8\u5173\u6027\u4f46\u503e\u5411\u4e8e\u7cfb\u7edf\u6027\u5730\u653e\u5927\u5b83\u4eec\u3002\u6b64\u5916\uff0c\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u5bfc\u81f4\u5e45\u5ea6\u8868\u793a\u7684\u4e00\u81f4\u504f\u79fb\uff0c\u5176\u4e0b\u6e38\u5f71\u54cd\u56e0\u6a21\u578b\u89c4\u6a21\u800c\u5f02\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86LLM\u51b3\u7b56\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5e76\u4e3a\u5728\u591a\u5c5e\u6027\u7ea0\u7f20\u4e0b\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u8868\u793a\u611f\u77e5\u7684\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.03831", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03831", "abs": "https://arxiv.org/abs/2511.03831", "authors": ["James Enouen", "Yujia Zheng", "Ignavier Ng", "Yan Liu", "Kun Zhang"], "title": "Higher-Order Causal Structure Learning with Additive Models", "comment": null, "summary": "Causal structure learning has long been the central task of inferring causal\ninsights from data. Despite the abundance of real-world processes exhibiting\nhigher-order mechanisms, however, an explicit treatment of interactions in\ncausal discovery has received little attention. In this work, we focus on\nextending the causal additive model (CAM) to additive models with higher-order\ninteractions. This second level of modularity we introduce to the structure\nlearning problem is most easily represented by a directed acyclic hypergraph\nwhich extends the DAG. We introduce the necessary definitions and theoretical\ntools to handle the novel structure we introduce and then provide\nidentifiability results for the hyper DAG, extending the typical Markov\nequivalence classes. We next provide insights into why learning the more\ncomplex hypergraph structure may actually lead to better empirical results. In\nparticular, more restrictive assumptions like CAM correspond to easier-to-learn\nhyper DAGs and better finite sample complexity. We finally develop an extension\nof the greedy CAM algorithm which can handle the more complex hyper DAG search\nspace and demonstrate its empirical usefulness in synthetic experiments.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u56e0\u679c\u52a0\u6027\u6a21\u578b(CAM)\u4ee5\u5904\u7406\u9ad8\u9636\u4ea4\u4e92\u4f5c\u7528\uff0c\u5f15\u5165\u4e86\u6709\u5411\u65e0\u73af\u8d85\u56fe\u6765\u8868\u793a\u8fd9\u79cd\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u8d85DAG\u7684\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bb8\u591a\u8fc7\u7a0b\u90fd\u8868\u73b0\u51fa\u9ad8\u9636\u673a\u5236\uff0c\u4f46\u56e0\u679c\u53d1\u73b0\u4e2d\u5bf9\u4ea4\u4e92\u4f5c\u7528\u7684\u663e\u5f0f\u5904\u7406\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u5c06\u56e0\u679c\u52a0\u6027\u6a21\u578b\u6269\u5c55\u5230\u5305\u542b\u9ad8\u9636\u4ea4\u4e92\u7684\u52a0\u6027\u6a21\u578b\u3002", "method": "\u5f15\u5165\u6709\u5411\u65e0\u73af\u8d85\u56fe\u6765\u8868\u793a\u9ad8\u9636\u4ea4\u4e92\u7ed3\u6784\uff0c\u63d0\u4f9b\u5fc5\u8981\u7684\u5b9a\u4e49\u548c\u7406\u8bba\u5de5\u5177\uff0c\u6269\u5c55\u8d2a\u5a6aCAM\u7b97\u6cd5\u6765\u5904\u7406\u66f4\u590d\u6742\u7684\u8d85DAG\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u83b7\u5f97\u4e86\u8d85DAG\u7684\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u6269\u5c55\u4e86\u5178\u578b\u7684\u9a6c\u5c14\u53ef\u592b\u7b49\u4ef7\u7c7b\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u5b66\u4e60\u66f4\u590d\u6742\u7684\u8d85\u56fe\u7ed3\u6784\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ecf\u9a8c\u7ed3\u679c\uff0c\u7279\u522b\u662f\u66f4\u4e25\u683c\u7684\u5047\u8bbe\u5bf9\u5e94\u7740\u66f4\u5bb9\u6613\u5b66\u4e60\u7684\u8d85DAG\u548c\u66f4\u597d\u7684\u6709\u9650\u6837\u672c\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5c06\u56e0\u679c\u52a0\u6027\u6a21\u578b\u6269\u5c55\u5230\u9ad8\u9636\u4ea4\u4e92\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u6709\u5411\u65e0\u73af\u8d85\u56fe\u8868\u793a\uff0c\u672c\u6587\u4e3a\u5904\u7406\u590d\u6742\u56e0\u679c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.04362", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04362", "abs": "https://arxiv.org/abs/2511.04362", "authors": ["Chiara Telli", "Oleg Antropov", "Anne L\u00f6nnqvist", "Marco Lavalle"], "title": "High-Resolution Forest Mapping from L-Band Interferometric SAR Time Series using Deep Learning over Northern Spain", "comment": null, "summary": "In this study, we examine the potential of high-resolution forest mapping\nusing L-band interferometric time series datasets and deep learning modeling.\nOur SAR data are represented by a time series of nine ALOS-2 PALSAR-2 dual-pol\nSAR images acquired at near-zero spatial baseline over a study site in\nAsturias, Northern Spain. Reference data are collected using airborne laser\nscanning. We examine the performance of several candidate deep learning models\nfrom UNet-family with various combinations of input polarimetric and\ninterferometric features. In addition to basic Vanilla UNet, attention\nreinforced UNet model with squeeze-excitation blocks (SeU-Net) and advanced\nUNet model with nested structure and skip pathways are used. Studied features\ninclude dual pol interferometric observables additionally incorporating\nmodel-based derived measures. Results show that adding model-based inverted\nInSAR features or InSAR coherence layers improves retrieval accuracy compared\nto using backscatter intensity only. Use of attention mechanisms and nested\nconnection fusion provides better predictions than using Vanilla UNet or\ntraditional machine learning methods. Forest height retrieval accuracies range\nbetween 3.1-3.8 m (R2 = 0.45--0.55) at 20 m resolution when only intensity data\nare used, and improve to less than 2.8 m when both intensity and\ninterferometric coherence features are included. At 40 m and 60 m resolution,\nretrieval performance further improves, primarily due to higher SNR in both the\nintensity and interferometric layers. When using intensity at 60 m resolution,\nbest achieved RMSE is 2.2 m, while when using all suitable input features the\nachieved error is 1.95 m. We recommend this hybrid approach for L-band SAR\nretrievals also suitable for NISAR and future ROSE-L missions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528L\u6ce2\u6bb5\u5e72\u6d89\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540cUNet\u67b6\u6784\u548c\u8f93\u5165\u7279\u5f81\u7ec4\u5408\uff0c\u53d1\u73b0\u7ed3\u5408\u5f3a\u5ea6\u6570\u636e\u548c\u5e72\u6d89\u76f8\u5e72\u6027\u7279\u5f81\u80fd\u663e\u8457\u63d0\u9ad8\u68ee\u6797\u9ad8\u5ea6\u53cd\u6f14\u7cbe\u5ea6\u3002", "motivation": "\u5229\u7528L\u6ce2\u6bb5\u5e72\u6d89\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6539\u8fdb\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\uff0c\u4e3aNISAR\u548c\u672a\u6765ROSE-L\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u7684\u68ee\u6797\u9ad8\u5ea6\u53cd\u6f14\u65b9\u6cd5\u3002", "method": "\u4f7f\u75289\u5e45ALOS-2 PALSAR-2\u53cc\u6781\u5316SAR\u56fe\u50cf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6bd4\u8f83\u4e86Vanilla UNet\u3001\u5e26\u6ce8\u610f\u529b\u673a\u5236\u7684SeU-Net\u548c\u5d4c\u5957\u7ed3\u6784UNet\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u4e0d\u540c\u6781\u5316\u3001\u5e72\u6d89\u7279\u5f81\u7ec4\u5408\u8fdb\u884c\u68ee\u6797\u9ad8\u5ea6\u53cd\u6f14\u3002", "result": "\u4ec5\u4f7f\u7528\u5f3a\u5ea6\u6570\u636e\u65f6\uff0c20\u7c73\u5206\u8fa8\u7387\u7684\u68ee\u6797\u9ad8\u5ea6\u53cd\u6f14\u7cbe\u5ea6\u4e3a3.1-3.8\u7c73\uff08R2=0.45-0.55\uff09\uff1b\u7ed3\u5408\u5f3a\u5ea6\u4e0e\u5e72\u6d89\u76f8\u5e72\u6027\u7279\u5f81\u540e\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u81f3\u5c0f\u4e8e2.8\u7c73\u3002\u572860\u7c73\u5206\u8fa8\u7387\u4e0b\uff0c\u4ec5\u4f7f\u7528\u5f3a\u5ea6\u7684\u6700\u4f73RMSE\u4e3a2.2\u7c73\uff0c\u4f7f\u7528\u6240\u6709\u5408\u9002\u8f93\u5165\u7279\u5f81\u65f6\u8bef\u5dee\u964d\u81f31.95\u7c73\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u8fd9\u79cd\u7ed3\u5408\u5f3a\u5ea6\u6570\u636e\u548c\u5e72\u6d89\u76f8\u5e72\u6027\u7279\u5f81\u7684\u6df7\u5408\u65b9\u6cd5\u8fdb\u884cL\u6ce2\u6bb5SAR\u53cd\u6f14\uff0c\u8be5\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8eNISAR\u548c\u672a\u6765ROSE-L\u4efb\u52a1\u3002"}}
{"id": "2511.04220", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04220", "abs": "https://arxiv.org/abs/2511.04220", "authors": ["Alan Seroul", "Th\u00e9o Fagnoni", "In\u00e8s Adnani", "Dana O. Mohamed", "Phillip Kingston"], "title": "Opus: A Quantitative Framework for Workflow Evaluation", "comment": null, "summary": "This paper introduces the Opus Workflow Evaluation Framework, a\nprobabilistic-normative formulation for quantifying Workflow quality and\nefficiency. It integrates notions of correctness, reliability, and cost into a\ncoherent mathematical model that enables direct comparison, scoring, and\noptimization of Workflows. The framework combines the Opus Workflow Reward, a\nprobabilistic function estimating expected performance through success\nlikelihood, resource usage, and output gain, with the Opus Workflow Normative\nPenalties, a set of measurable functions capturing structural and informational\nquality across Cohesion, Coupling, Observability, and Information Hygiene. It\nsupports automated Workflow assessment, ranking, and optimization within modern\nautomation systems such as Opus and can be integrated into Reinforcement\nLearning loops to guide Workflow discovery and refinement. In this paper, we\nintroduce the Opus Workflow Reward model that formalizes Workflow success as a\nprobabilistic expectation over costs and outcomes. We define measurable Opus\nWorkflow Normative Penalties capturing structural, semantic, and signal-related\nproperties of Workflows. Finally, we propose a unified optimization formulation\nfor identifying and ranking optimal Workflows under joint Reward-Penalty\ntrade-offs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Opus\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u7387-\u89c4\u8303\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5de5\u4f5c\u6d41\u8d28\u91cf\u548c\u6548\u7387\uff0c\u7ed3\u5408\u4e86\u6b63\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u6210\u672c\u8bc4\u4f30\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u652f\u6301\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u8bc4\u4f30\u3001\u6392\u540d\u548c\u4f18\u5316\u3002", "method": "\u7ed3\u5408Opus\u5de5\u4f5c\u6d41\u5956\u52b1\uff08\u6982\u7387\u51fd\u6570\u4f30\u8ba1\u9884\u671f\u6027\u80fd\uff09\u548cOpus\u5de5\u4f5c\u6d41\u89c4\u8303\u60e9\u7f5a\uff08\u6d4b\u91cf\u7ed3\u6784\u4fe1\u606f\u8d28\u91cf\uff09\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u6570\u5b66\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u5de5\u4f5c\u6d41\u81ea\u52a8\u8bc4\u4f30\u3001\u6392\u540d\u548c\u4f18\u5316\u7684\u6846\u67b6\uff0c\u53ef\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u6307\u5bfc\u5de5\u4f5c\u6d41\u53d1\u73b0\u548c\u4f18\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u4f5c\u6d41\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\uff0c\u652f\u6301\u5728\u5956\u52b1-\u60e9\u7f5a\u6743\u8861\u4e0b\u8bc6\u522b\u548c\u6392\u540d\u6700\u4f18\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2511.04235", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.04235", "abs": "https://arxiv.org/abs/2511.04235", "authors": ["Zhengru Fang", "Yu Guo", "Jingjing Wang", "Yuang Zhang", "Haonan An", "Yinhai Wang", "Yuguang Fang"], "title": "Shared Spatial Memory Through Predictive Coding", "comment": "We have prepared the open-source code and video demonstration pages:\n  1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html", "summary": "Sharing and reconstructing a consistent spatial memory is a critical\nchallenge in multi-agent systems, where partial observability and limited\nbandwidth often lead to catastrophic failures in coordination. We introduce a\nmulti-agent predictive coding framework that formulate coordination as the\nminimization of mutual uncertainty among agents. Instantiated as an information\nbottleneck objective, it prompts agents to learn not only who and what to\ncommunicate but also when. At the foundation of this framework lies a\ngrid-cell-like metric as internal spatial coding for self-localization,\nemerging spontaneously from self-supervised motion prediction. Building upon\nthis internal spatial code, agents gradually develop a bandwidth-efficient\ncommunication mechanism and specialized neural populations that encode\npartners' locations: an artificial analogue of hippocampal social place cells\n(SPCs). These social representations are further enacted by a hierarchical\nreinforcement learning policy that actively explores to reduce joint\nuncertainty. On the Memory-Maze benchmark, our approach shows exceptional\nresilience to bandwidth constraints: success degrades gracefully from 73.5% to\n64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast\nbaseline collapses from 67.6% to 28.6%. Our findings establish a theoretically\nprincipled and biologically plausible basis for how complex social\nrepresentations emerge from a unified predictive drive, leading to social\ncollective intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u667a\u80fd\u4f53\u95f4\u7684\u4e92\u4e0d\u786e\u5b9a\u6027\u6765\u89e3\u51b3\u534f\u8c03\u95ee\u9898\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u7f51\u683c\u7ec6\u80de\u72b6\u7684\u7a7a\u95f4\u7f16\u7801\uff0c\u81ea\u53d1\u4ea7\u751f\u793e\u4ea4\u4f4d\u7f6e\u7ec6\u80de\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u6709\u9650\u5e26\u5bbd\u5e38\u5e38\u5bfc\u81f4\u534f\u8c03\u5931\u8d25\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5171\u4eab\u548c\u91cd\u5efa\u4e00\u81f4\u7a7a\u95f4\u8bb0\u5fc6\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u5c06\u534f\u8c03\u95ee\u9898\u516c\u5f0f\u5316\u4e3a\u4e92\u4e0d\u786e\u5b9a\u6027\u6700\u5c0f\u5316\uff0c\u4f7f\u7528\u4fe1\u606f\u74f6\u9888\u76ee\u6807\u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u4f55\u65f6\u3001\u4e0e\u8c01\u3001\u4ee5\u53ca\u5982\u4f55\u901a\u4fe1\u3002\u57fa\u4e8e\u81ea\u76d1\u7763\u8fd0\u52a8\u9884\u6d4b\u4ea7\u751f\u7684\u7f51\u683c\u7ec6\u80de\u72b6\u7a7a\u95f4\u7f16\u7801\uff0c\u667a\u80fd\u4f53\u9010\u6b65\u53d1\u5c55\u51fa\u5e26\u5bbd\u9ad8\u6548\u7684\u901a\u4fe1\u673a\u5236\u548c\u793e\u4ea4\u4f4d\u7f6e\u7ec6\u80de\u3002", "result": "\u5728Memory-Maze\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5e26\u5bbd\u9650\u5236\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u97e7\u6027\uff1a\u5f53\u5e26\u5bbd\u4ece128\u4f4d/\u6b65\u7f29\u51cf\u52304\u4f4d/\u6b65\u65f6\uff0c\u6210\u529f\u7387\u4ece73.5%\u4f18\u96c5\u4e0b\u964d\u523064.4%\uff0c\u800c\u5168\u5e7f\u64ad\u57fa\u7ebf\u4ece67.6%\u5d29\u6e83\u523028.6%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u590d\u6742\u793e\u4ea4\u8868\u5f81\u5982\u4f55\u4ece\u7edf\u4e00\u7684\u9884\u6d4b\u9a71\u52a8\u4e2d\u6d8c\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u539f\u5219\u548c\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u57fa\u7840\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u793e\u4ea4\u96c6\u4f53\u667a\u80fd\u3002"}}
{"id": "2511.04312", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04312", "abs": "https://arxiv.org/abs/2511.04312", "authors": ["Jacob Lysn\u00e6s-Larsen", "Marte Eggen", "Inga Str\u00fcmke"], "title": "Probing the Probes: Methods and Metrics for Concept Alignment", "comment": "29 pages, 17 figures", "summary": "In explainable AI, Concept Activation Vectors (CAVs) are typically obtained\nby training linear classifier probes to detect human-understandable concepts as\ndirections in the activation space of deep neural networks. It is widely\nassumed that a high probe accuracy indicates a CAV faithfully representing its\ntarget concept. However, we show that the probe's classification accuracy alone\nis an unreliable measure of concept alignment, i.e., the degree to which a CAV\ncaptures the intended concept. In fact, we argue that probes are more likely to\ncapture spurious correlations than they are to represent only the intended\nconcept. As part of our analysis, we demonstrate that deliberately misaligned\nprobes constructed to exploit spurious correlations, achieve an accuracy close\nto that of standard probes. To address this severe problem, we introduce a\nnovel concept localization method based on spatial linear attribution, and\nprovide a comprehensive comparison of it to existing feature visualization\ntechniques for detecting and mitigating concept misalignment. We further\npropose three classes of metrics for quantitatively assessing concept\nalignment: hard accuracy, segmentation scores, and augmentation robustness. Our\nanalysis shows that probes with translation invariance and spatial alignment\nconsistently increase concept alignment. These findings highlight the need for\nalignment-based evaluation metrics rather than probe accuracy, and the\nimportance of tailoring probes to both the model architecture and the nature of\nthe target concept.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5728\u53ef\u89e3\u91caAI\u4e2d\uff0c\u4ec5\u51ed\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08CAV\uff09\u63a2\u9488\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e0d\u80fd\u53ef\u9760\u8861\u91cf\u6982\u5ff5\u5bf9\u9f50\u5ea6\uff0c\u56e0\u4e3a\u63a2\u9488\u66f4\u5bb9\u6613\u6355\u6349\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u6982\u5ff5\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u7a7a\u95f4\u7ebf\u6027\u5f52\u56e0\u7684\u65b0\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u7c7b\u5b9a\u91cf\u8bc4\u4f30\u6982\u5ff5\u5bf9\u9f50\u7684\u6307\u6807\u3002", "motivation": "\u5f53\u524dCAV\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u63a2\u9488\u5206\u7c7b\u51c6\u786e\u7387\u6765\u8bc4\u4f30\u6982\u5ff5\u5bf9\u9f50\u5ea6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u63a2\u9488\u53ef\u80fd\u6355\u6349\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u6982\u5ff5\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fddCAV\u771f\u6b63\u4ee3\u8868\u76ee\u6807\u6982\u5ff5\u3002", "method": "1\uff09\u5206\u6790\u6807\u51c6\u63a2\u9488\u4e0e\u6545\u610f\u9519\u4f4d\u63a2\u9488\u7684\u6027\u80fd\u5dee\u5f02\uff1b2\uff09\u63d0\u51fa\u57fa\u4e8e\u7a7a\u95f4\u7ebf\u6027\u5f52\u56e0\u7684\u65b0\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\uff1b3\uff09\u5f15\u5165\u4e09\u7c7b\u6982\u5ff5\u5bf9\u9f50\u8bc4\u4f30\u6307\u6807\uff1a\u786c\u51c6\u786e\u7387\u3001\u5206\u5272\u5206\u6570\u548c\u589e\u5f3a\u9c81\u68d2\u6027\uff1b4\uff09\u6bd4\u8f83\u73b0\u6709\u7279\u5f81\u53ef\u89c6\u5316\u6280\u672f\u3002", "result": "\u6545\u610f\u9519\u4f4d\u63a2\u9488\u5229\u7528\u865a\u5047\u76f8\u5173\u6027\u4e5f\u80fd\u8fbe\u5230\u63a5\u8fd1\u6807\u51c6\u63a2\u9488\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4ec5\u51ed\u51c6\u786e\u7387\u4e0d\u53ef\u9760\u3002\u5177\u6709\u5e73\u79fb\u4e0d\u53d8\u6027\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u63a2\u9488\u80fd\u6301\u7eed\u63d0\u9ad8\u6982\u5ff5\u5bf9\u9f50\u5ea6\u3002\u65b0\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u80fd\u66f4\u53ef\u9760\u5730\u68c0\u6d4b\u6982\u5ff5\u5bf9\u9f50\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u5bf9\u9f50\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\u800c\u975e\u63a2\u9488\u51c6\u786e\u7387\uff0c\u63a2\u9488\u8bbe\u8ba1\u5e94\u8003\u8651\u6a21\u578b\u67b6\u6784\u548c\u76ee\u6807\u6982\u5ff5\u7279\u6027\u3002\u7a7a\u95f4\u5bf9\u9f50\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u5bf9\u63d0\u9ad8\u6982\u5ff5\u5bf9\u9f50\u5ea6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.03929", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03929", "abs": "https://arxiv.org/abs/2511.03929", "authors": ["NVIDIA", ":", "Amala Sanjay Deshmukh", "Kateryna Chumachenko", "Tuomas Rintamaki", "Matthieu Le", "Tyler Poon", "Danial Mohseni Taheri", "Ilia Karmanov", "Guilin Liu", "Jarno Seppanen", "Guo Chen", "Karan Sapra", "Zhiding Yu", "Adi Renduchintala", "Charles Wang", "Peter Jin", "Arushi Goel", "Mike Ranzinger", "Lukas Voegtle", "Philipp Fischer", "Timo Roman", "Wei Ping", "Boxin Wang", "Zhuolin Yang", "Nayeon Lee", "Shaokun Zhang", "Fuxiao Liu", "Zhiqi Li", "Di Zhang", "Greg Heinrich", "Hongxu", "Yin", "Song Han", "Pavlo Molchanov", "Parth Mannan", "Yao Xu", "Jane Polak Scowcroft", "Tom Balough", "Subhashree Radhakrishnan", "Paris Zhang", "Sean Cha", "Ratnesh Kumar", "Zaid Pervaiz Bhat", "Jian Zhang", "Darragh Hanley", "Pritam Biswas", "Jesse Oliver", "Kevin Vasques", "Roger Waleffe", "Duncan Riach", "Oluwatobi Olabiyi", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Pritam Gundecha", "Khanh Nguyen", "Alexandre Milesi", "Eugene Khvedchenia", "Ran Zilberstein", "Ofri Masad", "Natan Bagrov", "Nave Assaf", "Tomer Asida", "Daniel Afrimi", "Amit Zuker", "Netanel Haber", "Zhiyu Cheng", "Jingyu", "Xin", "Di", "Wu", "Nik Spirin", "Maryam Moosaei", "Roman Ageev", "Vanshil Atul Shah", "Yuting Wu", "Daniel Korzekwa", "Unnikrishnan Kizhakkemadam Sreekumar", "Wanli Jiang", "Padmavathy Subramanian", "Alejandra Rico", "Sandip Bhaskar", "Saeid Motiian", "Kedi Wu", "Annie Surla", "Chia-Chih Chen", "Hayden Wolff", "Matthew Feinberg", "Melissa Corpuz", "Marek Wawrzos", "Eileen Long", "Aastha Jhunjhunwala", "Paul Hendricks", "Farzan Memarian", "Benika Hall", "Xin-Yu Wang", "David Mosallanezhad", "Soumye Singhal", "Luis Vega", "Katherine Cheung", "Krzysztof Pawelec", "Michael Evans", "Katherine Luna", "Jie Lou", "Erick Galinkin", "Akshay Hazare", "Kaustubh Purandare", "Ann Guan", "Anna Warno", "Chen Cui", "Yoshi Suhara", "Shibani Likhite", "Seph Mard", "Meredith Price", "Laya Sleiman", "Saori Kaji", "Udi Karpas", "Kari Briski", "Joey Conway", "Michael Lightstone", "Jan Kautz", "Mohammad Shoeybi", "Mostofa Patwary", "Jonathen Cohen", "Oleksii Kuchaiev", "Andrew Tao", "Bryan Catanzaro"], "title": "NVIDIA Nemotron Nano V2 VL", "comment": null, "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.", "AI": {"tldr": "Nemotron Nano V2 VL\u662f\u6700\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u6587\u6863\u7406\u89e3\u3001\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u524d\u4ee3\u6a21\u578b\u5728\u89c6\u89c9\u548c\u6587\u672c\u9886\u57df\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u91c7\u7528\u6df7\u5408Mamba-Transformer\u67b6\u6784\u548c\u521b\u65b0\u7684token\u51cf\u5c11\u6280\u672f\u6765\u63d0\u9ad8\u957f\u6587\u6863\u548c\u89c6\u9891\u573a\u666f\u7684\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7406\u89e3\u3001\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u6765\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "method": "\u57fa\u4e8eNemotron Nano V2\uff08\u6df7\u5408Mamba-Transformer LLM\uff09\uff0c\u91c7\u7528\u521b\u65b0\u7684token\u51cf\u5c11\u6280\u672f\uff0c\u4f18\u5316\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u914d\u65b9\u3002", "result": "\u76f8\u6bd4\u524d\u4ee3\u6a21\u578bLlama-3.1-Nemotron-Nano-VL-8B\uff0c\u5728\u6240\u6709\u89c6\u89c9\u548c\u6587\u672c\u9886\u57df\u90fd\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5728\u957f\u6587\u6863\u548c\u89c6\u9891\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u63a8\u7406\u541e\u5410\u91cf\u3002", "conclusion": "Nemotron Nano V2 VL\u662f\u4e00\u4e2a\u6027\u80fd\u663e\u8457\u63d0\u5347\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u9002\u5408\u6587\u6863\u7406\u89e3\u548c\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u56e2\u961f\u5c06\u53d1\u5e03BF16\u3001FP8\u548cFP4\u683c\u5f0f\u7684\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u5e76\u5206\u4eab\u5927\u90e8\u5206\u6570\u636e\u96c6\u3001\u914d\u65b9\u548c\u8bad\u7ec3\u4ee3\u7801\u3002"}}
{"id": "2511.04316", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04316", "abs": "https://arxiv.org/abs/2511.04316", "authors": ["Tim Beyer", "Jonas Dornbusch", "Jakob Steimle", "Moritz Ladenburger", "Leo Schwinn", "Stephan G\u00fcnnemann"], "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research", "comment": null, "summary": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety.", "AI": {"tldr": "AdversariaLLM\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u8d8a\u72f1\u9c81\u68d2\u6027\u7814\u7a76\u7684\u5de5\u5177\u7bb1\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dLLM\u5b89\u5168\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u788e\u7247\u5316\u3001\u53ef\u590d\u73b0\u6027\u5dee\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u548c\u9c81\u68d2\u6027\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u788e\u7247\u5316\u3001bug\u591a\u3001\u5b9e\u73b0\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7814\u7a76\u7ed3\u679c\u96be\u4ee5\u590d\u73b0\u548c\u6bd4\u8f83\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u5b9e\u8d28\u6027\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ee5\u53ef\u590d\u73b0\u6027\u3001\u6b63\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e3a\u6838\u5fc3\u7684\u5de5\u5177\u7bb1\uff0c\u5b9e\u73b0\u4e8612\u79cd\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff0c\u96c6\u6210\u4e867\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6709\u5bb3\u6027\u3001\u8fc7\u5ea6\u62d2\u7edd\u548c\u6548\u7528\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7Hugging Face\u63d0\u4f9b\u5bf9\u591a\u79cd\u5f00\u6e90LLM\u7684\u8bbf\u95ee\u3002", "result": "\u6846\u67b6\u63d0\u4f9b\u4e86\u8ba1\u7b97\u8d44\u6e90\u8ddf\u8e2a\u3001\u786e\u5b9a\u6027\u7ed3\u679c\u548c\u5206\u5e03\u8bc4\u4f30\u6280\u672f\u7b49\u9ad8\u7ea7\u529f\u80fd\uff0c\u786e\u4fdd\u53ef\u6bd4\u6027\u548c\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u4e0eJudgeZoo\u96c6\u6210\u8fdb\u884c\u8bc4\u5224\u3002", "conclusion": "AdversariaLLM\u4e3aLLM\u5b89\u5168\u7814\u7a76\u5efa\u7acb\u4e86\u900f\u660e\u3001\u53ef\u6bd4\u548c\u53ef\u590d\u73b0\u7684\u575a\u5b9e\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2511.03938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03938", "abs": "https://arxiv.org/abs/2511.03938", "authors": ["Sanggeon Yun", "Hyunwoo Oh", "Ryozo Masukawa", "Pietro Mercati", "Nathaniel D. Bastian", "Mohsen Imani"], "title": "LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction", "comment": "Accepted to DATE 2026", "summary": "Hyperdimensional computing (HDC) suits memory, energy, and\nreliability-constrained systems, yet the standard \"one prototype per class\"\ndesign requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior\ncompaction reduces $D$ (feature axis), improving storage/compute but weakening\nrobustness. We introduce LogHD, a logarithmic class-axis reduction that\nreplaces the $C$ per-class prototypes with $n\\!\\approx\\!\\lceil\\log_k C\\rceil$\nbundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional\nactivation space, cutting memory to $O(D\\log_k C)$ while preserving $D$. LogHD\nuses a capacity-aware codebook and profile-based decoding, and composes with\nfeature-axis sparsification. Across datasets and injected bit flips, LogHD\nattains competitive accuracy with smaller models and higher resilience at\nmatched memory. Under equal memory, it sustains target accuracy at roughly\n$2.5$-$3.0\\times$ higher bit-flip rates than feature-axis compression; an ASIC\ninstantiation delivers $498\\times$ energy efficiency and $62.6\\times$ speedup\nover an AMD Ryzen 9 9950X and $24.3\\times$/$6.58\\times$ over an NVIDIA RTX\n4090, and is $4.06\\times$ more energy-efficient and $2.19\\times$ faster than a\nfeature-axis HDC ASIC baseline.", "AI": {"tldr": "LogHD\u662f\u4e00\u79cd\u8d85\u7ef4\u8ba1\u7b97\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u7c7b\u522b\u8f74\u8fdb\u884c\u5bf9\u6570\u538b\u7f29\uff0c\u5c06\u5185\u5b58\u9700\u6c42\u4eceO(CD)\u964d\u4f4e\u5230O(Dlog_k C)\uff0c\u540c\u65f6\u4fdd\u6301\u7ef4\u5ea6D\u4e0d\u53d8\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u6807\u51c6\u8d85\u7ef4\u8ba1\u7b97\u7684\"\u6bcf\u7c7b\u4e00\u4e2a\u539f\u578b\"\u8bbe\u8ba1\u9700\u8981O(CD)\u5185\u5b58\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u51cf\u5c11\u7279\u5f81\u7ef4\u5ea6D\uff0c\u4f46\u8fd9\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u538b\u7f29\u5185\u5b58\u53c8\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5bf9\u6570\u7c7b\u522b\u8f74\u538b\u7f29\uff0c\u7528n\u2248\u2308log_k C\u2309\u4e2a\u6346\u7ed1\u8d85\u5411\u91cf\u66ff\u6362C\u4e2a\u6bcf\u7c7b\u539f\u578b\uff0c\u5728n\u7ef4\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89e3\u7801\uff0c\u7ed3\u5408\u5bb9\u91cf\u611f\u77e5\u7801\u672c\u548c\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u53ef\u4e0e\u7279\u5f81\u8f74\u7a00\u758f\u5316\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6ce8\u5165\u4f4d\u7ffb\u8f6c\u7684\u60c5\u51b5\u4e0b\uff0cLogHD\u4ee5\u66f4\u5c0f\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u51c6\u786e\u6027\uff0c\u5728\u76f8\u540c\u5185\u5b58\u4e0b\u6bd4\u7279\u5f81\u8f74\u538b\u7f29\u65b9\u6cd5\u80fd\u627f\u53d72.5-3.0\u500d\u7684\u4f4d\u7ffb\u8f6c\u7387\u3002ASIC\u5b9e\u73b0\u76f8\u6bd4AMD Ryzen 9 9950X\u5b9e\u73b0\u4e86498\u500d\u80fd\u6548\u548c62.6\u500d\u52a0\u901f\uff0c\u76f8\u6bd4NVIDIA RTX 4090\u5b9e\u73b0\u4e8624.3\u500d/6.58\u500d\u63d0\u5347\u3002", "conclusion": "LogHD\u901a\u8fc7\u7c7b\u522b\u8f74\u7684\u5bf9\u6570\u538b\u7f29\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u7ef4\u8ba1\u7b97\u7684\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u786c\u4ef6\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03939", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03939", "abs": "https://arxiv.org/abs/2511.03939", "authors": ["Raghav Sharma", "Manan Mehta", "Sai Tiger Raina"], "title": "RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is the standard for\naligning Large Language Models (LLMs), yet recent progress has moved beyond\ncanonical text-based methods. This survey synthesizes the new frontier of\nalignment research by addressing critical gaps in multi-modal alignment,\ncultural fairness, and low-latency optimization. To systematically explore\nthese domains, we first review foundational algo- rithms, including PPO, DPO,\nand GRPO, before presenting a detailed analysis of the latest innovations. By\nproviding a comparative synthesis of these techniques and outlining open\nchallenges, this work serves as an essential roadmap for researchers building\nmore robust, efficient, and equitable AI systems.", "AI": {"tldr": "\u672c\u8c03\u67e5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8d85\u8d8a\u4f20\u7edf\u6587\u672c\u65b9\u6cd5\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7814\u7a76\u65b0\u524d\u6cbf\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u6587\u5316\u516c\u5e73\u6027\u548c\u4f4e\u5ef6\u8fdf\u4f18\u5316\u7b49\u5173\u952e\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u7684RLHF\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u6587\u5316\u504f\u89c1\u548c\u6548\u7387\u4f18\u5316\u7b49\u5173\u952e\u6311\u6218\uff0c\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u516c\u5e73\u7684AI\u7cfb\u7edf\u3002", "method": "\u9996\u5148\u56de\u987e\u57fa\u7840\u7b97\u6cd5\uff08PPO\u3001DPO\u3001GRPO\uff09\uff0c\u7136\u540e\u8be6\u7ec6\u5206\u6790\u6700\u65b0\u521b\u65b0\u6280\u672f\uff0c\u63d0\u4f9b\u6bd4\u8f83\u6027\u7efc\u5408\u5206\u6790\u548c\u5f00\u653e\u6311\u6218\u7684\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u5bf9\u9f50\u7814\u7a76\u7684\u65b0\u524d\u6cbf\u6846\u67b6\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u6587\u5316\u516c\u5e73\u6027\u548c\u4f4e\u5ef6\u8fdf\u4f18\u5316\u7b49\u5173\u952e\u9886\u57df\u7684\u6280\u672f\u8fdb\u5c55\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u7814\u7a76\u4eba\u5458\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u516c\u5e73\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8def\u7ebf\u56fe\uff0c\u6307\u51fa\u4e86\u672a\u6765\u5bf9\u9f50\u7814\u7a76\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2511.03981", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03981", "abs": "https://arxiv.org/abs/2511.03981", "authors": ["Yuxiao Wang", "Di Wu", "Feng Liu", "Zhimin Qiu", "Chenrui Hu"], "title": "Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models", "comment": null, "summary": "This paper proposes a composable fine-tuning method that integrates graph\nstructural priors with modular adapters to address the high computational cost\nand structural instability faced by large-scale pre-trained models in\nmulti-task adaptation. The method introduces a relation matrix to model\ndependencies among tasks, explicitly encoding correlations between nodes and\npaths into graph structural priors, which provide unified structural\nconstraints for adapter weight allocation and path selection. Modular adapters\nare embedded into different layers through low-rank mapping and a pluggable\nmechanism, enabling efficient cross-task composition and reuse under prior\nguidance. This mechanism not only improves parameter efficiency and training\nstability but also alleviates path conflicts and redundant computation in\nmulti-task scenarios. Furthermore, experiments on hyperparameter sensitivity,\nenvironmental sensitivity, and data sensitivity are conducted to systematically\nanalyze key factors such as routing temperature, gating thresholds, and\nrelation matrix regularization strength, verifying the consistency and superior\nperformance of the method under structural constraints. The results demonstrate\nthat the proposed framework significantly enhances task prediction accuracy,\nadapter weight allocation precision, and overall computational efficiency while\nmaintaining model lightweight design, highlighting the synergistic advantages\nof graph priors and modular mechanisms in composable fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7ec4\u5408\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5148\u9a8c\u548c\u6a21\u5757\u5316\u9002\u914d\u5668\u89e3\u51b3\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u65f6\u9762\u4e34\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u5173\u7cfb\u77e9\u9635\u5efa\u6a21\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u8282\u70b9\u548c\u8def\u5f84\u76f8\u5173\u6027\u7f16\u7801\u4e3a\u56fe\u7ed3\u6784\u5148\u9a8c\uff0c\u901a\u8fc7\u4f4e\u79e9\u6620\u5c04\u548c\u53ef\u63d2\u62d4\u673a\u5236\u5c06\u6a21\u5757\u5316\u9002\u914d\u5668\u5d4c\u5165\u4e0d\u540c\u5c42\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u4efb\u52a1\u7ec4\u5408\u548c\u91cd\u7528\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u9884\u6d4b\u7cbe\u5ea6\u3001\u9002\u914d\u5668\u6743\u91cd\u5206\u914d\u7cbe\u5ea6\u548c\u6574\u4f53\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u56fe\u5148\u9a8c\u548c\u6a21\u5757\u5316\u673a\u5236\u5728\u53ef\u7ec4\u5408\u5fae\u8c03\u4e2d\u5177\u6709\u534f\u540c\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u8def\u5f84\u51b2\u7a81\u548c\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002"}}
{"id": "2511.04662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04662", "abs": "https://arxiv.org/abs/2511.04662", "authors": ["Yu Feng", "Nathaniel Weir", "Kaj Bostrom", "Sam Bayless", "Darion Cassel", "Sapana Chaudhary", "Benjamin Kiesl-Reiter", "Huzefa Rangwala"], "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks", "comment": null, "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.", "AI": {"tldr": "VeriCoT\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u4ece\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u63d0\u53d6\u548c\u9a8c\u8bc1\u5f62\u5f0f\u903b\u8f91\u8bba\u8bc1\uff0c\u901a\u8fc7\u4e00\u9636\u903b\u8f91\u5f62\u5f0f\u5316\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u7528\u81ea\u52a8\u6c42\u89e3\u5668\u9a8c\u8bc1\u903b\u8f91\u6709\u6548\u6027\u3002", "motivation": "LLMs\u901a\u8fc7\u601d\u7ef4\u94fe\u8fdb\u884c\u591a\u6b65\u63a8\u7406\uff0c\u4f46\u65e0\u6cd5\u53ef\u9760\u9a8c\u8bc1\u81ea\u8eab\u903b\u8f91\uff0c\u5373\u4f7f\u5728\u6b63\u786e\u7b54\u6848\u4e0b\uff0c\u5e95\u5c42\u63a8\u7406\u53ef\u80fd\u5b58\u5728\u7f3a\u9677\uff0c\u5f71\u54cd\u9ad8\u98ce\u9669\u573a\u666f\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5c06\u6bcf\u4e2a\u601d\u7ef4\u94fe\u63a8\u7406\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e00\u7ea7\u903b\u8f91\uff0c\u8bc6\u522b\u57fa\u4e8e\u6e90\u4e0a\u4e0b\u6587\u3001\u5e38\u8bc6\u77e5\u8bc6\u6216\u5148\u524d\u63a8\u7406\u6b65\u9aa4\u7684\u524d\u63d0\uff0c\u4f7f\u7528\u7b26\u53f7\u8868\u793a\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u3002", "result": "\u5728ProofWriter\u3001LegalBench\u548cBioASQ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cVeriCoT\u80fd\u6709\u6548\u8bc6\u522b\u6709\u7f3a\u9677\u7684\u63a8\u7406\uff0c\u5e76\u4f5c\u4e3a\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684\u5f3a\u9884\u6d4b\u5668\u3002", "conclusion": "VeriCoT\u7684\u9a8c\u8bc1\u4fe1\u53f7\u53ef\u7528\u4e8e\u63a8\u7406\u65f6\u81ea\u6211\u53cd\u601d\u3001\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u5fae\u8c03\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u63a8\u7406\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.04063", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04063", "abs": "https://arxiv.org/abs/2511.04063", "authors": ["Yuantian Shao", "Yuanteng Chen", "Peisong Wang", "Jianlin Yu", "Jing Lin", "Yiwu Yao", "Zhihui Wei", "Jian Cheng"], "title": "DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization", "comment": "NeurIPS 2025, 10 pages, 12 figures", "summary": "Quantization plays a crucial role in accelerating the inference of\nlarge-scale models, and rotational matrices have been shown to effectively\nimprove quantization performance by smoothing outliers. However, end-to-end\nfine-tuning of rotational optimization algorithms incurs high computational\ncosts and is prone to overfitting. To address this challenge, we propose an\nefficient distribution-aware rotational calibration method, DartQuant, which\nreduces the complexity of rotational optimization by constraining the\ndistribution of the activations after rotation. This approach also effectively\nreduces reliance on task-specific losses, thereby mitigating the risk of\noverfitting. Additionally, we introduce the QR-Orth optimization scheme, which\nreplaces expensive alternating optimization with a more efficient solution. In\na variety of model quantization experiments, DartQuant demonstrates superior\nperformance. Compared to existing methods, it achieves 47$\\times$ acceleration\nand 10$\\times$ memory savings for rotational optimization on a 70B model.\nFurthermore, it is the first to successfully complete rotational calibration\nfor a 70B model on a single 3090 GPU, making quantization of large language\nmodels feasible in resource-constrained environments. Code is available at\nhttps://github.com/CAS-CLab/DartQuant.git.", "AI": {"tldr": "DartQuant\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5e03\u611f\u77e5\u65cb\u8f6c\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u65cb\u8f6c\u540e\u6fc0\u6d3b\u503c\u7684\u5206\u5e03\u6765\u964d\u4f4e\u65cb\u8f6c\u4f18\u5316\u7684\u590d\u6742\u5ea6\uff0c\u51cf\u5c11\u5bf9\u4efb\u52a1\u7279\u5b9a\u635f\u5931\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165QR-Orth\u4f18\u5316\u65b9\u6848\u66ff\u4ee3\u6602\u8d35\u7684\u4ea4\u66ff\u4f18\u5316\u3002\u572870B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8647\u500d\u52a0\u901f\u548c10\u500d\u5185\u5b58\u8282\u7701\uff0c\u9996\u6b21\u5728\u5355\u5f203090 GPU\u4e0a\u5b8c\u621070B\u6a21\u578b\u7684\u65cb\u8f6c\u6821\u51c6\u3002", "motivation": "\u91cf\u5316\u5728\u52a0\u901f\u5927\u89c4\u6a21\u6a21\u578b\u63a8\u7406\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u65cb\u8f6c\u77e9\u9635\u80fd\u6709\u6548\u6539\u5584\u91cf\u5316\u6027\u80fd\u4f46\u7aef\u5230\u7aef\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u9700\u8981\u89e3\u51b3\u65cb\u8f6c\u4f18\u5316\u7b97\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u611f\u77e5\u65cb\u8f6c\u6821\u51c6\u65b9\u6cd5DartQuant\uff0c\u901a\u8fc7\u7ea6\u675f\u65cb\u8f6c\u540e\u6fc0\u6d3b\u503c\u7684\u5206\u5e03\u6765\u964d\u4f4e\u65cb\u8f6c\u4f18\u5316\u590d\u6742\u5ea6\uff1b\u5f15\u5165QR-Orth\u4f18\u5316\u65b9\u6848\u66ff\u4ee3\u6602\u8d35\u7684\u4ea4\u66ff\u4f18\u5316\u3002", "result": "\u5728\u5404\u79cd\u6a21\u578b\u91cf\u5316\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u572870B\u6a21\u578b\u4e0a\u5b9e\u73b047\u500d\u52a0\u901f\u548c10\u500d\u5185\u5b58\u8282\u7701\uff0c\u9996\u6b21\u5728\u5355\u5f203090 GPU\u4e0a\u6210\u529f\u5b8c\u621070B\u6a21\u578b\u7684\u65cb\u8f6c\u6821\u51c6\u3002", "conclusion": "DartQuant\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u53d8\u5f97\u53ef\u884c\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04094", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04094", "abs": "https://arxiv.org/abs/2511.04094", "authors": ["Hyungjong Na", "Wonho Song", "Seungyong Han", "Donghyeon Jo", "Sejin Myung", "Hyungjoon Kim"], "title": "KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea", "comment": "18 pages, 3 figures, 8 tables. Submitted to Scientific Data;\n  currently under review. Data and codebook available at Zenodo (DOI:\n  10.5281/zenodo.17149808)", "summary": "This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term\npanel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011\nand 2024. After excluding financial firms, firms with non-December fiscal year\nends, capital impairment, and negative pre-tax income, the final dataset\nconsists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed\nto treat corporate tax avoidance as a predictor variable and link it to\nmultiple domains, including earnings management (accrual- and activity-based),\nprofitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,\nINVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance\nitself is measured using complementary indicators cash effective tax rate\n(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,\nTSDA) with adjustments to ensure interpretability. A key strength of KoTaP is\nits balanced panel structure with standardized variables and its consistency\nwith international literature on the distribution and correlation of core\nindicators. At the same time, it reflects distinctive institutional features of\nKorean firms, such as concentrated ownership, high foreign shareholding, and\nelevated liquidity ratios, providing both international comparability and\ncontextual uniqueness. KoTaP enables applications in benchmarking econometric\nand deep learning models, external validity checks, and explainable AI\nanalyses. It further supports policy evaluation, audit planning, and investment\nanalysis, making it a critical open resource for accounting, finance, and\ninterdisciplinary research.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u97e9\u56fd\u907f\u7a0e\u9762\u677f(KoTaP)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b2011-2024\u5e74KOSPI\u548cKOSDAQ\u4e0a\u5e02\u975e\u91d1\u878d\u4f01\u4e1a\u7684\u957f\u671f\u9762\u677f\u6570\u636e\u96c6\uff0c\u5305\u542b12,653\u4e2a\u516c\u53f8\u5e74\u5ea6\u89c2\u6d4b\u503c\u3002\u8be5\u6570\u636e\u96c6\u5c06\u4f01\u4e1a\u907f\u7a0e\u4f5c\u4e3a\u9884\u6d4b\u53d8\u91cf\uff0c\u4e0e\u76c8\u4f59\u7ba1\u7406\u3001\u76c8\u5229\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u3001\u589e\u957f\u6027\u548c\u516c\u53f8\u6cbb\u7406\u7b49\u591a\u4e2a\u9886\u57df\u76f8\u5173\u8054\u3002", "motivation": "\u521b\u5efa\u6807\u51c6\u5316\u7684\u957f\u671f\u9762\u677f\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u4f01\u4e1a\u907f\u7a0e\u884c\u4e3a\u53ca\u5176\u4e0e\u5176\u4ed6\u516c\u53f8\u7279\u5f81\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u6301\u56fd\u9645\u53ef\u6bd4\u6027\u548c\u53cd\u6620\u97e9\u56fd\u7279\u6709\u7684\u5236\u5ea6\u7279\u5f81\u3002", "method": "\u6784\u5efa\u5e73\u8861\u9762\u677f\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u73b0\u91d1\u6709\u6548\u7a0e\u7387(CETR)\u3001GAAP\u6709\u6548\u7a0e\u7387(GETR)\u548c\u8d26\u9762\u7a0e\u6536\u5dee\u5f02(TSTA\u3001TSDA)\u7b49\u591a\u79cd\u4e92\u8865\u6307\u6807\u6765\u8861\u91cf\u4f01\u4e1a\u907f\u7a0e\uff0c\u5e76\u6807\u51c6\u5316\u6240\u6709\u53d8\u91cf\u3002", "result": "KoTaP\u6570\u636e\u96c6\u5177\u6709\u5e73\u8861\u9762\u677f\u7ed3\u6784\uff0c\u6838\u5fc3\u6307\u6807\u7684\u5206\u5e03\u548c\u76f8\u5173\u6027\u4e0e\u56fd\u9645\u6587\u732e\u4e00\u81f4\uff0c\u540c\u65f6\u53cd\u6620\u4e86\u97e9\u56fd\u4f01\u4e1a\u7279\u6709\u7684\u96c6\u4e2d\u6240\u6709\u6743\u3001\u9ad8\u5916\u8d44\u6301\u80a1\u548c\u9ad8\u6d41\u52a8\u6027\u6bd4\u7387\u7b49\u7279\u5f81\u3002", "conclusion": "KoTaP\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5f00\u653e\u8d44\u6e90\uff0c\u652f\u6301\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u3001\u5916\u90e8\u6709\u6548\u6027\u68c0\u9a8c\u3001\u53ef\u89e3\u91caAI\u5206\u6790\uff0c\u4ee5\u53ca\u653f\u7b56\u8bc4\u4f30\u3001\u5ba1\u8ba1\u89c4\u5212\u548c\u6295\u8d44\u5206\u6790\u7b49\u5e94\u7528\u3002"}}
{"id": "2511.04214", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04214", "abs": "https://arxiv.org/abs/2511.04214", "authors": ["Yuantian Shao", "Peisong Wang", "Yuanteng Chen", "Chang Xu", "Zhihui Wei", "Jian Cheng"], "title": "Block Rotation is All You Need for MXFP4 Quantization", "comment": "9 pages, 10 figures", "summary": "Large language models (LLMs) have achieved remarkable success, but their\nrapidly growing scale imposes prohibitive costs in memory, computation, and\nenergy. Post-training quantization (PTQ) is a promising solution for efficient\ndeployment, yet achieving accurate W4A4 quantization remains an open challenge.\nWhile most existing methods are designed for INT4 formats, the emergence of\nMXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--\nraises questions about the applicability of current techniques. In this work,\nwe establish a comprehensive benchmark of PTQ methods under the MXFP4 format.\nThrough systematic evaluation, we find that methods like GPTQ consistently\ndeliver strong performance, whereas rotation-based approaches, which are almost\nused by all state-of-the-art approaches, suffer from severe incompatibility\nwith MXFP4. We further provide the first in-depth analysis of this conflict,\ntracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)\nblock scaling and the redistribution of outlier energy via global rotation.\nBuilding on this insight, we propose a simple yet effective block rotation\nstrategy that adapts rotation-based methods to MXFP4, leading to substantial\naccuracy improvements across diverse LLMs. Our findings not only offer clear\nguidance for practitioners but also set a foundation for advancing PTQ research\nunder emerging low-precision formats.", "AI": {"tldr": "\u672c\u6587\u5bf9MXFP4\u683c\u5f0f\u4e0b\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0GPTQ\u7b49\u65b9\u6cd5\u8868\u73b0\u826f\u597d\uff0c\u800c\u57fa\u4e8e\u65cb\u8f6c\u7684\u65b9\u6cd5\u4e0eMXFP4\u683c\u5f0f\u5b58\u5728\u4e25\u91cd\u4e0d\u517c\u5bb9\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u5757\u65cb\u8f6c\u7b56\u7565\u6765\u9002\u914dMXFP4\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u80fd\u6e90\u6210\u672c\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\u662f\u9ad8\u6548\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002\u968f\u7740MXFP4\u8fd9\u79cd\u65b0\u578bFP4\u683c\u5f0f\u7684\u51fa\u73b0\uff0c\u9700\u8981\u8bc4\u4f30\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u65b0\u683c\u5f0f\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u5efa\u7acb\u4e86MXFP4\u683c\u5f0f\u4e0bPTQ\u65b9\u6cd5\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0\u65cb\u8f6c\u65b9\u6cd5\u4e0eMXFP4\u683c\u5f0f\u7684\u4e0d\u517c\u5bb9\u6027\uff0c\u5e76\u63d0\u51fa\u5757\u65cb\u8f6c\u7b56\u7565\u6765\u9002\u914dMXFP4\u7684PoT\u5757\u7f29\u653e\u7279\u6027\u3002", "result": "GPTQ\u7b49\u65b9\u6cd5\u5728MXFP4\u683c\u5f0f\u4e0b\u8868\u73b0\u7a33\u5b9a\uff0c\u800c\u65cb\u8f6c\u65b9\u6cd5\u7531\u4e8e\u4e0eMXFP4\u7684PoT\u5757\u7f29\u653e\u4e0d\u517c\u5bb9\u800c\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u63d0\u51fa\u7684\u5757\u65cb\u8f6c\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u65cb\u8f6c\u65b9\u6cd5\u5728MXFP4\u683c\u5f0f\u4e0b\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6307\u5bfc\uff0c\u5e76\u4e3a\u65b0\u5174\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e0b\u7684PTQ\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u9002\u914d\u65b0\u786c\u4ef6\u683c\u5f0f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.04286", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04286", "abs": "https://arxiv.org/abs/2511.04286", "authors": ["Matteo Cercola", "Valeria Capretti", "Simone Formentin"], "title": "Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference", "comment": null, "summary": "Learning from human preferences is a cornerstone of aligning machine learning\nmodels with subjective human judgments. Yet, collecting such preference data is\noften costly and time-consuming, motivating the need for more efficient\nlearning paradigms. Two established approaches offer complementary advantages:\nRLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,\nwhile PBO achieves greater sample efficiency through active querying. We\npropose a hybrid framework that unifies RLHF's scalability with PBO's query\nefficiency by integrating an acquisition-driven module into the RLHF pipeline,\nthereby enabling active and sample-efficient preference gathering. We validate\nthe proposed approach on two representative domains: (i) high-dimensional\npreference optimization and (ii) LLM fine-tuning. Experimental results\ndemonstrate consistent improvements in both sample efficiency and overall\nperformance across these tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RLHF\u53ef\u6269\u5c55\u6027\u548cPBO\u6837\u672c\u6548\u7387\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e3b\u52a8\u67e5\u8be2\u6a21\u5757\u96c6\u6210\u5230RLHF\u6d41\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u504f\u597d\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u4ece\u4eba\u7c7b\u504f\u597d\u4e2d\u5b66\u4e60\u662f\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u4e3b\u89c2\u4eba\u7c7b\u5224\u65ad\u5bf9\u9f50\u7684\u5173\u952e\uff0c\u4f46\u6536\u96c6\u504f\u597d\u6570\u636e\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u8303\u5f0f\u3002", "method": "\u5728RLHF\u6d41\u7a0b\u4e2d\u96c6\u6210\u4e3b\u52a8\u67e5\u8be2\u6a21\u5757\uff0c\u7ed3\u5408RLHF\u7684\u53ef\u6269\u5c55\u6027\u548cPBO\u7684\u6837\u672c\u6548\u7387\u4f18\u52bf\u3002", "result": "\u5728\u9ad8\u7ef4\u504f\u597d\u4f18\u5316\u548cLLM\u5fae\u8c03\u4e24\u4e2a\u4ee3\u8868\u6027\u9886\u57df\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86RLHF\u7684\u53ef\u6269\u5c55\u6027\u548cPBO\u7684\u67e5\u8be2\u6548\u7387\uff0c\u4e3a\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04332", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.04332", "abs": "https://arxiv.org/abs/2511.04332", "authors": ["Antti Koskela", "Tejas Kulkarni", "Laith Zumot"], "title": "Differentially Private In-Context Learning with Nearest Neighbor Search", "comment": "NeurIPS Lock-LLM Workshop 2025", "summary": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5206\u9690\u79c1\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u79c1\u611f\u77e5\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u68c0\u7d22\u76f8\u5173\u793a\u4f8b\uff0c\u5728\u6587\u672c\u5206\u7c7b\u548c\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4e86\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u6d41\u7a0b\u4e2d\u7684\u5173\u952e\u7ec4\u4ef6\u2014\u2014\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u6570\u636e\u7684\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002", "method": "\u91c7\u7528\u6700\u8fd1\u90bb\u68c0\u7d22\u4ece\u4e0a\u4e0b\u6587\u6570\u636e\u5e93\u4e2d\u83b7\u53d6\u76f8\u5173\u793a\u4f8b\uff0c\u7ed3\u5408\u9690\u79c1\u8fc7\u6ee4\u5668\u8ddf\u8e2a\u6240\u9009\u6837\u672c\u7684\u7d2f\u79ef\u9690\u79c1\u6210\u672c\uff0c\u786e\u4fdd\u7b26\u5408\u4e2d\u5fc3\u5dee\u5206\u9690\u79c1\u9884\u7b97\u3002", "result": "\u5728\u6587\u672c\u5206\u7c7b\u548c\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u57fa\u51c6\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u5dee\u5206\u9690\u79c1\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u6574\u5408\u9690\u79c1\u611f\u77e5\u7684\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.04401", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04401", "abs": "https://arxiv.org/abs/2511.04401", "authors": ["Subeen Park", "Joowang Kim", "Hakyung Lee", "Sunjae Yoo", "Kyungwoo Song"], "title": "Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness", "comment": null, "summary": "Deep learning models achieve strong performance across various domains but\noften rely on spurious correlations, making them vulnerable to distribution\nshifts. This issue is particularly severe in subpopulation shift scenarios,\nwhere models struggle in underrepresented groups. While existing methods have\nmade progress in mitigating this issue, their performance gains are still\nconstrained. They lack a rigorous theoretical framework connecting the\nembedding space representations with worst-group error. To address this\nlimitation, we propose Spurious Correlation-Aware Embedding Regularization for\nWorst-Group Robustness (SCER), a novel approach that directly regularizes\nfeature representations to suppress spurious cues. We show theoretically that\nworst-group error is influenced by how strongly the classifier relies on\nspurious versus core directions, identified from differences in group-wise mean\nembeddings across domains and classes. By imposing theoretical constraints at\nthe embedding level, SCER encourages models to focus on core features while\nreducing sensitivity to spurious patterns. Through systematic evaluation on\nmultiple vision and language, we show that SCER outperforms prior\nstate-of-the-art studies in worst-group accuracy. Our code is available at\n\\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.", "AI": {"tldr": "\u63d0\u51faSCER\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u7279\u5f81\u8868\u793a\u6765\u6291\u5236\u865a\u5047\u76f8\u5173\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u6700\u5dee\u7fa4\u4f53\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\uff0c\u5728\u5206\u5e03\u504f\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5b50\u7fa4\u4f53\u504f\u79fb\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c06\u5d4c\u5165\u7a7a\u95f4\u8868\u793a\u4e0e\u6700\u5dee\u7fa4\u4f53\u9519\u8bef\u8054\u7cfb\u8d77\u6765\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "SCER\u65b9\u6cd5\u76f4\u63a5\u5728\u7279\u5f81\u8868\u793a\u5c42\u9762\u65bd\u52a0\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u5206\u6790\u8de8\u57df\u548c\u8de8\u7c7b\u7684\u7fa4\u4f53\u5747\u503c\u5d4c\u5165\u5dee\u5f02\u6765\u8bc6\u522b\u6838\u5fc3\u65b9\u5411\u548c\u865a\u5047\u65b9\u5411\uff0c\u5e76\u65bd\u52a0\u7406\u8bba\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0cSCER\u5728\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SCER\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u5d4c\u5165\u6b63\u5219\u5316\uff0c\u6709\u6548\u6291\u5236\u865a\u5047\u76f8\u5173\u6027\uff0c\u63d0\u5347\u6a21\u578b\u5728\u6700\u5dee\u7fa4\u4f53\u4e0a\u7684\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2511.04418", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04418", "abs": "https://arxiv.org/abs/2511.04418", "authors": ["Tim Tomov", "Dominik Fuchsgruber", "Tom Wollschl\u00e4ger", "Stephan G\u00fcnnemann"], "title": "The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity", "comment": null, "summary": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is\ncritical for trustworthy deployment. While real-world language is inherently\nambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically\nbenchmarked against tasks with no ambiguity. In this work, we demonstrate that\nwhile current uncertainty estimators perform well under the restrictive\nassumption of no ambiguity, they degrade to close-to-random performance on\nambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first\nambiguous question-answering (QA) datasets equipped with ground-truth answer\ndistributions estimated from factual co-occurrence. We find this performance\ndeterioration to be consistent across different estimation paradigms: using the\npredictive distribution itself, internal representations throughout the model,\nand an ensemble of models. We show that this phenomenon can be theoretically\nexplained, revealing that predictive-distribution and ensemble-based estimators\nare fundamentally limited under ambiguity. Overall, our study reveals a key\nshortcoming of current UQ methods for LLMs and motivates a rethinking of\ncurrent modeling paradigms.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524dLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728\u6a21\u7cca\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u914d\u5907\u771f\u5b9e\u7b54\u6848\u5206\u5e03\u7684\u6a21\u7ccaQA\u6570\u636e\u96c6\uff0c\u5e76\u8bc1\u660e\u4e86\u73b0\u6709UQ\u65b9\u6cd5\u5728\u6a21\u7cca\u6027\u4e0b\u7684\u7406\u8bba\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709UQ\u65b9\u6cd5\u901a\u5e38\u5728\u65e0\u6a21\u7cca\u6027\u5047\u8bbe\u4e0b\u8bc4\u4f30\uff0c\u4f46\u771f\u5b9e\u8bed\u8a00\u5177\u6709\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff0c\u9700\u8981\u7814\u7a76UQ\u65b9\u6cd5\u5728\u6a21\u7cca\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165MAQA*\u548cAmbigQA*\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e8b\u5b9e\u5171\u73b0\u4f30\u8ba1\u771f\u5b9e\u7b54\u6848\u5206\u5e03\uff0c\u8bc4\u4f30\u4e0d\u540cUQ\u8303\u5f0f\u5728\u6a21\u7cca\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u5728\u65e0\u6a21\u7cca\u5047\u8bbe\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6a21\u7cca\u6570\u636e\u4e0a\u6027\u80fd\u9000\u5316\u81f3\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u8fd9\u79cd\u73b0\u8c61\u5728\u4e0d\u540c\u4f30\u8ba1\u8303\u5f0f\u4e2d\u4e00\u81f4\u5b58\u5728\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM UQ\u65b9\u6cd5\u7684\u5173\u952e\u7f3a\u9677\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5f53\u524d\u5efa\u6a21\u8303\u5f0f\u4ee5\u5e94\u5bf9\u8bed\u8a00\u6a21\u7cca\u6027\u3002"}}
{"id": "2511.04485", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.04485", "abs": "https://arxiv.org/abs/2511.04485", "authors": ["Ipsita Ghosh", "Ethan Nguyen", "Christian K\u00fcmmerle"], "title": "Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training", "comment": null, "summary": "Parameter-efficient training, based on low-rank optimization, has become a\nhighly successful tool for fine-tuning large deep-learning models. However,\nthese methods fail at low-rank pre-training tasks where maintaining the\nlow-rank structure and the objective remains a challenging task. We propose the\nQuadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel\nlow-rank inducing training strategy inspired by the iteratively reweighted\nleast squares (IRLS) framework. Q3R is based on a quadratic regularizer term\nwhich majorizes a smoothed log determinant serving as rank surrogate objective.\nUnlike other low-rank training techniques, Q3R is able to train weight matrices\nwith prescribed, low target ranks of models that achieve comparable predictive\nperformance as dense models, with small computational overhead, while remaining\nfully compatible with existing architectures. For example, we demonstrated one\nexperiment where we are able to truncate $60\\%$ and $80\\%$ of the parameters of\na ViT-Tiny model with $~1.3\\%$ and $~4\\%$ accuracy drop in CIFAR-10 performance\nrespectively. The efficacy of Q3R is confirmed on Transformers across both\nimage and language tasks, including for low-rank fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQ3R\u7684\u4e8c\u6b21\u91cd\u52a0\u6743\u79e9\u6b63\u5219\u5316\u5668\uff0c\u7528\u4e8e\u4f4e\u79e9\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4f4e\u79e9\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\u5728\u4f4e\u79e9\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u4f4e\u79e9\u7ed3\u6784\u548c\u6a21\u578b\u76ee\u6807\uff0c\u9700\u8981\u65b0\u7684\u4f4e\u79e9\u8bf1\u5bfc\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u8fed\u4ee3\u91cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6846\u67b6\uff0c\u63d0\u51fa\u4e8c\u6b21\u6b63\u5219\u5316\u9879Q3R\uff0c\u8be5\u6b63\u5219\u5316\u9879\u4e3b\u8981\u5316\u4e86\u4e00\u4e2a\u5e73\u6ed1\u5bf9\u6570\u884c\u5217\u5f0f\u4f5c\u4e3a\u79e9\u66ff\u4ee3\u76ee\u6807\u3002", "result": "\u5728ViT-Tiny\u6a21\u578b\u4e0a\u80fd\u591f\u622a\u65ad60%\u548c80%\u7684\u53c2\u6570\uff0c\u5728CIFAR-10\u4e0a\u4ec5\u5206\u522b\u635f\u5931\u7ea61.3%\u548c4%\u7684\u51c6\u786e\u7387\uff1b\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u7684Transformer\u6a21\u578b\u4e0a\u5747\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "Q3R\u80fd\u591f\u4ee5\u89c4\u5b9a\u7684\u4f4e\u76ee\u6807\u79e9\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u4e0e\u73b0\u6709\u67b6\u6784\u5b8c\u5168\u517c\u5bb9\uff0c\u5728\u4f4e\u79e9\u5fae\u8c03\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.04505", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.04505", "abs": "https://arxiv.org/abs/2511.04505", "authors": ["Shaolong Wu", "James Blume", "Geshi Yeung"], "title": "Alternative Fairness and Accuracy Optimization in Criminal Justice", "comment": "Accepted for presentation at the AAAI 2026 AI Governance Workshop\n  (AIGOV). 24 pages", "summary": "Algorithmic fairness has grown rapidly as a research area, yet key concepts\nremain unsettled, especially in criminal justice. We review group, individual,\nand process fairness and map the conditions under which they conflict. We then\ndevelop a simple modification to standard group fairness. Rather than exact\nparity across protected groups, we minimize a weighted error loss while keeping\ndifferences in false negative rates within a small tolerance. This makes\nsolutions easier to find, can raise predictive accuracy, and surfaces the\nethical choice of error costs. We situate this proposal within three classes of\ncritique: biased and incomplete data, latent affirmative action, and the\nexplosion of subgroup constraints. Finally, we offer a practical framework for\ndeployment in public decision systems built on three pillars: need-based\ndecisions, Transparency and accountability, and narrowly tailored definitions\nand solutions. Together, these elements link technical design to legitimacy and\nprovide actionable guidance for agencies that use risk assessment and related\ntools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4fdd\u62a4\u7fa4\u4f53\u95f4\u4fdd\u6301\u5047\u9634\u6027\u7387\u5dee\u5f02\u5728\u8f83\u5c0f\u5bb9\u5fcd\u5ea6\u5185\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u52a0\u6743\u9519\u8bef\u635f\u5931\uff0c\u6765\u89e3\u51b3\u7b97\u6cd5\u516c\u5e73\u6027\u5728\u5211\u4e8b\u53f8\u6cd5\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u7b97\u6cd5\u516c\u5e73\u6027\u7814\u7a76\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5173\u952e\u6982\u5ff5\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728\u5211\u4e8b\u53f8\u6cd5\u9886\u57df\u3002\u7fa4\u4f53\u3001\u4e2a\u4f53\u548c\u8fc7\u7a0b\u516c\u5e73\u6027\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b9e\u7528\u7684\u516c\u5e73\u6027\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u65b9\u6cd5\uff1a\u4e0d\u662f\u8981\u6c42\u4fdd\u62a4\u7fa4\u4f53\u95f4\u5b8c\u5168\u5e73\u7b49\uff0c\u800c\u662f\u6700\u5c0f\u5316\u52a0\u6743\u9519\u8bef\u635f\u5931\uff0c\u540c\u65f6\u5c06\u5047\u9634\u6027\u7387\u5dee\u5f02\u63a7\u5236\u5728\u8f83\u5c0f\u5bb9\u5fcd\u5ea6\u5185\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u89e3\u51b3\u65b9\u6848\u66f4\u5bb9\u6613\u627e\u5230\uff0c\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u51f8\u663e\u9519\u8bef\u6210\u672c\u7684\u4f26\u7406\u9009\u62e9\u3002\u5728\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u5b8c\u6574\u6570\u636e\u3001\u6f5c\u5728\u5e73\u6743\u884c\u52a8\u548c\u5b50\u7fa4\u7ea6\u675f\u6fc0\u589e\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e09\u4e2a\u652f\u67f1\u7684\u5b9e\u7528\u90e8\u7f72\u6846\u67b6\uff1a\u57fa\u4e8e\u9700\u6c42\u7684\u51b3\u7b56\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u3001\u4ee5\u53ca\u7cbe\u786e\u5b9a\u4e49\u548c\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u4e9b\u8981\u7d20\u5c06\u6280\u672f\u8bbe\u8ba1\u4e0e\u5408\u6cd5\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u4f7f\u7528\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u7684\u673a\u6784\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2511.04638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04638", "abs": "https://arxiv.org/abs/2511.04638", "authors": ["Satchel Grant", "Simon Jerome Han", "Alexa Tartaglini", "Christopher Potts"], "title": "Addressing divergent representations from causal interventions on neural networks", "comment": null, "summary": "A common approach to mechanistic interpretability is to causally manipulate\nmodel representations via targeted interventions in order to understand what\nthose representations encode. Here we ask whether such interventions create\nout-of-distribution (divergent) representations, and whether this raises\nconcerns about how faithful their resulting explanations are to the target\nmodel in its natural state. First, we demonstrate empirically that common\ncausal intervention techniques often do shift internal representations away\nfrom the natural distribution of the target model. Then, we provide a\ntheoretical analysis of two classes of such divergences: `harmless' divergences\nthat occur in the null-space of the weights and from covariance within\nbehavioral decision boundaries, and `pernicious' divergences that activate\nhidden network pathways and cause dormant behavioral changes. Finally, in an\neffort to mitigate the pernicious cases, we modify the Counterfactual Latent\n(CL) loss from Grant (2025) that regularizes interventions to remain closer to\nthe natural distributions, reducing the likelihood of harmful divergences while\npreserving the interpretive power of interventions. Together, these results\nhighlight a path towards more reliable interpretability methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e2d\u56e0\u679c\u5e72\u9884\u6280\u672f\u662f\u5426\u4f1a\u4ea7\u751f\u5206\u5e03\u5916\u8868\u793a\uff0c\u5206\u6790\u4e86\u65e0\u5bb3\u548c\u6709\u5bb3\u4e24\u7c7b\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684CL\u635f\u5931\u51fd\u6570\u6765\u51cf\u5c11\u6709\u5bb3\u504f\u79fb\u3002", "motivation": "\u7814\u7a76\u56e0\u679c\u5e72\u9884\u6280\u672f\u662f\u5426\u4f1a\u4ea7\u751f\u5206\u5e03\u5916\u8868\u793a\uff0c\u4ee5\u53ca\u8fd9\u662f\u5426\u5f71\u54cd\u89e3\u91ca\u7ed3\u679c\u5bf9\u76ee\u6807\u6a21\u578b\u81ea\u7136\u72b6\u6001\u7684\u5fe0\u5b9e\u5ea6\u3002", "method": "\u9996\u5148\u5b9e\u8bc1\u9a8c\u8bc1\u5e38\u89c1\u56e0\u679c\u5e72\u9884\u6280\u672f\u4f1a\u5bfc\u81f4\u5185\u90e8\u8868\u793a\u504f\u79bb\u81ea\u7136\u5206\u5e03\uff0c\u7136\u540e\u7406\u8bba\u5206\u6790\u65e0\u5bb3\u548c\u6709\u5bb3\u4e24\u7c7b\u5206\u5e03\u504f\u79fb\uff0c\u6700\u540e\u6539\u8fdbCL\u635f\u5931\u51fd\u6570\u4ee5\u6b63\u5219\u5316\u5e72\u9884\u3002", "result": "\u53d1\u73b0\u5e38\u89c1\u56e0\u679c\u5e72\u9884\u6280\u672f\u786e\u5b9e\u4f1a\u504f\u79fb\u5185\u90e8\u8868\u793a\uff0c\u7406\u8bba\u533a\u5206\u4e86\u65e0\u5bb3\u548c\u6709\u5bb3\u504f\u79fb\uff0c\u6539\u8fdb\u7684CL\u635f\u5931\u80fd\u51cf\u5c11\u6709\u5bb3\u504f\u79fb\u540c\u65f6\u4fdd\u6301\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.04573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04573", "abs": "https://arxiv.org/abs/2511.04573", "authors": ["Vasco V. Branco", "Jand\u00f3 Benedek", "Lidia Pivovarova", "Lu\u00eds Correia", "Pedro Cardoso"], "title": "ARETE: an R package for Automated REtrieval from TExt with large language models", "comment": null, "summary": "1. A hard stop for the implementation of rigorous conservation initiatives is\nour lack of key species data, especially occurrence data. Furthermore,\nresearchers have to contend with an accelerated speed at which new information\nmust be collected and processed due to anthropogenic activity. Publications\nranging from scientific papers to gray literature contain this crucial\ninformation but their data are often not machine-readable, requiring extensive\nhuman work to be retrieved. 2. We present the ARETE R package, an open-source\nsoftware aiming to automate data extraction of species occurrences powered by\nlarge language models, namely using the chatGPT Application Programming\nInterface. This R package integrates all steps of the data extraction and\nvalidation process, from Optical Character Recognition to detection of outliers\nand output in tabular format. Furthermore, we validate ARETE through systematic\ncomparison between what is modelled and the work of human annotators. 3. We\ndemonstrate the usefulness of the approach by comparing range maps produced\nusing GBIF data and with those automatically extracted for 100 species of\nspiders. Newly extracted data allowed to expand the known Extent of Occurrence\nby a mean three orders of magnitude, revealing new areas where the species were\nfound in the past, which mayhave important implications for spatial\nconservation planning and extinction risk assessments. 4. ARETE allows faster\naccess to hitherto untapped occurrence data, a potential game changer in\nprojects requiring such data. Researchers will be able to better prioritize\nresources, manually verifying selected species while maintaining automated\nextraction for the majority. This workflow also allows predicting available\nbibliographic data during project planning.", "AI": {"tldr": "ARETE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684R\u5305\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u63d0\u53d6\u7269\u79cd\u51fa\u73b0\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u83b7\u53d6\u6548\u7387\uff0c\u5728\u8718\u86db\u7269\u79cd\u7814\u7a76\u4e2d\u5c06\u5df2\u77e5\u5206\u5e03\u8303\u56f4\u6269\u5927\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4fdd\u62a4\u751f\u7269\u5b66\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u662f\u7f3a\u4e4f\u7269\u79cd\u51fa\u73b0\u6570\u636e\uff0c\u4e14\u73b0\u6709\u6587\u732e\u6570\u636e\u96be\u4ee5\u673a\u5668\u8bfb\u53d6\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\u6765\u63d0\u53d6\u3002", "method": "\u5f00\u53d1ARETE R\u5305\uff0c\u5229\u7528chatGPT API\u5b9e\u73b0\u7269\u79cd\u51fa\u73b0\u6570\u636e\u7684\u81ea\u52a8\u5316\u63d0\u53d6\u548c\u9a8c\u8bc1\uff0c\u5305\u62ec\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u3001\u5f02\u5e38\u503c\u68c0\u6d4b\u548c\u8868\u683c\u8f93\u51fa\u3002", "result": "\u5bf9100\u79cd\u8718\u86db\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u65b0\u63d0\u53d6\u6570\u636e\u4f7f\u5df2\u77e5\u5206\u5e03\u8303\u56f4\u5e73\u5747\u6269\u5927\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u5386\u53f2\u5206\u5e03\u533a\u57df\u3002", "conclusion": "ARETE\u80fd\u591f\u5feb\u901f\u83b7\u53d6\u672a\u5f00\u53d1\u7684\u7269\u79cd\u51fa\u73b0\u6570\u636e\uff0c\u6539\u53d8\u9700\u8981\u6b64\u7c7b\u6570\u636e\u7684\u9879\u76ee\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u66f4\u597d\u5730\u5206\u914d\u8d44\u6e90\u3002"}}
{"id": "2511.04590", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.04590", "abs": "https://arxiv.org/abs/2511.04590", "authors": ["Oshri Naparstek"], "title": "Complexity as Advantage: A Regret-Based Perspective on Emergent Structure", "comment": "15 pages. Under preparation for submission to ICML 2026. Feedback\n  welcome", "summary": "We introduce Complexity as Advantage (CAA), a framework that defines the\ncomplexity of a system relative to a family of observers. Instead of measuring\ncomplexity as an intrinsic property, we evaluate how much predictive regret a\nsystem induces for different observers attempting to model it. A system is\ncomplex when it is easy for some observers and hard for others, creating an\ninformation advantage. We show that this formulation unifies several notions of\nemergent behavior, including multiscale entropy, predictive information, and\nobserver-dependent structure. The framework suggests that \"interesting\" systems\nare those positioned to create differentiated regret across observers,\nproviding a quantitative grounding for why complexity can be functionally\nvaluable. We demonstrate the idea through simple dynamical models and discuss\nimplications for learning, evolution, and artificial agents.", "AI": {"tldr": "CAA\u6846\u67b6\u5c06\u7cfb\u7edf\u590d\u6742\u6027\u5b9a\u4e49\u4e3a\u76f8\u5bf9\u4e8e\u89c2\u5bdf\u8005\u5bb6\u65cf\u7684\u9884\u6d4b\u9057\u61be\u5dee\u5f02\uff0c\u8ba4\u4e3a\u590d\u6742\u6027\u6e90\u4e8e\u7cfb\u7edf\u5bf9\u4e0d\u540c\u89c2\u5bdf\u8005\u5efa\u6a21\u96be\u5ea6\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u521b\u9020\u4fe1\u606f\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u590d\u6742\u6027\u5ea6\u91cf\u5c06\u590d\u6742\u6027\u89c6\u4e3a\u7cfb\u7edf\u5185\u5728\u5c5e\u6027\uff0c\u800cCAA\u6846\u67b6\u65e8\u5728\u4ece\u89c2\u5bdf\u8005\u4f9d\u8d56\u7684\u89d2\u5ea6\u91cd\u65b0\u5b9a\u4e49\u590d\u6742\u6027\uff0c\u5f3a\u8c03\u590d\u6742\u6027\u5982\u4f55\u5728\u4e0d\u540c\u89c2\u5bdf\u8005\u4e4b\u95f4\u4ea7\u751f\u9884\u6d4b\u80fd\u529b\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u7cfb\u7edf\u76f8\u5bf9\u4e8e\u89c2\u5bdf\u8005\u5bb6\u65cf\u7684\u9884\u6d4b\u9057\u61be\uff0c\u8bc4\u4f30\u7cfb\u7edf\u5bf9\u4e0d\u540c\u89c2\u5bdf\u8005\u5efa\u6a21\u7684\u96be\u5ea6\u5dee\u5f02\uff0c\u5efa\u7acb\u590d\u6742\u6027\u4f5c\u4e3a\u4fe1\u606f\u4f18\u52bf\u7684\u6982\u5ff5\u6846\u67b6\u3002", "result": "CAA\u6846\u67b6\u7edf\u4e00\u4e86\u591a\u79cd\u6d8c\u73b0\u884c\u4e3a\u6982\u5ff5\uff08\u5982\u591a\u5c3a\u5ea6\u71b5\u3001\u9884\u6d4b\u4fe1\u606f\u3001\u89c2\u5bdf\u8005\u4f9d\u8d56\u7ed3\u6784\uff09\uff0c\u8868\u660e\"\u6709\u8da3\"\u7cfb\u7edf\u662f\u90a3\u4e9b\u80fd\u5728\u89c2\u5bdf\u8005\u95f4\u4ea7\u751f\u5dee\u5f02\u5316\u9884\u6d4b\u9057\u61be\u7684\u7cfb\u7edf\u3002", "conclusion": "\u590d\u6742\u6027\u53ef\u4ee5\u88ab\u529f\u80fd\u6027\u5730\u5229\u7528\uff0c\u56e0\u4e3a\u5b83\u521b\u9020\u4e86\u89c2\u5bdf\u8005\u4e4b\u95f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u6027\u5728\u5b66\u4e60\u548c\u8fdb\u5316\u4e2d\u7684\u4ef7\u503c\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u7840\u3002"}}
{"id": "2511.04594", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04594", "abs": "https://arxiv.org/abs/2511.04594", "authors": ["Utkarsh U. Chavan", "Prashant Trivedi", "Nandyala Hemachandra"], "title": "Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems", "comment": "To appear in 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Multi-agent systems (MAS) are central to applications such as swarm robotics\nand traffic routing, where agents must coordinate in a decentralized manner to\nachieve a common objective. Stochastic Shortest Path (SSP) problems provide a\nnatural framework for modeling decentralized control in such settings. While\nthe problem of learning in SSP has been extensively studied in single-agent\nsettings, the decentralized multi-agent variant remains largely unexplored. In\nthis work, we take a step towards addressing that gap. We study decentralized\nmulti-agent SSPs (Dec-MASSPs) under linear function approximation, where the\ntransition dynamics and costs are represented using linear models. Applying\nnovel symmetry-based arguments, we identify the structure of optimal policies.\nOur main contribution is the first regret lower bound for this setting based on\nthe construction of hard-to-learn instances for any number of agents, $n$. Our\nregret lower bound of $\\Omega(\\sqrt{K})$, over $K$ episodes, highlights the\ninherent learning difficulty in Dec-MASSPs. These insights clarify the learning\ncomplexity of decentralized control and can further guide the design of\nefficient learning algorithms in multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u968f\u673a\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff08Dec-MASSPs\uff09\uff0c\u5728\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u6846\u67b6\u4e0b\u5206\u6790\u4e86\u6700\u4f18\u7b56\u7565\u7ed3\u6784\uff0c\u5e76\u9996\u6b21\u5efa\u7acb\u4e86\u8be5\u8bbe\u7f6e\u7684\u9057\u61be\u4e0b\u754c\u03a9(\u221aK)\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u7fa4\u4f53\u673a\u5668\u4eba\u548c\u4ea4\u901a\u8def\u7531\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u968f\u673a\u6700\u77ed\u8def\u5f84\u95ee\u9898\u7684\u5b66\u4e60\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u6846\u67b6\u4e0b\uff0c\u5e94\u7528\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u65b0\u9896\u8bba\u8bc1\u6765\u8bc6\u522b\u6700\u4f18\u7b56\u7565\u7ed3\u6784\uff0c\u5e76\u6784\u5efa\u96be\u4ee5\u5b66\u4e60\u7684\u5b9e\u4f8b\u6765\u5efa\u7acb\u9057\u61be\u4e0b\u754c\u3002", "result": "\u5efa\u7acb\u4e86\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u968f\u673a\u6700\u77ed\u8def\u5f84\u95ee\u9898\u7684\u9996\u4e2a\u9057\u61be\u4e0b\u754c\u03a9(\u221aK)\uff0c\u63ed\u793a\u4e86\u8be5\u8bbe\u7f6e\u4e2d\u56fa\u6709\u7684\u5b66\u4e60\u96be\u5ea6\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u9610\u660e\u4e86\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u7684\u5b66\u4e60\u590d\u6742\u6027\uff0c\u53ef\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.04667", "categories": ["cs.LG", "97C70, 62P25, 62H30, 68T05"], "pdf": "https://arxiv.org/pdf/2511.04667", "abs": "https://arxiv.org/abs/2511.04667", "authors": ["Julian D. Allagan", "Dasia A. Singleton", "Shanae N. Perry", "Gabrielle C. Morgan", "Essence A. Morgan"], "title": "Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches", "comment": "28 pages, 8 table, 4figures, NAM conference", "summary": "This study evaluates a 40-item mathematics placement examination administered\nto 198 students using a multi-method framework combining Classical Test Theory,\nmachine learning, and unsupervised clustering. Classical Test Theory analysis\nreveals that 55\\% of items achieve excellent discrimination ($D \\geq 0.40$)\nwhile 30\\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.\nQuestion 6 (Graph Interpretation) emerges as the examination's most powerful\ndiscriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA\nF-statistic ($F = 4609.1$), and maximum Random Forest feature importance\n(0.206), accounting for 20.6\\% of predictive power. Machine learning algorithms\ndemonstrate exceptional performance, with Random Forest and Gradient Boosting\nachieving 97.5\\% and 96.0\\% cross-validation accuracy. K-means clustering\nidentifies a natural binary competency structure with a boundary at 42.5\\%,\ndiverging from the institutional threshold of 55\\% and suggesting potential\noverclassification into remedial categories. The two-cluster solution exhibits\nexceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster\npurity. Convergent evidence across methods supports specific refinements:\nreplace poorly discriminating items, implement a two-stage assessment, and\nintegrate Random Forest predictions with transparency mechanisms. These\nfindings demonstrate that multi-method integration provides a robust empirical\nfoundation for evidence-based mathematics placement optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u91c7\u7528\u7ecf\u5178\u6d4b\u9a8c\u7406\u8bba\u3001\u673a\u5668\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u805a\u7c7b\u76f8\u7ed3\u5408\u7684\u591a\u65b9\u6cd5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86198\u540d\u5b66\u751f\u53c2\u52a0\u768440\u9879\u6570\u5b66\u5206\u73ed\u8003\u8bd5\u3002\u7ed3\u679c\u663e\u793a55%\u7684\u9879\u76ee\u5177\u6709\u4f18\u79c0\u533a\u5206\u5ea6\uff0c30%\u9700\u8981\u66ff\u6362\uff1b\u7b2c6\u9898\u662f\u6700\u4f73\u533a\u5206\u9879\uff1b\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u51c6\u786e\u7387\u8fbe97.5%\uff1b\u805a\u7c7b\u5206\u6790\u53d1\u73b042.5%\u7684\u81ea\u7136\u80fd\u529b\u8fb9\u754c\uff0c\u5efa\u8bae\u4f18\u5316\u5206\u73ed\u6807\u51c6\u3002", "motivation": "\u901a\u8fc7\u591a\u65b9\u6cd5\u6574\u5408\u4e3a\u6570\u5b66\u5206\u73ed\u8003\u8bd5\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\uff0c\u4f18\u5316\u5206\u73ed\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u79d1\u5b66\u6027\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u6d4b\u9a8c\u7406\u8bba\u3001\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\uff09\u548c\u65e0\u76d1\u7763\u805a\u7c7b\uff08K-means\uff09\uff0c\u5206\u679040\u9879\u6570\u5b66\u5206\u73ed\u8003\u8bd5\u6570\u636e\u3002", "result": "55%\u9879\u76ee\u533a\u5206\u5ea6\u4f18\u79c0\uff0c30%\u9700\u8981\u66ff\u6362\uff1b\u7b2c6\u9898\u533a\u5206\u5ea6\u6700\u4f73\uff1b\u673a\u5668\u5b66\u4e60\u51c6\u786e\u738797.5%\uff1b\u805a\u7c7b\u53d1\u73b042.5%\u81ea\u7136\u80fd\u529b\u8fb9\u754c\uff0c\u4e0e\u673a\u678455%\u6807\u51c6\u5b58\u5728\u5dee\u5f02\uff1b\u4e24\u805a\u7c7b\u65b9\u6848\u7a33\u5b9a\u6027\u9ad8\u3002", "conclusion": "\u591a\u65b9\u6cd5\u6574\u5408\u4e3a\u57fa\u4e8e\u8bc1\u636e\u7684\u6570\u5b66\u5206\u73ed\u4f18\u5316\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u5efa\u8bae\u66ff\u6362\u4f4e\u533a\u5206\u5ea6\u9879\u76ee\u3001\u5b9e\u65bd\u4e24\u9636\u6bb5\u8bc4\u4f30\u3001\u6574\u5408\u968f\u673a\u68ee\u6797\u9884\u6d4b\u5e76\u786e\u4fdd\u900f\u660e\u5ea6\u3002"}}

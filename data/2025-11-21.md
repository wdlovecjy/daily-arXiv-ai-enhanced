<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 2]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.AI](#cs.AI) [Total: 16]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Atlas Gaussian processes on restricted domains and point clouds](https://arxiv.org/abs/2511.15822)
*Mu Niu,Yue Zhang,Ke Ye,Pokman Cheung,Yizhu Wang,Xiaochen Yang*

Main category: stat.ML

TL;DR: 本文提出了Atlas布朗运动框架来估计未知几何和拓扑结构点云上的热核，并构建了Riemannian校正核，形成了Riemannian校正Atlas高斯过程(RC-AGPs)，在回归任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据常位于边界未知的受限域中，或作为高维点云位于低维未知流形上。传统高斯过程难以捕捉此类几何结构，现有方法要么假设平坦空间嵌入点云，要么在稀疏或不规则采样点云上表现不佳。

Method: 建立Atlas布朗运动框架估计未知几何点云上的热核；构建Riemannian校正核，将全局热核与局部RBF核结合，形成Riemannian校正Atlas高斯过程(RC-AGPs)。

Result: 在合成和真实数据集上的回归任务中，RC-AGPs在热核估计和回归精度方面均优于现有方法，有效弥合了复杂高维观测与基于流形推断之间的差距。

Conclusion: RC-AGPs通过结合全局热核和局部RBF核，在未知几何结构的点云上实现了更准确的统计推断，提升了回归性能。

Abstract: In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.

</details>


### [2] [Rate-optimal community detection near the KS threshold via node-robust algorithms](https://arxiv.org/abs/2511.16613)
*Jingqiu Ding,Yiding Hua,Kasper Lindberg,David Steurer,Aleksandr Storozhenko*

Main category: stat.ML

TL;DR: 本文提出了一种多项式时间算法，在对称k-随机块模型中实现了最小最大最优的误分类率，该算法在Kesten-Stigum阈值附近工作，并且能够抵抗节点损坏攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么计算效率低下，要么需要更强的假设条件（如C ≥ Kk³），特别是在节点鲁棒性设置中，已知最佳算法需要C ≥ Kk¹⁰²的强条件。本文旨在填补这一空白。

Method: 结合两个关键技术贡献：(1) 通过Sum-of-Squares框架鲁棒化多数投票；(2) 开发基于鲁棒多数投票的新型图二分算法，显著改善了初始估计的误分类率。

Result: 算法在C ≥ Kk²logk的条件下实现了最小最大最优误分类率exp(-(1±o(1))C/k)，即使当对手损坏η ≤ exp(-(1±o(1))C/k)比例的节点时仍然有效。

Conclusion: 这是第一个在Kesten-Stigum阈值附近实现最小最大最优误分类率的多项式时间算法，同时在标准设置和节点鲁棒设置中都有效，填补了现有研究的空白。

Abstract: We study community detection in the \emph{symmetric $k$-stochastic block model}, where $n$ nodes are evenly partitioned into $k$ clusters with intra- and inter-cluster connection probabilities $p$ and $q$, respectively.
  Our main result is a polynomial-time algorithm that achieves the minimax-optimal misclassification rate
  \begin{equation*}
  \exp \Bigl(-\bigl(1 \pm o(1)\bigr) \tfrac{C}{k}\Bigr),
  \quad \text{where } C = (\sqrt{pn} - \sqrt{qn})^2,
  \end{equation*}
  whenever $C \ge K\,k^2\,\log k$ for some universal constant $K$, matching the Kesten--Stigum (KS) threshold up to a $\log k$ factor.
  Notably, this rate holds even when an adversary corrupts an $η\le \exp\bigl(- (1 \pm o(1)) \tfrac{C}{k}\bigr)$ fraction of the nodes.
  To the best of our knowledge, the minimax rate was previously only attainable either via computationally inefficient procedures [ZZ15] or via polynomial-time algorithms that require strictly stronger assumptions such as $C \ge K k^3$ [GMZZ17].
  In the node-robust setting, the best known algorithm requires the substantially stronger condition $C \ge K k^{102}$ [LM22].
  Our results close this gap by providing the first polynomial-time algorithm that achieves the minimax rate near the KS threshold in both settings.
  Our work has two key technical contributions:
  (1) we robustify majority voting via the Sum-of-Squares framework,
  (2) we develop a novel graph bisection algorithm via robust majority voting, which allows us to significantly improve the misclassification rate to $1/\mathrm{poly}(k)$ for the initial estimation near the KS threshold.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [3] [Rapid and Accurate Changepoint Detection of Power System Forced Oscillations](https://arxiv.org/abs/2511.15812)
*Luke Dosiek,Akaash Karn,Frank Liu*

Main category: eess.SP

TL;DR: 本文提出了一种使用变点检测(CPD)来估计电力系统数据中强制振荡起止时间的新方法，采用PELT算法但通过手动设置惩罚参数，大幅减少计算时间而不损失精度。


<details>
  <summary>Details</summary>
Motivation: 改进现有CPD方法在强制振荡检测中的计算效率，减少输入参数需求，并提供数据驱动的方法来设置最小FO段长度。

Method: 使用PELT算法进行变点检测，但手动提供惩罚参数而非自动调参，结合低阶ARMAX模型进行测试验证。

Result: 在minniWECC模型测试中，计算时间减少了98%，同时保持了高估计精度。

Conclusion: 该方法显著提高了强制振荡检测的计算效率，减少了参数需求，为机电模式仪表提供了更实用的解决方案。

Abstract: This paper describes a new approach for using changepoint detection (CPD) to estimate the starting and stopping times of a forced oscillation (FO) in measured power system data. As with a previous application of CPD to this problem, the pruned exact linear time (PELT) algorithm is used. However, instead of allowing PELT to automatically tune its penalty parameter, a method of manually providing it is presented that dramatically reduces computation time without sacrificing accuracy. Additionally, the new algorithm requires fewer input parameters and provides a formal, data-driven approach to setting the minimum FO segment length to consider as troublesome for an electromechanical mode meter. A low-order ARMAX representation of the minniWECC model is used to test the approach, where a 98\% reduction in computation time is enjoyed with high estimation accuracy.

</details>


### [4] [EEG Emotion Recognition Through Deep Learning](https://arxiv.org/abs/2511.15902)
*Roman Dolgopolyi,Antonis Chatzipanagiotou*

Main category: eess.SP

TL;DR: 开发了一个基于CNN-Transformer架构的脑电波情绪分类模型，能够有效识别积极、中性和消极三种情绪状态，测试准确率达到91%，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统情绪识别方法在面部表情或语音线索受限场景下的局限性，为医疗、健康监测等应用提供更可靠的情绪识别技术。

Method: 使用CNN-Transformer混合架构处理EEG信号，在合并SEED、SEED-FRA和SEED-GER数据集的基础上训练模型，仅需5个电极而非传统的62个。

Result: 模型在测试集上达到91%的准确率，显著优于SVM、DNN和逻辑回归等传统方法，同时大幅降低了硬件要求和计算成本。

Conclusion: 该研究为开发经济实惠的消费级EEG设备奠定了基础，有望在医疗健康、家庭监测等领域实现连续被动的情绪监测，特别适用于传统行为线索受限的临床或护理场景。

Abstract: An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions...

</details>


### [5] [UT-OSANet: A Multimodal Deep Learning model for Evaluating and Classifying Obstructive Sleep Apnea](https://arxiv.org/abs/2511.16169)
*Zijian Wang,Xiaoyu Bao,Chenhao Zhao,Jihui Zhang,Sizhi Ai,Yuanqing Li*

Main category: eess.SP

TL;DR: UT OSANet是一个基于深度学习的阻塞性睡眠呼吸暂停(OSA)事件级诊断模型，支持多模态输入和多种应用场景，在9,021个多导睡眠图记录上训练，性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有OSA诊断方法只能粗略分类严重程度或检测孤立呼吸事件，缺乏高分辨率的事件级诊断精度和全面性。

Method: 开发了UT OSANet深度学习模型，采用随机掩码模态组合训练策略，支持灵活调整的输入模态(EEG、气流、SpO2)，能够理解跨模态关系并在不同模态条件下保持稳定性能。

Result: 在五个独立数据集的9,021个PSG记录上训练评估，在家庭、临床和研究场景中达到最高0.93的敏感度和0.84、0.85的宏F1分数。

Conclusion: 该模型可作为OSA事件级、多场景诊断工具用于实际应用，同时为深入理解睡眠障碍中呼吸过程的机制及其广泛健康影响提供了手段。

Abstract: Obstructive sleep apnea (OSA) is a highly prevalent sleep disorder that is associated with increased risks of cardiovascular morbidity and all-cause mortality. While existing diagnostic approaches can roughly classify OSA severity or detect isolated respiratory events, they lack the precision and comprehensiveness required for high resolution, event level diagnosis. Here, we present UT OSANet, a deep learning based model designed as a event level, multi scenario diagnostic tool for OSA. This model facilitates detailed identification of events associated with OSA, including apnea, hypopnea, oxygen desaturation, and arousal. Moreover, the model employs flexibly adjustable input modalities such as electroencephalography (EEG), airflow, and SpO 2. It utilizes a random masked modality combination training strategy, allowing it to comprehend cross-modal relationships while sustaining consistent performance across varying modality conditions. This model was trained and evaluated utilizing 9,021 polysomnography (PSG) recordings from five independent datasets. achieving sensitivities up to 0.93 and macro F1 scores of 0.84, 0.85 across home, clinical, and research scenarios. This model serves as an event-level, multi-scenario diagnostic instrument for real-world applications of OSA, while also establishing itself as a means to deepen the mechanistic comprehension of respiratory processes in sleep disorders and their extensive health implications.

</details>


### [6] [3-20 GHz Wideband Tightly-Coupled Dual-Polarized Vivaldi Antenna Array](https://arxiv.org/abs/2511.16472)
*Niko Lindvall,Mikko Heino,Mikko Valkama*

Main category: eess.SP

TL;DR: 本文提出了一种新型紧密耦合双极化对跖维瓦尔第天线，通过重叠维瓦尔第叶片实现紧密耦合，将-6dB阻抗带宽扩展到3-20GHz，相比孤立天线将下边带从3.75GHz扩展到3GHz和2.75GHz，改进幅度达20-25%。


<details>
  <summary>Details</summary>
Motivation: 在定位、传感、频谱监测和现代扩频系统（如跳频系统）中需要非常宽带的孔径。维瓦尔第天线因其天然的宽带特性成为这些系统的突出选择，但现有研究主要集中在偶极子元件的紧密耦合阵列，尚未涉及双极化维瓦尔第天线。

Method: 设计了一种紧密耦合双极化对跖维瓦尔第天线，通过重叠维瓦尔第叶片实现元件间的紧密耦合，从而扩展天线的低频性能。

Result: 该天线实现了3-20GHz的-6dB阻抗带宽，相比孤立维瓦尔第天线元件，将下边带从3.75GHz扩展到3GHz和2.75GHz，对两种极化分别实现了20%和25%的改进。

Conclusion: 紧密耦合技术可有效扩展双极化维瓦尔第天线的低频性能，为宽带天线系统提供了一种有效的设计方法。

Abstract: Very wideband apertures are needed in positioning, sensing, spectrum monitoring, and modern spread spectrum, e.g., frequency hopping systems. Vivaldi antennas are one of the prominent choices for the aforementioned systems due to their natural wideband characteristics. Furthermore, tightly-coupled antenna arrays have been researched in the recent years to extend the lower band edge of compact arrays by taking advantage of the strong mutual coupling between the elements especially with dipole elements, but not with dual-polarized Vivaldi antennas. This paper presents a novel tightly-coupled dual-polarized antipodal Vivaldi antenna (TC-AVA) with -6 dB impedance bandwidth of 3 to 20 GHz. The tight coupling by overlapping the Vivaldi leaves is shown to extend the lower band edge from 3.75 to 3 GHz and 2.75 GHz, an improvement of 20% to 25% for both polarizations, compared with an isolated antipodal Vivaldi element.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Achieving Skilled and Reliable Daily Probabilistic Forecasts of Wind Power at Subseasonal-to-Seasonal Timescales over France](https://arxiv.org/abs/2511.16164)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: cs.LG

TL;DR: 该研究提出了一个将ECMWF次季节到季节天气预报转化为风功率预测的框架，预测范围从1天到46天，通过后处理改善预报偏差和离散度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 准确可靠的风功率预测对电网稳定、供需平衡和市场风险管理至关重要，但涉及较长预测周期的风功率预测仍需深入研究，尽管次季节到季节天气概率预报已有进展。

Method: 开发了一个预测流程，将ECMWF次季节到季节天气预报转换为风功率预测，包括对结果功率集合进行后处理以解决天气预报的偏差和离散度不足问题。

Result: 该方法在连续排名概率技能评分和集合均方误差方面比气候学基准提高了50%，在15到46天的预测范围内提供了近乎完美的校准。

Conclusion: 该框架能够有效将次季节到季节天气预报转化为风功率预测，显著优于基准方法，并为中长期风功率预测提供了可靠解决方案。

Abstract: Accurate and reliable wind power forecasts are crucial for grid stability, balancing supply and demand, and market risk management. Even though short-term weather forecasts have been thoroughly used to provide short-term renewable power predictions, forecasts involving longer prediction horizons still need investigations. Despite the recent progress in subseasonal-to-seasonal weather probabilistic forecasting, their use for wind power prediction usually involves both temporal and spatial aggregation achieve reasonable skill. In this study, we present a forecasting pipeline enabling to transform ECMWF subseasonal-to-seasonal weather forecasts into wind power forecasts for lead times ranging from 1 day to 46 days at daily resolution. This framework also include post-processing of the resulting power ensembles to account for the biases and lack of dispersion of the weather forecasts. We show that our method is able to outperform a climatological baseline by 50 % in terms of both Continuous Ranked Probability Skill Score and Ensemble Mean Squared Error while also providing near perfect calibration of the forecasts for lead times ranging from 15 to 46 days.

</details>


### [8] [Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn](https://arxiv.org/abs/2511.15738)
*Chao Yu,Qixin Tan,Jiaxuan Gao,Shi Yu,Hong Lu,Xinting Yang,Zelai Xu,Yu Wang,Yi Wu,Eugene Vinitsky*

Main category: cs.LG

TL;DR: 论文提出了三维测试时扩展框架，将上下文长度、批量采样和迭代轮次三个维度的测试时扩展效应结合起来，显著提升了推理模型的性能，并展示了在具身学习中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展受限于基础模型的有限上下文长度，远小于训练时消耗的token数量。需要探索更多维度的测试时扩展方法来突破这一限制。

Method: 提出了三维测试时扩展框架，包括：上下文长度扩展（增加推理上下文）、批量扩展（并行采样提升准确率）、轮次扩展（迭代自我优化提升推理质量）。

Result: 每个维度都表现出测试时扩展效应但容量有限；三个维度结合显著提升了IOI、IMO、CPHO等挑战性测试集的推理性能；人类反馈进一步提升了效果；框架可扩展到具身学习领域。

Conclusion: 三维测试时扩展框架有效突破了单一上下文扩展的限制，为推理强化学习提供了新的扩展维度，在复杂推理任务和开放领域应用中展现出良好潜力。

Abstract: Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors.

</details>


### [9] [Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors](https://arxiv.org/abs/2511.16340)
*Alan Yufei Dong,Jihao Andreas Lin,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出了一种改进高斯过程可扩展性的新方法，通过利用小线性系统的已知解来加速大线性系统的求解器收敛，特别适用于增量数据添加任务。


<details>
  <summary>Details</summary>
Motivation: 高斯过程推理的可扩展性对于序列决策任务至关重要，但改进高斯过程可扩展性仍然是一个具有许多开放研究方向的挑战性问题。

Method: 使用迭代线性求解器（如共轭梯度、随机梯度下降或交替投影）来近似高斯过程后验，通过利用包含在其中的较小系统的已知解来改进大线性系统的求解器收敛。

Result: 该方法在容差求解时实现了加速，并在固定计算预算下改善了贝叶斯优化性能。

Conclusion: 所提出的技术对于增量数据添加任务具有重要意义，能够有效提高高斯过程推理的可扩展性。

Abstract: Scalable Gaussian process (GP) inference is essential for sequential decision-making tasks, yet improving GP scalability remains a challenging problem with many open avenues of research. This paper focuses on iterative GPs, where iterative linear solvers, such as conjugate gradients, stochastic gradient descent or alternative projections, are used to approximate the GP posterior. We propose a new method which improves solver convergence of a large linear system by leveraging the known solution to a smaller system contained within. This is significant for tasks with incremental data additions, and we show that our technique achieves speed-ups when solving to tolerance, as well as improved Bayesian optimisation performance under a fixed compute budget.

</details>


### [10] [Transparent Early ICU Mortality Prediction with Clinical Transformer and Per-Case Modality Attribution](https://arxiv.org/abs/2511.15847)
*Alexander Bakumenko,Janine Hoelscher,Hudson Smith*

Main category: cs.LG

TL;DR: 提出了一种轻量级、透明的多模态集成方法，结合ICU患者前48小时的生理时间序列数据和临床笔记，通过逻辑回归融合双向LSTM和ClinicalModernBERT的预测结果，实现多级可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法虽然预测性能高，但缺乏透明性和鲁棒性，限制了临床采用。需要开发既准确又可解释的ICU死亡率预测系统。

Method: 使用逻辑回归模型融合两个模态特定模型的预测：双向LSTM处理生命体征数据，微调的ClinicalModernBERT处理临床笔记。采用可追溯架构实现多级可解释性。

Result: 在MIMIC-III基准测试中，晚期融合集成比最佳单一模型表现更好（AUPRC 0.565 vs. 0.526；AUROC 0.891 vs. 0.876），同时保持良好校准的预测。系统在模态缺失时仍保持鲁棒性。

Conclusion: 该方法实现了竞争性性能，提供可靠、可审计的风险估计和透明、可预测的操作，这对临床应用至关重要。

Abstract: Early identification of intensive care patients at risk of in-hospital mortality enables timely intervention and efficient resource allocation. Despite high predictive performance, existing machine learning approaches lack transparency and robustness, limiting clinical adoption. We present a lightweight, transparent multimodal ensemble that fuses physiological time-series measurements with unstructured clinical notes from the first 48 hours of an ICU stay. A logistic regression model combines predictions from two modality-specific models: a bidirectional LSTM for vitals and a finetuned ClinicalModernBERT transformer for notes. This traceable architecture allows for multilevel interpretability: feature attributions within each modality and direct per-case modality attributions quantifying how vitals and notes influence each decision. On the MIMIC-III benchmark, our late-fusion ensemble improves discrimination over the best single model (AUPRC 0.565 vs. 0.526; AUROC 0.891 vs. 0.876) while maintaining well-calibrated predictions. The system remains robust through a calibrated fallback when a modality is missing. These results demonstrate competitive performance with reliable, auditable risk estimates and transparent, predictable operation, which together are crucial for clinical use.

</details>


### [11] [Optimal Fairness under Local Differential Privacy](https://arxiv.org/abs/2511.16377)
*Hrad Ghoukasian,Shahab Asoodeh*

Main category: cs.LG

TL;DR: 本文研究了如何设计最优的本地差分隐私机制来减少数据不公平性，从而改善下游分类的公平性。提出了针对二元和多值敏感属性的优化机制，并证明了减少数据不公平性必然降低分类不公平性。


<details>
  <summary>Details</summary>
Motivation: 当前本地差分隐私机制在保护敏感属性隐私的同时，可能加剧数据不公平性，影响下游分类的公平性。需要开发既能保护隐私又能改善公平性的机制。

Method: 首先推导了二元敏感属性的闭式最优机制，然后开发了可处理的优化框架来处理多值属性。建立了隐私感知预处理与分类公平性之间的理论联系。

Result: 实验表明，该方法在多种数据集和公平性指标上始终优于现有LDP机制，在减少数据不公平性的同时保持接近非私有模型的准确性。相比领先的预处理和后处理公平方法，实现了更好的准确性-公平性权衡。

Conclusion: 本地差分隐私是一种原则性且有效的预处理公平干预技术，能够在保护敏感属性隐私的同时显著改善分类公平性。

Abstract: We investigate how to optimally design local differential privacy (LDP) mechanisms that reduce data unfairness and thereby improve fairness in downstream classification. We first derive a closed-form optimal mechanism for binary sensitive attributes and then develop a tractable optimization framework that yields the corresponding optimal mechanism for multi-valued attributes. As a theoretical contribution, we establish that for discrimination-accuracy optimal classifiers, reducing data unfairness necessarily leads to lower classification unfairness, thus providing a direct link between privacy-aware pre-processing and classification fairness. Empirically, we demonstrate that our approach consistently outperforms existing LDP mechanisms in reducing data unfairness across diverse datasets and fairness metrics, while maintaining accuracy close to that of non-private models. Moreover, compared with leading pre-processing and post-processing fairness methods, our mechanism achieves a more favorable accuracy-fairness trade-off while simultaneously preserving the privacy of sensitive attributes. Taken together, these results highlight LDP as a principled and effective pre-processing fairness intervention technique.

</details>


### [12] [ECPv2: Fast, Efficient, and Scalable Global Optimization of Lipschitz Functions](https://arxiv.org/abs/2511.16575)
*Fares Fourati,Mohamed-Slim Alouini,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: ECPv2是一个可扩展的全局优化算法，用于优化Lipschitz连续函数，解决了ECP算法的高计算成本和保守行为问题，通过自适应下界、Worst-m记忆机制和固定随机投影等创新，在保持理论保证的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决ECP算法在全局优化中的高计算成本、早期行为过于保守以及未知Lipschitz常数等关键限制，开发了ECPv2算法。

Method: ECPv2引入了三个创新：(1)自适应下界避免无效接受区域；(2)Worst-m记忆机制限制与固定大小历史评估子集的比较；(3)固定随机投影加速高维距离计算。

Result: 理论证明ECPv2保持ECP的无悔保证和最优有限时间界限，实验验证在广泛的高维非凸优化问题中，ECPv2始终匹配或优于最先进的优化器，同时显著减少运行时间。

Conclusion: ECPv2是一个理论上有保证且实用的全局优化算法，通过关键创新解决了ECP的局限性，在保持理论性能的同时显著提升了计算效率。

Abstract: We propose ECPv2, a scalable and theoretically grounded algorithm for global optimization of Lipschitz-continuous functions with unknown Lipschitz constants. Building on the Every Call is Precious (ECP) framework, which ensures that each accepted function evaluation is potentially informative, ECPv2 addresses key limitations of ECP, including high computational cost and overly conservative early behavior. ECPv2 introduces three innovations: (i) an adaptive lower bound to avoid vacuous acceptance regions, (ii) a Worst-m memory mechanism that restricts comparisons to a fixed-size subset of past evaluations, and (iii) a fixed random projection to accelerate distance computations in high dimensions. We theoretically show that ECPv2 retains ECP's no-regret guarantees with optimal finite-time bounds and expands the acceptance region with high probability. We further empirically validate these findings through extensive experiments and ablation studies. Using principled hyperparameter settings, we evaluate ECPv2 across a wide range of high-dimensional, non-convex optimization problems. Across benchmarks, ECPv2 consistently matches or outperforms state-of-the-art optimizers, while significantly reducing wall-clock time.

</details>


### [13] [Descend or Rewind? Stochastic Gradient Descent Unlearning](https://arxiv.org/abs/2511.15983)
*Siqiao Mu,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文为随机版本的R2D和D2D机器遗忘算法提供了(ε, δ)认证遗忘保证，涵盖强凸、凸和非凸损失函数，通过分析受扰梯度系统的收缩特性来实现理论保证。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘算法旨在无需从头重新训练就能从模型中移除选定训练数据的影响，但现有的随机D2D算法虽被广泛用作基准却缺乏非凸函数上的理论支持。

Method: 通过分析受扰或有偏梯度系统（分别对应收缩、半收缩或扩张特性），采用最优耦合随机遗忘和重新训练轨迹的方法，结合新型松弛高斯机制实现(ε, δ)遗忘。

Result: 证明了D2D在强凸函数上比R2D有更紧的保证，而R2D在凸和非凸设置下也能实现遗忘，因为它通过逆转累积扰动将遗忘模型拉近重新训练模型。

Conclusion: 为随机R2D和D2D算法提供了全面的理论遗忘保证，填补了现有基准算法缺乏理论支持的空白，并阐明了不同算法在不同函数类型下的适用性。

Abstract: Machine unlearning algorithms aim to remove the impact of selected training data from a model without the computational expenses of retraining from scratch. Two such algorithms are ``Descent-to-Delete" (D2D) and ``Rewind-to-Delete" (R2D), full-batch gradient descent algorithms that are easy to implement and satisfy provable unlearning guarantees. In particular, the stochastic version of D2D is widely implemented as the ``finetuning" unlearning baseline, despite lacking theoretical backing on nonconvex functions. In this work, we prove $(ε, δ)$ certified unlearning guarantees for stochastic R2D and D2D for strongly convex, convex, and nonconvex loss functions, by analyzing unlearning through the lens of disturbed or biased gradient systems, which may be contracting, semi-contracting, or expansive respectively. Our argument relies on optimally coupling the random behavior of the unlearning and retraining trajectories, resulting in a probabilistic sensitivity bound that can be combined with a novel relaxed Gaussian mechanism to achieve $(ε, δ)$ unlearning. We determine that D2D can yield tighter guarantees for strongly convex functions compared to R2D by relying on contraction to a unique global minimum. However, unlike D2D, R2D can achieve unlearning in the convex and nonconvex setting because it draws the unlearned model closer to the retrained model by reversing the accumulated disturbances.

</details>


### [14] [A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning](https://arxiv.org/abs/2511.16073)
*Shreyansh Jain,Madhav Singhvi,Shreya Rahul Jain,Pranav S,Dishaa Lokesh,Naren Chittibabu,Akash Anandhan*

Main category: cs.LG

TL;DR: 本文提出了一种基于小型语言模型（<600M参数）的两步简历评估方法，通过监督微调（SFT）和GRPO强化学习优化，解决了传统ATS系统过于依赖关键词匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ATS系统过于僵化，仅依赖关键词匹配，导致有才华的候选人因微小语义不匹配而被拒绝。需要开发更精细的简历评估模型。

Method: 采用两步法：首先使用监督微调（SFT）建立基线模型，然后通过GRPO强化学习优化，使用多组件奖励函数全面评估候选人。

Result: 最终模型在未见测试数据上达到91%准确率，对'SELECTED'类别的召回率为0.85，精确度为1.0，显示出强大的候选识别能力和可靠性。

Conclusion: 经过适当执行的两步微调程序可以有效优化小型语言模型，使其能够进行精细且类似人类的候选人评分，克服传统ATS和简单RL使用的缺点。

Abstract: Conventional Applicant Tracking Systems (ATS) tend to be inflexible keyword-matchers, and deny gifted candidates a role due to a few minor semantic mismatches. This article describes a new two-step process to design a more refined resume evaluation model based on a small language model (<600M parameters) that is finetuned using GRPO on a custom reward function. To begin with, Supervised Fine-Tuning (SFT) was used to build a solid baseline model. Second, this SFT model was also optimized with the help of Reinforcement Learning (RL) through GRPO under the guidance of a new, multi-component reward function that can holistically assess candidates beyond simple keyword matching. We indicate that the RL application presents a critical problem of reward hacking due to the initial experiments of aggressive penalties, which produces faulty, excessively negative model behaviors. We have overcome this challenge by refining the reward function repeatedly and training hyperparameters into a stable "gentle polishing process" of the reward function. Our resulting GRPO-polished model demonstrates significant real-world efficacy, achieving a final accuracy of 91% on unseen test data. The model shows a strong ability to correctly identify qualified candidates (recall of 0.85 for the 'SELECTED' class) while also showing exceptional precision (1.0), confirming its reliability. These results indicate that a properly executed, two-step fine-tuning procedure can indeed effectively refine a small language model to be able to conduct fine-tuned and human-like candidate scoring, overcoming the drawbacks of both traditional ATS and naive RL usage.

</details>


### [15] [Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection](https://arxiv.org/abs/2511.16145)
*Zhijie Zhong,Zhiwen Yu,Kaixiang Yang,C. L. Philip Chen*

Main category: cs.LG

TL;DR: 本文挑战时间序列异常检测中架构复杂性的必要性，通过系统比较监督与无监督方法，提出简单监督基线STAND，证明在有限标签下监督方法显著优于复杂无监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究过度依赖复杂架构建模正常数据分布，忽视了实际场景中有限异常标签带来的性能提升潜力。

Method: 提出STAND作为流线型监督基线，并进行监督与无监督范式的首次系统性比较。

Result: 在五个公共数据集上的实验表明：(1)标签比模型更重要；(2)监督带来更高回报；(3)STAND具有更优的预测一致性和异常定位能力。

Conclusion: 研究倡导时间序列异常检测向以数据为中心的转变，强调标签利用而非纯粹算法复杂性。

Abstract: Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at https://github.com/EmorZz1G/STAND.

</details>


### [16] [Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective](https://arxiv.org/abs/2511.16231)
*Yang Yu*

Main category: cs.LG

TL;DR: 本文分析了pass@k指标作为强化学习优化目标的局限性，指出它本质上是pass@1目标的正向重加权，在需要探索的关键区域提供消失的学习信号，可能导致探索崩溃。


<details>
  <summary>Details</summary>
Motivation: 评估和增强大语言模型的多步推理能力是AI研究的核心焦点，pass@k指标已被广泛采用作为评估标准和优化目标，但其作为直接优化目标的适用性需要深入分析。

Method: 分析pass@k目标函数，推导其梯度，证明其本质上是pass@1目标的逐样本正向重加权，并分析探索崩溃的动态过程。

Result: 发现pass@k目标在探索最关键的机制中提供消失的学习信号，随着策略概率质量集中，pass@k与pass@1之间的差距会减小。

Conclusion: pass@k作为诊断工具有用，但不适合作为直接优化目标，应使用明确鼓励高效探索的机制来推进推理任务中的强化学习。

Abstract: The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of "exploration collapse", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.

</details>


### [17] [Learning-Enhanced Observer for Linear Time-Invariant Systems with Parametric Uncertainty](https://arxiv.org/abs/2511.16318)
*Hao Shu*

Main category: cs.LG

TL;DR: 提出了一种用于线性时不变系统的学习增强观测器(LEO)，通过梯度优化系统矩阵来改进传统观测器设计，在参数不确定性情况下显著降低估计误差。


<details>
  <summary>Details</summary>
Motivation: 传统观测器设计依赖标称模型，在系统参数不确定时性能下降。需要结合现代学习机制来增强观测器的鲁棒性和准确性。

Method: 将系统矩阵作为可优化变量，通过梯度最小化稳态输出差异损失来精炼模型，构建数据驱动的替代模型，同时保留经典观测器结构。

Result: 蒙特卡洛研究表明，在多种系统维度下，开环和Luenberger观测器的归一化估计误差均显著降低，通常超过15%。

Conclusion: 现代学习机制可以作为传统观测器设计的强大补充，在不确定系统中实现更准确和鲁棒的状态估计。

Abstract: This work introduces a learning-enhanced observer (LEO) for linear time-invariant systems with uncertain dynamics. Rather than relying solely on nominal models, the proposed framework treats the system matrices as optimizable variables and refines them through gradient-based minimization of a steady-state output discrepancy loss. The resulting data-informed surrogate model enables the construction of an improved observer that effectively compensates for moderate parameter uncertainty while preserving the structure of classical designs. Extensive Monte Carlo studies across diverse system dimensions show systematic and statistically significant reductions, typically exceeding 15\%, in normalized estimation error for both open-loop and Luenberger observers. These results demonstrate that modern learning mechanisms can serve as a powerful complement to traditional observer design, yielding more accurate and robust state estimation in uncertain systems. Codes are available at https://github.com/Hao-B-Shu/LTI_LEO.

</details>


### [18] [Loss Functions Robust to the Presence of Label Errors](https://arxiv.org/abs/2511.16512)
*Nicholas Pellegrino,David Szczecina,Paul Fieguth*

Main category: cs.LG

TL;DR: 提出两种新的损失函数，通过降低或忽略可能含有标签错误的困难样本权重，来改进训练数据中的标签错误检测。


<details>
  <summary>Details</summary>
Motivation: 检测训练数据中的标签错误需要能够抵抗标签错误的模型，但获取这种模型通常需要在被污染的数据上训练，这带来了挑战。受Focal Loss启发，希望通过调整损失函数来改进这一问题。

Method: 提出了两种新颖而简单的损失函数，这些函数对可能含有标签错误的困难样本进行降权或忽略处理。

Result: 在人工污染数据上的实验结果显示，与传统的分类交叉熵和Focal Loss基线相比，提出的方法在检测标签错误方面的F1分数有所提升。

Conclusion: 通过调整损失函数来处理困难样本（可能含有标签错误），可以有效改进标签错误检测的性能。

Abstract: Methods for detecting label errors in training data require models that are robust to label errors (i.e., not fit to erroneously labelled data points). However, acquiring such models often involves training on corrupted data, which presents a challenge. Adjustments to the loss function present an opportunity for improvement. Motivated by Focal Loss (which emphasizes difficult-to-classify samples), two novel, yet simple, loss functions are proposed that de-weight or ignore these difficult samples (i.e., those likely to have label errors). Results on artificially corrupted data show promise, such that F1 scores for detecting errors are improved from the baselines of conventional categorical Cross Entropy and Focal Loss.

</details>


### [19] [Saving Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2511.16520)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: cs.LG

TL;DR: FMPlug是一个插件框架，通过实例引导的时间相关预热启动策略和锐利高斯正则化，显著提升了基础流匹配模型在逆问题求解中的性能。


<details>
  <summary>Details</summary>
Motivation: 基础流匹配模型在解决逆问题时表现不如领域特定甚至无训练先验，需要解锁其潜力以成为实用的通用先验。

Method: 结合实例引导的时间相关预热启动策略和锐利高斯正则化，在保持高斯结构的同时添加问题特定指导。

Result: 在图像恢复和科学逆问题中实现了显著的性能提升。

Conclusion: FMPlug为将基础流匹配模型转化为实用的、可重用的逆问题求解先验指明了路径。

Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.

</details>


### [20] [FairLRF: Achieving Fairness through Sparse Low Rank Factorization](https://arxiv.org/abs/2511.16549)
*Yuanbo Guo,Jun Xia,Yiyu Shi*

Main category: cs.LG

TL;DR: 提出了一种基于低秩分解的公平性增强框架FairLRF，利用SVD分解选择性移除导致偏差的矩阵元素来提升深度学习模型的公平性，在保持性能的同时减少群体差异。


<details>
  <summary>Details</summary>
Motivation: 现有偏差缓解方法通常计算成本高昂或导致模型准确率显著下降，限制了在资源受限环境中的实用性。作者发现SVD分解得到的酉矩阵元素对敏感属性定义的群体偏差贡献不均等，这启发了利用SVD进行公平性增强的新思路。

Method: 提出FairLRF方法，通过SVD分解模型权重矩阵，识别并选择性移除导致群体偏差的酉矩阵元素，从而减少模型对不同敏感属性群体的预测差异。

Result: 大量实验表明该方法优于传统低秩分解方法和最先进的公平性增强技术，同时通过消融研究分析了主要超参数对处理模型性能的影响。

Conclusion: 这是首个主要利用SVD进行公平性增强而非压缩的研究，证明了SVD在提升模型公平性方面的有效性，为资源受限环境下的公平AI提供了实用解决方案。

Abstract: As deep learning (DL) techniques become integral to various applications, ensuring model fairness while maintaining high performance has become increasingly critical, particularly in sensitive fields such as medical diagnosis. Although a variety of bias-mitigation methods have been proposed, many rely on computationally expensive debiasing strategies or suffer substantial drops in model accuracy, which limits their practicality in real-world, resource-constrained settings. To address this issue, we propose a fairness-oriented low rank factorization (LRF) framework that leverages singular value decomposition (SVD) to improve DL model fairness. Unlike traditional SVD, which is mainly used for model compression by decomposing and reducing weight matrices, our work shows that SVD can also serve as an effective tool for fairness enhancement. Specifically, we observed that elements in the unitary matrices obtained from SVD contribute unequally to model bias across groups defined by sensitive attributes. Motivated by this observation, we propose a method, named FairLRF, that selectively removes bias-inducing elements from unitary matrices to reduce group disparities, thus enhancing model fairness. Extensive experiments show that our method outperforms conventional LRF methods as well as state-of-the-art fairness-enhancing techniques. Additionally, an ablation study examines how major hyper-parameters may influence the performance of processed models. To the best of our knowledge, this is the first work utilizing SVD not primarily for compression but for fairness enhancement.

</details>


### [21] [Almost Sure Convergence Analysis of Differentially Private Stochastic Gradient Methods](https://arxiv.org/abs/2511.16587)
*Amartya Mukherjee,Jun Liu*

Main category: cs.LG

TL;DR: 本文证明了差分隐私随机梯度下降（DP-SGD）及其动量变体在标准平滑性假设下几乎必然收敛，为差分隐私优化提供了更强的理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管DP-SGD已成为训练具有严格隐私保证的机器学习模型的标准算法，但其长期行为的理论理解仍然有限。现有分析通常只建立期望收敛或高概率收敛，未能解决单条轨迹的几乎必然收敛问题。

Method: 在标准平滑性假设下，通过满足标准衰减条件的步长，证明了DP-SGD在非凸和强凸设置下的几乎必然收敛。分析扩展到动量变体如随机重球法（DP-SHB）和Nesterov加速梯度（DP-NAG），通过精心设计的能量构造获得类似保证。

Result: 证明了DP-SGD及其动量变体在非凸和强凸设置下几乎必然收敛，表明尽管存在隐私引起的失真，算法在凸和非凸区域都保持路径稳定性。

Conclusion: 这些结果为差分隐私优化提供了更强的理论基础，表明尽管有隐私引起的扭曲，DP-SGD算法在凸和非凸情况下都保持路径稳定性。

Abstract: Differentially private stochastic gradient descent (DP-SGD) has become the standard algorithm for training machine learning models with rigorous privacy guarantees. Despite its widespread use, the theoretical understanding of its long-run behavior remains limited: existing analyses typically establish convergence in expectation or with high probability, but do not address the almost sure convergence of single trajectories. In this work, we prove that DP-SGD converges almost surely under standard smoothness assumptions, both in nonconvex and strongly convex settings, provided the step sizes satisfy some standard decaying conditions. Our analysis extends to momentum variants such as the stochastic heavy ball (DP-SHB) and Nesterov's accelerated gradient (DP-NAG), where we show that careful energy constructions yield similar guarantees. These results provide stronger theoretical foundations for differentially private optimization and suggest that, despite privacy-induced distortions, the algorithm remains pathwise stable in both convex and nonconvex regimes.

</details>


### [22] [Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies](https://arxiv.org/abs/2511.16596)
*Zohar Rimon,Elisei Shafer,Tal Tepper,Efrat Shimron,Aviv Tamar*

Main category: cs.LG

TL;DR: 提出了一种基于自监督学习的人工触诊方法，通过编码器-解码器框架从触觉测量序列中学习包含被触诊物体所有相关信息的表示，用于触觉成像和变化检测等下游任务。


<details>
  <summary>Details</summary>
Motivation: 当前触诊几乎完全由人类执行，需要开发能够捕捉触觉测量中复杂模式的人工触诊方法，超越简单的力映射图。

Method: 开发模拟环境并收集真实世界数据集，使用配备触觉传感器的机器人收集触诊序列，训练模型预测物体不同位置的感官读数，研究学习到的表示。

Result: 验证了该方法能够学习到有效的表示，并展示了其在成像和变化检测中的应用。

Conclusion: 自监督学习方法能够从触觉测量中学习到包含丰富信息的表示，为人工触诊提供了有前景的概念验证。

Abstract: Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a $\textit{representation}$ from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization](https://arxiv.org/abs/2511.15714)
*Ariel Kamen,Yakov Kamen*

Main category: cs.AI

TL;DR: 本文提出了一个基于大语言模型的非结构化文本分类集成框架(eLLM)，通过整合多个模型解决了单个系统的常见弱点，在IAB分层分类法上实现了高达65%的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 解决单个大语言模型在文本分类中的不一致性、幻觉、类别膨胀和误分类等问题，提高分类的可靠性和准确性。

Method: 采用集成学习方法，通过数学建模集体决策过程并建立原则性聚合准则，在IAB分层分类法上评估了10个最先进的大语言模型，使用8,660个人工标注样本进行零样本测试。

Result: eLLM框架相比最强单模型实现了高达65%的F1分数提升，达到接近人类专家水平的性能，同时提高了鲁棒性和准确性。

Conclusion: eLLM为基于分类法的分类提供了一个可扩展且可靠的解决方案，可能显著减少对人类专家标注的依赖。

Abstract: This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.

</details>


### [24] [MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding](https://arxiv.org/abs/2511.15716)
*Abraham Itzhak Weinberg*

Main category: cs.AI

TL;DR: MACIE是一个多智能体因果智能解释框架，结合结构因果模型、干预反事实和Shapley值，为多智能体强化学习系统提供全面解释，解决个体因果贡献、系统级涌现智能和可操作解释三个问题。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体强化学习系统在安全关键应用中使用，理解智能体决策原因和集体行为实现方式变得至关重要。现有可解释AI方法在多智能体环境中表现不佳，无法将集体结果归因于个体、量化涌现行为或捕捉复杂交互。

Method: MACIE框架结合结构因果模型、干预反事实和Shapley值，通过干预归因分数评估每个智能体的因果贡献，使用协同指标分离集体效应与个体贡献来量化系统级涌现智能，并通过自然语言叙述合成因果洞察提供可操作解释。

Result: 在四种MARL场景（合作、竞争和混合动机）中评估显示：准确的结果归因（平均φ_i=5.07，标准差<0.05），在合作任务中检测到正向涌现（协同指数高达0.461），计算效率高（CPU上每个数据集0.79秒）。

Conclusion: MACIE独特地结合了因果严谨性、涌现量化和多智能体支持，同时保持实时使用的实用性，代表了向可解释、可信赖和负责任的多智能体AI迈出的一步。

Abstract: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.

</details>


### [25] [Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models](https://arxiv.org/abs/2511.15720)
*Islem Sahraoui*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态AI框架，结合文本和图像分析来识别建筑工地的安全隐患，通过两个案例研究评估了大型语言模型和视觉语言模型在自动化危险识别中的能力。


<details>
  <summary>Details</summary>
Motivation: 在建筑工地等安全关键环境中，事故数据通常以多种格式存在（如书面报告、检查记录和现场图像），传统方法难以综合识别危险源。

Method: 开发了多模态AI框架，包含两个案例研究：1）使用GPT-4o和GPT-4o mini从28,000份OSHA事故报告中提取结构化见解；2）使用轻量级开源视觉语言模型Molmo 7B和Qwen2 VL 2B在ConstructionSite10k数据集上进行规则级安全违规检测。

Result: 尽管模型规模较小，Molmo 7B和Qwen2 VL 2B在某些提示配置下表现出有竞争力的性能，证明了低资源多模态系统用于规则感知安全监控的可行性。

Conclusion: 多模态AI框架能够有效结合文本和视觉数据分析，为建筑安全监控提供可行的解决方案，特别是轻量级开源模型在成本敏感场景中具有应用潜力。

Abstract: This thesis explores a multimodal AI framework for enhancing construction safety through the combined analysis of textual and visual data. In safety-critical environments such as construction sites, accident data often exists in multiple formats, such as written reports, inspection records, and site imagery, making it challenging to synthesize hazards using traditional approaches. To address this, this thesis proposed a multimodal AI framework that combines text and image analysis to assist in identifying safety hazards on construction sites. Two case studies were consucted to evaluate the capabilities of large language models (LLMs) and vision-language models (VLMs) for automated hazard identification.The first case study introduces a hybrid pipeline that utilizes GPT 4o and GPT 4o mini to extract structured insights from a dataset of 28,000 OSHA accident reports (2000-2025). The second case study extends this investigation using Molmo 7B and Qwen2 VL 2B, lightweight, open-source VLMs. Using the public ConstructionSite10k dataset, the performance of the two models was evaluated on rule-level safety violation detection using natural language prompts. This experiment served as a cost-aware benchmark against proprietary models and allowed testing at scale with ground-truth labels. Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance in certain prompt configurations, reinforcing the feasibility of low-resource multimodal systems for rule-aware safety monitoring.

</details>


### [26] [Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods](https://arxiv.org/abs/2511.15722)
*Weichen Liu,Qiyao Xue,Haoming Wang,Xiangyu Yin,Boyuan Yang,Wei Gao*

Main category: cs.AI

TL;DR: 本文从认知角度构建空间智能分类法，按推理复杂度组织任务，将现有基准映射到该分类中，分析评估方法，并比较训练式和推理式提升空间能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有调查多基于输入模态分类，但空间能力不仅取决于输入格式。本文从认知角度出发，旨在提供更原则性的跨任务比较，揭示当前模型能力与人类推理之间的关键差距。

Method: 引入基于认知视角的分类法，按推理复杂度划分任务并关联认知功能；将文本、视觉语言和具身环境下的现有基准映射到该分类中；分析评估指标和方法；比较训练式和推理式提升空间能力的方法。

Result: 构建了从认知角度组织空间智能的分类体系，实现了现有基准的映射分析，明确了评估方法，揭示了训练式和推理式方法的互补机制。

Conclusion: 认知视角为空间推理研究提供了更原则性的分析框架，有助于新研究者全面理解该领域并为未来研究提供可行方向。

Abstract: Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.

</details>


### [27] [Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications](https://arxiv.org/abs/2511.15763)
*Raymond K. Sheh,Karen Geappen*

Main category: cs.AI

TL;DR: 本文提出了一种AI供应链风险分类法，填补了当前AI风险评估在供应链环节的空白，帮助非技术背景的利益相关者系统识别AI系统依赖关系。


<details>
  <summary>Details</summary>
Motivation: 当前AI风险评估主要关注算法偏见和模型幻觉等问题，但缺乏对AI供应链（数据源、预训练模型、服务等）的系统风险评估，这在关键应用中尤为危险。

Method: 调研当前AI风险评估和管理现状，重点关注AI供应链风险，并提出专门用于分类AI供应链实体的分类法。

Result: 开发了一个AI供应链实体分类法，帮助利益相关者系统性地识别和盘点组织AI系统的依赖关系。

Conclusion: 该研究填补了当前AI治理与关键应用中AI使用风险评估之间的空白，提供了可操作的风险评估和管理方法。

Abstract: Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.
  We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to "consider the right questions" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications.

</details>


### [28] [Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights](https://arxiv.org/abs/2511.15778)
*Paulina Tworek,Miłosz Bargieł,Yousef Khan,Tomasz Pełech-Pilichowski,Marek Mikołajczyk,Roman Lewandowski,Jose Sousa*

Main category: cs.AI

TL;DR: 比较基于规则的NLP方法和大型语言模型在从波兰电子健康记录中提取医疗信息的性能，分析文本标准化和翻译对信息提取的影响。


<details>
  <summary>Details</summary>
Motivation: 解决非英语环境下从非结构化临床文本中提取结构化医疗信息的挑战，特别是在资源稀缺的情况下。

Method: 使用基于规则的低计算NLP方法和大型语言模型从波兰电子健康记录中提取患者人口统计信息、临床发现和处方药物，评估文本标准化缺失和翻译导致的信息损失影响。

Result: 基于规则的方法在信息检索任务中准确率更高，特别是在年龄和性别提取方面；而大型语言模型在药物名称识别方面表现更好，具有更强的适应性和可扩展性。

Conclusion: 建议采用混合方法，结合基于规则系统的精确性和大型语言模型的适应性，为现实医院环境提供更可靠和资源高效的临床NLP解决方案。

Abstract: Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.

</details>


### [29] [Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions](https://arxiv.org/abs/2511.15830)
*Stéphane Aroca-Ouellette,Ian Berlot-Attwell,Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Tongqi Zhu,Herin Kang,Kaheer Suleman,Sam Pasupalak*

Main category: cs.AI

TL;DR: 该论文介绍了Mini Amusement Parks (MAPs)模拟器，用于评估智能体在复杂商业环境中的决策能力，发现人类表现远超现有LLM智能体。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在现实世界决策中面临挑战，但现有基准测试孤立评估各项能力，无法全面评估整体决策能力。

Method: 开发了MAPs游乐园模拟器，统一评估环境建模、长期规划、空间推理等能力，并提供人类基准和最新LLM智能体的全面评估。

Result: 人类在简单模式下表现优于AI系统6.5倍，中等模式下优于9.8倍，揭示了AI在长期优化、样本效率学习等方面的持续弱点。

Conclusion: MAPs为评估适应性决策能力的智能体提供了新的基准基础，统一了现实决策中的多种挑战。

Abstract: Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs

</details>


### [30] [Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs](https://arxiv.org/abs/2511.15895)
*Ivan Chulo,Ananya Joshi*

Main category: cs.AI

TL;DR: 本文通过对比激活引导与基线LLMs的激活，使用线性探针分析45种认知行为，发现情感理解而非分析推理是LLMs成功心智理论能力的关键机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明激活引导能显著改善语言模型的心智理论能力，但其内部机制导致不同输出的具体变化仍不清楚。

Method: 应用对比激活加法引导Gemma-3-4B模型，在1000个BigToM前向信念场景上评估，使用线性探针分析45种认知行为。

Result: 信念归因任务准确率从32.5%提升至46.7%，这种改善由情感内容处理（情感感知+2.23，情感评价+2.20）介导，同时抑制分析过程（质疑-0.78，收敛思维-1.59）。

Conclusion: LLMs中成功的心智理论能力由情感理解介导，而非分析推理。

Abstract: Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\% to 46.7\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.

</details>


### [31] [JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation](https://arxiv.org/abs/2511.15958)
*Zhenyu Bi,Gaurav Srivastava,Yang Li,Meng Lu,Swastik Roy,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: JudgeBoard是一个新颖的评估框架，直接查询模型来评估候选答案的正确性，无需额外答案比较。通过多智能体评判(MAJ)框架，使用多个交互的小型语言模型来近似大型语言模型的判断准确性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在判断答案正确性方面的能力尚不明确，现有基于比较的评估方法间接且难以完全自动化，无法支持细粒度和可扩展的推理输出评估。

Method: 提出JudgeBoard评估管道，直接评估候选答案正确性；开发MAJ多智能体评估框架，利用多个具有不同推理特征的小型语言模型通过协作审议来提升判断性能。

Result: 实验结果显示小型语言模型与大型语言模型在独立判断任务中存在显著性能差距，但MAJ框架显著提高了小型语言模型的可靠性和一致性。在MATH数据集上，基于小型模型的MAJ表现与大型模型相当甚至更好。

Conclusion: 多智能体小型语言模型系统在判断任务中可能匹配或超越大型语言模型的性能，为可扩展和高效评估提供了新思路。

Abstract: While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.

</details>


### [32] [An Aligned Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size](https://arxiv.org/abs/2511.16045)
*Jorge A. Huertas,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: 本文提出了一种新的约束规划模型，用于解决考虑最小批次大小的串行批次调度问题，避免了现有方法依赖预定义虚拟批次集合的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有约束规划模型依赖预定义的虚拟批次集合，这会导致维度灾难并增加问题复杂性。在半导体制造等实际应用中，最小批次大小是常见要求，需要更高效的解决方案。

Method: 提出不依赖虚拟批次集合的新约束规划模型，使用关键对齐参数直接在机器上调度同族作业序列，形成更紧凑的表述。通过定制搜索阶段和加强约束传播器的推理级别来改进模型。

Result: 在近5000个实例上的计算实验表明，所提模型在最多100个作业的小到中型实例上表现优越，在最多500个作业、10个家族和10台机器的大规模实例上，能找到比现有方法优25%的解决方案。

Conclusion: 新约束规划模型在串行批次调度问题上显著优于现有方法，包括混合整数规划、禁忌搜索元启发式和传统约束规划方法，特别是在大规模实例上表现突出。

Abstract: In serial batch (s-batch) scheduling, jobs from similar families are grouped into batches and processed sequentially to avoid repetitive setups that are required when processing consecutive jobs of different families. Despite its large success in scheduling, only three Constraint Programming (CP) models have been proposed for this problem considering minimum batch sizes, which is a common requirement in many practical settings, including the ion implantation area in semiconductor manufacturing. These existing CP models rely on a predefined virtual set of possible batches that suffers from the curse of dimensionality and adds complexity to the problem. This paper proposes a novel CP model that does not rely on this virtual set. Instead, it uses key alignment parameters that allow it to reason directly on the sequences of same-family jobs scheduled on the machines, resulting in a more compact formulation. This new model is further improved by exploiting the problem's structure with tailored search phases and strengthened inference levels of the constraint propagators. The extensive computational experiments on nearly five thousand instances compare the proposed models against existing methods in the literature, including mixed-integer programming formulations, tabu search meta-heuristics, and CP approaches. The results demonstrate the superiority of the proposed models on small-to-medium instances with up to 100 jobs, and their ability to find solutions up to 25\% better than the ones produces by existing methods on large-scale instances with up to 500 jobs, 10 families, and 10 machines.

</details>


### [33] [A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management](https://arxiv.org/abs/2511.16075)
*Hrikshesh Kumar,Anika Garg,Anshul Gupta,Yashika Agarwal*

Main category: cs.AI

TL;DR: 提出了一种结合CNN-LSTM时间序列预测和多智能体深度强化学习的混合架构，用于云边工作负载资源管理，实现从被动响应到主动预测的转变。


<details>
  <summary>Details</summary>
Motivation: 传统云边工作负载资源管理过于被动，依赖静态阈值导致资源过度配置或性能下降，需要转向主动解决方案。

Method: 设计混合架构，将CNN-LSTM模型的时间序列预测嵌入到基于多智能体深度强化学习的编排器状态空间中，使AI管理器能够预见未来并做出长期规划决策。

Result: 测试表明该系统明显优于传统方法，能够有效解决复杂决策问题，同时平衡成本节约、系统健康和用户体验等多个目标。

Conclusion: 通过将预测能力嵌入强化学习智能体，系统能够预见未来并找到资源管理的最优平衡点，实现从被动响应到主动规划的转变。

Abstract: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable

</details>


### [34] [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)
*Pei Yang,Ke Zhang,Ji Wang,Xiao Chen,Yuxin Tang,Eric Yang,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: CRM是一个多智能体协作奖励模型框架，用专家评估器团队替代单一黑盒奖励模型，提高RLHF的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型难以同时优化多个可能冲突的偏好维度（如事实性、帮助性、安全性），且评分透明度有限。

Method: 将偏好评估分解为领域特定的智能体生成部分信号，结合全局评估器，通过中央聚合器融合信号，平衡逐步正确性、多智能体一致性和重复惩罚等因素。

Result: CRM与rewardBench基准套件一起提供了更透明的奖励建模和更稳定优化的实用模块化路径。

Conclusion: 该框架无需额外人工标注即可实现多视角奖励塑造，与标准RL流程兼容。

Abstract: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.

</details>


### [35] [Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes](https://arxiv.org/abs/2511.16548)
*Guanchen Wu,Yuzhang Xie,Huanwei Wu,Zhe He,Hui Shao,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: CLOZE是一个使用大语言模型从临床笔记中自动提取医学实体并整合到层次化医学本体中的零样本框架，无需额外训练或标注数据，保护患者隐私，具有准确、可扩展的特点。


<details>
  <summary>Details</summary>
Motivation: 临床笔记作为富含详细患者观察的非结构化文档，为医学本体扩展提供了有价值但未被充分利用的资源，直接利用临床笔记进行本体扩展的研究尚不充分。

Method: 利用预训练大语言模型的强大语言理解和广泛生物医学知识，CLOZE框架自动识别疾病相关概念并捕获复杂的层次关系，采用零样本方法无需额外训练，通过自动化移除受保护健康信息确保患者隐私。

Result: 实验结果表明CLOZE提供了一个准确、可扩展且保护隐私的本体扩展框架，在生物医学研究和临床信息学中具有广泛下游应用潜力。

Conclusion: CLOZE框架成功展示了利用大语言模型从临床笔记中自动扩展医学本体的可行性，为零样本、隐私保护的生物医学本体构建提供了有效解决方案。

Abstract: Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.

</details>


### [36] [Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization](https://arxiv.org/abs/2511.16602)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Yingji Zhang,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Haozhe Shan,Junbo Qi,Yan Bai,Dengjie Li,Jiachen Luo,Yidong Wang,Yong Dai,Zenglin Xu,Bin Shen,Qifan Wang,Jian Tang,Xiaozhu Ju*

Main category: cs.AI

TL;DR: DPPO是一个元认知训练框架，通过动态交替监督微调（能力扩展）和强化学习（技能精炼）来解决具身智能中的数据瓶颈和算法效率问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能系统的两大挑战：真实世界数据稀缺昂贵的数据瓶颈，以及现有方法资源消耗大的算法效率问题。

Method: 提出Deliberate Practice Policy Optimization (DPPO)框架，采用元认知"Metaloop"训练方法，动态交替监督微调和强化学习，实现自动弱点识别和针对性资源分配。

Result: 训练出的Pelican-VL 1.0模型相比基础模型性能提升20.3%，在100B参数规模上超越开源模型10.6%。

Conclusion: DPPO是首个系统性解决数据和资源瓶颈的框架，能够高效构建多功能具身智能体，相关模型和代码已开源。

Abstract: Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.

</details>


### [37] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite是一个轻量级贝叶斯增强框架，用于基于transformer的临床语言模型，旨在产生可靠、不确定性感知的预测，无需重新训练或架构修改。


<details>
  <summary>Details</summary>
Motivation: transformer模型在临床决策支持中显示出强大潜力，但在模糊医疗案例中容易过度自信，而校准的不确定性对医疗应用至关重要。

Method: 框架集成三个组件：(i)使用蒙特卡洛dropout进行贝叶斯嵌入校准以获取认知不确定性，(ii)不确定性加权注意力机制对token可靠性进行边际化，(iii)受临床风险最小化启发的置信度引导决策塑造。

Result: 在生物医学QA和临床预测基准测试中，MedBayes-Lite持续改善校准和可信度，将过度自信减少32-48%。在模拟临床设置中，通过标记不确定预测供人工审查，可以预防高达41%的诊断错误。

Conclusion: 该框架在医疗AI系统中实现了可靠的不确定性传播，并提高了可解释性。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


### [38] [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660)
*Priyanka Kargupta,Shuyue Stella Li,Haocheng Wang,Jinu Lee,Shan Chen,Orevaoghene Ahia,Dean Light,Thomas L. Griffiths,Max Kleiman-Weiner,Jiawei Han,Asli Celikyilmaz,Yulia Tsvetkov*

Main category: cs.AI

TL;DR: 该研究提出了一个包含28个认知元素的分类法，分析了17个模型和人类在推理过程中的行为差异，发现人类使用层次嵌套和元认知监控，而模型依赖浅层前向链式推理。研究开发了测试时推理指导方法，可将复杂问题性能提升60%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂问题上表现良好但在简单变体上失败，表明它们通过不同于人类推理的机制获得正确输出。研究旨在弥合认知科学与LLM研究之间的差距，建立基于认知机制而非记忆或虚假推理捷径的模型开发基础。

Method: 综合认知科学研究构建28个认知元素的分类法，分析17个模型的170K推理轨迹和54个人类思维轨迹，并进行1,598篇LLM推理论文的元分析。开发测试时推理指导方法来自动构建成功推理结构。

Result: 发现人类和模型在推理结构上存在系统性差异：人类使用层次嵌套和元认知监控，模型依赖浅层前向链式推理。元分析显示研究社区关注可量化行为而忽视元认知控制。推理指导方法可将复杂问题性能提升60%。

Conclusion: 通过连接认知科学和LLM研究，为开发基于原则性认知机制而非脆弱虚假推理捷径或记忆的模型奠定了基础，为改进模型能力和大规模测试人类认知理论开辟了新方向。

Abstract: Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.

</details>

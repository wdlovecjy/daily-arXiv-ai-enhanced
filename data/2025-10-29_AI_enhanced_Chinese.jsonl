{"id": "2510.23832", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23832", "abs": "https://arxiv.org/abs/2510.23832", "authors": ["Evan Allen", "Karim Said", "Robert Calderbank", "Lingjia Liu"], "title": "Communication in a Fractional World: MIMO MC-OTFS Precoder Prediction", "comment": null, "summary": "As 6G technologies advance, international bodies and regulatory agencies are\nintensifying efforts to extend seamless connectivity especially for\nhigh-mobility scenarios such as Mobile Ad-Hoc Networks (\\textit{MANETs}) types\nsuch as Vehicular Ad-Hoc Networks (\\textit{VANETs}) and Flying Ad-Hoc Networks\n(\\textit{FANETs}). For these environments to be considered for long term\nadoption and use they must support Multiple-Input-Multiple- (MIMO) technology,\nrapidly fluctuating channel conditions in these environments place a heavy\nburden on traditional time-frequency CSI feedback schemes required for MIMO\nprecoding. This motivates a shift toward delay-Doppler representations like\nthose employed by Orthogonal Time-Frequency Space(OTFS) modulation, which\noffers greater stability under mobility. We derive an expression for the\nvariation over time in the OTFS I/O relationship. We then use this to create a\nphysics informed complex exponential basis expansion model prediction framework\nthat maximizes the usefulness of outdated Channel State Information (CSI) in\nthe presence of integer and fractional delay-Doppler channels and facilitates\nhigh mobility MIMO communication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOTFS\u8c03\u5236\u7684\u7269\u7406\u4fe1\u606f\u590d\u6742\u6307\u6570\u57fa\u6269\u5c55\u6a21\u578b\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0bMIMO\u901a\u4fe1\u4e2d\u7684CSI\u53cd\u9988\u95ee\u9898\u3002", "motivation": "\u968f\u77406G\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u5982VANETs\u548cFANETs\u9700\u8981\u652f\u6301MIMO\u6280\u672f\uff0c\u4f46\u4f20\u7edf\u65f6\u9891CSI\u53cd\u9988\u65b9\u6848\u5728\u5feb\u901f\u53d8\u5316\u7684\u4fe1\u9053\u6761\u4ef6\u4e0b\u8d1f\u62c5\u8fc7\u91cd\uff0c\u56e0\u6b64\u9700\u8981\u8f6c\u5411\u66f4\u7a33\u5b9a\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63a8\u5bfc\u4e86OTFS\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u968f\u65f6\u95f4\u53d8\u5316\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u521b\u5efa\u4e86\u7269\u7406\u4fe1\u606f\u590d\u6742\u6307\u6570\u57fa\u6269\u5c55\u6a21\u578b\u9884\u6d4b\u6846\u67b6\uff0c\u6700\u5927\u5316\u8fc7\u65f6CSI\u7684\u6709\u6548\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u6574\u6570\u548c\u5206\u6570\u5ef6\u8fdf-\u591a\u666e\u52d2\u4fe1\u9053\uff0c\u4fc3\u8fdb\u9ad8\u79fb\u52a8\u6027MIMO\u901a\u4fe1\u3002", "conclusion": "OTFS\u8c03\u5236\u5728\u79fb\u52a8\u6027\u73af\u5883\u4e0b\u63d0\u4f9b\u66f4\u5927\u7a33\u5b9a\u6027\uff0c\u6240\u63d0\u51fa\u7684\u9884\u6d4b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684CSI\u53cd\u9988\u6311\u6218\u3002"}}
{"id": "2510.23837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23837", "abs": "https://arxiv.org/abs/2510.23837", "authors": ["Ali Amhaz", "Shreya Khisa", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Coordinated Multipoint Transmission in Pinching Antenna Systems", "comment": null, "summary": "We study a coordinated multi-point (CoMP) transmission where two base\nstations (BSs), each supported by a pinching antenna system (PASS), are\ndeployed to jointly serve communication users under spatial division multiple\naccess (SDMA) technology. Pinching Antenna technology was introduced as a\npromising solution to overcome the large-scale fading that has been shown to be\nan impediment in multiple-input multiple-output (MIMO) systems. To realize the\nadvantages of this technology in CoMP systems, which suffer from an upperbound\nrate limitation when traditional uniform linear arrays (ULAs) are adopted, we\nformulate an optimization problem with the aim of maximizing the achievable sum\nrate by jointly determining the transmit beamforming vectors and pinching\nlocations on the waveguides while respecting the quality of service (QoS)\nrequirements of users. This problem is inherently non-convex due to the strong\ncoupling among its decision parameters, making it challenging to solve using\ntraditional optimization methods. Thus, we utilize a gradient-based\nmeta-learning (GML) strategy specifically designed for large-scale optimization\ntasks. Finally, numerical analysis demonstrates the effectiveness of the\nproposed GML approach, achieving 92 percent of the optimal solution, and the\nsuperiority of the solution presented compared to other benchmarks. In\naddition, it achieves a higher upper bound on the achievable rate compared to\nconventional CoMP systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u534f\u8c03\u591a\u70b9\u4f20\u8f93\u7cfb\u7edf\u4e2d\u4f7f\u7528\u634f\u5408\u5929\u7ebf\u6280\u672f\u6765\u514b\u670d\u5927\u89c4\u6a21\u8870\u843d\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u548c\u634f\u5408\u4f4d\u7f6e\u6765\u6700\u5927\u5316\u53ef\u5b9e\u73b0\u7684\u548c\u901f\u7387\uff0c\u5e76\u91c7\u7528\u68af\u5ea6\u5143\u5b66\u4e60\u7b56\u7565\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u534f\u8c03\u591a\u70b9\u4f20\u8f93\u7cfb\u7edf\u5728\u4f7f\u7528\u5747\u5300\u7ebf\u6027\u9635\u5217\u65f6\u5b58\u5728\u901f\u7387\u4e0a\u9650\u9650\u5236\uff0c\u800c\u634f\u5408\u5929\u7ebf\u6280\u672f\u88ab\u8bc1\u660e\u662f\u514b\u670dMIMO\u7cfb\u7edf\u4e2d\u5927\u89c4\u6a21\u8870\u843d\u95ee\u9898\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5c06\u8be5\u6280\u672f\u4f18\u52bf\u5e94\u7528\u4e8eCoMP\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u5143\u5b66\u4e60\u7b56\u7565\u6765\u8054\u5408\u4f18\u5316\u4f20\u8f93\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u548c\u6ce2\u5bfc\u4e0a\u7684\u634f\u5408\u4f4d\u7f6e\uff0c\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u670d\u52a1\u8d28\u91cf\u8981\u6c42\uff0c\u89e3\u51b3\u7531\u4e8e\u51b3\u7b56\u53c2\u6570\u5f3a\u8026\u5408\u5bfc\u81f4\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u5206\u6790\u8868\u660e\u6240\u63d0\u51fa\u7684GML\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u4f18\u89e3\u768492%\uff0c\u76f8\u6bd4\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u76f8\u6bd4\u4f20\u7edfCoMP\u7cfb\u7edf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ef\u5b9e\u73b0\u901f\u7387\u4e0a\u9650\u3002", "conclusion": "\u634f\u5408\u5929\u7ebf\u6280\u672f\u5728CoMP\u7cfb\u7edf\u4e2d\u80fd\u591f\u6709\u6548\u514b\u670d\u5927\u89c4\u6a21\u8870\u843d\uff0c\u6240\u63d0\u51fa\u7684\u68af\u5ea6\u5143\u5b66\u4e60\u65b9\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.23666", "categories": ["stat.ML", "cs.LG", "stat.ME", "I.2.6; G.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.23666", "abs": "https://arxiv.org/abs/2510.23666", "authors": ["Junpeng Gong", "Chunkai Wang", "Hao Li", "Jinyong Ma", "Haoxuan Li", "Xu He"], "title": "Beyond Normality: Reliable A/B Testing with Non-Gaussian Data", "comment": "11 pages, 3 figures", "summary": "A/B testing has become the cornerstone of decision-making in online markets,\nguiding how platforms launch new features, optimize pricing strategies, and\nimprove user experience. In practice, we typically employ the pairwise $t$-test\nto compare outcomes between the treatment and control groups, thereby assessing\nthe effectiveness of a given strategy. To be trustworthy, these experiments\nmust keep Type I error (i.e., false positive rate) under control; otherwise, we\nmay launch harmful strategies. However, in real-world applications, we find\nthat A/B testing often fails to deliver reliable results. When the data\ndistribution departs from normality or when the treatment and control groups\ndiffer in sample size, the commonly used pairwise $t$-test is no longer\ntrustworthy. In this paper, we quantify how skewed, long tailed data and\nunequal allocation distort error rates and derive explicit formulas for the\nminimum sample size required for the $t$-test to remain valid. We find that\nmany online feedback metrics require hundreds of millions samples to ensure\nreliable A/B testing. Thus we introduce an Edgeworth-based correction that\nprovides more accurate $p$-values when the available sample size is limited.\nOffline experiments on a leading A/B testing platform corroborate the practical\nvalue of our theoretical minimum sample size thresholds and demonstrate that\nthe corrected method substantially improves the reliability of A/B testing in\nreal-world conditions.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86A/B\u6d4b\u8bd5\u4e2dt\u68c0\u9a8c\u5728\u6570\u636e\u975e\u6b63\u6001\u5206\u5e03\u6216\u6837\u672c\u91cf\u4e0d\u5747\u8861\u65f6\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6700\u5c0f\u6837\u672c\u91cf\u8ba1\u7b97\u516c\u5f0f\u548cEdgeworth\u6821\u6b63\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86A/B\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u3002", "motivation": "A/B\u6d4b\u8bd5\u5df2\u6210\u4e3a\u5728\u7ebf\u5e02\u573a\u51b3\u7b56\u7684\u6838\u5fc3\u5de5\u5177\uff0c\u4f46\u5f53\u6570\u636e\u5206\u5e03\u504f\u79bb\u6b63\u6001\u6027\u6216\u5904\u7406\u7ec4\u4e0e\u5bf9\u7167\u7ec4\u6837\u672c\u91cf\u4e0d\u540c\u65f6\uff0c\u5e38\u7528\u7684\u6210\u5bf9t\u68c0\u9a8c\u4e0d\u518d\u53ef\u9760\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u7b56\u7565\u7684\u53d1\u5e03\u3002", "method": "\u91cf\u5316\u504f\u659c\u3001\u957f\u5c3e\u6570\u636e\u548c\u4e0d\u7b49\u5206\u914d\u5bf9\u9519\u8bef\u7387\u7684\u5f71\u54cd\uff0c\u63a8\u5bfct\u68c0\u9a8c\u6709\u6548\u6240\u9700\u7684\u6700\u5c0f\u6837\u672c\u91cf\u663e\u5f0f\u516c\u5f0f\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eEdgeworth\u7684\u6821\u6b63\u65b9\u6cd5\u6765\u63d0\u4f9b\u66f4\u51c6\u786e\u7684p\u503c\u3002", "result": "\u53d1\u73b0\u8bb8\u591a\u5728\u7ebf\u53cd\u9988\u6307\u6807\u9700\u8981\u6570\u4ebf\u6837\u672c\u624d\u80fd\u786e\u4fdd\u53ef\u9760\u7684A/B\u6d4b\u8bd5\uff0c\u79bb\u7ebf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u7406\u8bba\u6700\u5c0f\u6837\u672c\u91cf\u9608\u503c\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u6821\u6b63\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u6d4b\u8bd5\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6821\u6b63\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86A/B\u6d4b\u8bd5\u5728\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7edf\u8ba1\u57fa\u7840\u3002"}}
{"id": "2510.23617", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23617", "abs": "https://arxiv.org/abs/2510.23617", "authors": ["Phuong Q. Dao", "Mark Roantree", "Vuong M. Ngo"], "title": "An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis", "comment": "The paper has been accepted for presentation at the MEDES 2025\n  conference", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by\njointly analyzing data from multiple modalities typically text and images\noffering a richer and more accurate interpretation than unimodal approaches. In\nthis paper, we first propose BERT-ViT-EF, a novel model that combines powerful\nTransformer-based encoders BERT for textual input and ViT for visual input\nthrough an early fusion strategy. This approach facilitates deeper cross-modal\ninteractions and more effective joint representation learning. To further\nenhance the model's capability, we propose an extension called the Dual\nTransformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN\nincorporates an additional Transformer encoder layer after BERT to refine\ntextual context (before fusion) and employs contrastive learning to align text\nand image representations, fostering robust multimodal feature learning.\nEmpirical results on two widely used MSA benchmarks MVSA-Single and TumEmo\ndemonstrate the effectiveness of our approach. DTCN achieves best accuracy\n(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on\nMVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements\nhighlight the benefits of early fusion and deeper contextual modeling in\nTransformer-based multimodal sentiment analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86BERT-ViT-EF\u6a21\u578b\u53ca\u5176\u6269\u5c55DTCN\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u3002DTCN\u901a\u8fc7\u65e9\u671f\u878d\u5408\u7b56\u7565\u7ed3\u5408BERT\u548cViT\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u6765\u5bf9\u9f50\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\uff0c\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u901a\u8fc7\u8054\u5408\u5206\u6790\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\uff0c\u80fd\u591f\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\u63d0\u4f9b\u66f4\u4e30\u5bcc\u548c\u51c6\u786e\u7684\u60c5\u611f\u7406\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u8054\u5408\u8868\u793a\u5b66\u4e60\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u9996\u5148\u63d0\u51faBERT-ViT-EF\u6a21\u578b\uff0c\u4f7f\u7528Transformer\u7f16\u7801\u5668\uff08BERT\u7528\u4e8e\u6587\u672c\uff0cViT\u7528\u4e8e\u56fe\u50cf\uff09\u901a\u8fc7\u65e9\u671f\u878d\u5408\u7b56\u7565\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u7136\u540e\u63d0\u51faDTCN\u6269\u5c55\uff0c\u5728BERT\u540e\u6dfb\u52a0\u989d\u5916\u7684Transformer\u7f16\u7801\u5668\u5c42\u6765\u4f18\u5316\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u5e76\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u5bf9\u9f50\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u3002", "result": "\u5728MVSA-Single\u548cTumEmo\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDTCN\u5728TumEmo\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u51c6\u786e\u7387\uff0878.4%\uff09\u548cF1\u5206\u6570\uff0878.3%\uff09\uff0c\u5728MVSA-Single\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u4e3a76.6%\uff0cF1\u5206\u6570\u4e3a75.9%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u65e9\u671f\u878d\u5408\u548c\u66f4\u6df1\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23621", "abs": "https://arxiv.org/abs/2510.23621", "authors": ["Alexandre Benoit"], "title": "Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields", "comment": "78 pages, 21 figures", "summary": "Machine-learning force fields can deliver accurate molecular dynamics (MD) at\nhigh computational cost. For SO(3)-equivariant models such as MACE, there is\nlittle systematic evidence on whether reduced-precision arithmetic and\nGPU-optimized kernels can cut this cost without harming physical fidelity. This\nthesis aims to make MACE cheaper and faster while preserving accuracy by\nidentifying computational bottlenecks and evaluating low-precision execution\npolicies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA\ncuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32\naccumulation) for inference, short NVT and long NPT water simulations, and toy\ntraining runs under reproducible, steady-state timing. cuEquivariance reduces\ninference latency by about $3\\times$. Casting only linear layers to BF16/FP16\nwithin an FP32 model yields roughly 4x additional speedups, while energies and\nthermodynamic observables in NVT/NPT MD remain within run-to-run variability.\nHalf-precision weights during training degrade force RMSE. Mixing e3nn and cuEq\nmodules without explicit adapters causes representation mismatches. Fused\nequivariant kernels and mixed-precision inference can substantially accelerate\nstate-of-the-art force fields with negligible impact on downstream MD. A\npractical policy is to use cuEquivariance with FP32 by default and enable\nBF16/FP16 for linear layers (keeping FP32 accumulations) for maximum\nthroughput, while training remains in FP32. Further gains are expected on\nAmpere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and\npipeline fusion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u964d\u4f4e\u7cbe\u5ea6\u7b97\u672f\u548cGPU\u4f18\u5316\u5185\u6838\u6765\u52a0\u901fSO(3)-\u7b49\u53d8\u673a\u5668\u5b66\u4e60\u529b\u573aMACE\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u4fdd\u771f\u5ea6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u529b\u573a\u867d\u7136\u80fd\u63d0\u4f9b\u7cbe\u786e\u7684\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u964d\u4f4e\u7cbe\u5ea6\u548cGPU\u4f18\u5316\u662f\u5426\u4f1a\u5f71\u54cd\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u7cfb\u7edf\u6027\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u7aef\u5230\u7aef\u548c\u9010\u5757\u5206\u6790MACE\u6027\u80fd\uff0c\u6bd4\u8f83e3nn\u548ccuEquivariance\u540e\u7aef\uff0c\u8bc4\u4f30FP64/FP32/BF16/FP16\u7cbe\u5ea6\u8bbe\u7f6e\uff0c\u5728\u53ef\u91cd\u590d\u7a33\u6001\u65f6\u5e8f\u4e0b\u8fdb\u884c\u63a8\u7406\u3001\u77edNVT\u548c\u957fNPT\u6c34\u6a21\u62df\u4ee5\u53ca\u73a9\u5177\u8bad\u7ec3\u8fd0\u884c\u3002", "result": "cuEquivariance\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u7ea63\u500d\uff0c\u4ec5\u5c06\u7ebf\u6027\u5c42\u8f6c\u6362\u4e3aBF16/FP16\u53ef\u83b7\u5f97\u989d\u59164\u500d\u52a0\u901f\uff0cNVT/NPT MD\u4e2d\u7684\u80fd\u91cf\u548c\u70ed\u529b\u5b66\u89c2\u6d4b\u503c\u4fdd\u6301\u5728\u8fd0\u884c\u95f4\u53d8\u5f02\u6027\u8303\u56f4\u5185\u3002\u8bad\u7ec3\u4e2d\u4f7f\u7528\u534a\u7cbe\u5ea6\u6743\u91cd\u4f1a\u964d\u4f4e\u529bRMSE\u3002", "conclusion": "\u878d\u5408\u7b49\u53d8\u5185\u6838\u548c\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6700\u5148\u8fdb\u7684\u529b\u573a\uff0c\u5bf9\u4e0b\u6e38MD\u5f71\u54cd\u53ef\u5ffd\u7565\u3002\u5efa\u8bae\u9ed8\u8ba4\u4f7f\u7528cuEquivariance\u548cFP32\uff0c\u4e3a\u7ebf\u6027\u5c42\u542f\u7528BF16/FP16\u4ee5\u83b7\u5f97\u6700\u5927\u541e\u5410\u91cf\uff0c\u8bad\u7ec3\u4fdd\u6301FP32\u7cbe\u5ea6\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u6307\u51faAI\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53d6\u4ee3\u800c\u975e\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\u3002", "motivation": "\u7814\u7a76AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89d2\u8272\uff0c\u7279\u522b\u662f\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u62c5\u5fc3AI\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u521b\u9020\u529b\u7406\u8bba\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5f15\u5165\u5b66\u79d1\u521b\u9020\u529b\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u9886\u57df\u7684\u4e24\u4e2a\u6848\u4f8b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u8868\u660e\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b\u6d89\u53caAI\u7684\u65b9\u6cd5\u53ef\u80fd\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\u3002", "conclusion": "AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f7f\u7528\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u53ef\u80fd\u524a\u5f31\u8fd9\u79cd\u4ef7\u503c\u3002"}}
{"id": "2510.23900", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23900", "abs": "https://arxiv.org/abs/2510.23900", "authors": ["Kuan-Po Chiu", "Sumit Roy"], "title": "LEO Downlink Channel Model Revisited: Scattering Geometry-Inspired Derivation", "comment": "Accepted to Globecom 2025", "summary": "This paper presents a new derivation of LEO-to-ground receiver channel model\nto address a clear gap in the prior art: the lack of an appropriate geometry\naware characterization of non LOS (NLOS) link model represented by the power\nspectral density (PSD). Specifically, the main contribution is a coherent\nderivation of the PSD from 1st principles that is able to reproduce results in\nprior art and explain the causal relationship of main PSD features to the\npropagation geometry parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LEO\u5230\u5730\u9762\u63a5\u6536\u5668\u4fe1\u9053\u6a21\u578b\u63a8\u5bfc\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u4e2d\u7f3a\u4e4f\u5bf9\u975e\u89c6\u8ddd(NLOS)\u94fe\u8def\u529f\u7387\u8c31\u5bc6\u5ea6(PSD)\u7684\u51e0\u4f55\u611f\u77e5\u8868\u5f81\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u7f3a\u4e4f\u5bf9\u975e\u89c6\u8ddd(NLOS)\u94fe\u8def\u529f\u7387\u8c31\u5bc6\u5ea6(PSD)\u7684\u9002\u5f53\u51e0\u4f55\u611f\u77e5\u8868\u5f81\uff0c\u8fd9\u662f\u4e00\u4e2a\u660e\u663e\u7684\u7a7a\u767d\u3002", "method": "\u4ece\u7b2c\u4e00\u539f\u7406\u51fa\u53d1\u5bf9PSD\u8fdb\u884c\u76f8\u5e72\u63a8\u5bfc\uff0c\u80fd\u591f\u91cd\u73b0\u5148\u524d\u6280\u672f\u7684\u7ed3\u679c\uff0c\u5e76\u89e3\u91ca\u4e3b\u8981PSD\u7279\u5f81\u4e0e\u4f20\u64ad\u51e0\u4f55\u53c2\u6570\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u65b0\u63a8\u5bfc\u7684\u6a21\u578b\u80fd\u591f\u91cd\u73b0\u5148\u524d\u6280\u672f\u7684\u7ed3\u679c\uff0c\u5e76\u6e05\u6670\u5730\u89e3\u91caPSD\u4e3b\u8981\u7279\u5f81\u4e0e\u4f20\u64ad\u51e0\u4f55\u53c2\u6570\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u63a8\u5bfc\u65b9\u6cd5\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4e3aLEO\u5230\u5730\u9762\u63a5\u6536\u5668\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u51e0\u4f55\u611f\u77e5PSD\u8868\u5f81\u3002"}}
{"id": "2510.23821", "categories": ["stat.AP", "62P05", "G.3"], "pdf": "https://arxiv.org/pdf/2510.23821", "abs": "https://arxiv.org/abs/2510.23821", "authors": ["\u0141ukasz Delong", "Mario W\u00fcthrich"], "title": "Universal Inference for Testing Calibration of Mean Estimates within the Exponential Dispersion Family", "comment": "34 pages", "summary": "Calibration of mean estimates for predictions is a crucial property in many\napplications, particularly in the fields of financial and actuarial\ndecision-making. In this paper, we first review classical approaches for\nvalidating mean-calibration, and we discuss the Likelihood Ratio Test (LRT)\nwithin the Exponential Dispersion Family (EDF). Then, we investigate the\nframework of universal inference to test for mean-calibration. We develop a\nsub-sampled split LRT within the EDF that provides finite sample guarantees\nwith universally valid critical values. We investigate type I error, power and\ne-power of this sub-sampled split LRT, we compare it to the classical LRT, and\nwe propose a novel test statistics based on the sub-sampled split LRT to\nenhance the performance of the calibration test. A numerical analysis verifies\nthat our proposal is an attractive alternative to the classical LRT achieving a\nhigh power in detecting miscalibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6307\u6570\u5206\u6563\u65cf\u4e2d\u7684\u5b50\u91c7\u6837\u5206\u5272\u4f3c\u7136\u6bd4\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u9884\u6d4b\u7684\u5747\u503c\u6821\u51c6\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u548c\u666e\u904d\u6709\u6548\u7684\u4e34\u754c\u503c\uff0c\u6bd4\u4f20\u7edfLRT\u5177\u6709\u66f4\u9ad8\u7684\u68c0\u6d4b\u9519\u8bef\u6821\u51c6\u7684\u80fd\u529b\u3002", "motivation": "\u5728\u91d1\u878d\u548c\u7cbe\u7b97\u51b3\u7b56\u7b49\u9886\u57df\uff0c\u9884\u6d4b\u7684\u5747\u503c\u6821\u51c6\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u548c\u666e\u904d\u6709\u6548\u4e34\u754c\u503c\u7684\u6821\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u6307\u6570\u5206\u6563\u65cf\u4e2d\u7684\u5b50\u91c7\u6837\u5206\u5272\u4f3c\u7136\u6bd4\u68c0\u9a8c\uff0c\u7ed3\u5408\u4e86\u5206\u5272\u4f3c\u7136\u6bd4\u548c\u5b50\u91c7\u6837\u6280\u672f\uff0c\u63d0\u51fa\u57fa\u4e8e\u6b64\u7684\u65b0\u6d4b\u8bd5\u7edf\u8ba1\u91cf\u6765\u63d0\u5347\u6821\u51c6\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "\u6570\u503c\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u9519\u8bef\u6821\u51c6\u65b9\u9762\u5177\u6709\u9ad8\u529f\u6548\uff0c\u662f\u4f20\u7edfLRT\u7684\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b50\u91c7\u6837\u5206\u5272LRT\u65b9\u6cd5\u4e3a\u5747\u503c\u6821\u51c6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u548c\u4f18\u8d8a\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2510.23622", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23622", "abs": "https://arxiv.org/abs/2510.23622", "authors": ["Alyssa Gerhart", "Balaji Iyangar"], "title": "Adversarially-Aware Architecture Design for Robust Medical AI Systems", "comment": null, "summary": "Adversarial attacks pose a severe risk to AI systems used in healthcare,\ncapable of misleading models into dangerous misclassifications that can delay\ntreatments or cause misdiagnoses. These attacks, often imperceptible to human\nperception, threaten patient safety, particularly in underserved populations.\nOur study explores these vulnerabilities through empirical experimentation on a\ndermatological dataset, where adversarial methods significantly reduce\nclassification accuracy. Through detailed threat modeling, experimental\nbenchmarking, and model evaluation, we demonstrate both the severity of the\nthreat and the partial success of defenses like adversarial training and\ndistillation. Our results show that while defenses reduce attack success rates,\nthey must be balanced against model performance on clean data. We conclude with\na call for integrated technical, ethical, and policy-based approaches to build\nmore resilient, equitable AI in healthcare.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u533b\u7597AI\u7cfb\u7edf\u4e2d\u7684\u5bf9\u6297\u6027\u653b\u51fb\u98ce\u9669\uff0c\u901a\u8fc7\u5728\u76ae\u80a4\u75c5\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u5bf9\u6297\u6027\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5206\u7c7b\u51c6\u786e\u6027\u7684\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u9632\u5fa1\u63aa\u65bd\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u533b\u7597AI\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u7684\u8bef\u5206\u7c7b\uff0c\u5ef6\u8bef\u6cbb\u7597\u6216\u8bef\u8bca\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u4eba\u7fa4\u4e2d\u5a01\u80c1\u60a3\u8005\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7684\u5a01\u80c1\u5efa\u6a21\u3001\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u8bc4\u4f30\uff0c\u5728\u76ae\u80a4\u75c5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u84b8\u998f\u7b49\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u9632\u5fa1\u63aa\u65bd\u867d\u7136\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u4f46\u9700\u8981\u5728\u5bf9\u6297\u6027\u653b\u51fb\u9632\u5fa1\u548c\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u547c\u5401\u91c7\u7528\u96c6\u6210\u7684\u6280\u672f\u3001\u4f26\u7406\u548c\u653f\u7b56\u65b9\u6cd5\uff0c\u6784\u5efa\u66f4\u5177\u5f39\u6027\u548c\u516c\u5e73\u6027\u7684\u533b\u7597AI\u7cfb\u7edf\u3002"}}
{"id": "2510.23935", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23935", "abs": "https://arxiv.org/abs/2510.23935", "authors": ["Enze Shi", "Pankaj Bhagwat", "Zhixian Yang", "Linglong Kong", "Bei Jiang"], "title": "Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis", "comment": null, "summary": "Machine learning models have achieved widespread success but often inherit\nand amplify historical biases, resulting in unfair outcomes. Traditional\nfairness methods typically impose constraints at the prediction level, without\naddressing underlying biases in data representations. In this work, we propose\na principled framework that adjusts data representations to balance predictive\nutility and fairness. Using sufficient dimension reduction, we decompose the\nfeature space into target-relevant, sensitive, and shared components, and\ncontrol the fairness-utility trade-off by selectively removing sensitive\ninformation. We provide a theoretical analysis of how prediction error and\nfairness gaps evolve as shared subspaces are added, and employ influence\nfunctions to quantify their effects on the asymptotic behavior of parameter\nestimates. Experiments on both synthetic and real-world datasets validate our\ntheoretical insights and show that the proposed method effectively improves\nfairness while preserving predictive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8c03\u6574\u6570\u636e\u8868\u793a\u6765\u5e73\u8861\u9884\u6d4b\u6548\u7528\u548c\u516c\u5e73\u6027\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5145\u5206\u964d\u7ef4\u5206\u89e3\u7279\u5f81\u7a7a\u95f4\uff0c\u9009\u62e9\u6027\u79fb\u9664\u654f\u611f\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7ecf\u5e38\u7ee7\u627f\u548c\u653e\u5927\u5386\u53f2\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7ed3\u679c\u3002\u4f20\u7edf\u516c\u5e73\u6027\u65b9\u6cd5\u901a\u5e38\u5728\u9884\u6d4b\u5c42\u9762\u65bd\u52a0\u7ea6\u675f\uff0c\u800c\u6ca1\u6709\u89e3\u51b3\u6570\u636e\u8868\u793a\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u5145\u5206\u964d\u7ef4\u5c06\u7279\u5f81\u7a7a\u95f4\u5206\u89e3\u4e3a\u76ee\u6807\u76f8\u5173\u3001\u654f\u611f\u548c\u5171\u4eab\u7ec4\u4ef6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u79fb\u9664\u654f\u611f\u4fe1\u606f\u6765\u63a7\u5236\u516c\u5e73\u6027-\u6548\u7528\u6743\u8861\uff0c\u5e76\u91c7\u7528\u5f71\u54cd\u51fd\u6570\u91cf\u5316\u53c2\u6570\u4f30\u8ba1\u7684\u6e10\u8fd1\u884c\u4e3a\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\u6765\u5904\u7406\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u6574\u6570\u636e\u8868\u793a\u800c\u975e\u4ec5\u7ea6\u675f\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u516c\u5e73\u6027-\u6548\u7528\u5e73\u8861\u3002"}}
{"id": "2510.23624", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23624", "abs": "https://arxiv.org/abs/2510.23624", "authors": ["Tiago Mendon\u00e7a dos Santos", "Rafael Izbicki", "Lu\u00eds Gustavo Esteves"], "title": "DiNo and RanBu: Lightweight Predictions from Shallow Random Forests", "comment": null, "summary": "Random Forest ensembles are a strong baseline for tabular prediction tasks,\nbut their reliance on hundreds of deep trees often results in high inference\nlatency and memory demands, limiting deployment in latency-sensitive or\nresource-constrained environments. We introduce DiNo (Distance with Nodes) and\nRanBu (Random Bushes), two shallow-forest methods that convert a small set of\ndepth-limited trees into efficient, distance-weighted predictors. DiNo measures\ncophenetic distances via the most recent common ancestor of observation pairs,\nwhile RanBu applies kernel smoothing to Breiman's classical proximity measure.\nBoth approaches operate entirely after forest training: no additional trees are\ngrown, and tuning of the single bandwidth parameter $h$ requires only\nlightweight matrix-vector operations. Across three synthetic benchmarks and 25\npublic datasets, RanBu matches or exceeds the accuracy of full-depth random\nforests-particularly in high-noise settings-while reducing training plus\ninference time by up to 95\\%. DiNo achieves the best bias-variance trade-off in\nlow-noise regimes at a modest computational cost. Both methods extend directly\nto quantile regression, maintaining accuracy with substantial speed gains. The\nimplementation is available as an open-source R/C++ package at\nhttps://github.com/tiagomendonca/dirf. We focus on structured tabular random\nsamples (i.i.d.), leaving extensions to other modalities for future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DiNo\u548cRanBu\u4e24\u79cd\u6d45\u5c42\u68ee\u6797\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5c11\u91cf\u6df1\u5ea6\u53d7\u9650\u7684\u6811\u8f6c\u6362\u4e3a\u9ad8\u6548\u7684\u8ddd\u79bb\u52a0\u6743\u9884\u6d4b\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u968f\u673a\u68ee\u6797\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u968f\u673a\u68ee\u6797\u96c6\u6210\u5728\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f9d\u8d56\u6570\u767e\u68f5\u6df1\u5ea6\u6811\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5728\u5ef6\u8fdf\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "DiNo\u901a\u8fc7\u89c2\u6d4b\u5bf9\u7684\u6700\u8fdc\u5171\u540c\u7956\u5148\u6d4b\u91cf\u540c\u6e90\u8ddd\u79bb\uff0cRanBu\u5bf9Breiman\u7ecf\u5178\u90bb\u8fd1\u5ea6\u5ea6\u91cf\u5e94\u7528\u6838\u5e73\u6ed1\u3002\u4e24\u79cd\u65b9\u6cd5\u5728\u68ee\u6797\u8bad\u7ec3\u540e\u5b8c\u5168\u64cd\u4f5c\uff0c\u65e0\u9700\u989d\u5916\u751f\u957f\u6811\uff0c\u4ec5\u9700\u8f7b\u91cf\u7ea7\u77e9\u9635\u5411\u91cf\u64cd\u4f5c\u8c03\u6574\u5355\u4e2a\u5e26\u5bbd\u53c2\u6570h\u3002", "result": "\u57283\u4e2a\u5408\u6210\u57fa\u51c6\u548c25\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cRanBu\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5168\u6df1\u5ea6\u968f\u673a\u68ee\u6797\u7684\u7cbe\u5ea6\uff08\u7279\u522b\u662f\u5728\u9ad8\u566a\u58f0\u8bbe\u7f6e\u4e2d\uff09\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u52a0\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe95%\u3002DiNo\u5728\u4f4e\u566a\u58f0\u673a\u5236\u4e2d\u4ee5\u9002\u5ea6\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u6700\u4f73\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002\u4e24\u79cd\u65b9\u6cd5\u53ef\u76f4\u63a5\u6269\u5c55\u5230\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "DiNo\u548cRanBu\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6d45\u5c42\u68ee\u6797\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5ef6\u8fdf\u654f\u611f\u548c\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2510.24153", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24153", "abs": "https://arxiv.org/abs/2510.24153", "authors": ["Yuya Takada", "Kiyoshi Izumi"], "title": "Machine Learning for the Production of Official Statistics: Density Ratio Estimation using Biased Transaction Data for Japanese labor statistics", "comment": "23 pages, 9 figures. Under review at Journal of Computational Social\n  Science", "summary": "National statistical institutes are beginning to use non-traditional data\nsources to produce official statistics. These sources, originally collected for\nnon-statistical purposes, include point-of-sales(POS) data and mobile phone\nglobal positioning system(GPS) data. Such data have the potential to\nsignificantly enhance the usefulness of official statistics. In the era of big\ndata, many private companies are accumulating vast amounts of transaction data.\nExploring how to leverage these data for official statistics is increasingly\nimportant. However, progress has been slower than expected, mainly because such\ndata are not collected through sample-based survey methods and therefore\nexhibit substantial selection bias. If this bias can be properly addressed,\nthese data could become a valuable resource for official statistics,\nsubstantially expanding their scope and improving the quality of\ndecision-making, including economic policy. This paper demonstrates that even\nbiased transaction data can be useful for producing official statistics for\nprompt release, by drawing on the concepts of density ratio estimation and\nsupervised learning under covariate shift, both developed in the field of\nmachine learning. As a case study, we show that preliminary statistics can be\nproduced in a timely manner using biased data from a Japanese private\nemployment agency. This approach enables the early release of a key labor\nmarket indicator that would otherwise be delayed by up to a year, thereby\nmaking it unavailable for timely decision-making.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5373\u4f7f\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u4ea4\u6613\u6570\u636e\uff0c\u901a\u8fc7\u5e94\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u548c\u534f\u53d8\u91cf\u504f\u79fb\u76d1\u7763\u5b66\u4e60\u6982\u5ff5\uff0c\u4e5f\u80fd\u7528\u4e8e\u53ca\u65f6\u751f\u6210\u5b98\u65b9\u7edf\u8ba1\u6570\u636e\u3002\u4ee5\u65e5\u672c\u79c1\u8425\u5c31\u4e1a\u673a\u6784\u6570\u636e\u4e3a\u4f8b\uff0c\u53ef\u4ee5\u63d0\u524d\u53d1\u5e03\u5173\u952e\u52b3\u52a8\u529b\u5e02\u573a\u6307\u6807\u3002", "motivation": "\u56fd\u5bb6\u7edf\u8ba1\u673a\u6784\u5f00\u59cb\u4f7f\u7528\u975e\u4f20\u7edf\u6570\u636e\u6e90\uff08\u5982\u9500\u552e\u70b9\u6570\u636e\u548c\u624b\u673aGPS\u6570\u636e\uff09\u6765\u751f\u6210\u5b98\u65b9\u7edf\u8ba1\u6570\u636e\u3002\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u663e\u8457\u589e\u5f3a\u5b98\u65b9\u7edf\u8ba1\u6709\u7528\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u4e0d\u662f\u901a\u8fc7\u62bd\u6837\u8c03\u67e5\u65b9\u6cd5\u6536\u96c6\u800c\u5b58\u5728\u4e25\u91cd\u9009\u62e9\u504f\u5dee\u3002\u5982\u679c\u80fd\u591f\u89e3\u51b3\u8fd9\u79cd\u504f\u5dee\uff0c\u8fd9\u4e9b\u6570\u636e\u5c06\u6210\u4e3a\u5b98\u65b9\u7edf\u8ba1\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u548c\u534f\u53d8\u91cf\u504f\u79fb\u76d1\u7763\u5b66\u4e60\u6982\u5ff5\uff0c\u5904\u7406\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u4ea4\u6613\u6570\u636e\u3002\u4ee5\u65e5\u672c\u79c1\u8425\u5c31\u4e1a\u673a\u6784\u7684\u6570\u636e\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u6709\u504f\u6570\u636e\u751f\u6210\u521d\u6b65\u7edf\u8ba1\u6570\u636e\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u4f7f\u7528\u5b58\u5728\u504f\u5dee\u7684\u4ea4\u6613\u6570\u636e\uff0c\u4e5f\u80fd\u53ca\u65f6\u751f\u6210\u5b98\u65b9\u7edf\u8ba1\u6570\u636e\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u524d\u53d1\u5e03\u5173\u952e\u52b3\u52a8\u529b\u5e02\u573a\u6307\u6807\uff0c\u5426\u5219\u8fd9\u4e9b\u6307\u6807\u53ef\u80fd\u4f1a\u5ef6\u8fdf\u957f\u8fbe\u4e00\u5e74\u624d\u53d1\u5e03\uff0c\u65e0\u6cd5\u7528\u4e8e\u53ca\u65f6\u51b3\u7b56\u3002", "conclusion": "\u5373\u4f7f\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u975e\u4f20\u7edf\u6570\u636e\u6e90\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5904\u7406\uff0c\u4e5f\u80fd\u6210\u4e3a\u5b98\u65b9\u7edf\u8ba1\u7684\u6709\u4ef7\u503c\u8d44\u6e90\uff0c\u663e\u8457\u6269\u5927\u7edf\u8ba1\u8303\u56f4\u5e76\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\uff0c\u5305\u62ec\u7ecf\u6d4e\u653f\u7b56\u7684\u5236\u5b9a\u3002"}}
{"id": "2510.24017", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24017", "abs": "https://arxiv.org/abs/2510.24017", "authors": ["Brian Skoglind", "Travis Roberts", "Sourabh Karmakar", "Cameron Turner", "Laine Mears"], "title": "Localized Acoustic-Event Measurement Probe: Connector Confirmation Utilizing Acoustic Signatures", "comment": null, "summary": "Modern consumer products are full of interconnected electrical and electronic\nmodules to fulfill direct and indirect needs. In an automated assembly line\nstill, most of these interconnections are required to be done manually due to\nthe large variety of connector types, connector positions, and the soft,\nflexible nature of their structures. The manual connection points are the\nsource of partial or completely loose connections. Sometimes connections are\nmissed due to the application of unequal mating forces and natural human\nfatigue. Subsequently, these defects can lead to unexpected downtime and\nexpensive rework. For successful connection detection, past approaches such as\nvision verification, Augmented Reality, or circuit parameter-based measurements\nhave shown limited ability to detect the correct connection state. Though most\nconnections emit a specific noise for successful mating, the acoustic-based\nverification system for electrical connection confirmation has not been\nextensively researched. The main discouraging reason for such research is the\ntypically low signal-to-noise ratio (SNR) between the sound of a pair of\nelectrical connector mating and the diverse soundscape of the plant. In this\nstudy, the authors investigated increasing the SNR between the electrical\nconnector mating sound and the plant soundscape to improve connection success\ndetection by employing a physical system for background noise mitigation and\nthe successful met noise signature amplification algorithm. The solution is\nover 75% effective at detecting and classifying connection state. The solution\nhas been constructed without any modification to the existing manual\ninterconnection process.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u68c0\u6d4b\u7684\u7535\u6c14\u8fde\u63a5\u72b6\u6001\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u7406\u964d\u566a\u548c\u58f0\u97f3\u7279\u5f81\u653e\u5927\u7b97\u6cd5\u63d0\u9ad8\u4fe1\u566a\u6bd4\uff0c\u6709\u6548\u68c0\u6d4b\u7535\u6c14\u8fde\u63a5\u5668\u7684\u8fde\u63a5\u72b6\u6001\u3002", "motivation": "\u73b0\u4ee3\u6d88\u8d39\u4ea7\u54c1\u4e2d\u7684\u7535\u6c14\u8fde\u63a5\u5728\u81ea\u52a8\u5316\u88c5\u914d\u7ebf\u4e0a\u4ecd\u9700\u4eba\u5de5\u5b8c\u6210\uff0c\u624b\u52a8\u8fde\u63a5\u5bb9\u6613\u51fa\u73b0\u677e\u52a8\u6216\u9057\u6f0f\u95ee\u9898\uff0c\u5bfc\u81f4\u8bbe\u5907\u505c\u673a\u548c\u8fd4\u5de5\u6210\u672c\u3002\u73b0\u6709\u89c6\u89c9\u9a8c\u8bc1\u3001\u589e\u5f3a\u73b0\u5b9e\u6216\u7535\u8def\u53c2\u6570\u6d4b\u91cf\u65b9\u6cd5\u5728\u68c0\u6d4b\u8fde\u63a5\u72b6\u6001\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u7269\u7406\u7cfb\u7edf\u8fdb\u884c\u80cc\u666f\u566a\u58f0\u6291\u5236\uff0c\u5e76\u4f7f\u7528\u6210\u529f\u914d\u5bf9\u566a\u58f0\u7279\u5f81\u653e\u5927\u7b97\u6cd5\u6765\u63d0\u9ad8\u7535\u6c14\u8fde\u63a5\u5668\u914d\u5bf9\u58f0\u97f3\u4e0e\u5de5\u5382\u73af\u5883\u58f0\u97f3\u4e4b\u95f4\u7684\u4fe1\u566a\u6bd4\u3002", "result": "\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u68c0\u6d4b\u548c\u5206\u7c7b\u8fde\u63a5\u72b6\u6001\u65b9\u9762\u6709\u6548\u6027\u8d85\u8fc775%\uff0c\u4e14\u65e0\u9700\u5bf9\u73b0\u6709\u624b\u52a8\u4e92\u8fde\u8fc7\u7a0b\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u3002", "conclusion": "\u58f0\u5b66\u9a8c\u8bc1\u7cfb\u7edf\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u7535\u6c14\u8fde\u63a5\u72b6\u6001\uff0c\u901a\u8fc7\u63d0\u9ad8\u4fe1\u566a\u6bd4\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u5de5\u5382\u73af\u5883\u4e2d\u58f0\u97f3\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2510.24394", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24394", "abs": "https://arxiv.org/abs/2510.24394", "authors": ["Sandra Barrag\u00e1n", "Adri\u00e1n P\u00e9rez-Bote", "Carlos S\u00e1ez", "David Salgado", "Luis Sanguiao-Sande"], "title": "Streamlining business functions in official statistical production with Machine Learning", "comment": "42 pages, 14 figures, preprint version to appear as a chapter of F.\n  Dumpert (ed.), Foundations and Advances of Machine Learning in Official\n  Statistics", "summary": "We provide a description of pilot and production experiences to streamline\nsome business functions in the official statistical production process using\nstatistical learning models. Our approach is quality-oriented searching for an\nimprovement on accuracy, cost-efficiency, timeliness, granularity, response\nburden reduction, and frequency. Pilot experiences have been conducted with\ndata from real surveys in Statistics Spain (INE).", "AI": {"tldr": "\u672c\u6587\u63cf\u8ff0\u4e86\u5728\u5b98\u65b9\u7edf\u8ba1\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u7b80\u5316\u4e1a\u52a1\u529f\u80fd\u7684\u8bd5\u70b9\u548c\u751f\u4ea7\u7ecf\u9a8c\uff0c\u65e8\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u3001\u6210\u672c\u6548\u76ca\u3001\u53ca\u65f6\u6027\u3001\u7c92\u5ea6\u3001\u51cf\u8f7b\u54cd\u5e94\u8d1f\u62c5\u548c\u9891\u7387\u3002", "motivation": "\u901a\u8fc7\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u6539\u8fdb\u5b98\u65b9\u7edf\u8ba1\u751f\u4ea7\u8fc7\u7a0b\u7684\u8d28\u91cf\uff0c\u5305\u62ec\u51c6\u786e\u6027\u3001\u6210\u672c\u6548\u76ca\u3001\u53ca\u65f6\u6027\u3001\u7c92\u5ea6\u3001\u54cd\u5e94\u8d1f\u62c5\u548c\u9891\u7387\u7b49\u65b9\u9762\u3002", "method": "\u5728\u897f\u73ed\u7259\u7edf\u8ba1\u5c40(INE)\u7684\u771f\u5b9e\u8c03\u67e5\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bd5\u70b9\u5b9e\u9a8c\uff0c\u91c7\u7528\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u6765\u7b80\u5316\u4e1a\u52a1\u529f\u80fd\u3002", "result": "\u8bd5\u70b9\u7ecf\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u5177\u6709\u6539\u8fdb\u6f5c\u529b\u3002", "conclusion": "\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u5728\u5b98\u65b9\u7edf\u8ba1\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7edf\u8ba1\u4e1a\u52a1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2510.24058", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24058", "abs": "https://arxiv.org/abs/2510.24058", "authors": ["Zihan Zhao", "Masood Mortazavi", "Ning Yan"], "title": "PULSE: Privileged Knowledge Transfer from Electrodermal Activity to Low-Cost Sensors for Stress Monitoring", "comment": "Accepted as a finders paper at ML4H 2025", "summary": "Electrodermal activity (EDA), the primary signal for stress detection,\nrequires costly hardware often unavailable in real-world wearables. In this\npaper, we propose PULSE, a framework that utilizes EDA exclusively during\nself-supervised pretraining, while enabling inference without EDA but with more\nreadily available modalities such as ECG, BVP, ACC, and TEMP. Our approach\nseparates encoder outputs into shared and private embeddings. We align shared\nembeddings across modalities and fuse them into a modality-invariant\nrepresentation. The private embeddings carry modality-specific information to\nsupport the reconstruction objective. Pretraining is followed by knowledge\ntransfer where a frozen EDA teacher transfers sympathetic-arousal\nrepresentations into student encoders. On WESAD, our method achieves strong\nstress-detection performance, showing that representations of privileged EDA\ncan be transferred to low-cost sensors to improve accuracy while reducing\nhardware cost.", "AI": {"tldr": "PULSE\u6846\u67b6\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5229\u7528EDA\u4fe1\u53f7\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u4ec5\u4f7f\u7528\u66f4\u6613\u83b7\u53d6\u7684ECG\u3001BVP\u3001ACC\u548cTEMP\u6a21\u6001\uff0c\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u4f20\u611f\u5668\u7684\u538b\u529b\u68c0\u6d4b\u3002", "motivation": "EDA\u4f5c\u4e3a\u538b\u529b\u68c0\u6d4b\u7684\u4e3b\u8981\u4fe1\u53f7\u9700\u8981\u6602\u8d35\u786c\u4ef6\uff0c\u800c\u73b0\u5b9e\u53ef\u7a7f\u6234\u8bbe\u5907\u5f80\u5f80\u4e0d\u5177\u5907\u8fd9\u79cd\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6ca1\u6709EDA\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u51c6\u786e\u538b\u529b\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u7f16\u7801\u5668\u8f93\u51fa\u5206\u4e3a\u5171\u4eab\u548c\u79c1\u6709\u5d4c\u5165\uff0c\u5bf9\u9f50\u8de8\u6a21\u6001\u7684\u5171\u4eab\u5d4c\u5165\u5e76\u878d\u5408\u4e3a\u6a21\u6001\u4e0d\u53d8\u8868\u793a\uff0c\u79c1\u6709\u5d4c\u5165\u643a\u5e26\u6a21\u6001\u7279\u5b9a\u4fe1\u606f\u652f\u6301\u91cd\u5efa\u76ee\u6807\u3002\u9884\u8bad\u7ec3\u540e\u901a\u8fc7\u51bb\u7ed3\u7684EDA\u6559\u5e08\u6a21\u578b\u5c06\u4ea4\u611f\u795e\u7ecf\u5524\u9192\u8868\u793a\u8f6c\u79fb\u5230\u5b66\u751f\u7f16\u7801\u5668\u3002", "result": "\u5728WESAD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u538b\u529b\u68c0\u6d4b\u6027\u80fd\uff0c\u8868\u660e\u7279\u6743EDA\u7684\u8868\u793a\u53ef\u4ee5\u8f6c\u79fb\u5230\u4f4e\u6210\u672c\u4f20\u611f\u5668\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u5b9e\u73b0\u51c6\u786e\u7684\u538b\u529b\u68c0\u6d4b\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u53ef\u7a7f\u6234\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24187", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24187", "abs": "https://arxiv.org/abs/2510.24187", "authors": ["Lucas L\u00e9vy", "Jean-Lou Valeau", "Arya Akhavan", "Patrick Rebeschini"], "title": "Self-Concordant Perturbations for Linear Bandits", "comment": null, "summary": "We study the adversarial linear bandits problem and present a unified\nalgorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and\nFollow-the-Perturbed-Leader (FTPL) methods, extending the known connection\nbetween them from the full-information setting. Within this framework, we\nintroduce self-concordant perturbations, a family of probability distributions\nthat mirror the role of self-concordant barriers previously employed in the\nFTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based\nalgorithm that combines self-concordant regularization with efficient\nstochastic exploration. Our approach achieves a regret of $O(d\\sqrt{n \\ln n})$\non both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean\nball, this matches the rate attained by existing self-concordant FTRL methods.\nFor the hypercube, this represents a $\\sqrt{d}$ improvement over these methods\nand matches the optimal bound up to logarithmic factors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7b97\u6cd5\u6846\u67b6\uff0c\u5c06FTRL\u548cFTPL\u65b9\u6cd5\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u5f15\u5165\u81ea\u534f\u8c03\u6270\u52a8\u6982\u5ff5\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u7684\u65b0FTPL\u7b97\u6cd5\u5728d\u7ef4\u8d85\u7acb\u65b9\u4f53\u548c\u6b27\u51e0\u91cc\u5f97\u7403\u4e0a\u5b9e\u73b0\u4e86O(d\u221a(n ln n))\u7684\u9057\u61be\u754c\uff0c\u5728\u8d85\u7acb\u65b9\u4f53\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u221ad\u7684\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u5bf9\u6297\u6027\u7ebf\u6027\u8d4c\u535a\u95ee\u9898\uff0c\u65e8\u5728\u5efa\u7acbFTRL\u548cFTPL\u65b9\u6cd5\u4e4b\u95f4\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6269\u5c55\u5b83\u4eec\u5728\u5b8c\u5168\u4fe1\u606f\u8bbe\u7f6e\u4e2d\u7684\u5df2\u77e5\u8054\u7cfb\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u534f\u8c03\u6270\u52a8\u6982\u5ff5\uff0c\u7ed3\u5408\u81ea\u534f\u8c03\u6b63\u5219\u5316\u548c\u9ad8\u6548\u968f\u673a\u63a2\u7d22\uff0c\u8bbe\u8ba1\u57fa\u4e8eFTPL\u7684\u65b0\u7b97\u6cd5\u3002", "result": "\u5728d\u7ef4\u8d85\u7acb\u65b9\u4f53\u548c\u6b27\u51e0\u91cc\u5f97\u7403\u4e0a\u5747\u5b9e\u73b0\u4e86O(d\u221a(n ln n))\u7684\u9057\u61be\u754c\u3002\u5728\u6b27\u51e0\u91cc\u5f97\u7403\u4e0a\u5339\u914d\u73b0\u6709\u81ea\u534f\u8c03FTRL\u65b9\u6cd5\uff0c\u5728\u8d85\u7acb\u65b9\u4f53\u4e0a\u6bd4\u8fd9\u4e9b\u65b9\u6cd5\u6709\u221ad\u7684\u6539\u8fdb\uff0c\u4e14\u8fbe\u5230\u6700\u4f18\u754c\uff08\u9664\u5bf9\u6570\u56e0\u5b50\u5916\uff09\u3002", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e86FTRL\u548cFTPL\u65b9\u6cd5\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u534f\u8c03\u6270\u52a8\u8bbe\u8ba1\u7684\u65b0\u7b97\u6cd5\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u6765\u4e3a\u4e2a\u4f53\u60a3\u8005\u63d0\u4f9b\u4e2a\u6027\u5316\u533b\u7597\u51b3\u7b56\uff0c\u514b\u670d\u4f20\u7edfAI\u533b\u7597\u7cfb\u7edf\u53ea\u5173\u6ce8\u5e73\u5747\u60a3\u8005\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfAI\u533b\u7597\u7cfb\u7edf\u901a\u8fc7\u6700\u5c0f\u5316\u5927\u6570\u636e\u96c6\u4e0a\u7684\u9519\u8bef\u6765\u63d0\u4f9b\u603b\u4f53\u51c6\u786e\u6027\uff0c\u4f46\u5728\u8fb9\u7f18\u75c5\u4f8b\uff08\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u5171\u5b58\u3001\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u79cd\u5e73\u5747\u60a3\u8005\u8c2c\u8bef\u635f\u5bb3\u4e86\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u667a\u80fd\u4f53\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u5f0f\u805a\u7c7b\uff0c\u5171\u4eab\u6a21\u578b\u5e93\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\u3002\u901a\u8fc7\u534f\u8c03\u5c42\u6743\u8861\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u5305\u3002", "result": "\u9a8c\u8bc1\u4ece\u7fa4\u4f53\u5e73\u5747\u8f6c\u5411\u4e2a\u4f53\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u6d4b\u91cf\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u9519\u8bef\u3001\u5c0f\u6837\u672c\u6821\u51c6\u548c\u98ce\u9669-\u8986\u76d6\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u4f7f\u533b\u7597AI\u4e0e\u533b\u5b66\u9996\u8981\u539f\u5219\u4fdd\u6301\u4e00\u81f4\uff1a\u63d0\u4f9b\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u670d\u52a1\u3002"}}
{"id": "2510.23631", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23631", "abs": "https://arxiv.org/abs/2510.23631", "authors": ["Yuxuan Tang", "Yifan Feng"], "title": "Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling", "comment": null, "summary": "Alignment of large language models (LLMs) has predominantly relied on\npairwise preference optimization, where annotators select the better of two\nresponses to a prompt. While simple, this approach overlooks the opportunity to\nlearn from richer forms of human feedback, such as multiwise comparisons and\ntop-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a\nunified framework that bridges preference optimization with (ranked) choice\nmodeling via maximum likelihood estimation. The framework is flexible,\nsupporting both utility-based and rank-based choice models. It subsumes several\nexisting pairwise methods (e.g., DPO, SimPO), while providing principled\ntraining objectives for richer feedback formats. We instantiate this framework\nwith two representative ranked choice models (Multinomial Logit and\nMallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across\nAlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms\ncompetitive baselines. RCPO shows how directly leveraging ranked preference\ndata, combined with the right choice models, yields more effective alignment.\nIt offers a versatile and extensible foundation for incorporating (ranked)\nchoice modeling into LLM training.", "AI": {"tldr": "RCPO\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u540d\uff09\u9009\u62e9\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u57fa\u4e8e\u6548\u7528\u548c\u57fa\u4e8e\u6392\u540d\u7684\u9009\u62e9\u6a21\u578b\uff0c\u80fd\u591f\u5229\u7528\u591a\u9009\u548c\u6392\u540d\u53cd\u9988\u6570\u636e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6210\u5bf9\u504f\u597d\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u4ece\u66f4\u4e30\u5bcc\u7684\u4eba\u7c7b\u53cd\u9988\u5f62\u5f0f\uff08\u5982\u591a\u9009\u6bd4\u8f83\u548ctop-k\u6392\u540d\uff09\u4e2d\u5b66\u4e60\u7684\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u6392\u540d\u9009\u62e9\u504f\u597d\u4f18\u5316\uff08RCPO\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u652f\u6301\u57fa\u4e8e\u6548\u7528\u548c\u57fa\u4e8e\u6392\u540d\u7684\u9009\u62e9\u6a21\u578b\uff0c\u5305\u542bMultinomial Logit\u548cMallows-RMJ\u4e24\u79cd\u4ee3\u8868\u6027\u6392\u540d\u9009\u62e9\u6a21\u578b\u3002", "result": "\u5728Llama-3-8B-Instruct\u548cGemma-2-9B-it\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cRCPO\u5728AlpacaEval 2\u548cArena-Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RCPO\u5c55\u793a\u4e86\u5982\u4f55\u76f4\u63a5\u5229\u7528\u6392\u540d\u504f\u597d\u6570\u636e\uff0c\u7ed3\u5408\u9002\u5f53\u7684\u9009\u62e9\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5bf9\u9f50\uff0c\u4e3a\u5c06\uff08\u6392\u540d\uff09\u9009\u62e9\u5efa\u6a21\u878d\u5165LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23634", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23634", "abs": "https://arxiv.org/abs/2510.23634", "authors": ["Soutrik Sarangi", "Yonatan Sverdlov", "Nadav Dym", "Abir De"], "title": "Monotone and Separable Set Functions: Characterizations and Neural Models", "comment": null, "summary": "Motivated by applications for set containment problems, we consider the\nfollowing fundamental problem: can we design set-to-vector functions so that\nthe natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if\nand only if } F(S)\\leq F(T) $. We call functions satisfying this property\nMonotone and Separating (MAS) set functions. % We establish lower and upper\nbounds for the vector dimension necessary to obtain MAS functions, as a\nfunction of the cardinality of the multisets and the underlying ground set. In\nthe important case of an infinite ground set, we show that MAS functions do not\nexist, but provide a model called our which provably enjoys a relaxed MAS\nproperty we name \"weakly MAS\" and is stable in the sense of Holder continuity.\nWe also show that MAS functions can be used to construct universal models that\nare monotone by construction and can approximate all monotone set functions.\nExperimentally, we consider a variety of set containment tasks. The experiments\nshow the benefit of using our our model, in comparison with standard set models\nwhich do not incorporate set containment as an inductive bias. Our code is\navailable in https://github.com/yonatansverdlov/Monotone-Embedding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5355\u8c03\u5206\u79bb\u96c6\u51fd\u6570\u7684\u8bbe\u8ba1\uff0c\u8fd9\u7c7b\u51fd\u6570\u80fd\u4fdd\u6301\u96c6\u5408\u5305\u542b\u5173\u7cfb\u7684\u504f\u5e8f\u7ed3\u6784\uff0c\u5373S\u2286T\u5f53\u4e14\u4ec5\u5f53F(S)\u2264F(T)\u3002\u5728\u65e0\u9650\u57fa\u7840\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86MAS\u51fd\u6570\u4e0d\u5b58\u5728\uff0c\u4f46\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f31MAS\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u96c6\u5408\u5305\u542b\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u53d7\u96c6\u5408\u5305\u542b\u95ee\u9898\u5e94\u7528\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1\u80fd\u4fdd\u6301\u96c6\u5408\u504f\u5e8f\u5173\u7cfb\u7684\u96c6\u5408\u5230\u5411\u91cf\u51fd\u6570\uff0c\u5373\u6ee1\u8db3S\u2286T\u5f53\u4e14\u4ec5\u5f53F(S)\u2264F(T)\u7684\u51fd\u6570\u3002", "method": "\u5efa\u7acb\u4e86MAS\u51fd\u6570\u5411\u91cf\u7ef4\u5ea6\u9700\u6c42\u7684\u4e0b\u754c\u548c\u4e0a\u754c\u3002\u5728\u65e0\u9650\u57fa\u7840\u96c6\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u5f31MAS\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709Holder\u8fde\u7eed\u6027\u7a33\u5b9a\u6027\uff0c\u5e76\u53ef\u7528\u4e8e\u6784\u5efa\u80fd\u8fd1\u4f3c\u6240\u6709\u5355\u8c03\u96c6\u51fd\u6570\u7684\u901a\u7528\u6a21\u578b\u3002", "result": "\u5728\u65e0\u9650\u57fa\u7840\u96c6\u4e0a\u8bc1\u660e\u4e86MAS\u51fd\u6570\u4e0d\u5b58\u5728\uff0c\u4f46\u63d0\u51fa\u7684\u5f31MAS\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u96c6\u5408\u5305\u542b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u672a\u5305\u542b\u96c6\u5408\u5305\u542b\u5f52\u7eb3\u504f\u7f6e\u7684\u6807\u51c6\u96c6\u5408\u6a21\u578b\u3002", "conclusion": "MAS\u51fd\u6570\u4e3a\u96c6\u5408\u5305\u542b\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f31MAS\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6709\u6548\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2510.23635", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23635", "abs": "https://arxiv.org/abs/2510.23635", "authors": ["Andrea Bontempelli", "Matteo Busso", "Leonardo Javier Malcotti", "Fausto Giunchiglia"], "title": "Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning", "comment": null, "summary": "Any digital personal assistant, whether used to support task performance,\nanswer questions, or manage work and daily life, including fitness schedules,\nrequires high-quality annotations to function properly. However, user\nannotations, whether actively produced or inferred from context (e.g., data\nfrom smartphone sensors), are often subject to errors and noise. Previous\nresearch on Skeptical Learning (SKEL) addressed the issue of noisy labels by\ncomparing offline active annotations with passive data, allowing for an\nevaluation of annotation accuracy. However, this evaluation did not include\nconfirmation from end-users, the best judges of their own context. In this\nstudy, we evaluate SKEL's performance in real-world conditions with actual\nusers who can refine the input labels based on their current perspectives and\nneeds. The study involves university students using the iLog mobile application\non their devices over a period of four weeks. The results highlight the\nchallenges of finding the right balance between user effort and data quality,\nas well as the potential benefits of using SKEL, which include reduced\nannotation effort and improved quality of collected data.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Skeptical Learning (SKEL)\u5728\u771f\u5b9e\u7528\u6237\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5927\u5b66\u751f\u4f7f\u7528iLog\u79fb\u52a8\u5e94\u7528\u56db\u5468\u7684\u5b9e\u9a8c\uff0c\u53d1\u73b0SKEL\u80fd\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u5728\u7528\u6237\u52aa\u529b\u548c\u6570\u636e\u8d28\u91cf\u95f4\u627e\u5230\u5e73\u8861\u3002", "motivation": "\u6570\u5b57\u4e2a\u4eba\u52a9\u7406\u9700\u8981\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u4f46\u7528\u6237\u6807\u6ce8\u5e38\u5b58\u5728\u9519\u8bef\u548c\u566a\u58f0\u3002\u5148\u524dSKEL\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u79bb\u7ebf\u4e3b\u52a8\u6807\u6ce8\u4e0e\u88ab\u52a8\u6570\u636e\u8bc4\u4f30\u6807\u6ce8\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u6700\u7ec8\u7528\u6237\u786e\u8ba4\uff0c\u800c\u7528\u6237\u662f\u81ea\u8eab\u60c5\u5883\u7684\u6700\u4f73\u5224\u65ad\u8005\u3002", "method": "\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u8bc4\u4f30SKEL\u6027\u80fd\uff0c\u8ba9\u5b9e\u9645\u7528\u6237\u57fa\u4e8e\u5f53\u524d\u89c6\u89d2\u548c\u9700\u6c42\u7cbe\u70bc\u8f93\u5165\u6807\u7b7e\u3002\u7814\u7a76\u6d89\u53ca\u5927\u5b66\u751f\u5728\u5176\u8bbe\u5907\u4e0a\u4f7f\u7528iLog\u79fb\u52a8\u5e94\u7528\uff0c\u6301\u7eed\u56db\u5468\u65f6\u95f4\u3002", "result": "\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u7528\u6237\u52aa\u529b\u548c\u6570\u636e\u8d28\u91cf\u95f4\u627e\u5230\u9002\u5f53\u5e73\u8861\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u4f7f\u7528SKEL\u7684\u6f5c\u5728\u597d\u5904\uff0c\u5305\u62ec\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u548c\u63d0\u9ad8\u6536\u96c6\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "SKEL\u5728\u771f\u5b9e\u7528\u6237\u73af\u5883\u4e2d\u80fd\u6709\u6548\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u6743\u8861\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0e\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u65e8\u5728\u652f\u6301\u5b89\u5168\u8bbe\u8ba1\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "motivation": "\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8bb0\u5fc6\u548c\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u4e3a\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u4e0e\u4f20\u7edfAI\u5b89\u5168\u548c\u8f6f\u4ef6\u5b89\u5168\u4e0d\u540c\u7684\u65b0\u578b\u653e\u5927\u5b89\u5168\u98ce\u9669\u3002", "method": "\u91c7\u7528\u8c03\u67e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u73b0\u6709\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u4e24\u4e2a\u89d2\u5ea6\u8ba8\u8bba\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u4e86\u667a\u80fd\u4f53AI\u7279\u6709\u7684\u5a01\u80c1\u7c7b\u578b\uff0c\u7efc\u5408\u4e86\u5f53\u524d\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u660e\u786e\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728web\u3001\u8f6f\u4ef6\u548c\u7269\u7406\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6267\u884c\u80fd\u529b\u521b\u9020\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u8bbe\u8ba1\u548c\u6cbb\u7406\u6846\u67b6\u6765\u786e\u4fdd\u5176\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2510.24287", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24287", "abs": "https://arxiv.org/abs/2510.24287", "authors": ["Richard Koebe", "Noah Saibel", "Juan Miguel Lopez Alcaraz", "Simon Sch\u00e4fer", "Nils Strodthoff"], "title": "Towards actionable hypotension prediction- predicting catecholamine therapy initiation in the intensive care unit", "comment": "27 pages, 8 figures, source code under\n  https://github.com/AI4HealthUOL/actionable-hypotension", "summary": "Hypotension in critically ill ICU patients is common and life-threatening.\nEscalation to catecholamine therapy marks a key management step, with both\nundertreatment and overtreatment posing risks. Most machine learning (ML)\nmodels predict hypotension using fixed MAP thresholds or MAP forecasting,\noverlooking the clinical decision behind treatment escalation. Predicting\ncatecholamine initiation, the start of vasoactive or inotropic agent\nadministration offers a more clinically actionable target reflecting real\ndecision-making. Using the MIMIC-III database, we modeled catecholamine\ninitiation as a binary event within a 15-minute prediction window. Input\nfeatures included statistical descriptors from a two-hour sliding MAP context\nwindow, along with demographics, biometrics, comorbidities, and ongoing\ntreatments. An Extreme Gradient Boosting (XGBoost) model was trained and\ninterpreted via SHapley Additive exPlanations (SHAP). The model achieved an\nAUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,\nAUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP\ntrends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant\npredictors. Subgroup analysis showed higher performance in males, younger\npatients (<53 years), those with higher BMI (>32), and patients without\ncomorbidities or concurrent medications. Predicting catecholamine initiation\nbased on MAP dynamics, treatment context, and patient characteristics supports\nthe critical decision of when to escalate therapy, shifting focus from\nthreshold-based alarms to actionable decision support. This approach is\nfeasible across a broad ICU cohort under natural event imbalance. Future work\nshould enrich temporal and physiological context, extend label definitions to\ninclude therapy escalation, and benchmark against existing hypotension\nprediction systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u9884\u6d4bICU\u60a3\u8005\u4e2d\u513f\u8336\u915a\u80fa\u6cbb\u7597\u7684\u542f\u52a8\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u4f4e\u8840\u538b\u9884\u6d4b\u3002\u6a21\u578b\u57fa\u4e8e\u5e73\u5747\u52a8\u8109\u538b\u52a8\u6001\u3001\u6cbb\u7597\u80cc\u666f\u548c\u60a3\u8005\u7279\u5f81\uff0c\u5728\u9884\u6d4b\u513f\u8336\u915a\u80fa\u542f\u52a8\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u56fa\u5b9aMAP\u9608\u503c\u7684\u4f4e\u8840\u538b\u9884\u6d4b\u3002", "motivation": "ICU\u60a3\u8005\u4f4e\u8840\u538b\u5e38\u89c1\u4e14\u5371\u53ca\u751f\u547d\uff0c\u513f\u8336\u915a\u80fa\u6cbb\u7597\u662f\u5173\u952e\u7684\u5e72\u9884\u6b65\u9aa4\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u9884\u6d4b\u57fa\u4e8e\u56fa\u5b9aMAP\u9608\u503c\u7684\u4f4e\u8840\u538b\uff0c\u5ffd\u89c6\u4e86\u6cbb\u7597\u5347\u7ea7\u7684\u4e34\u5e8a\u51b3\u7b56\u8fc7\u7a0b\u3002\u9884\u6d4b\u513f\u8336\u915a\u80fa\u542f\u52a8\u80fd\u63d0\u4f9b\u66f4\u5177\u4e34\u5e8a\u53ef\u64cd\u4f5c\u6027\u7684\u76ee\u6807\u3002", "method": "\u4f7f\u7528MIMIC-III\u6570\u636e\u5e93\uff0c\u5c06\u513f\u8336\u915a\u80fa\u542f\u52a8\u5efa\u6a21\u4e3a15\u5206\u949f\u9884\u6d4b\u7a97\u53e3\u5185\u7684\u4e8c\u5143\u4e8b\u4ef6\u3002\u8f93\u5165\u7279\u5f81\u5305\u62ec\u4e24\u5c0f\u65f6\u6ed1\u52a8MAP\u7a97\u53e3\u7684\u7edf\u8ba1\u63cf\u8ff0\u7b26\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u751f\u7269\u7279\u5f81\u3001\u5408\u5e76\u75c7\u548c\u5f53\u524d\u6cbb\u7597\u3002\u91c7\u7528XGBoost\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7SHAP\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u6a21\u578bAUROC\u8fbe\u52300.822\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eMAP<65\u7684\u4f4e\u8840\u538b\u57fa\u7ebf\u9884\u6d4b\uff08AUROC 0.686\uff09\u3002SHAP\u5206\u6790\u663e\u793a\u8fd1\u671fMAP\u503c\u3001MAP\u8d8b\u52bf\u548c\u5f53\u524d\u6cbb\u7597\u662f\u4e3b\u8981\u9884\u6d4b\u56e0\u5b50\u3002\u5728\u7537\u6027\u3001\u5e74\u8f7b\u60a3\u8005\u3001\u9ad8BMI\u60a3\u8005\u4ee5\u53ca\u65e0\u5408\u5e76\u75c7\u6216\u5e76\u53d1\u7528\u836f\u7684\u60a3\u8005\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u57fa\u4e8eMAP\u52a8\u6001\u3001\u6cbb\u7597\u80cc\u666f\u548c\u60a3\u8005\u7279\u5f81\u9884\u6d4b\u513f\u8336\u915a\u80fa\u542f\u52a8\uff0c\u652f\u6301\u6cbb\u7597\u5347\u7ea7\u7684\u5173\u952e\u51b3\u7b56\uff0c\u5c06\u91cd\u70b9\u4ece\u57fa\u4e8e\u9608\u503c\u7684\u8b66\u62a5\u8f6c\u5411\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u652f\u6301\u3002\u8be5\u65b9\u6cd5\u5728\u81ea\u7136\u4e8b\u4ef6\u4e0d\u5e73\u8861\u7684\u5e7f\u6cdbICU\u4eba\u7fa4\u4e2d\u53ef\u884c\u3002"}}
{"id": "2510.23810", "categories": ["cs.LG", "math.AP", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23810", "abs": "https://arxiv.org/abs/2510.23810", "authors": ["Sumanta Roy", "Bahador Bahmani", "Ioannis G. Kevrekidis", "Michael D. Shields"], "title": "A Physics-informed Multi-resolution Neural Operator", "comment": "26 pages, 14 figures, 4 tables", "summary": "The predictive accuracy of operator learning frameworks depends on the\nquality and quantity of available training data (input-output function pairs),\noften requiring substantial amounts of high-fidelity data, which can be\nchallenging to obtain in some real-world engineering applications. These\ndatasets may be unevenly discretized from one realization to another, with the\ngrid resolution varying across samples. In this study, we introduce a\nphysics-informed operator learning approach by extending the Resolution\nIndependent Neural Operator (RINO) framework to a fully data-free setup,\naddressing both challenges simultaneously. Here, the arbitrarily (but\nsufficiently finely) discretized input functions are projected onto a latent\nembedding space (i.e., a vector space of finite dimensions), using pre-trained\nbasis functions. The operator associated with the underlying partial\ndifferential equations (PDEs) is then approximated by a simple multi-layer\nperceptron (MLP), which takes as input a latent code along with spatiotemporal\ncoordinates to produce the solution in the physical space. The PDEs are\nenforced via a finite difference solver in the physical space. The validation\nand performance of the proposed method are benchmarked on several numerical\nexamples with multi-resolution data, where input functions are sampled at\nvarying resolutions, including both coarse and fine discretizations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u5206\u8fa8\u7387\u65e0\u5173\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\u6269\u5c55\u5230\u5b8c\u5168\u65e0\u6570\u636e\u8bbe\u7f6e\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u548c\u5206\u8fa8\u7387\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u771f\u5b9e\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u83b7\u53d6\u5927\u91cf\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u6570\u636e\u96c6\u53ef\u80fd\u5728\u4e0d\u540c\u5b9e\u73b0\u4e2d\u5177\u6709\u4e0d\u5747\u5300\u7684\u79bb\u6563\u5316\uff0c\u7f51\u683c\u5206\u8fa8\u7387\u5728\u6837\u672c\u95f4\u53d8\u5316\u3002", "method": "\u5c06\u4efb\u610f\u79bb\u6563\u5316\u7684\u8f93\u5165\u51fd\u6570\u6295\u5f71\u5230\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u51fd\u6570\u3002\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u673a\u8fd1\u4f3c\u504f\u5fae\u5206\u65b9\u7a0b\u7b97\u5b50\uff0c\u5728\u7269\u7406\u7a7a\u95f4\u901a\u8fc7\u6709\u9650\u5dee\u5206\u6c42\u89e3\u5668\u5f3a\u5236\u6267\u884cPDE\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u591a\u5206\u8fa8\u7387\u6570\u636e\u7684\u6570\u503c\u793a\u4f8b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5305\u62ec\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u79bb\u6563\u5316\u7684\u8f93\u5165\u51fd\u6570\u91c7\u6837\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u5206\u8fa8\u7387\u6570\u636e\uff0c\u5728\u65e0\u6570\u636e\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u7b97\u5b50\u5b66\u4e60\u3002"}}
{"id": "2510.24356", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24356", "abs": "https://arxiv.org/abs/2510.24356", "authors": ["Suman Sanyal"], "title": "Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning", "comment": null, "summary": "We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.", "AI": {"tldr": "\u63d0\u51fa\u611f\u77e5\u5b66\u4e60\uff08PeL\uff09\u8303\u5f0f\uff0c\u5c06\u667a\u80fd\u4f53\u7684\u611f\u77e5\u63a5\u53e3\u4f18\u5316\u4e0e\u4e0b\u6e38\u51b3\u7b56\u5b66\u4e60\u89e3\u8026\uff0c\u4f7f\u7528\u4efb\u52a1\u65e0\u5173\u4fe1\u53f7\u76f4\u63a5\u4f18\u5316\u611f\u77e5\u5c5e\u6027\u5982\u7a33\u5b9a\u6027\u3001\u4fe1\u606f\u6027\u548c\u51e0\u4f55\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u611f\u77e5\u548c\u51b3\u7b56\u8026\u5408\u4f18\u5316\uff0c\u5bfc\u81f4\u611f\u77e5\u8d28\u91cf\u96be\u4ee5\u8bc4\u4f30\u548c\u4fdd\u8bc1\u3002PeL\u65e8\u5728\u5206\u79bb\u611f\u77e5\u5b66\u4e60\uff0c\u4f7f\u5176\u72ec\u7acb\u4e8e\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5b9a\u4e49\u611f\u77e5\u63a5\u53e3f_\u03c6\u548c\u51b3\u7b56\u51fd\u6570g_\u03b8\u7684\u5206\u79bb\u6846\u67b6\uff0c\u4f7f\u7528\u4efb\u52a1\u65e0\u5173\u4fe1\u53f7\u4f18\u5316\u611f\u77e5\u5c5e\u6027\uff0c\u63d0\u51fa\u8868\u793a\u4e0d\u53d8\u6027\u6307\u6807\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u8bc1\u660ePeL\u66f4\u65b0\u4e0e\u8d1d\u53f6\u65af\u4efb\u52a1\u98ce\u9669\u68af\u5ea6\u6b63\u4ea4\u3002", "result": "\u5efa\u7acb\u4e86\u611f\u77e5\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8ba4\u8bc1\u611f\u77e5\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u611f\u77e5\u5b66\u4e60\u66f4\u65b0\u4e0e\u51b3\u7b56\u5b66\u4e60\u66f4\u65b0\u7684\u6b63\u4ea4\u6027\u3002", "conclusion": "PeL\u8303\u5f0f\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u7684\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u611f\u77e5\u5b66\u4e60\u4e0e\u51b3\u7b56\u5b66\u4e60\u7684\u6709\u6548\u89e3\u8026\u3002"}}
{"id": "2510.23650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23650", "abs": "https://arxiv.org/abs/2510.23650", "authors": ["Wei Xia"], "title": "Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs", "comment": null, "summary": "We proposed Static and Dynamic -- two zero-shot logits-layer debiasing\nmethods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits\nintervention outperforms hidden-layer approaches. We show semantic-aware logits\nintervention is stable and effective for debiasing aligned LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u96f6\u6837\u672clogits\u5c42\u53bb\u504f\u65b9\u6cd5\uff1aStatic\u548cDynamic\uff0c\u5176\u4e2dDynamic\u65b9\u6cd5\u80fd\u51cf\u5c11\u9ad8\u8fbe70%\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5c0f\u7684\u6d41\u7545\u6027\u635f\u5931\u3002", "motivation": "\u5f00\u53d1\u6709\u6548\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5df2\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u611f\u77e5\u7684logits\u5e72\u9884\u65b9\u6cd5\uff0c\u5728logits\u5c42\u8fdb\u884c\u53bb\u504f\u5904\u7406\uff0c\u800c\u4e0d\u662f\u5728\u9690\u85cf\u5c42\u3002", "result": "Dynamic\u65b9\u6cd5\u5728\u51cf\u5c11\u504f\u89c1\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u504f\u89c1\u51cf\u5c11\u8fbe70%\uff0c\u4e14\u6d41\u7545\u6027\u635f\u5931\u6700\u5c0f\uff1blogits\u5e72\u9884\u65b9\u6cd5\u4f18\u4e8e\u9690\u85cf\u5c42\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u4e49\u611f\u77e5\u7684logits\u5e72\u9884\u662f\u4e00\u79cd\u7a33\u5b9a\u4e14\u6709\u6548\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5df2\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2510.23652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23652", "abs": "https://arxiv.org/abs/2510.23652", "authors": ["Yao Lu", "Yuqi Li", "Wenbin Xie", "Shanqing Yu", "Qi Xuan", "Zhaowei Zhu", "Shiping Wen"], "title": "The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models", "comment": null, "summary": "Although large language models (LLMs) have achieved revolutionary\nbreakthroughs in many fields, their large model size and high computational\ncost pose significant challenges for practical deployment on\nresource-constrained edge devices. To this end, layer pruning has been proposed\nto reduce the computational overhead by directly removing redundant layers.\nHowever, existing layer pruning methods typically rely on hand-crafted metrics\nto evaluate and remove individual layers, while ignoring the dependencies\nbetween layers. This can disrupt the model's information flow and severely\ndegrade performance. To address these issues, we propose CLP, a novel\ncontinuous layer pruning framework that introduces two key innovations: a\ndifferentiable concave gate algorithm that automatically identifies the best\ncontinuous layer segments for pruning via gradient-based optimization; and a\ncutoff endpoint tuning strategy that effectively restores model performance by\nfine-tuning only the layers adjacent to the pruned segments. Extensive\nexperiments across multiple model architectures (including LLaMA2, LLaMA3 and\nQwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly\noutperforms existing state-of-the-art baselines. For example, at a pruning rate\nof $20\\%$, CLP achieves an average performance retention of $95.34\\%$ on\nLLaMA3-70B, outperforming baselines by $4.29\\%$-$30.52\\%$. Furthermore, CLP can\nbe seamlessly combined with quantization to further compress the model with\nonly a slight performance loss.", "AI": {"tldr": "CLP\u662f\u4e00\u4e2a\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u51f9\u95e8\u7b97\u6cd5\u81ea\u52a8\u8bc6\u522b\u6700\u4f73\u8fde\u7eed\u5c42\u6bb5\u8fdb\u884c\u526a\u679d\uff0c\u5e76\u4f7f\u7528\u622a\u6b62\u7aef\u70b9\u8c03\u4f18\u7b56\u7565\u6062\u590d\u6a21\u578b\u6027\u80fd\uff0c\u5728\u591a\u4e2aLLM\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u6a21\u578b\u5c3a\u5bf8\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u73b0\u6709\u5c42\u526a\u679d\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6307\u6807\u8bc4\u4f30\u5355\u4e2a\u5c42\uff0c\u5ffd\u7565\u4e86\u5c42\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f1a\u7834\u574f\u4fe1\u606f\u6d41\u5e76\u4e25\u91cd\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faCLP\u6846\u67b6\uff1a1\uff09\u53ef\u5fae\u51f9\u95e8\u7b97\u6cd5\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u81ea\u52a8\u8bc6\u522b\u6700\u4f73\u8fde\u7eed\u5c42\u6bb5\u8fdb\u884c\u526a\u679d\uff1b2\uff09\u622a\u6b62\u7aef\u70b9\u8c03\u4f18\u7b56\u7565\u901a\u8fc7\u5fae\u8c03\u526a\u679d\u6bb5\u76f8\u90bb\u5c42\u6765\u6709\u6548\u6062\u590d\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\uff08LLaMA2\u3001LLaMA3\u3001Qwen\uff09\u548c\u5c3a\u5bf8\uff087B\u523070B\u53c2\u6570\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u572820%\u526a\u679d\u7387\u4e0b\uff0cLLaMA3-70B\u5e73\u5747\u6027\u80fd\u4fdd\u6301\u7387\u8fbe\u523095.34%\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa4.29%-30.52%\u3002", "conclusion": "CLP\u80fd\u6709\u6548\u51cf\u5c11LLM\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u5e76\u53ef\u65e0\u7f1d\u4e0e\u91cf\u5316\u6280\u672f\u7ed3\u5408\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\uff0c\u4ec5\u5e26\u6765\u8f7b\u5fae\u6027\u80fd\u635f\u5931\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u9a70\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u7269\u7406\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u666e\u53ca\uff0c\u9700\u8981\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86IDM\u3001OVM\u3001OVRV\u548c\u7b80\u5316CACC\u7b49\u7ecf\u5178\u6a21\u578b\u4e0e\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff0c\u901a\u8fc7\u6700\u5c0f\u5316RMSE\u6765\u6821\u51c6\u6a21\u578b\u53c2\u6570\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5728\u4e2d\u7b49\u3001\u957f\u548c\u8d85\u957f\u8f66\u8ddd\u4e0b\u7684RMSE\u5206\u522b\u4e3a0.0046\u30010.0016\u548c0.0025\uff1b\u7ecf\u5178\u6a21\u578b\u4e2dCACC\u8868\u73b0\u6700\u597d\uff0c\u957f\u8f66\u8ddd\u4e0bRMSE\u4e3a2.67\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u4e3b\u4ea4\u901a\u52a8\u6001\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u7535\u52a8\u6c7d\u8f66\u96c6\u6210\u73af\u5883\u4e2d\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens\u662f\u4e00\u4e2a\u900f\u660e\u7684AI\u75c5\u7406\u5b66\u52a9\u624b\uff0c\u5141\u8bb8\u75c5\u7406\u5b66\u5bb6\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\u7ec4\u7ec7\u5207\u7247\u95ee\u9898\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u62a5\u544a\u548c\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u5e2e\u52a9\u533b\u751f\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u5730\u505a\u51fa\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efb\u4eba\u5de5\u667a\u80fd\uff0cAI\u4e0d\u80fd\u662f\u9ed1\u76d2\u7cfb\u7edf\uff0c\u9700\u8981\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u7406\u89e3\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u521b\u5efaHistoLens\u7cfb\u7edf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u667a\u80fd\u7ffb\u8bd1\u4e3aAI\u5f15\u64ce\u7684\u7cbe\u786e\u67e5\u8be2\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u62a5\u544a\u548c\u70ed\u529b\u56fe\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u5e76\u8bad\u7ec3AI\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\u800c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u75c5\u7406\u5b66\u5bb6\u4fdd\u6301\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f7f\u7528\u53ef\u4fe1\u8d56\u7684AI\u52a9\u624b\u9a8c\u8bc1\u4ed6\u4eec\u7684\u89c1\u89e3\uff0c\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u7684\u8bca\u65ad\u3002", "conclusion": "HistoLens\u4f5c\u4e3a\u900f\u660e\u7684\u534f\u4f5c\u4f19\u4f34\uff0c\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684AI\u63a8\u7406\u548c\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u589e\u5f3a\u4e86\u533b\u751f\u5bf9AI\u7684\u4fe1\u4efb\uff0c\u540c\u65f6\u4fdd\u6301\u533b\u751f\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002"}}
{"id": "2510.23657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23657", "abs": "https://arxiv.org/abs/2510.23657", "authors": ["Saklain Niam", "Tashfiqur Rahman", "Md. Amjad Patwary", "Mukarram Hossain"], "title": "A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops", "comment": null, "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u9996\u4e2a\u673a\u5668\u5b66\u4e60\u6846\u67b6\u6765\u9884\u6d4b\u51b7\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u4e0b\u4e94\u79cd\u4f5c\u7269\u7684\u53d1\u82bd\u63d0\u5347\u6548\u679c\uff0c\u5176\u4e2dExtra Trees\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63ed\u793a\u4e86\u7535\u538b\u548c\u5904\u7406\u65f6\u95f4\u7684\u6fc0\u7d20\u6548\u5e94\u54cd\u5e94\u6a21\u5f0f\u3002", "motivation": "\u51b7\u7b49\u79bb\u5b50\u4f53\u662f\u4e00\u79cd\u73af\u4fdd\u7684\u4fc3\u8fdb\u79cd\u5b50\u53d1\u82bd\u7684\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u79cd\u5b50-\u7b49\u79bb\u5b50\u4f53-\u73af\u5883\u76f8\u4e92\u4f5c\u7528\uff0c\u7ed3\u679c\u96be\u4ee5\u9884\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9884\u6d4b\u6027\u5de5\u5177\u6765\u4f18\u5316\u5904\u7406\u53c2\u6570\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u63d0\u5347(GB)\u3001\u6781\u9650\u68af\u5ea6\u63d0\u5347(XGB)\u3001\u6781\u7aef\u968f\u673a\u6811(ET)\u53ca\u5176\u6df7\u5408\u6a21\u578b\u7b49\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4ecb\u7535\u963b\u6321\u653e\u7535\u7b49\u79bb\u5b50\u4f53\u6761\u4ef6\u4e0b\u9884\u6d4b\u5927\u8c46\u3001\u5927\u9ea6\u3001\u5411\u65e5\u8475\u3001\u841d\u535c\u548c\u756a\u8304\u7684\u53d1\u82bd\u63d0\u5347\u3002", "result": "Extra Trees\u6a21\u578b\u8868\u73b0\u6700\u4f73(R\u00b2=0.919)\uff0c\u7279\u5f81\u51cf\u5c11\u540e\u63d0\u5347\u81f3R\u00b2=0.925\u3002\u5de5\u7a0b\u5206\u6790\u663e\u793a\u6fc0\u7d20\u6548\u5e94\u54cd\u5e94\uff1a<7kV\u6216<200s\u6548\u679c\u53ef\u5ffd\u7565\uff0c7-15kV/200-500s\u53d1\u82bd\u7387\u6700\u5927\uff0c>20kV\u6216\u957f\u65f6\u95f4\u5904\u7406\u53d1\u82bd\u7387\u964d\u4f4e\u3002\u653e\u7535\u529f\u7387\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0c\u2265100W\u914d\u5408\u77ed\u65f6\u95f4\u5904\u7406\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u9884\u6d4b\u4e86\u4e0d\u540c\u7269\u79cd\u548c\u54c1\u79cd\u7684\u53d1\u82bd\u54cd\u5e94\uff0c\u5e76\u96c6\u6210\u5230MLflow\u4e2d\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u4e2d\u4f18\u5316\u51b7\u7b49\u79bb\u5b50\u4f53\u79cd\u5b50\u53d1\u82bd\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2510.23665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23665", "abs": "https://arxiv.org/abs/2510.23665", "authors": ["Juan C. Leon Alcazar", "Mattia Soldan", "Mohammad Saatialsoruji", "Alejandro Pardo", "Hani Itani", "Juan Camilo Perez", "Bernard Ghanem"], "title": "Transformers from Compressed Representations", "comment": null, "summary": "Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.", "AI": {"tldr": "TEMPEST\u662f\u4e00\u79cd\u5229\u7528\u538b\u7f29\u6587\u4ef6\u56fa\u6709\u5b57\u8282\u6d41\u7ed3\u6784\u8fdb\u884c\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d27\u51d1\u7f16\u7801\u4f7f\u6807\u51c6transformer\u80fd\u591f\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u5b66\u4e60\u8bed\u4e49\u8868\u793a\uff0c\u65e0\u9700\u539f\u59cb\u5b57\u8282\u5904\u7406\u6216\u5b8c\u6574\u5a92\u4f53\u89e3\u7801\u3002", "motivation": "\u538b\u7f29\u6587\u4ef6\u683c\u5f0f\u662f\u9ad8\u6548\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u7684\u57fa\u7840\uff0c\u4f46\u5176\u5728\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u5b57\u8282\u6d41\u7ed3\u6784\u8bbe\u8ba1\u6709\u6548\u7684\u6807\u8bb0\u5316\u548c\u7f16\u7801\u7b56\u7565\uff0c\u4f7f\u6807\u51c6transformer\u80fd\u591f\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u5b66\u4e60\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u7f16\u7801\u65b9\u6848\u548c\u6a21\u6001\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTEMPEST\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u6027\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bed\u4e49\u5206\u7c7b\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9MCTS\u4e2d\u7684\u62bd\u8c61\u65b9\u6cd5\u63d0\u51fa\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86\u5f53\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u591a\u4e2a\u52a8\u4f5c\u5177\u6709\u76f8\u540cUCB\u503c\u65f6\u9700\u8981\u6253\u7834\u5e73\u5c40\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u7684\u62bd\u8c61\u5185\u90e8\u7b56\u7565\u3002", "motivation": "MCTS\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u72b6\u6001\u548c\u52a8\u4f5c\u62bd\u8c61\u6765\u6539\u5584\uff0c\u4f46\u73b0\u6709\u62bd\u8c61\u65b9\u6cd5\uff08\u5982pruned OGA\uff09\u5728\u591a\u4e2a\u52a8\u4f5c\u5c5e\u4e8e\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u65f6\uff0c\u7531\u4e8eUCB\u503c\u76f8\u540c\u800c\u9700\u8981\u968f\u673a\u6253\u7834\u5e73\u5c40\uff0c\u8fd9\u5f71\u54cd\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u591a\u79cd\u62bd\u8c61\u5185\u90e8\u7b56\u7565\uff08intra-abstraction policies\uff09\uff0c\u7528\u4e8e\u5728\u62bd\u8c61\u8282\u70b9\u5185\u591a\u4e2a\u52a8\u4f5c\u5177\u6709\u76f8\u540cUCB\u503c\u65f6\u8fdb\u884c\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u51e0\u79cd\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u90fd\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u62bd\u8c61\u8282\u70b9\u5185\u7684\u52a8\u4f5c\u9009\u62e9\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347MCTS\u5728\u62bd\u8c61\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u62bd\u8c61MCTS\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u5c40\u6253\u7834\u673a\u5236\u3002"}}
{"id": "2510.23668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23668", "abs": "https://arxiv.org/abs/2510.23668", "authors": ["Fujiang Yuan", "Yangrui Fan", "Xiaohuan Bing", "Zhen Tian", "Chunhong Yuan", "Yankang Li"], "title": "Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems", "comment": null, "summary": "Accurate traffic flow forecasting is essential for intelligent transportation\nsystems and urban traffic management. However, single model approaches often\nfail to capture the complex, nonlinear, and multi scale temporal patterns in\ntraffic flow data. This study proposes a decomposition driven hybrid framework\nthat integrates Seasonal Trend decomposition using Loess (STL) with three\ncomplementary predictive models. STL first decomposes the original time series\ninto trend, seasonal, and residual components. Then, a Long Short Term Memory\n(LSTM) network models long term trends, an Autoregressive Integrated Moving\nAverage (ARIMA) model captures seasonal periodicity, and an Extreme Gradient\nBoosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The\nfinal forecast is obtained through multiplicative integration of the sub model\npredictions. Using 998 traffic flow records from a New York City intersection\nbetween November and December 2015, results show that the LSTM ARIMA XGBoost\nhybrid model significantly outperforms standalone models including LSTM, ARIMA,\nand XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy\neffectively isolates temporal characteristics, allowing each model to\nspecialize, thereby improving prediction accuracy, interpretability, and\nrobustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSTL\u5206\u89e3\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408LSTM\u3001ARIMA\u548cXGBoost\u4e09\u79cd\u6a21\u578b\u6765\u9884\u6d4b\u4ea4\u901a\u6d41\u91cf\uff0c\u901a\u8fc7\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u5e76\u8ba9\u5404\u6a21\u578b\u4e13\u6ce8\u4e8e\u4e0d\u540c\u65f6\u95f4\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5355\u4e00\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4ea4\u901a\u6d41\u91cf\u6570\u636e\u4e2d\u590d\u6742\u7684\u975e\u7ebf\u6027\u3001\u591a\u5c3a\u5ea6\u65f6\u95f4\u6a21\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u6df7\u5408\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528STL\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u548c\u6b8b\u5dee\u4e09\u4e2a\u5206\u91cf\uff0c\u5206\u522b\u7528LSTM\u5efa\u6a21\u957f\u671f\u8d8b\u52bf\u3001ARIMA\u6355\u6349\u5b63\u8282\u5468\u671f\u6027\u3001XGBoost\u9884\u6d4b\u975e\u7ebf\u6027\u6b8b\u5dee\u6ce2\u52a8\uff0c\u6700\u540e\u901a\u8fc7\u4e58\u6cd5\u96c6\u6210\u5f97\u5230\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u57fa\u4e8e\u7ebd\u7ea6\u5e02\u4ea4\u53c9\u53e3998\u6761\u4ea4\u901a\u6d41\u91cf\u8bb0\u5f55\uff0cLSTM-ARIMA-XGBoost\u6df7\u5408\u6a21\u578b\u5728MAE\u3001RMSE\u548cR\u5e73\u65b9\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684LSTM\u3001ARIMA\u548cXGBoost\u6a21\u578b\u3002", "conclusion": "\u5206\u89e3\u7b56\u7565\u80fd\u6709\u6548\u5206\u79bb\u65f6\u95f4\u7279\u5f81\uff0c\u4f7f\u5404\u6a21\u578b\u4e13\u4e1a\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684\u76f8\u5173\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u8def\u5f84\u53ef\u4fe1\u5ea6\u6307\u6807\u7684\u81ea\u6307\u793a\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u6709\u6548\u9a8c\u8bc1\u63a8\u7406\u6b63\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63a8\u7406\u80fd\u529b\u5f3a\uff0c\u4f46\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u548c\u5e7b\u89c9\uff0c\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u9002\u7528\u8303\u56f4\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u884c\u4e3a\uff0c\u53d1\u73b0\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u7684\u76f8\u5173\u77e9\u9635\u79e9\u662f\u63a8\u7406\u6b63\u786e\u6027\u7684\u7a33\u5065\u6307\u6807\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u81ea\u6307\u793a\u65b9\u6cd5\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u91cd\u52a0\u6743\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u8d85\u8fc775%\u7684\u51c6\u786e\u7387\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5c06\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "LLM\u5185\u90e8\u884c\u4e3a\u672c\u8eab\u5305\u542b\u63a8\u7406\u53ef\u4fe1\u5ea6\u4fe1\u606f\uff0c\u81ea\u6307\u793a\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6295\u7968\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027(PCS)\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\uff0c\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4f9d\u8d56LLM\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u5728\u566a\u58f0\u548c\u590d\u6742\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8ePCS\u539f\u5219\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u8d1f\u8d23\uff0c\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u5728\u4e5d\u4e2a\u4e0d\u540c\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528DeepSeek-V3\u548cGPT-4o\u4f5c\u4e3a\u540e\u7aef\uff0cVDSAgents\u6301\u7eed\u4f18\u4e8eAutoKaggle\u548cDataInterformer\u7684\u7ed3\u679c\u3002", "conclusion": "\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u662f\u53ef\u884c\u7684\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.23756", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23756", "abs": "https://arxiv.org/abs/2510.23756", "authors": ["Nicki Barari", "Edward Kim", "Christopher MacLellan"], "title": "Explaining Robustness to Catastrophic Forgetting Through Incremental Concept Formation", "comment": "18 pages, 5 figures, Advances in Cognitive Systems 2025", "summary": "Catastrophic forgetting remains a central challenge in continual learning,\nwhere models are required to integrate new knowledge over time without losing\nwhat they have previously learned. In prior work, we introduced Cobweb/4V, a\nhierarchical concept formation model that exhibited robustness to catastrophic\nforgetting in visual domains. Motivated by this robustness, we examine three\nhypotheses regarding the factors that contribute to such stability: (1)\nadaptive structural reorganization enhances knowledge retention, (2) sparse and\nselective updates reduce interference, and (3) information-theoretic learning\nbased on sufficiency statistics provides advantages over gradient-based\nbackpropagation. To test these hypotheses, we compare Cobweb/4V with neural\nbaselines, including CobwebNN, a neural implementation of the Cobweb framework\nintroduced in this work. Experiments on datasets of varying complexity (MNIST,\nFashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring\nenhances learning plasticity, sparse updates help mitigate interference, and\nthe information-theoretic learning process preserves prior knowledge without\nrevisiting past data. Together, these findings provide insight into mechanisms\nthat can mitigate catastrophic forgetting and highlight the potential of\nconcept-based, information-theoretic approaches for building stable and\nadaptive continual learning systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86Cobweb/4V\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u62b5\u6297\u707e\u96be\u6027\u9057\u5fd8\u7684\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e09\u4e2a\u5047\u8bbe\uff1a\u81ea\u9002\u5e94\u7ed3\u6784\u91cd\u7ec4\u3001\u7a00\u758f\u9009\u62e9\u6027\u66f4\u65b0\u548c\u4fe1\u606f\u7406\u8bba\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "motivation": "\u707e\u96be\u6027\u9057\u5fd8\u662f\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Cobweb/4V\u6a21\u578b\u62b5\u6297\u9057\u5fd8\u7684\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u7a33\u5b9a\u81ea\u9002\u5e94\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83Cobweb/4V\u4e0e\u795e\u7ecf\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684CobwebNN\uff09\uff0c\u5728MNIST\u3001Fashion-MNIST\u3001MedMNIST\u548cCIFAR-10\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e09\u4e2a\u5047\u8bbe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u81ea\u9002\u5e94\u7ed3\u6784\u91cd\u7ec4\u589e\u5f3a\u4e86\u5b66\u4e60\u53ef\u5851\u6027\uff0c\u7a00\u758f\u66f4\u65b0\u51cf\u5c11\u4e86\u5e72\u6270\uff0c\u4fe1\u606f\u7406\u8bba\u5b66\u4e60\u8fc7\u7a0b\u65e0\u9700\u56de\u987e\u5386\u53f2\u6570\u636e\u5373\u53ef\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u673a\u5236\uff0c\u7a81\u663e\u4e86\u57fa\u4e8e\u6982\u5ff5\u548c\u4fe1\u606f\u7406\u8bba\u65b9\u6cd5\u5728\u6784\u5efa\u7a33\u5b9a\u81ea\u9002\u5e94\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u7684\u56f0\u96be\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u6d89\u53ca\u7406\u89e3\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u903b\u8f91\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u4f7f\u7528\u516b\u9053\u81ea\u5b9a\u4e49\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86GPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u548cSabi'a\u7b49LLMs\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eLLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u5f85\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Law in Silico\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\uff0c\u9a8c\u8bc1\u4e86LLM\u80fd\u591f\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\u5e76\u4e3a\u6cd5\u5f8b\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u89c1\u89e3\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u6cd5\u5f8b\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u6216\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u6765\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\u3002\u5927\u8bed\u8a00\u6a21\u578b\u51ed\u501f\u5176\u4e16\u754c\u77e5\u8bc6\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u662f\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\u7684\u7406\u60f3\u5019\u9009\u3002", "method": "\u5f15\u5165Law in Silico\u6846\u67b6\uff0c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6a21\u62df\u6cd5\u5f8b\u573a\u666f\uff0c\u5305\u542b\u4e2a\u4f53\u51b3\u7b56\u548c\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u6cd5\u7b49\u5236\u5ea6\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM\u667a\u80fd\u4f53\u80fd\u591f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\uff0c\u63d0\u4f9b\u4e0e\u73b0\u5b9e\u89c2\u5bdf\u4e00\u81f4\u7684\u89c1\u89e3\uff1b\u5fae\u89c2\u6a21\u62df\u663e\u793a\u529f\u80fd\u826f\u597d\u3001\u900f\u660e\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6cd5\u5f8b\u7cfb\u7edf\u80fd\u66f4\u597d\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u6743\u5229\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u6a21\u62df\u6cd5\u5f8b\u7cfb\u7edf\uff0c\u4e3a\u6cd5\u5f8b\u7406\u8bba\u9a8c\u8bc1\u548c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u826f\u597d\u6cd5\u5f8b\u7cfb\u7edf\u5bf9\u4fdd\u62a4\u5f31\u52bf\u7fa4\u4f53\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u793a\u4f8b\u548c\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6536\u96c6\u9ad8\u8d28\u91cf\u793a\u4f8b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u5229\u7528\u8de8\u4efb\u52a1\u793a\u4f8b\u63d0\u793aLLM\u4f2a\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u4efb\u52a1\u5b9e\u4f8b\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u5c06\u6807\u7b7e\u4fe1\u606f\u4f20\u64ad\u5230\u5269\u4f59\u76ee\u6807\u793a\u4f8b\u4e2d\uff0c\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u8de8\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0LLM\u4f20\u64ad\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23818", "abs": "https://arxiv.org/abs/2510.23818", "authors": ["Yilang Zhang", "Xiaodong Yang", "Yiwei Cai", "Georgios B. Giannakis"], "title": "ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning", "comment": null, "summary": "As large language models (LLMs) continue to scale in size, the computational\noverhead has become a major bottleneck for task-specific fine-tuning. While\nlow-rank adaptation (LoRA) effectively curtails this cost by confining the\nweight updates to a low-dimensional subspace, such a restriction can hinder\neffectiveness and slow convergence. This contribution deals with these\nlimitations by accumulating progressively a high-rank weight update from\nconsecutive low-rank increments. Specifically, the per update optimal low-rank\nmatrix is identified to minimize the loss function and closely approximate full\nfine-tuning. To endow efficient and seamless optimization without restarting,\nthis optimal choice is formed by appropriately scaling the columns of the\noriginal low-rank matrix. Rigorous performance guarantees reveal that the\noptimal scaling can be found analytically. Extensive numerical tests with\npopular LLMs scaling up to 12 billion parameters demonstrate a consistent\nperformance gain and fast convergence relative to state-of-the-art LoRA\nvariants on diverse tasks including natural language understanding, commonsense\nreasoning, and mathematical problem solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LoRA\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d2f\u79ef\u8fde\u7eed\u7684\u4f4e\u79e9\u589e\u91cf\u6765\u6784\u5efa\u9ad8\u79e9\u6743\u91cd\u66f4\u65b0\uff0c\u4ece\u800c\u514b\u670d\u4f20\u7edfLoRA\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u6027\u80fd\u5e76\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u4e0d\u65ad\u6269\u5927\uff0c\u8ba1\u7b97\u5f00\u9500\u6210\u4e3a\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u4e3b\u8981\u74f6\u9888\u3002\u867d\u7136\u4f4e\u79e9\u9002\u5e94(LoRA)\u901a\u8fc7\u5c06\u6743\u91cd\u66f4\u65b0\u9650\u5236\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u6765\u6709\u6548\u63a7\u5236\u6210\u672c\uff0c\u4f46\u8fd9\u79cd\u9650\u5236\u4f1a\u963b\u788d\u6709\u6548\u6027\u548c\u51cf\u7f13\u6536\u655b\u901f\u5ea6\u3002", "method": "\u901a\u8fc7\u7d2f\u79ef\u8fde\u7eed\u7684\u4f4e\u79e9\u589e\u91cf\u6765\u9010\u6b65\u6784\u5efa\u9ad8\u79e9\u6743\u91cd\u66f4\u65b0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bc6\u522b\u6bcf\u4e2a\u66f4\u65b0\u6b65\u9aa4\u4e2d\u7684\u6700\u4f18\u4f4e\u79e9\u77e9\u9635\uff0c\u4ee5\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u5e76\u7d27\u5bc6\u903c\u8fd1\u5168\u53c2\u6570\u5fae\u8c03\u3002\u4e3a\u4e86\u5728\u4e0d\u91cd\u542f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u65e0\u7f1d\u4f18\u5316\uff0c\u901a\u8fc7\u9002\u5f53\u7f29\u653e\u539f\u59cb\u4f4e\u79e9\u77e9\u9635\u7684\u5217\u6765\u5f62\u6210\u6700\u4f18\u9009\u62e9\u3002", "result": "\u5728\u89c4\u6a21\u9ad8\u8fbe120\u4ebf\u53c2\u6570\u7684\u6d41\u884cLLMs\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u6570\u503c\u6d4b\u8bd5\u8868\u660e\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u7b49\u591a\u6837\u5316\u4efb\u52a1\u4e0a\uff0c\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684LoRA\u53d8\u4f53\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u548c\u5feb\u901f\u6536\u655b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7d2f\u79ef\u4f4e\u79e9\u589e\u91cf\u6784\u5efa\u9ad8\u79e9\u6743\u91cd\u66f4\u65b0\u7684\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfLoRA\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u4f5c\u7269\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\uff08SSDM\uff09\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u4e0e\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f5c\u7269\u75c5\u5bb3\u7ba1\u7406\u4ece\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u53d1\u5c55\u5230\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u7279\u5f81\u5b66\u4e60\u3002\u57fa\u7840\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u5904\u7406\u4f5c\u7269\u75c5\u5bb3\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u65b9\u5f0f\uff0c\u80fd\u591f\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u89e3\u91ca\u75c7\u72b6\u6587\u672c\uff0c\u63a8\u7406\u75c7\u72b6-\u7ba1\u7406\u5173\u7cfb\uff0c\u5e76\u4e3a\u79cd\u690d\u8005\u548c\u6559\u80b2\u8005\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u95ee\u7b54\u652f\u6301\u3002", "method": "\u901a\u8fc7\u7b5b\u9009\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u4e2d\u5e94\u7528\u7684\u6587\u732e\uff0c\u91cd\u70b9\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a\u57fa\u7840\u6a21\u578b\u5e94\u7528\u57282023-24\u5e74\u6587\u732e\u6fc0\u589e\uff1b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u5feb\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u51fa\u7248\u7269\u6570\u91cf\u589e\u957f5-10\u500d\uff1b\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u5728\u667a\u80fd\u55b7\u6d12\u4e2d\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff1b\u6570\u5b57\u5b6a\u751f\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u53ef\u6a21\u62df\u865a\u62df\u9776\u5411\u55b7\u6d12\uff1b\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff1b\u4eba\u673a\u534f\u4f5c\u4ecd\u7136\u6709\u9650\uff1b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u6b63\u5728\u63a8\u52a8\u4f5c\u7269\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u7684\u521b\u65b0\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u878d\u5408\u65b9\u9762\u3002\u672a\u6765\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8f6c\u5316\u3001\u4eba\u673a\u534f\u4f5c\u7684\u589e\u5f3a\u4ee5\u53ca\u5b9e\u65f6\u53cd\u9988\u7cfb\u7edf\u7684\u5b8c\u5584\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7530\u95f4\u75c5\u5bb3\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23912", "abs": "https://arxiv.org/abs/2510.23912", "authors": ["Marko Karbevski", "Antonij Mijoski"], "title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers", "comment": null, "summary": "The Query, Key, Value weight triplet is a building block of current attention\nmechanisms in state-of-the-art LLMs. We theoretically investigate whether this\ntriplet can be reduced, proving under simplifying assumptions that the Query\nweights are redundant, thereby reducing the number of non-embedding/lm-head\nparameters by over 8%. We validate the theory on full-complexity GPT-3 small\narchitectures (with layer normalization, skip connections, and weight decay)\ntrained from scratch, demonstrating that the reduced model achieves comparable\nvalidation loss to standard baselines. These findings motivate the\ninvestigation of the Query weight redundancy at scale.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0cQuery\u6743\u91cd\u662f\u5197\u4f59\u7684\uff0c\u53ef\u4ee5\u79fb\u9664\u800c\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4ece\u800c\u51cf\u5c118%\u4ee5\u4e0a\u7684\u975e\u5d4c\u5165\u53c2\u6570\u3002", "motivation": "\u63a2\u7d22\u5f53\u524dLLM\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684Query\u3001Key\u3001Value\u6743\u91cd\u4e09\u5143\u7ec4\u662f\u5426\u53ef\u4ee5\u88ab\u7b80\u5316\uff0c\u4ee5\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u6570\u91cf\u3002", "method": "\u5728\u7406\u8bba\u5206\u6790\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528\u5b8c\u6574\u7684GPT-3\u5c0f\u578b\u67b6\u6784\uff08\u5305\u542b\u5c42\u5f52\u4e00\u5316\u3001\u8df3\u8dc3\u8fde\u63a5\u548c\u6743\u91cd\u8870\u51cf\uff09\u8fdb\u884c\u4ece\u5934\u8bad\u7ec3\u9a8c\u8bc1\u3002", "result": "\u7b80\u5316\u540e\u7684\u6a21\u578b\uff08\u79fb\u9664Query\u6743\u91cd\uff09\u5728\u9a8c\u8bc1\u635f\u5931\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6807\u51c6\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "Query\u6743\u91cd\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u662f\u5197\u4f59\u7684\uff0c\u8fd9\u4e00\u53d1\u73b0\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u7684\u9002\u7528\u6027\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.23914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23914", "abs": "https://arxiv.org/abs/2510.23914", "authors": ["Arsenii Mustafin", "Xinyi Sheng", "Dominik Baumann"], "title": "Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs", "comment": "12 pages, 1 figure", "summary": "The theoretical analysis of Markov Decision Processes (MDPs) is commonly\nsplit into two cases - the average-reward case and the discounted-reward case -\nwhich, while sharing similarities, are typically analyzed separately. In this\nwork, we extend a recently introduced geometric interpretation of MDPs for the\ndiscounted-reward case to the average-reward case, thereby unifying both. This\nallows us to extend a major result known for the discounted-reward case to the\naverage-reward case: under a unique and ergodic optimal policy, the Value\nIteration algorithm achieves a geometric convergence rate.", "AI": {"tldr": "\u5c06\u6298\u6263\u5956\u52b1MDP\u7684\u51e0\u4f55\u89e3\u91ca\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u60c5\u51b5\uff0c\u7edf\u4e00\u4e86\u4e24\u79cd\u5206\u6790\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5728\u552f\u4e00\u4e14\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\u503c\u8fed\u4ee3\u7b97\u6cd5\u5177\u6709\u51e0\u4f55\u6536\u655b\u7387\u3002", "motivation": "MDP\u7684\u7406\u8bba\u5206\u6790\u901a\u5e38\u5206\u4e3a\u5e73\u5747\u5956\u52b1\u548c\u6298\u6263\u5956\u52b1\u4e24\u79cd\u60c5\u51b5\uff0c\u867d\u7136\u76f8\u4f3c\u4f46\u5206\u5f00\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u79cd\u60c5\u51b5\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u5c06\u6298\u6263\u5956\u52b1MDP\u7684\u51e0\u4f55\u89e3\u91ca\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u60c5\u51b5\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u51e0\u4f55\u5206\u6790\u6846\u67b6\u3002", "result": "\u6210\u529f\u5c06\u6298\u6263\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u4e3b\u8981\u7ed3\u679c\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u60c5\u51b5\uff1a\u5728\u552f\u4e00\u4e14\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u503c\u8fed\u4ee3\u7b97\u6cd5\u5177\u6709\u51e0\u4f55\u6536\u655b\u7387\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u89e3\u91ca\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5e73\u5747\u5956\u52b1\u548c\u6298\u6263\u5956\u52b1MDP\u5206\u6790\u7684\u6574\u5408\uff0c\u4e3a\u503c\u8fed\u4ee3\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.23931", "categories": ["cs.LG", "cs.CR", "cs.DC", "68T07 (Primary) 68M14, 68P27, 68Q32, 94A16, 62H35 (Secondary)", "I.2.11; I.2.6; C.2.4; D.4.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.23931", "abs": "https://arxiv.org/abs/2510.23931", "authors": ["Miguel Fernandez-de-Retana", "Unai Zulaika", "Rub\u00e9n S\u00e1nchez-Corcuera", "Aitor Almeida"], "title": "Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments", "comment": "17 pages, 12 figures", "summary": "Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u5dee\u5206\u9690\u79c1\u673a\u5236\uff08DP-SGD\u548cPDP-SGD\uff09\u4f5c\u4e3a\u9632\u5fa1\u68af\u5ea6\u6cc4\u6f0f\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0DP-SGD\u80fd\u663e\u8457\u964d\u4f4e\u653b\u51fb\u98ce\u9669\u4f46\u4f1a\u727a\u7272\u6a21\u578b\u6027\u80fd\uff0c\u800cPDP-SGD\u4fdd\u6301\u826f\u597d\u6027\u80fd\u4f46\u9632\u5fa1\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u62a4\u654f\u611f\u6570\u636e\u4e0d\u76f4\u63a5\u5171\u4eab\uff0c\u4f46\u4ecd\u9762\u4e34\u68af\u5ea6\u6cc4\u6f0f\u653b\u51fb\u7684\u98ce\u9669\uff0c\u9700\u8981\u8bc4\u4f30\u5dee\u5206\u9690\u79c1\u673a\u5236\u5728\u5b9e\u9645\u9632\u5fa1\u4e2d\u7684\u6548\u679c\u3002", "method": "\u5728\u6a21\u62df\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u8bc4\u4f30\u4e0d\u540c\u9690\u79c1\u7ea7\u522b\u4e0b\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4ece\u622a\u83b7\u68af\u5ea6\u4e2d\u91cd\u5efa\u79c1\u6709\u6570\u636e\u7684\u8d28\u91cf\u3002", "result": "DP-SGD\u663e\u8457\u51cf\u8f7b\u4e86\u68af\u5ea6\u6cc4\u6f0f\u653b\u51fb\u98ce\u9669\uff0c\u4f46\u6a21\u578b\u6548\u7528\u6709\u9002\u5ea6\u6298\u8877\uff1bPDP-SGD\u4fdd\u6301\u5f3a\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u9632\u5fa1\u91cd\u5efa\u653b\u51fb\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u7406\u8bba\u4fdd\u8bc1\uff0c\u7ecf\u9a8c\u6027\u5730\u8bc4\u4f30\u9690\u79c1\u673a\u5236\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u56e0\u4e3a\u4fe1\u606f\u6cc4\u6f0f\u53ef\u80fd\u5bf9\u6570\u636e\u5b89\u5168\u548c\u9690\u79c1\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002"}}
{"id": "2510.23948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23948", "abs": "https://arxiv.org/abs/2510.23948", "authors": ["Qianfeng Wen", "Zhenwei Tang", "Ashton Anderson"], "title": "ChessQA: Evaluating Large Language Models for Chess Understanding", "comment": "33 pages,8 figures", "summary": "Chess provides an ideal testbed for evaluating the reasoning, modeling, and\nabstraction capabilities of large language models (LLMs), as it has\nwell-defined structure and objective ground truth while admitting a wide\nspectrum of skill levels. However, existing evaluations of LLM ability in chess\nare ad hoc and narrow in scope, making it difficult to accurately measure LLM\nchess understanding and how it varies with scale, post-training methodologies,\nor architecture choices. We present ChessQA, a comprehensive benchmark that\nassesses LLM chess understanding across five task categories (Structural,\nMotifs, Short Tactics, Position Judgment, and Semantic), which approximately\ncorrespond to the ascending abstractions that players master as they accumulate\nchess knowledge, from understanding basic rules and learning tactical motifs to\ncorrectly calculating tactics, evaluating positions, and semantically\ndescribing high-level concepts. In this way, ChessQA captures a more\ncomprehensive picture of chess ability and understanding, going significantly\nbeyond the simple move quality evaluations done previously, and offers a\ncontrolled, consistent setting for diagnosis and comparison. Furthermore,\nChessQA is inherently dynamic, with prompts, answer keys, and construction\nscripts that can evolve as models improve. Evaluating a range of contemporary\nLLMs, we find persistent weaknesses across all five categories and provide\nresults and error analyses by category. We will release the code, periodically\nrefreshed datasets, and a public leaderboard to support further research.", "AI": {"tldr": "ChessQA\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8c61\u68cb\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff1a\u7ed3\u6784\u3001\u6a21\u5f0f\u3001\u77ed\u6218\u672f\u3001\u4f4d\u7f6e\u5224\u65ad\u548c\u8bed\u4e49\uff0c\u5bf9\u5e94\u73a9\u5bb6\u638c\u63e1\u8c61\u68cb\u77e5\u8bc6\u7684\u9012\u8fdb\u62bd\u8c61\u5c42\u6b21\u3002", "motivation": "\u73b0\u6709\u7684LLM\u8c61\u68cb\u80fd\u529b\u8bc4\u4f30\u662f\u4e34\u65f6\u6027\u7684\u4e14\u8303\u56f4\u72ed\u7a84\uff0c\u96be\u4ee5\u51c6\u786e\u8861\u91cfLLM\u5bf9\u8c61\u68cb\u7684\u7406\u89e3\u4ee5\u53ca\u8fd9\u79cd\u7406\u89e3\u5982\u4f55\u968f\u89c4\u6a21\u3001\u540e\u8bad\u7ec3\u65b9\u6cd5\u6216\u67b6\u6784\u9009\u62e9\u800c\u53d8\u5316\u3002", "method": "\u521b\u5efaChessQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff1a\u7ed3\u6784\uff08\u57fa\u672c\u89c4\u5219\uff09\u3001\u6a21\u5f0f\uff08\u6218\u672f\u6a21\u5f0f\uff09\u3001\u77ed\u6218\u672f\uff08\u6b63\u786e\u8ba1\u7b97\uff09\u3001\u4f4d\u7f6e\u5224\u65ad\uff08\u8bc4\u4f30\u4f4d\u7f6e\uff09\u548c\u8bed\u4e49\uff08\u63cf\u8ff0\u9ad8\u7ea7\u6982\u5ff5\uff09\u3002", "result": "\u8bc4\u4f30\u5f53\u4ee3LLM\u53d1\u73b0\u6240\u6709\u4e94\u4e2a\u7c7b\u522b\u90fd\u5b58\u5728\u6301\u7eed\u5f31\u70b9\uff0c\u63d0\u4f9b\u4e86\u6309\u7c7b\u522b\u7684\u7ed3\u679c\u548c\u9519\u8bef\u5206\u6790\u3002", "conclusion": "ChessQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8c61\u68cb\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7b80\u5355\u7684\u8d70\u5b50\u8d28\u91cf\u8bc4\u4f30\uff0c\u4e3a\u8bca\u65ad\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53d7\u63a7\u3001\u4e00\u81f4\u7684\u8bbe\u7f6e\uff0c\u5e76\u5c06\u53d1\u5e03\u4ee3\u7801\u3001\u5b9a\u671f\u66f4\u65b0\u7684\u6570\u636e\u96c6\u548c\u516c\u5171\u6392\u884c\u699c\u3002"}}
{"id": "2510.23966", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23966", "abs": "https://arxiv.org/abs/2510.23966", "authors": ["Scott Emmons", "Roland S. Zimmermann", "David K. Elson", "Rohin Shah"], "title": "A Pragmatic Way to Measure Chain-of-Thought Monitorability", "comment": "The first two authors contributed equally", "summary": "While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI\nsafety, this opportunity could be lost through shifts in training practices or\nmodel architecture. To help preserve monitorability, we propose a pragmatic way\nto measure two components of it: legibility (whether the reasoning can be\nfollowed by a human) and coverage (whether the CoT contains all the reasoning\nneeded for a human to also produce the final output). We implement these\nmetrics with an autorater prompt that enables any capable LLM to compute the\nlegibility and coverage of existing CoTs. After sanity-checking our prompted\nautorater with synthetic CoT degradations, we apply it to several frontier\nmodels on challenging benchmarks, finding that they exhibit high\nmonitorability. We present these metrics, including our complete autorater\nprompt, as a tool for developers to track how design decisions impact\nmonitorability. While the exact prompt we share is still a preliminary version\nunder ongoing development, we are sharing it now in the hopes that others in\nthe community will find it useful. Our method helps measure the default\nmonitorability of CoT - it should be seen as a complement, not a replacement,\nfor the adversarial stress-testing needed to test robustness against\ndeliberately evasive models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cf\u601d\u7ef4\u94fe\u53ef\u76d1\u63a7\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5305\u62ec\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u5ea6\u4e24\u4e2a\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u8bc4\u5206\u5668\u63d0\u793a\u5b9e\u73b0\u8bc4\u4f30\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u53ef\u76d1\u63a7\u6027\u3002", "motivation": "\u601d\u7ef4\u94fe\u76d1\u63a7\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\uff0c\u4f46\u8bad\u7ec3\u5b9e\u8df5\u6216\u6a21\u578b\u67b6\u6784\u7684\u53d8\u5316\u53ef\u80fd\u4f7f\u8fd9\u79cd\u673a\u4f1a\u4e27\u5931\u3002\u4e3a\u4e86\u5e2e\u52a9\u4fdd\u6301\u53ef\u76d1\u63a7\u6027\uff0c\u9700\u8981\u6d4b\u91cf\u5176\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u63d0\u51fa\u6d4b\u91cf\u53ef\u8bfb\u6027\uff08\u4eba\u7c7b\u80fd\u5426\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\uff09\u548c\u8986\u76d6\u5ea6\uff08\u601d\u7ef4\u94fe\u662f\u5426\u5305\u542b\u4eba\u7c7b\u4ea7\u751f\u6700\u7ec8\u8f93\u51fa\u6240\u9700\u7684\u6240\u6709\u63a8\u7406\uff09\u7684\u6307\u6807\uff0c\u4f7f\u7528\u81ea\u52a8\u8bc4\u5206\u5668\u63d0\u793a\u8ba9\u4efb\u4f55\u6709\u80fd\u529b\u7684LLM\u8ba1\u7b97\u73b0\u6709\u601d\u7ef4\u94fe\u7684\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u5ea6\u3002", "result": "\u5728\u5408\u6210\u601d\u7ef4\u94fe\u9000\u5316\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u81ea\u52a8\u8bc4\u5206\u5668\u7684\u6709\u6548\u6027\uff0c\u5e94\u7528\u4e8e\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53d1\u73b0\u5b83\u4eec\u8868\u73b0\u51fa\u9ad8\u53ef\u76d1\u63a7\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u6307\u6807\u53ef\u4f5c\u4e3a\u5f00\u53d1\u8005\u8ddf\u8e2a\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u53ef\u76d1\u63a7\u6027\u5f71\u54cd\u7684\u5de5\u5177\uff0c\u8be5\u65b9\u6cd5\u5e94\u88ab\u89c6\u4e3a\u5bf9\u6297\u6027\u538b\u529b\u6d4b\u8bd5\u7684\u8865\u5145\u800c\u975e\u66ff\u4ee3\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5bf9\u6297\u6545\u610f\u89c4\u907f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.23972", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23972", "abs": "https://arxiv.org/abs/2510.23972", "authors": ["Andra\u017e Jelin\u010di\u010d", "Owen Lockwood", "Akhil Garlapati", "Guillaume Verdon", "Trevor McCourt"], "title": "An efficient probabilistic hardware architecture for diffusion-like models", "comment": "9 pages, 6 figures", "summary": "The proliferation of probabilistic AI has promoted proposals for specialized\nstochastic computers. Despite promising efficiency gains, these proposals have\nfailed to gain traction because they rely on fundamentally limited modeling\ntechniques and exotic, unscalable hardware. In this work, we address these\nshortcomings by proposing an all-transistor probabilistic computer that\nimplements powerful denoising models at the hardware level. A system-level\nanalysis indicates that devices based on our architecture could achieve\nperformance parity with GPUs on a simple image benchmark using approximately\n10,000 times less energy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5168\u6676\u4f53\u7ba1\u6982\u7387\u8ba1\u7b97\u673a\u67b6\u6784\uff0c\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u5f3a\u5927\u7684\u53bb\u566a\u6a21\u578b\uff0c\u76f8\u6bd4GPU\u80fd\u5b9e\u73b0\u7ea610,000\u500d\u7684\u80fd\u6548\u63d0\u5347", "motivation": "\u73b0\u6709\u7684\u6982\u7387AI\u4e13\u7528\u8ba1\u7b97\u673a\u63d0\u6848\u4f9d\u8d56\u6709\u9650\u5efa\u6a21\u6280\u672f\u548c\u4e0d\u53ef\u6269\u5c55\u7684\u5f02\u8d28\u786c\u4ef6\uff0c\u672a\u80fd\u83b7\u5f97\u5e7f\u6cdb\u5e94\u7528", "method": "\u8bbe\u8ba1\u5168\u6676\u4f53\u7ba1\u6982\u7387\u8ba1\u7b97\u673a\u67b6\u6784\uff0c\u5728\u786c\u4ef6\u5c42\u9762\u76f4\u63a5\u5b9e\u73b0\u5f3a\u5927\u7684\u53bb\u566a\u6a21\u578b", "result": "\u7cfb\u7edf\u7ea7\u5206\u6790\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u7b80\u5355\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u80fd\u8fbe\u5230\u4e0eGPU\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u80fd\u8017\u964d\u4f4e\u7ea610,000\u500d", "conclusion": "\u8be5\u5168\u6676\u4f53\u7ba1\u6982\u7387\u8ba1\u7b97\u673a\u67b6\u6784\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6982\u7387AI\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u786c\u4ef6\u5b9e\u73b0\u65b9\u6848"}}
{"id": "2510.24027", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24027", "abs": "https://arxiv.org/abs/2510.24027", "authors": ["Zibo Liu", "Zhe Jiang", "Zelin Xu", "Tingsong Xiao", "Yupu Zhang", "Zhengkun Xiao", "Haibo Wang", "Shigang Chen"], "title": "Spatio-temporal Multivariate Time Series Forecast with Chosen Variables", "comment": "In submission", "summary": "Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series\nof $n$ spatially distributed variables in a period of recent past to forecast\ntheir values in a period of near future. It has important applications in\nspatio-temporal sensing forecast such as road traffic prediction and air\npollution prediction. Recent papers have addressed a practical problem of\nmissing variables in the model input, which arises in the sensing applications\nwhere the number $m$ of sensors is far less than the number $n$ of locations to\nbe monitored, due to budget constraints. We observe that the state of the art\nassumes that the $m$ variables (i.e., locations with sensors) in the model\ninput are pre-determined and the important problem of how to choose the $m$\nvariables in the input has never been studied. This paper fills the gap by\nstudying a new problem of STMF with chosen variables, which optimally selects\n$m$-out-of-$n$ variables for the model input in order to maximize the forecast\naccuracy. We propose a unified framework that jointly performs variable\nselection and model optimization for both forecast accuracy and model\nefficiency. It consists of three novel technical components: (1) masked\nvariable-parameter pruning, which progressively prunes less informative\nvariables and attention parameters through quantile-based masking; (2)\nprioritized variable-parameter replay, which replays low-loss past samples to\npreserve learned knowledge for model stability; (3) dynamic extrapolation\nmechanism, which propagates information from variables selected for the input\nto all other variables via learnable spatial embeddings and adjacency\ninformation. Experiments on five real-world datasets show that our work\nsignificantly outperforms the state-of-the-art baselines in both accuracy and\nefficiency, demonstrating the effectiveness of joint variable selection and\nmodel optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u7a7a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u4ecen\u4e2a\u4f4d\u7f6e\u4e2d\u4f18\u5316\u9009\u62e9m\u4e2a\u53d8\u91cf\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u7cbe\u5ea6\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u7684\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u6a21\u578b\u8f93\u5165\u4e2d\u7684m\u4e2a\u53d8\u91cf\u662f\u9884\u5148\u786e\u5b9a\u7684\uff0c\u4f46\u5982\u4f55\u9009\u62e9\u8fd9m\u4e2a\u53d8\u91cf\u7684\u95ee\u9898\u4ece\u672a\u88ab\u7814\u7a76\u3002\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u4e86\u5e26\u6709\u9009\u62e9\u53d8\u91cf\u7684STMF\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u4e09\u4e2a\u6280\u672f\u7ec4\u4ef6\u7684\u7edf\u4e00\u6846\u67b6\uff1a(1) \u63a9\u7801\u53d8\u91cf\u53c2\u6570\u526a\u679d\uff0c\u901a\u8fc7\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u63a9\u7801\u9010\u6b65\u526a\u679d\u4fe1\u606f\u91cf\u8f83\u5c11\u7684\u53d8\u91cf\u548c\u6ce8\u610f\u529b\u53c2\u6570\uff1b(2) \u4f18\u5148\u53d8\u91cf\u53c2\u6570\u91cd\u653e\uff0c\u91cd\u653e\u4f4e\u635f\u5931\u7684\u8fc7\u53bb\u6837\u672c\u4ee5\u4fdd\u6301\u5b66\u4e60\u77e5\u8bc6\u7684\u6a21\u578b\u7a33\u5b9a\u6027\uff1b(3) \u52a8\u6001\u5916\u63a8\u673a\u5236\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u5d4c\u5165\u548c\u90bb\u63a5\u4fe1\u606f\u5c06\u4fe1\u606f\u4ece\u8f93\u5165\u53d8\u91cf\u4f20\u64ad\u5230\u6240\u6709\u5176\u4ed6\u53d8\u91cf\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u7684\u65b9\u6cd5\u5728\u65f6\u7a7a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6a21\u578b\u6548\u7387\u3002"}}
{"id": "2510.24044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24044", "abs": "https://arxiv.org/abs/2510.24044", "authors": ["Hui Sun", "Zheng Xie", "Hao-Yuan He", "Ming Li"], "title": "Mitigating Negative Transfer via Reducing Environmental Disagreement", "comment": "13 pages, 5 figures", "summary": "Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a\nlabeled source domain to an unlabeled target domain, addressing the challenge\nof \\emph{domain shift}. Significant domain shifts hinder effective knowledge\ntransfer, leading to \\emph{negative transfer} and deteriorating model\nperformance. Therefore, mitigating negative transfer is essential. This study\nrevisits negative transfer through the lens of causally disentangled learning,\nemphasizing cross-domain discriminative disagreement on non-causal\nenvironmental features as a critical factor. Our theoretical analysis reveals\nthat overreliance on non-causal environmental features as the environment\nevolves can cause discriminative disagreements~(termed \\emph{environmental\ndisagreement}), thereby resulting in negative transfer. To address this, we\npropose Reducing Environmental Disagreement~(RED), which disentangles each\nsample into domain-invariant causal features and domain-specific non-causal\nenvironmental features via adversarially training domain-specific environmental\nfeature extractors in the opposite domains. Subsequently, RED estimates and\nreduces environmental disagreement based on domain-specific non-causal\nenvironmental features. Experimental results confirm that RED effectively\nmitigates negative transfer and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRED\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u89e3\u8026\u5b66\u4e60\u6765\u51cf\u5c11\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u56e0\u679c\u7279\u5f81\u548c\u73af\u5883\u7279\u5f81\u6765\u964d\u4f4e\u73af\u5883\u5206\u6b67\u3002", "motivation": "\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u663e\u8457\u7684\u9886\u57df\u504f\u79fb\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff0c\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u8de8\u9886\u57df\u5728\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u4e0a\u7684\u5224\u522b\u5206\u6b67\u662f\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51faRED\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u9886\u57df\u7279\u5b9a\u7684\u73af\u5883\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5c06\u6837\u672c\u89e3\u8026\u4e3a\u9886\u57df\u4e0d\u53d8\u7684\u56e0\u679c\u7279\u5f81\u548c\u9886\u57df\u7279\u5b9a\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u4f30\u8ba1\u5e76\u51cf\u5c11\u73af\u5883\u5206\u6b67\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eRED\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u89e3\u8026\u5b66\u4e60\u51cf\u5c11\u73af\u5883\u5206\u6b67\u662f\u89e3\u51b3\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u8d1f\u8fc1\u79fb\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\uff0cRED\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.24088", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24088", "abs": "https://arxiv.org/abs/2510.24088", "authors": ["Moongyu Jeon", "Sangwoo Shin", "Dongjae Jeon", "Albert No"], "title": "Information-Theoretic Discrete Diffusion", "comment": "Accepted at NeurIPS 2025", "summary": "We present an information-theoretic framework for discrete diffusion models\nthat yields principled estimators of log-likelihood using score-matching\nlosses. Inspired by the I-MMSE identity for the Gaussian setup, we derive\nanalogous results for the discrete setting. Specifically, we introduce the\nInformation-Minimum Denoising Score Entropy (I-MDSE) relation, which links\nmutual information between data and its diffused version to the minimum\ndenoising score entropy (DSE) loss. We extend this theory to masked diffusion\nand establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)\nrelation, connecting cross-entropy losses to mutual information in discrete\nmasked processes. These results provide a time-integral decomposition of the\nlog-likelihood of the data in terms of optimal score-based losses, showing that\ncommonly used losses such as DSE and DCE are not merely variational bounds but\ntight and principled estimators of log-likelihood. The I-MDCE decomposition\nfurther enables practical extensions, including time-free formula, conditional\nlikelihood estimation in prompt-response tasks, and coupled Monte Carlo\nestimation of likelihood ratios. Experiments on synthetic and real-world data\nconfirm the accuracy, variance stability, and utility of our estimators. The\ncode is publicly available at https://github.com/Dongjae0324/infodis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u635f\u5931\u5f97\u5230\u5bf9\u6570\u4f3c\u7136\u7684\u7406\u8bba\u4f30\u8ba1\u5668\uff0c\u5efa\u7acb\u4e86\u4fe1\u606f-\u6700\u5c0f\u53bb\u566a\u5206\u6570\u71b5\u5173\u7cfb\uff0c\u5e76\u6269\u5c55\u5230\u63a9\u7801\u6269\u6563\u8fc7\u7a0b\u3002", "motivation": "\u53d7\u9ad8\u65af\u8bbe\u7f6e\u4e2dI-MMSE\u6052\u7b49\u5f0f\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5728\u79bb\u6563\u8bbe\u7f6e\u4e2d\u5efa\u7acb\u7c7b\u4f3c\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u3002", "method": "\u5f15\u5165\u4fe1\u606f-\u6700\u5c0f\u53bb\u566a\u5206\u6570\u71b5\u5173\u7cfb\uff0c\u5c06\u6570\u636e\u4e0e\u5176\u6269\u6563\u7248\u672c\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4e0e\u6700\u5c0f\u53bb\u566a\u5206\u6570\u71b5\u635f\u5931\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u6269\u5c55\u5230\u63a9\u7801\u6269\u6563\u8fc7\u7a0b\u5efa\u7acb\u4fe1\u606f-\u6700\u5c0f\u53bb\u566a\u4ea4\u53c9\u71b5\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u4f30\u8ba1\u5668\u7684\u51c6\u786e\u6027\u3001\u65b9\u5dee\u7a33\u5b9a\u6027\u548c\u5b9e\u7528\u6027\uff0c\u8868\u660e\u5e38\u7528\u635f\u5931\u51fd\u6570\u4e0d\u4ec5\u662f\u53d8\u5206\u4e0b\u754c\uff0c\u800c\u4e14\u662f\u7d27\u81f4\u7684\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u5668\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5e38\u7528\u635f\u5931\u51fd\u6570\u7684\u7406\u8bba\u5408\u7406\u6027\uff0c\u5e76\u652f\u6301\u65f6\u95f4\u65e0\u5173\u516c\u5f0f\u3001\u6761\u4ef6\u4f3c\u7136\u4f30\u8ba1\u548c\u8026\u5408\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7b49\u5b9e\u9645\u6269\u5c55\u3002"}}
{"id": "2510.24200", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.24200", "abs": "https://arxiv.org/abs/2510.24200", "authors": ["Alexander Bakarsky", "Dimitar I. Dimitrov", "Maximilian Baader", "Martin Vechev"], "title": "SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning", "comment": "Published at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Federated Learning has seen an increased deployment in real-world scenarios\nrecently, as it enables the distributed training of machine learning models\nwithout explicit data sharing between individual clients. Yet, the introduction\nof the so-called gradient inversion attacks has fundamentally challenged its\nprivacy-preserving properties. Unfortunately, as these attacks mostly rely on\ndirect data optimization without any formal guarantees, the vulnerability of\nreal-world systems remains in dispute and requires tedious testing for each new\nfederated deployment. To overcome these issues, recently the SPEAR attack was\nintroduced, which is based on a theoretical analysis of the gradients of linear\nlayers with ReLU activations. While SPEAR is an important theoretical\nbreakthrough, the attack's practicality was severely limited by its exponential\nruntime in the batch size b. In this work, we fill this gap by applying\nState-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the\nproblem of gradient inversion on linear layers with ReLU activations tractable.\nOur experiments demonstrate that our new attack, SPEAR++, retains all desirable\nproperties of SPEAR, such as robustness to DP noise and FedAvg aggregation,\nwhile being applicable to 10x bigger batch sizes.", "AI": {"tldr": "SPEAR++\u653b\u51fb\u6539\u8fdb\u4e86\u539f\u6709\u7684SPEAR\u653b\u51fb\uff0c\u901a\u8fc7\u5e94\u7528\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u6280\u672f\uff0c\u4f7f\u57fa\u4e8e\u7ebf\u6027\u5c42\u548cReLU\u6fc0\u6d3b\u51fd\u6570\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u5728\u8ba1\u7b97\u4e0a\u53d8\u5f97\u53ef\u884c\uff0c\u80fd\u591f\u5904\u740610\u500d\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9DP\u566a\u58f0\u548cFedAvg\u805a\u5408\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u800c\u4e0d\u5171\u4eab\u6570\u636e\uff0c\u4f46\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u6311\u6218\u4e86\u5176\u9690\u79c1\u4fdd\u62a4\u7279\u6027\u3002\u73b0\u6709\u7684SPEAR\u653b\u51fb\u867d\u7136\u7406\u8bba\u7a81\u7834\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5728\u6279\u6b21\u5927\u5c0f\u4e0a\u5448\u6307\u6570\u7ea7\u8fd0\u884c\u65f6\u95f4\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u5e94\u7528\u6700\u5148\u8fdb\u7684\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u6280\u672f\u6765\u89e3\u51b3\u7ebf\u6027\u5c42\u548cReLU\u6fc0\u6d3b\u51fd\u6570\u7684\u68af\u5ea6\u53cd\u6f14\u95ee\u9898\uff0c\u4f7f\u653b\u51fb\u5728\u8ba1\u7b97\u4e0a\u53ef\u884c\u3002", "result": "SPEAR++\u653b\u51fb\u4fdd\u6301\u4e86SPEAR\u7684\u6240\u6709\u7406\u60f3\u7279\u6027\uff08\u5982\u5bf9DP\u566a\u58f0\u548cFedAvg\u805a\u5408\u7684\u9c81\u68d2\u6027\uff09\uff0c\u540c\u65f6\u80fd\u591f\u5904\u740610\u500d\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\u3002", "conclusion": "SPEAR++\u901a\u8fc7\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u7684\u5b9e\u7528\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24217", "abs": "https://arxiv.org/abs/2510.24217", "authors": ["Alisher Turubayev", "Anna Shopova", "Fabian Lange", "Mahmut Kamalak", "Paul Mattes", "Victoria Ayvasky", "Bert Arnrich", "Bjarne Pfitzner", "Robin P. van de Water"], "title": "Closing Gaps: An Imputation Analysis of ICU Vital Signs", "comment": "Preprint", "summary": "As more Intensive Care Unit (ICU) data becomes available, the interest in\ndeveloping clinical prediction models to improve healthcare protocols\nincreases. However, the lack of data quality still hinders clinical prediction\nusing Machine Learning (ML). Many vital sign measurements, such as heart rate,\ncontain sizeable missing segments, leaving gaps in the data that could\nnegatively impact prediction performance. Previous works have introduced\nnumerous time-series imputation techniques. Nevertheless, more comprehensive\nwork is needed to compare a representative set of methods for imputing ICU\nvital signs and determine the best practice. In reality, ad-hoc imputation\ntechniques that could decrease prediction accuracy, like zero imputation, are\nstill used. In this work, we compare established imputation techniques to guide\nresearchers in improving the performance of clinical prediction models by\nselecting the most accurate imputation technique. We introduce an extensible\nand reusable benchmark with currently 15 imputation and 4 amputation methods,\ncreated for benchmarking on major ICU datasets. We hope to provide a\ncomparative basis and facilitate further ML development to bring more models\ninto clinical practice.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86ICU\u751f\u547d\u4f53\u5f81\u6570\u636e\u7684\u591a\u79cd\u63d2\u8865\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u9009\u62e9\u6700\u51c6\u786e\u7684\u63d2\u8865\u6280\u672f\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b15\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u622a\u65ad\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u57fa\u51c6\u3002", "motivation": "ICU\u6570\u636e\u4e2d\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u5b58\u5728\u5927\u91cf\u7f3a\u5931\u6bb5\uff0c\u8fd9\u4f1a\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6027\u80fd\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9ICU\u751f\u547d\u4f53\u5f81\u63d2\u8865\u65b9\u6cd5\u7684\u5168\u9762\u6bd4\u8f83\uff0c\u4e14\u5b9e\u9645\u4e2d\u4ecd\u5728\u4f7f\u7528\u53ef\u80fd\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u6027\u7684\u4e34\u65f6\u63d2\u8865\u6280\u672f\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u7528\u7684\u57fa\u51c6\uff0c\u5305\u542b15\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u622a\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e3b\u8981ICU\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u63d2\u8865\u6280\u672f\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u51c6\u786e\u63d2\u8865\u65b9\u6cd5\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aICU\u751f\u547d\u4f53\u5f81\u6570\u636e\u63d2\u8865\u63d0\u4f9b\u4e86\u6bd4\u8f83\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u66f4\u591a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u5165\u4e34\u5e8a\u5b9e\u8df5\u3002"}}
{"id": "2510.24234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24234", "abs": "https://arxiv.org/abs/2510.24234", "authors": ["Ludovic Schwartz", "Hamish Flynn", "Gergely Neu"], "title": "Sparse Optimistic Information Directed Sampling", "comment": null, "summary": "Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial depen- dence on the ambient dimension is unavoidable, or the\ndata-poor regime, where dimension-independence is possible at the cost of worse\ndependence on the num- ber of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, pro- viding the first\nalgorithm that simultaneously achieves optimal worst-case regret in both the\ndata-rich and data-poor regimes. We empirically demonstrate the good\nperformance of SOIDS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7a00\u758f\u4e50\u89c2\u4fe1\u606f\u5bfc\u5411\u91c7\u6837\uff08SOIDS\uff09\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u532e\u4e4f\u4e24\u79cd\u60c5\u51b5\u4e0b\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6700\u574f\u60c5\u51b5\u9057\u61be\uff0c\u65e0\u9700\u8d1d\u53f6\u65af\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u7ebf\u6027\u8d4c\u535a\u673a\u7b97\u6cd5\u8981\u4e48\u5728\u6570\u636e\u4e30\u5bcc\u65f6\u8fbe\u5230\u6700\u4f18\u6700\u574f\u60c5\u51b5\u9057\u61be\u4f46\u4f9d\u8d56\u7ef4\u5ea6\u591a\u9879\u5f0f\uff0c\u8981\u4e48\u5728\u6570\u636e\u532e\u4e4f\u65f6\u5b9e\u73b0\u7ef4\u5ea6\u65e0\u5173\u4f46\u56de\u5408\u6570\u4f9d\u8d56\u8f83\u5dee\u3002\u7a00\u758fIDS\u7b97\u6cd5\u867d\u7136\u80fd\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u540c\u65f6\u8fbe\u5230\u6700\u4f18\u8d1d\u53f6\u65af\u9057\u61be\uff0c\u4f46\u9700\u8981\u8d1d\u53f6\u65af\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u4e50\u89c2\u4fe1\u606f\u5bfc\u5411\u91c7\u6837\uff08SOIDS\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4f9d\u8d56\u5b66\u4e60\u7387\u7684\u65b0\u9896\u5206\u6790\uff0c\u80fd\u591f\u5e73\u8861\u4fe1\u606f\u548c\u9057\u61be\u3002", "result": "SOIDS\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u6269\u5c55\u4e86IDS\u7684\u4fdd\u8bc1\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u532e\u4e4f\u4e24\u79cd\u60c5\u51b5\u4e0b\u540c\u65f6\u8fbe\u5230\u6700\u4f18\u6700\u574f\u60c5\u51b5\u9057\u61be\u7684\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u826f\u597d\u6027\u80fd\u3002", "conclusion": "SOIDS\u7b97\u6cd5\u80fd\u591f\u5728\u65e0\u9700\u8d1d\u53f6\u65af\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u7a00\u758f\u7ebf\u6027\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\u540c\u65f6\u9002\u5e94\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u532e\u4e4f\u4e24\u79cd\u60c5\u51b5\uff0c\u5b9e\u73b0\u6700\u4f18\u6700\u574f\u60c5\u51b5\u9057\u61be\u3002"}}
{"id": "2510.24273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24273", "abs": "https://arxiv.org/abs/2510.24273", "authors": ["Junlin Mu", "Hantao Huang", "Jihang Zhang", "Minghui Yu", "Tao Wang", "Yidong Li"], "title": "SALS: Sparse Attention in Latent Space for KV cache Compression", "comment": null, "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.", "AI": {"tldr": "SALS\u662f\u4e00\u4e2a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u5c06KV\u7f13\u5b58\u538b\u7f29\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5728\u8be5\u7a7a\u95f4\u4e2d\u4f7f\u7528\u65e0RoPE\u7684\u67e5\u8be2-\u952e\u4ea4\u4e92\u8fdb\u884c\u7a00\u758ftoken\u9009\u62e9\uff0c\u4ece\u800c\u907f\u514d\u5b8c\u6574KV\u7f13\u5b58\u91cd\u5efa\u7684\u5f00\u9500\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u9762\u4e34KV\u7f13\u5b58\u5927\u5c0f\u548c\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\u7684\u6311\u6218\uff0c\u73b0\u6709\u4f4e\u79e9\u538b\u7f29\u65b9\u6cd5\u7531\u4e8eRoPE\u673a\u5236\u5bfc\u81f4\u7cbe\u5ea6\u4e25\u91cd\u4e0b\u964d\u6216\u4ea7\u751f\u65b0\u7684\u901f\u5ea6\u74f6\u9888\u3002", "method": "\u63d0\u51faSALS\u6846\u67b6\uff1a1\uff09\u5c06KV\u7f13\u5b58\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u65e0RoPE\u7684\u67e5\u8be2-\u952e\u4ea4\u4e92\u8fdb\u884c\u7a00\u758ftoken\u9009\u62e9\uff1b3\uff09\u4ec5\u91cd\u5efa\u91cd\u8981token\u5b50\u96c6\u3002", "result": "\u5728LLaMA2-7b-chat\u548cMistral-7b\u7b49\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cSALS\u5b9e\u73b0\u4e866.4\u500dKV\u7f13\u5b58\u538b\u7f29\u548c5.7\u500d\u6ce8\u610f\u529b\u7b97\u5b50\u52a0\u901f\uff0c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u76f8\u6bd4GPT-fast\u57284k\u548c32K\u5e8f\u5217\u4e0a\u5206\u522b\u63d0\u53471.4\u500d\u548c4.5\u500d\u3002", "conclusion": "SALS\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7a00\u758f\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u5ea6\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2510.24310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24310", "abs": "https://arxiv.org/abs/2510.24310", "authors": ["Guus Toussaint", "Arno Knobbe"], "title": "EDC: Equation Discovery for Classification", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Lecture Notes in Computer Science, and is available online at\n  https://doi.org/10.1007/978-3-032-05461-6_9", "summary": "Equation Discovery techniques have shown considerable success in regression\ntasks, where they are used to discover concise and interpretable models\n(\\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary\nclassification framework. Our proposed method EDC finds analytical functions of\nmanageable size that specify the location and shape of the decision boundary.\nIn extensive experiments on artificial and real-life data, we demonstrate how\nEDC is able to discover both the structure of the target equation as well as\nthe value of its parameters, outperforming the current state-of-the-art\nED-based classification methods in binary classification and achieving\nperformance comparable to the state of the art in binary classification. We\nsuggest a grammar of modest complexity that appears to work well on the tested\ndatasets but argue that the exact grammar -- and thus the complexity of the\nmodels -- is configurable, and especially domain-specific expressions can be\nincluded in the pattern language, where that is required. The presented grammar\nconsists of a series of summands (additive terms) that include linear,\nquadratic and exponential terms, as well as products of two features (producing\nhyperbolic curves ideal for capturing XOR-like dependencies). The experiments\ndemonstrate that this grammar allows fairly flexible decision boundaries while\nnot so rich to cause overfitting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u7a0b\u53d1\u73b0(ED)\u7684\u4e8c\u5143\u5206\u7c7b\u6846\u67b6EDC\uff0c\u80fd\u591f\u53d1\u73b0\u6307\u5b9a\u51b3\u7b56\u8fb9\u754c\u4f4d\u7f6e\u548c\u5f62\u72b6\u7684\u89e3\u6790\u51fd\u6570\uff0c\u5728\u4eba\u5de5\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709ED\u5206\u7c7b\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u65b9\u7a0b\u53d1\u73b0\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u5df2\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u4f46\u9700\u8981\u5c06\u5176\u6269\u5c55\u5230\u5206\u7c7b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u53d1\u73b0\u80fd\u591f\u660e\u786e\u63cf\u8ff0\u51b3\u7b56\u8fb9\u754c\u7684\u89e3\u6790\u51fd\u6570\u3002", "method": "\u4f7f\u7528\u9002\u5ea6\u590d\u6742\u5ea6\u7684\u8bed\u6cd5\u6784\u5efa\u6a21\u578b\uff0c\u5305\u62ec\u7ebf\u6027\u9879\u3001\u4e8c\u6b21\u9879\u3001\u6307\u6570\u9879\u4ee5\u53ca\u7279\u5f81\u4e58\u79ef\u9879\uff08\u7528\u4e8e\u6355\u6349XOR\u7c7b\u4f9d\u8d56\u5173\u7cfb\uff09\uff0c\u901a\u8fc7EDC\u6846\u67b6\u53d1\u73b0\u51b3\u7b56\u8fb9\u754c\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u5728\u4eba\u5de5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEDC\u80fd\u591f\u540c\u65f6\u53d1\u73b0\u76ee\u6807\u65b9\u7a0b\u7684\u7ed3\u6784\u548c\u53c2\u6570\u503c\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709ED\u5206\u7c7b\u65b9\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u7075\u6d3b\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u8fc7\u62df\u5408\uff0c\u4e14\u8bed\u6cd5\u590d\u6742\u5ea6\u53ef\u914d\u7f6e\uff0c\u5141\u8bb8\u5305\u542b\u9886\u57df\u7279\u5b9a\u8868\u8fbe\u5f0f\uff0c\u4e3a\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89e3\u6790\u6a21\u578b\u3002"}}
{"id": "2510.24331", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24331", "abs": "https://arxiv.org/abs/2510.24331", "authors": ["Gabriel O. dos Santos", "Esther Colombini", "Sandra Avila"], "title": "What do vision-language models see in the context? Investigating multimodal in-context learning", "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\uff0c\u5f53\u524dVLMs\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u7ebf\u7d22\u800c\u672a\u80fd\u6709\u6548\u6574\u5408\u89c6\u89c9\u4fe1\u606f\uff0c\u5b58\u5728\u591a\u6a21\u6001\u878d\u5408\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u6790VLMs\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e86\u6db5\u76d6\u56db\u79cd\u67b6\u6784\u7684\u4e03\u4e2aVLMs\u6a21\u578b\uff0c\u5728\u4e09\u4e2a\u56fe\u50cf\u63cf\u8ff0\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u5206\u6790\u4e86\u63d0\u793a\u8bbe\u8ba1\u3001\u67b6\u6784\u9009\u62e9\u548c\u8bad\u7ec3\u7b56\u7565\u5bf9\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u5206\u6790\u4e86\u6ce8\u610f\u529b\u6a21\u5f0f\u968f\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\u7684\u53d8\u5316\u3002", "result": "\u8bad\u7ec3\u4e8e\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u6570\u636e\u80fd\u63d0\u5347\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff0c\u4f46\u4e0d\u610f\u5473\u7740\u80fd\u6709\u6548\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002\u6307\u4ee4\u8c03\u4f18\u6539\u5584\u4e86\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u51cf\u5c11\u5bf9\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684\u4f9d\u8d56\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u5f53\u524dVLMs\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u7ebf\u7d22\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u4e0a\u5b58\u5728\u5173\u952e\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u6574\u5408\u65b9\u9762\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u589e\u5f3aVLMs\u4ece\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.24368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24368", "abs": "https://arxiv.org/abs/2510.24368", "authors": ["Maria Gabriela Valeriano", "David Kohan Marzag\u00e3o", "Alfredo Montelongo", "Carlos Roberto Veiga Kiffer", "Natan Katz", "Ana Carolina Lorena"], "title": "Filtering instances and rejecting predictions to obtain reliable models in healthcare", "comment": "This paper is under review at Machine Learning (Springer)", "summary": "Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u6b65\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u548c\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u6765\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u3002\u7b2c\u4e00\u6b65\u4f7f\u7528\u5b9e\u4f8b\u96be\u5ea6\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\uff0c\u7b2c\u4e8c\u6b65\u5728\u63a8\u7406\u65f6\u5f15\u5165\u7f6e\u4fe1\u5ea6\u62d2\u7edd\u673a\u5236\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u4fdd\u5065\uff09\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u8003\u8651\u4e0d\u786e\u5b9a\u6027\uff0c\u5373\u4f7f\u5728\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u4e5f\u63d0\u4f9b\u9884\u6d4b\uff0c\u5f71\u54cd\u9884\u6d4b\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a1\uff09\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5b9e\u4f8b\u96be\u5ea6\uff08IH\uff09\u8fc7\u6ee4\u95ee\u9898\u5b9e\u4f8b\u6765\u6539\u8fdb\u6570\u636e\u96c6\uff1b2\uff09\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u62d2\u7edd\u673a\u5236\uff0c\u53ea\u4fdd\u7559\u53ef\u9760\u9884\u6d4b\u3002\u4f7f\u7528\u4e09\u4e2a\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5c06IH\u8fc7\u6ee4\u4e0e\u7f6e\u4fe1\u5ea6\u62d2\u7edd\u76f8\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u5927\u90e8\u5206\u5b9e\u4f8b\u3002\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u62d2\u7edd\u7387\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24432", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24432", "abs": "https://arxiv.org/abs/2510.24432", "authors": ["Seyed Mahdi Basiri Azad", "Joschka Boedecker"], "title": "Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings", "comment": null, "summary": "Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u5c11\u91cf\u6210\u529f\u6f14\u793a\u521d\u59cb\u5316\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4ef7\u503c\u51fd\u6570\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u63d0\u4f9b\u6709\u7528\u7684\u52a8\u4f5c\u5148\u9a8c\u6765\u52a0\u901f\u6536\u655b", "motivation": "\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u7531\u4e8e\u7f3a\u4e4f\u4fe1\u606f\u53cd\u9988\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u51cf\u5c11\u63a2\u7d22\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6837\u672c\u6548\u7387", "method": "\u4f7f\u7528\u5c11\u91cf\u6210\u529f\u6f14\u793a\u9884\u8ba1\u7b97\u4ef7\u503c\u4f30\u8ba1\u4f5c\u4e3a\u65e9\u671f\u5b66\u4e60\u76ee\u6807\uff0c\u91c7\u7528\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u6df7\u5408\u8303\u5f0f\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u6807\u51c6\u5728\u7ebf\u4ea4\u4e92\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u4f30\u8ba1", "result": "\u5728\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u52a0\u901f\u4e86\u6536\u655b\u5e76\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u5c11\u6216\u6b21\u4f18\u7684\u6f14\u793a\u6570\u636e\u4e5f\u80fd\u53d6\u5f97\u826f\u597d\u6548\u679c", "conclusion": "\u901a\u8fc7\u6f14\u793a\u521d\u59cb\u5316\u7684\u6df7\u5408\u79bb\u7ebf-\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387"}}
{"id": "2510.24473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24473", "abs": "https://arxiv.org/abs/2510.24473", "authors": ["Lucas Buk Cardoso", "Simone Aldrey Angelo", "Yasmin Pacheco Gil Bonilha", "Fernando Maia", "Adeylson Guimar\u00e3es Ribeiro", "Maria Paula Curado", "Gisele Aparecida Fernandes", "Vanderlei Cunha Parro", "Fl\u00e1vio Almeida de Magalh\u00e3es Cipparrone", "Alexandre Dias Porto Chiavegatto Filho", "Tatiana Natasha Toporcov"], "title": "Methodology for Comparing Machine Learning Algorithms for Survival Analysis", "comment": null, "summary": "This study presents a comparative methodological analysis of six machine\nlearning models for survival analysis (MLSA). Using data from nearly 45,000\ncolorectal cancer patients in the Hospital-Based Cancer Registries of S\\~ao\nPaulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for\nSurvival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),\nXGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival\nconsidering censored data. Hyperparameter optimization was performed with\ndifferent samplers, and model performance was assessed using the Concordance\nIndex (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score\n(IBS). Survival curves produced by the models were compared with predictions\nfrom classification algorithms, and predictor interpretation was conducted\nusing SHAP and permutation importance. XGB-AFT achieved the best performance\n(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results\nhighlight the potential and applicability of MLSA to improve survival\nprediction and support decision making.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u673a\u5668\u5b66\u4e60\u751f\u5b58\u5206\u6790\u6a21\u578b\u5728\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0XGBoost-AFT\u6a21\u578b\u6027\u80fd\u6700\u4f73\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u751f\u5b58\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u751f\u5b58\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5220\u5931\u6570\u636e\u65f6\u7684\u80fd\u529b\uff0c\u4ee5\u6539\u8fdb\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u7684\u751f\u5b58\u9884\u6d4b\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002", "method": "\u4f7f\u7528\u5df4\u897f\u5723\u4fdd\u7f57\u533b\u9662\u764c\u75c7\u767b\u8bb0\u5904\u7684\u8fd145,000\u540d\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u673a\u5668\u5b66\u4e60\u751f\u5b58\u5206\u6790\u6a21\u578b\uff08RSF\u3001GBSA\u3001SSVM\u3001XGB-Cox\u3001XGB-AFT\u3001LGBM\uff09\uff0c\u91c7\u7528\u4e0d\u540c\u91c7\u6837\u5668\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u4f7f\u7528C-Index\u3001C-Index IPCW\u3001\u65f6\u95f4\u76f8\u5173AUC\u548cIBS\u7b49\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "XGB-AFT\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08C-Index = 0.7618; IPCW = 0.7532\uff09\uff0c\u5176\u6b21\u662fGBSA\u548cRSF\u6a21\u578b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u751f\u5b58\u5206\u6790\u6a21\u578b\u5728\u6539\u5584\u751f\u5b58\u9884\u6d4b\u548c\u652f\u6301\u51b3\u7b56\u5236\u5b9a\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24500", "abs": "https://arxiv.org/abs/2510.24500", "authors": ["Yong Huang", "Zhongqi Yang", "Amir Rahmani"], "title": "MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU", "comment": null, "summary": "Sepsis is a leading cause of mortality in intensive care units (ICUs), yet\nexisting research often relies on outdated datasets, non-reproducible\npreprocessing pipelines, and limited coverage of clinical interventions. We\nintroduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from\nthe MIMIC-IV database, designed to support reproducible modeling of sepsis\ntrajectories. Our cohort includes 35,239 ICU patients with time-aligned\nclinical variables and standardized treatment data, including vasopressors,\nfluids, mechanical ventilation and antibiotics. We describe a transparent\npreprocessing pipeline-based on Sepsis-3 criteria, structured imputation\nstrategies, and treatment inclusion-and release it alongside benchmark tasks\nfocused on early mortality prediction, length-of-stay estimation, and shock\nonset classification. Empirical results demonstrate that incorporating\ntreatment variables substantially improves model performance, particularly for\nTransformer-based architectures. MIMIC-Sepsis serves as a robust platform for\nevaluating predictive and sequential models in critical care research.", "AI": {"tldr": "MIMIC-Sepsis\u662f\u4e00\u4e2a\u4eceMIMIC-IV\u6570\u636e\u5e93\u884d\u751f\u7684\u8113\u6bd2\u75c7\u60a3\u8005\u961f\u5217\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u8113\u6bd2\u75c7\u75c5\u7a0b\u7684\u53ef\u91cd\u590d\u5efa\u6a21\uff0c\u5305\u542b35,239\u540dICU\u60a3\u8005\u7684\u65f6\u95f4\u5bf9\u9f50\u4e34\u5e8a\u53d8\u91cf\u548c\u6807\u51c6\u5316\u6cbb\u7597\u6570\u636e\u3002", "motivation": "\u8113\u6bd2\u75c7\u662fICU\u4e2d\u6b7b\u4ea1\u7387\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5e38\u4f9d\u8d56\u8fc7\u65f6\u6570\u636e\u96c6\u3001\u4e0d\u53ef\u590d\u73b0\u7684\u9884\u5904\u7406\u6d41\u7a0b\u548c\u6709\u9650\u7684\u4e34\u5e8a\u5e72\u9884\u8986\u76d6\u3002", "method": "\u57fa\u4e8eSepsis-3\u6807\u51c6\u6784\u5efa\u900f\u660e\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u63d2\u8865\u7b56\u7565\u548c\u6cbb\u7597\u7eb3\u5165\uff0c\u5e76\u53d1\u5e03\u4e13\u6ce8\u4e8e\u65e9\u671f\u6b7b\u4ea1\u7387\u9884\u6d4b\u3001\u4f4f\u9662\u65f6\u95f4\u4f30\u8ba1\u548c\u4f11\u514b\u53d1\u4f5c\u5206\u7c7b\u7684\u57fa\u51c6\u4efb\u52a1\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u7eb3\u5165\u6cbb\u7597\u53d8\u91cf\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u3002", "conclusion": "MIMIC-Sepsis\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u91cd\u75c7\u76d1\u62a4\u7814\u7a76\u4e2d\u9884\u6d4b\u548c\u5e8f\u5217\u6a21\u578b\u7684\u7a33\u5065\u5e73\u53f0\u3002"}}
{"id": "2510.24574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24574", "abs": "https://arxiv.org/abs/2510.24574", "authors": ["Hao Wang", "Licheng Pan", "Yuan Lu", "Zhixuan Chu", "Xiaoxi Li", "Shuting He", "Zhichao Chen", "Haoxuan Li", "Qingsong Wen", "Zhouchen Lin"], "title": "DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment", "comment": null, "summary": "Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.", "AI": {"tldr": "DistDF\u662f\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6761\u4ef6\u9884\u6d4b\u5206\u5e03\u4e0e\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u89e3\u51b3\u4f20\u7edf\u76f4\u63a5\u9884\u6d4b\u65b9\u6cd5\u5728\u5b58\u5728\u6807\u7b7e\u81ea\u76f8\u5173\u65f6\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76f4\u63a5\u9884\u6d4b\u65b9\u6cd5\u5728\u5b58\u5728\u6807\u7b7e\u81ea\u76f8\u5173\u65f6\uff0c\u6761\u4ef6\u8d1f\u5bf9\u6570\u4f3c\u7136\u7684\u4f30\u8ba1\u5b58\u5728\u504f\u5dee\uff0c\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faDistDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6761\u4ef6\u9884\u6d4b\u5206\u5e03\u4e0e\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5b9e\u73b0\u5bf9\u9f50\u3002\u5f15\u5165\u8054\u5408\u5206\u5e03Wasserstein\u5dee\u5f02\u6765\u4e0a\u754c\u6761\u4ef6\u5dee\u5f02\uff0c\u8be5\u5dee\u5f02\u53ef\u4ece\u7ecf\u9a8c\u6837\u672c\u4e2d\u8fdb\u884c\u53ef\u5fae\u4f30\u8ba1\uff0c\u5e76\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDistDF\u63d0\u5347\u4e86\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "DistDF\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u5728\u6807\u7b7e\u81ea\u76f8\u5173\u4e0b\u7684\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24639", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24639", "abs": "https://arxiv.org/abs/2510.24639", "authors": ["Pedro P. Sanchez", "Damian Machlanski", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "Causal Ordering for Structure Learning From Time Series", "comment": "32 pages", "summary": "Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.", "AI": {"tldr": "DOTS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65f6\u5e8f\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u6709\u6548\u56e0\u679c\u6392\u5e8f\u800c\u975e\u5355\u4e00\u6392\u5e8f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6392\u5e8f\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u5e8f\u6570\u636e\u4e2d\u7684\u56e0\u679c\u53d1\u73b0\u5bf9\u4e8e\u7406\u89e3\u751f\u7406\u5b66\u3001\u8111\u8fde\u63a5\u3001\u6c14\u5019\u52a8\u529b\u5b66\u548c\u793e\u4f1a\u7ecf\u6d4e\u884c\u4e3a\u7b49\u590d\u6742\u73b0\u8c61\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6392\u5e8f\u65b9\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faDOTS\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u65f6\u5e8f\u56e0\u679c\u53d1\u73b0\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u6709\u6548\u56e0\u679c\u6392\u5e8f\u6765\u6062\u590d\u5e95\u5c42\u6709\u5411\u65e0\u73af\u56fe\u7684\u4f20\u9012\u95ed\u5305\uff0c\u5e76\u57fa\u4e8e\u5e73\u7a33\u6027\u548c\u52a0\u6027\u566a\u58f0\u6a21\u578b\u5047\u8bbe\uff0c\u4f7f\u7528\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u5206\u6570\u5339\u914d\u4ee5\u5b9e\u73b0\u9ad8\u6548Hessian\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDOTS\u5c06\u5e73\u5747\u7a97\u53e3\u56feF1\u4ece0.63\u63d0\u5347\u52300.81\uff1b\u5728CausalTime\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDOTS\u83b7\u5f97\u6700\u9ad8\u7684\u5e73\u5747\u6458\u8981\u56feF1\uff0c\u540c\u65f6\u5c06\u8fd0\u884c\u65f6\u95f4\u76f8\u5bf9\u4e8e\u56fe\u4f18\u5316\u65b9\u6cd5\u51cf\u534a\u3002", "conclusion": "DOTS\u4e3a\u65f6\u5e8f\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u6392\u5e8f\u96c6\u6210\u6709\u6548\u7f13\u89e3\u4e86\u5355\u6392\u5e8f\u65b9\u6cd5\u4e2d\u7684\u4f2a\u5f71\u95ee\u9898\u3002"}}
{"id": "2510.24633", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.24633", "abs": "https://arxiv.org/abs/2510.24633", "authors": ["Mingyue Liu", "Andrew Cropper"], "title": "Symbolic Snapshot Ensembles", "comment": null, "summary": "Inductive logic programming (ILP) is a form of logical machine learning. Most\nILP algorithms learn a single hypothesis from a single training run. Ensemble\nmethods train an ILP algorithm multiple times to learn multiple hypotheses. In\nthis paper, we train an ILP algorithm only once and save intermediate\nhypotheses. We then combine the hypotheses using a minimum description length\nweighting scheme. Our experiments on multiple benchmarks, including game\nplaying and visual reasoning, show that our approach improves predictive\naccuracy by 4% with less than 1% computational overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u4fdd\u5b58\u4e2d\u95f4\u5047\u8bbe\uff0c\u5e76\u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u52a0\u6743\u65b9\u6848\u7ec4\u5408\u8fd9\u4e9b\u5047\u8bbe\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e864%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8ba1\u7b97\u5f00\u9500\u4e0d\u52301%\u3002", "motivation": "\u4f20\u7edf\u7684\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\u901a\u5e38\u4ece\u5355\u6b21\u8bad\u7ec3\u4e2d\u5b66\u4e60\u5355\u4e2a\u5047\u8bbe\uff0c\u800c\u96c6\u6210\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u8bad\u7ec3\u6765\u5b66\u4e60\u591a\u4e2a\u5047\u8bbe\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u591a\u4e2a\u5047\u8bbe\u3002", "method": "\u8bad\u7ec3ILP\u7b97\u6cd5\u4ec5\u4e00\u6b21\uff0c\u4fdd\u5b58\u4e2d\u95f4\u751f\u6210\u7684\u5047\u8bbe\uff0c\u7136\u540e\u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u52a0\u6743\u65b9\u6848\u6765\u7ec4\u5408\u8fd9\u4e9b\u5047\u8bbe\u3002", "result": "\u5728\u5305\u62ec\u6e38\u620f\u73a9\u6cd5\u548c\u89c6\u89c9\u63a8\u7406\u5728\u5185\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e864%\uff0c\u800c\u8ba1\u7b97\u5f00\u9500\u4e0d\u52301%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684ILP\u96c6\u6210\u5b66\u4e60\u65b9\u5f0f\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9700\u8981\u591a\u6b21\u8bad\u7ec3\u7684\u96c6\u6210\u65b9\u6cd5\u3002"}}

<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 该论文通过实验发现，2012-2023年间AI训练效率提升的22,000倍增长中，大部分来源于算法的规模依赖性效率改进，特别是LSTM到Transformer的转变，而非传统认为的小规模算法创新。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为AI训练效率的大幅提升主要来自算法创新，但作者发现现有文献对小规模算法创新的贡献估计过高，存在显著的效率差距需要解释。

Method: 通过小规模消融实验分析关键创新贡献，进行扩展实验比较LSTM和Transformer在不同规模下的效率，使用实验外推和文献估计来量化效率增益。

Result: 研究发现规模依赖性算法效率改进解释了大部分效率增益，LSTM到Transformer的转变贡献了6,930倍效率提升中的主要部分，而小规模算法创新仅贡献不到100倍。

Conclusion: 算法效率的衡量具有强烈的参考依赖性，小规模模型的算法进展远慢于预期，规模依赖性效率改进是AI训练效率提升的主要驱动力。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [2] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 通过降维技术提取、处理和可视化Transformer语言模型的潜在状态几何结构，揭示了注意力与MLP组件在中间层的分离模式，以及位置嵌入的高维螺旋结构等几何特征。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言任务中表现出色，但其内部机制仍难以解释。本研究旨在通过几何分析来理解Transformer模型的内部工作方式。

Method: 使用主成分分析（PCA）和均匀流形近似（UMAP）等降维技术，捕获Transformer块内多个点的层间激活，对GPT-2和LLaMa模型进行系统分析。

Result: 发现了中间层注意力与MLP组件输出的明显分离模式，识别了初始序列位置潜在状态的高范数特征，可视化了位置嵌入的高维螺旋结构以及序列级几何模式。

Conclusion: 该方法支持对Transformer内部机制的系统分析，为可复现的可解释性研究提供了工具，相关代码已开源。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [3] [SIR Analysis for Affine Filter Bank Modulation](https://arxiv.org/abs/2511.21615)
*Henrique L. Senger,Gustavo P. Gonçalves,Bruno S. Chang,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Didier Le Ruyet*

Main category: eess.SP

TL;DR: 分析了AFBM波形在MMSE均衡下的SIR性能，发现在滤波时域中会出现信道干扰与正交性近似误差的有趣抵消现象，这解释了滤波时域检测方案相比仿射域等效方案性能显著提升的原因。


<details>
  <summary>Details</summary>
Motivation: 研究AFBM波形在不同域中的SIR性能差异，特别是理解滤波时域检测方案为何比仿射域等效方案具有更好的性能表现。

Method: 在MMSE均衡条件下，分别在仿射域和滤波时域分析AFBM波形的SIR性能，通过离散仿射傅里叶变换和去扩展/映射处理来研究干扰抵消现象。

Result: 发现滤波时域中存在信道诱导干扰与正交性近似误差的有趣抵消现象，这种现象在仿射域中不会发生，直接影响了误码率性能。

Conclusion: 滤波时域检测方案相比仿射域等效方案具有显著性能优势，这归因于滤波时域中独特的干扰抵消机制，该分析为AFBM波形的性能优化提供了理论依据。

Abstract: The signal-to-interference ratio (SIR) of the Affine Filter Bank Modulation (AFBM) waveform is analyzed under minimum mean square error (MMSE) equalization in two domains; namely, the affine domain and the filtered time-domain (TD). Due to the incorporation of the discrete affine Fourier transform (DAFT) and despreading/mapping, an interesting and counter-intuitive cancellation of the unwanted combination of the channel induced interference with the orthogonality approximation error is seen in the filtered TD, a process which does not occur in the affine domain. The direct impact on bit error rate (BER) provides a thorough validation of the proposed analysis and explains the substantial gains in performance of the filtered TD detection scheme as opposed to its affine domain equivalent

</details>


### [4] [Optimal Bit Detection in Thermal Noise Communication Systems Under Rician Fading](https://arxiv.org/abs/2511.21649)
*Mohamed El Jbari,Fernando D. A. García,Hugerles S. Silva,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 本文提出了在Rician衰落信道下热噪声通信系统的最优比特检测分析框架，使用卡方统计推导最大似然检测阈值和误码率表达式，相比传统高斯近似方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有热噪声通信分析多依赖高斯近似且忽略衰落效应，限制了分析精度。需要建立准确的分析框架来支持B5G/6G和物联网系统的能量高效接收机设计。

Method: 基于卡方统计推导最大似然检测阈值，通过Gauss-Laguerre数值积分计算误码率，消除近似误差并准确表征有限样本量下的性能。

Result: 蒙特卡洛仿真验证了分析结果的准确性，相比次优的高斯基检测方法，误码率有显著改善。量化了样本量、电阻比和Rician K因子等关键参数的影响。

Conclusion: 所提框架为未来B5G/6G和大规模物联网系统中设计能量高效的热噪声通信接收机提供了坚实基础。

Abstract: Thermal noise communication (TNC) enables ultra-low-power wireless links for Internet of Things (IoT) devices by modulating the variance of thermal noise, rather than using active carriers. Existing analyses often rely on Gaussian approximations and overlook fading effects, which limits their accuracy. This paper presents an accurate analytical framework for optimal bit detection in TNC systems under Rician fading. Using chi-squared statistics, we derive the optimal maximum-likelihood detection threshold and an expression for the bit error probability (BEP) via Gauss-Laguerre quadrature. The proposed model eliminates approximation errors and accurately characterizes performance for finite sample sizes. Monte Carlo simulations confirm the analytical results and demonstrate significant improvements in BEP compared with suboptimal Gaussian-based detection. Furthermore, the influence of key parameters, sample size, resistance ratio, and Rician K-factor, is quantified. The proposed framework provides a solid foundation for designing energy-efficient TNC receivers in future B5G/6G and large-scale IoT systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在8拼图任务中的规划和状态推理能力，发现即使有反馈机制和外部验证器辅助，模型仍存在内部状态表示脆弱和启发式规划能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在许多基准测试中表现出色，但其规划和状态推理能力尚不清楚。作者希望通过8拼图这一经典任务来直接评估这些能力，该任务需要状态跟踪和目标导向的规划。

Method: 测试了四种模型在常见提示条件下（零样本、思维链、算法思维）以及分层纠正反馈下的表现。还使用外部移动验证器仅提供有效移动来检查模型。

Result: 反馈机制对某些模型-提示组合提高了成功率，但许多成功运行时间长、计算成本高且间接。即使有外部移动验证器辅助，所有模型都无法解决任何拼图。定性分析揭示了两个主要缺陷：脆弱的内部状态表示和弱启发式规划能力。

Conclusion: 在没有外部工具（如代码解释器）的情况下，当前大型语言模型在规划方面存在重大限制，进一步进展可能需要维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>

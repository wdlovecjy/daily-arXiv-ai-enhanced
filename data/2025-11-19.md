<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 7]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.SP](#eess.SP) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: SpatiaLite是一个合成基准测试，用于评估视觉语言模型的空间推理能力，发现当前先进VLMs主要依赖语言表示，在视觉中心任务上存在显著缺陷，且推理效率低下。


<details>
  <summary>Details</summary>
Motivation: 当前先进视觉语言模型在空间推理（如心理旋转、导航和空间关系理解）方面存在显著挑战，作者假设想象力（空间状态的内部模拟）是空间世界模型中的主导推理机制。

Method: 引入SpatiaLite合成基准测试，联合测量空间推理准确性和效率；提出图像驱动框架（IDF）进行数据合成和训练，隐式构建内部世界模型。

Result: 实验发现：1）先进VLMs主要依赖语言表示，在需要感知空间关系和3D几何变换的任务上表现不佳；2）当前空间推理机制效率低下，token使用随复杂度快速增加；3）IDF框架能有效构建内部世界模型。

Conclusion: 本研究明确了先进VLMs的空间推理局限性和模式，识别了关键缺陷，并为未来改进提供了方向，强调了构建内部世界模型对空间推理的重要性。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [2] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: ALEX是一个轻量级知识编辑框架，通过分层内存架构将知识更新组织成语义簇，将检索复杂度从O(N)降低到O(K+N/C)，并集成推理查询合成模块和动态证据裁决引擎，显著提高多跳问答准确性和推理路径可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的知识是静态的，难以适应不断变化的信息，现有方法在处理需要多步推理的复杂多跳问题时面临可扩展性和检索效率的挑战。

Method: 提出ALEX框架，采用分层内存架构组织知识编辑，集成推理查询合成模块弥合查询与事实之间的语义鸿沟，以及动态证据裁决引擎执行两阶段检索过程。

Result: 在MQUAKE基准测试中，ALEX显著提高了多跳答案的准确性和推理路径的可靠性，同时将所需搜索空间减少了80%以上。

Conclusion: ALEX为构建可扩展、高效和准确的知识编辑系统提供了一条有前景的路径。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [3] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 本文提出了一种由教师主导的反馈循环系统，将概念级评估证据转化为经过验证的微干预措施，通过三个保障机制（充分性、注意力预算、多样性）实现自适应学习。


<details>
  <summary>Details</summary>
Motivation: 自适应学习系统通常诊断准确但干预效果弱，导致帮助时机不当或内容不匹配。需要一种能够有效闭合诊断与教学循环的系统。

Method: 将干预分配形式化为二元整数规划，包含覆盖度、时间、难度窗口、先决条件概念矩阵和反冗余约束。采用贪心选择、基于梯度的松弛方法和混合方法来解决不同资源丰富度和延迟要求。

Result: 在模拟和1204名学生的物理课程部署中，两种求解器都能在有限观看时间内为几乎所有学习者实现完整的技能覆盖。基于梯度的方法比贪心方法减少约12%的冗余覆盖，并在资源稀缺环境下以较低计算成本实现相当的充分性。

Conclusion: 该系统提供了一个可追踪和可审计的控制器，能够闭合诊断-教学循环，在课堂规模上实现公平、负载感知的个性化教学。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [4] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 本文测试了大语言模型在时间顺序任务中的表现，发现模型在理解时间顺序方面存在局限性，特别是在处理长序列时。GPT-5和Claude-3.7 Sonnet在显式推理预算下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融和经济学中的广泛应用，需要测试模型是否真正理解时间顺序，以避免前瞻性偏差。

Method: 设计了一系列时间顺序排序任务，包括时间顺序排序、条件排序（先筛选后排序）和时代错误检测，评估了GPT-4.1、Claude-3.7 Sonnet和GPT-5在不同推理设置下的表现。

Result: 随着序列长度增加，精确匹配率显著下降，但等级相关性保持较高。条件排序中的失败主要来自筛选步骤。GPT-5在中/高推理努力下在所有长度上实现了完美排序和条件排序。

Conclusion: 当前大语言模型在时间顺序任务中存在局限性，显式推理预算有助于改善性能，这对金融领域的实时应用具有重要意义。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [5] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 提出两阶段架构来减少Whisper模型在噪声环境下的幻觉错误：第一阶段通过自适应层注意力增强编码器鲁棒性，第二阶段使用多目标知识蒸馏抑制幻觉。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在噪声声学条件下经常出现幻觉错误，而之前的研究主要集中在音频预处理或转录后处理，对模型本身的修改探索不足。

Method: 1. 自适应层注意力(ALA)：通过层间相关性分析将编码器层分组为语义连贯块，使用可学习多头注意力融合块表示；2. 多目标知识蒸馏框架：在噪声音频上训练学生模型，使其语义和注意力分布与处理干净输入的教师模型对齐。

Result: 在噪声语音基准测试中显著减少了幻觉和词错误率，同时在干净语音上保持了性能。

Conclusion: ALA和KD提供了一个原则性策略来改善Whisper在真实世界噪声条件下的可靠性。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [6] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 提出一种新颖的时间序列预测框架，用于预测区域层面的Airbnb关键指标（收入、预订天数和预订数量），通过结合房源特征与外部环境因素生成区域嵌入，使用LLM和先进时序模型提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 短期租赁平台（如Airbnb）的扩张显著扰乱了当地住房市场，导致租金上涨和住房负担能力问题。准确预测区域Airbnb市场趋势可为政策制定者和城市规划者提供关键见解以减轻这些影响。

Method: 使用滑动窗口方法预测1-3个月趋势，构建区域表示时整合房源特征与外部环境因素（如城市可达性和人员流动性），将结构化表格数据转换为基于提示的LLM输入以生成全面区域嵌入，然后输入到先进时间序列模型（RNN、LSTM、Transformer）中。

Result: 在首尔Airbnb数据集上的实验表明，与传统基线（包括传统统计和机器学习模型）相比，该方法将平均RMSE和MAE降低了约48%。

Conclusion: 该框架不仅提高了预测准确性，还为检测供应过剩区域和支持数据驱动的城市政策决策提供了实用见解。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [7] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 本研究探讨了在LLM对齐过程中融入多元社会价值观的影响，通过系统评估人口统计差异和设计参数，发现不同社会群体对模型响应的评价存在系统性差异，技术设计选择对模型行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐决策往往忽视人类社会的多样性，本研究旨在探索如何通过纳入多元价值观来影响LLM行为，以平衡安全性与公平代表性。

Method: 从美国和德国收集了1,095名参与者的27,375个评分，评估LLM响应在毒性、情感意识、敏感性、刻板偏见和帮助性五个维度。使用不同社会群体的偏好微调多个大语言模型和大推理模型，同时变化评分尺度、分歧处理方法和优化技术。

Result: 发现系统性人口统计效应：男性参与者对毒性的评分比女性低18%；保守派和黑人参与者对情感意识的评分分别比自由派和白人参与者高27.9%和44%。技术设计选择显示强烈影响：保留评分者分歧比多数投票实现约53%更大的毒性减少；5点量表比二元格式产生约22%更多减少；DPO在多值优化中始终优于GRPO。

Conclusion: 这些发现为回答关键问题提供了初步步骤：对齐应如何平衡专家驱动和用户驱动信号，以确保安全性和公平代表性。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [8] [Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants](https://arxiv.org/abs/2511.13763)
*Anthony Kiggundu,Bin Han,Hans D. Schotten*

Main category: stat.ML

TL;DR: 研究两种信息反馈（马尔可夫估计器和在线训练的演员-评论家）如何影响双M/M/1系统中的放弃和跳槽行为，分析了不同服务率和总时间耐心下的渐近特性。


<details>
  <summary>Details</summary>
Motivation: 探讨信息反馈对排队系统中用户行为（放弃和跳槽）的影响，为低成本、支持跳槽的系统设计轻量级遥测和决策逻辑提供理论指导。

Method: 结合理论分析和实证验证，使用马尔可夫估计器和在线训练的演员-评论家两种信息模型，研究双M/M/1系统在不同服务率和总时间耐心下的行为。

Result: 理论证明总等待时间线性增长导致放弃不可避免，成功跳槽概率在积压趋于无穷时消失；两种信息模型在满足温和次线性误差条件下具有相同的渐近极限；实证验证了有限积压下的差异和渐近收敛。

Conclusion: 信息价值在有限规模下重要（产生不同的延迟、放弃率和瞬态跳槽行为），但在渐近情况下不重要，为系统设计提供了理论依据。

Abstract: We study how two information feeds, a closed-form Markov estimator of residual sojourn and an online trained actor-critic, affect reneging and jockeying in a dual M/M/1 system. Analytically, for unequal service rates and total-time patience, we show that total wait grows linearly so abandonment is inevitable and the probability of a successful jockey vanishes as the backlog approaches towards infinity. Furthermore, under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness). We empirically validate these limits and quantify finite backlog differences. Our findings show that learned and analytic feeds produce different delays, reneging rates and transient jockeying behavior at practical sizes, but converge to the same asymptotic outcome implied by our theory. The results characterize when value-of-information matters (finite regimes) and when it does not (asymptotics), informing lightweight telemetry and decision-logic design for low-cost, jockeying-aware systems.

</details>


### [9] [Empirical Likelihood for Random Forests and Ensembles](https://arxiv.org/abs/2511.13934)
*Harold D. Chiang,Yukitoshi Matsushita,Taisuke Otsu*

Main category: stat.ML

TL;DR: 本文开发了一个用于随机森林和相关集成方法的经验似然框架，通过利用集成预测中固有的不完全U统计量结构，构建了渐近卡方分布的经验似然统计量，并提出修正版本以在稀疏子采样情况下恢复枢轴性。


<details>
  <summary>Details</summary>
Motivation: 为随机森林和集成方法提供基于似然的统计不确定性量化方法，解决现有推理方法的局限性。

Method: 利用集成预测的不完全U统计量结构构建经验似然统计量，在稀疏子采样情况下通过简单调整提出修正经验似然以恢复枢轴性。

Result: 理论分析和模拟表明，修正经验似然相对于现有推理方法实现了准确的覆盖率和实际可靠性。

Conclusion: 该方法保留了经验似然的关键特性，同时保持计算效率，为随机森林提供了有效的统计不确定性量化框架。

Abstract: We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.

</details>


### [10] [SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation](https://arxiv.org/abs/2511.14146)
*Renjie Chen,Viet Anh Nguyen,Huifu Xu*

Main category: stat.ML

TL;DR: 提出一种分布鲁棒联合估计方法，同时估计随机向量的协方差矩阵和精度矩阵，通过最小化最坏情况下的加权损失函数来获得非线性收缩估计器。


<details>
  <summary>Details</summary>
Motivation: 为了解决经验协方差/精度矩阵估计器的谱偏差问题，提出一种能够同时优化协方差和精度矩阵估计的鲁棒方法。

Method: 采用分布鲁棒优化框架，构建以名义分布为中心的模糊集，最小化Frobenius损失和Stein损失的最坏情况加权和，通过凸谱散度衡量模糊集半径。

Result: 提出的SCOPE估计器能够将特征值非线性收缩到正标量，改善估计器的条件数，在合成和真实数据实验中表现优于现有方法。

Conclusion: 该方法通过精心调整权重系数，有效校正谱偏差，提供了一种参数调整方案，在应用中表现出竞争优势。

Abstract: We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.

</details>


### [11] [Causal Discovery on Higher-Order Interactions](https://arxiv.org/abs/2511.14206)
*Alessio Zanga,Marco Scutari,Fabio Stella*

Main category: stat.ML

TL;DR: 本文提出了一种基于高阶结构的因果发现新框架和DAG聚合算法，在数据稀缺情况下通过bagging方法提升因果图构建的置信度，特别在低样本量和高维设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在数据稀缺时使用bagging技术，但聚合步骤仅考虑单个边的置信度，忽略了复杂的高阶边结构，导致聚合效果受限。

Method: 引入基于高阶结构的理论框架，开发新的DAG聚合算法，通过考虑复杂的高阶边结构来改进因果图的聚合过程。

Result: 模拟研究表明，该方法计算效率高且效果显著，在低样本量和高维设置下表现优于现有最先进解决方案。

Conclusion: 提出的高阶结构框架和DAG聚合算法为因果发现提供了更有效的工具，特别是在数据稀缺情况下能够更准确地构建因果图。

Abstract: Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [12] [DualLaguerreNet: A Decoupled Spectral Filter GNN and the Uncovering of the Flexibility-Stability Trade-off](https://arxiv.org/abs/2511.13729)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: DualLaguerreNet通过引入解耦谱灵活性，将图拉普拉斯算子分解为低频和高频两个算子，分别学习独立的Laguerre多项式滤波器，解决了单滤波器模型的折中问题，在复杂异质任务上达到SOTA性能，但揭示了灵活性-稳定性权衡。


<details>
  <summary>Details</summary>
Motivation: 解决基于谱滤波器的GNNs（如LaguerreNet）中单滤波器模型的折中问题，即单个自适应参数必须在整个图谱上学习次优的平均响应。

Method: 提出DualLaguerreNet架构，将图拉普拉斯算子分解为L_low（低频）和L_high（高频）两个算子，分别学习由alpha_1和alpha_2参数化的独立自适应Laguerre多项式滤波器。

Result: 在复杂异质任务上达到最先进性能（优于LaguerreNet），但在简单同质任务上表现不佳，揭示了灵活性-稳定性权衡。增加的参数化导致在简单任务上过拟合。

Conclusion: 提出了用于异质性的新SOTA架构，同时批判性分析了自适应GNN滤波器设计中固有的偏差-方差权衡，表明简单模型的折中起到了关键的正则化作用。

Abstract: Graph Neural Networks (GNNs) based on spectral filters, such as the Adaptive Orthogonal Polynomial Filter (AOPF) class (e.g., LaguerreNet), have shown promise in unifying the solutions for heterophily and over-smoothing. However, these single-filter models suffer from a "compromise" problem, as their single adaptive parameter (e.g., alpha) must learn a suboptimal, averaged response across the entire graph spectrum. In this paper, we propose DualLaguerreNet, a novel GNN architecture that solves this by introducing "Decoupled Spectral Flexibility." DualLaguerreNet splits the graph Laplacian into two operators, L_low (low-frequency) and L_high (high-frequency), and learns two independent, adaptive Laguerre polynomial filters, parameterized by alpha_1 and alpha_2, respectively. This work, however, uncovers a deeper finding. While our experiments show DualLaguerreNet's flexibility allows it to achieve state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet), it simultaneously underperforms on simpler, homophilic tasks. We identify this as a fundamental "Flexibility-Stability Trade-off". The increased parameterization (2x filter parameters and 2x model parameters) leads to overfitting on simple tasks, demonstrating that the "compromise" of simpler models acts as a crucial regularizer. This paper presents a new SOTA architecture for heterophily while providing a critical analysis of the bias-variance trade-off inherent in adaptive GNN filter design.

</details>


### [13] [Cross-Sparsity-Enabled Multipath Perception via Structured Bayesian Inference for Multi-Target Estimation](https://arxiv.org/abs/2511.14051)
*Xiang Chen,Ming-Min Zhao,An Liu,Min Li,Qingjiang Shi,Min-Jian Zhao*

Main category: eess.SP

TL;DR: 提出了一种利用多径环境中目标间散射产生的一阶反射路径来增强目标方向估计的方法，通过构建交叉稀疏结构和分层先验模型，结合高效的变分贝叶斯推理算法，在降低计算复杂度的同时提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 在多目标感知系统中，多径环境下的目标间散射会产生一阶反射路径，这些路径的角度信息与不同目标的直接路径角度重合，可以作为额外的先验知识来改进目标方向估计。

Method: 构建多目标感知信道的稀疏表示，提出三层分层结构化先验模型下的交叉稀疏结构，并开发结构化快速涡轮变分贝叶斯推理算法，结合高效消息传递策略和双时间尺度更新方案。

Result: 仿真结果表明，利用所提出的交叉稀疏结构能够显著提高目标角度估计精度，SF-TVBI算法在达到与Turbo-VBI相当估计性能的同时，具有更低的计算复杂度。

Conclusion: 通过利用多径环境中的一阶反射路径构建交叉稀疏结构，可以有效提升多目标方向估计性能，所提出的SF-TVBI算法在保持高精度的同时降低了计算负担。

Abstract: In this paper, we investigate a multi-target sensing system in multipath environment, where inter-target scattering gives rise to first-order reflected paths whose angles of departure (AoDs) and angles of arrival (AoAs) coincide with the direct-path angles of different targets. Unlike other multipath components, these first-order paths carry structural information that can be exploited as additional prior knowledge for target direction estimation. To exploit this property, we construct a sparse representation of the multi-target sensing channel and propose a novel cross sparsity structure under a three-layer hierarchical structured (3LHS) prior model, which leverages the first-order paths to enhance the prior probability of the direct paths and thereby improve the estimation accuracy. Building on this model, we propose a structured fast turbo variational Bayesian inference (SF-TVBI) algorithm, which integrates an efficient message-passing strategy to enable tractable probabilistic exchange within the cross sparsity, and a two-timescale update scheme to reduce the update frequency of the high-dimensional sparse vector. Simulation results demonstrate that leveraging the proposed cross sparsity structure is able to improve the target angle estimation accuracy substantially, and the SF-TVBI algorithm achieves estimation performance comparable to that of the Turbo-VBI, but with lower computational complexity.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Darts Analysis](https://arxiv.org/abs/2511.14537)
*Ayham Makhamra,Yelyzaveta Satynska,Michael Weselcouch*

Main category: stat.AP

TL;DR: 本文评估了五种预测业余飞镖比赛结果的数学模型，包括空模型、逻辑回归模型、基础模拟模型、时间调整模拟模型和基于当前比分更新的Massey模型变体。通过Brier评分和投注游戏两种方法评估，得分依赖的Massey模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同数学模型在预测业余飞镖比赛结果方面的有效性，特别是这些模型如何随着比赛比分变化而更新预测。

Method: 使用五种模型：空模型、逻辑回归模型、基础模拟模型、时间调整模拟模型和得分依赖的Massey模型。通过Brier评分和投注游戏两种方法进行评估，不仅评估比赛开始时的预测，还评估每轮开始时的预测。

Result: 在两种评估方法中，得分依赖的Massey模型表现最佳。

Conclusion: 得分依赖的Massey模型框架可以适应飞镖以外的其他竞争环境。

Abstract: In this paper we examine the effectiveness of five mathematical models used to predict the outcomes of amateur darts games. These models not only predict the outcomes at the start of the game, but also update their estimations as the game score changes. The models were trained and tested on a dataset consisting of games played by amateur players involving students, faculty, and staff at Roanoke College. The five models are: the null model, which is based only on the live scores, a logistic regression model, a basic simulation model, a time-adjusted simulation model, and a new variation of the Massey model which updates based on the current score. We evaluate these models using two approaches. First, we compare their Brier scores. Second, we conduct head-to-head comparisons in a betting game in which one model sets the betting odds while the other places bets. In both cases, model performance is assessed not only at the start of the game but also at the start of each round. Across both evaluation methods, the score-dependent Massey model performs the best. We conclude by illustrating how this score-dependent Massey model framework can be adapted to other competitive settings beyond darts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Blurred Encoding for Trajectory Representation Learning](https://arxiv.org/abs/2511.13741)
*Silin Zhou,Yao Chen,Shuo Shang,Lisi Chen,Bingsheng He,Ryosuke Shibasaki*

Main category: cs.LG

TL;DR: BLUE是一种轨迹表示学习方法，通过逐渐降低GPS坐标精度创建多级分层补丁，在保持细粒度时空细节的同时捕获整体出行模式，在多个下游任务中平均比最佳基线方法提升30.90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的轨迹表示学习方法将原始GPS轨迹转换为网格或道路轨迹来捕获高级出行语义，但会丢失细粒度的时空细节，因为多个GPS点被分组到单个网格单元或道路段中。

Method: 提出BLUrred Encoding方法(BLUE)，通过逐渐降低GPS坐标精度创建分层补丁，使用具有金字塔结构的编码器-解码器模型，在每个补丁级别使用Transformer学习轨迹嵌入，通过池化准备更高级别的输入，通过上分辨率提供更低级别的指导，使用轨迹重建任务和MSE损失进行训练。

Result: 与8种最先进的轨迹表示学习方法在3个下游任务上进行比较，BLUE始终比所有基线方法获得更高的准确率，平均比最佳基线方法高出30.90%。

Conclusion: BLUE方法通过分层补丁机制有效平衡了细粒度时空细节和整体出行模式的捕获，在轨迹表示学习任务中表现出优越性能。

Abstract: Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLUrred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90%. Our code is available at https://github.com/slzhou-xy/BLUE.

</details>


### [16] [Robustness of LLM-enabled vehicle trajectory prediction under data security threats](https://arxiv.org/abs/2511.13753)
*Feilong Wang,Fuqiang Liu*

Main category: cs.LG

TL;DR: 本文对基于大语言模型的车辆轨迹预测系统进行了系统性漏洞分析，提出单特征差分进化攻击方法，发现在黑盒设置下即使微小扰动也能显著破坏模型输出，揭示了LLM驱动自动驾驶模型的安全脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调后的LLM在预测车辆轨迹和变道意图方面表现出色，但其在安全关键驾驶系统中的鲁棒性尚未得到充分研究，特别是考虑到LLM可信度日益受到关注的问题。

Method: 提出单特征差分进化攻击方法，在LLM输入提示中扰动周围车辆的单个运动学特征，采用黑盒设置进行实验。

Result: 在高D数据集上的实验表明，即使微小且物理上合理的扰动也能显著破坏模型输出，揭示了LLM预测器对对抗性操纵的易感性。进一步分析揭示了准确性和鲁棒性之间的权衡关系。

Conclusion: 这是首次揭示LLM驱动自动驾驶模型在车辆交互背景下的对抗性漏洞，强调了未来基于LLM的智能交通系统需要采用面向鲁棒性的设计方法。

Abstract: The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.

</details>


### [17] [Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement](https://arxiv.org/abs/2511.13755)
*Zhe Yang,Wenrui Li,Hongtao Chen,Penghong Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.LG

TL;DR: 本文提出RedReg方法解决多模态学习中优势模态主导导致的优化不平衡问题，通过冗余阶段监控和共信息门控机制自适应调节梯度，在多数场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中由于模态偏差，优势模态在反向传播中占据主导地位，导致优化不平衡。现有方法存在两个问题：1）优势模态长期主导削弱了表示-输出耦合，导致冗余信息积累；2）直接均匀调整优势模态梯度，忽略模态间的语义和方向性。

Method: 提出自适应冗余调节方法RedReg，基于信息瓶颈原理：1）构建冗余阶段监控器，使用有效增益增长率和冗余联合标准触发干预；2）设计共信息门控机制，基于跨模态语义估计优势模态贡献；3）将优势模态梯度投影到联合多模态梯度子空间的正交补空间，根据冗余度抑制梯度。

Result: 实验表明该方法在大多数场景下优于当前主流方法，消融实验验证了方法的有效性。

Conclusion: RedReg方法通过自适应冗余调节有效解决了多模态学习中的优化不平衡问题，在保持模态特定信息的同时实现了更好的性能表现。

Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git

</details>


### [18] [Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection](https://arxiv.org/abs/2511.13759)
*Han Wang,Deyi Ji,Junyu Lu,Lanyun Zhu,Hailong Zhang,Haiyang Wu,Liqun Liu,Peng Shu,Roy Ka-Wei Lee*

Main category: cs.LG

TL;DR: 提出了一种自训练框架，利用多智能体视觉语言模型进行协作伪标注，通过PNU损失函数优化分类器，在有限监督下显著提升社交媒体冒犯性内容检测性能


<details>
  <summary>Details</summary>
Motivation: 社交媒体冒犯性内容检测需要高质量标注数据，但由于冒犯性实例稀少且人工标注成本高，标注数据往往稀缺，需要解决低资源场景下的检测挑战

Method: 使用轻量级分类器和多智能体视觉语言模型进行迭代伪标注，形成一致未知集和分歧未知集，通过模拟监管者和用户双视角增强标签可靠性，采用PNU损失函数联合优化

Result: 在基准数据集上的实验表明，该框架在有限监督下显著优于基线方法，接近大规模模型的性能

Conclusion: 所提出的自训练框架有效解决了低资源场景下的冒犯性内容检测问题，通过协作伪标注和噪声抑制实现了优越性能

Abstract: Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models

</details>


### [19] [Credal Ensemble Distillation for Uncertainty Quantification](https://arxiv.org/abs/2511.13766)
*Kaizheng Wang,Fabio Cuzzolin,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: 该论文提出了credal ensemble distillation (CED)框架，将深度集成压缩为单个模型CREDIT，通过预测类别概率区间来量化不确定性，显著降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 深度集成虽然能有效量化预测不确定性，但其高计算和内存成本限制了实际部署。

Method: 提出credal ensemble distillation (CED)框架，将深度集成压缩为单个模型CREDIT，预测类别概率区间而非单一概率分布。

Result: 在分布外检测基准测试中，CED实现了优于或可比现有基线的uncertainty估计性能，同时大幅降低了推理开销。

Conclusion: CED框架能有效压缩深度集成，在保持uncertainty估计性能的同时显著降低计算成本，具有实际部署价值。

Abstract: Deep ensembles (DE) have emerged as a powerful approach for quantifying predictive uncertainty and distinguishing its aleatoric and epistemic components, thereby enhancing model robustness and reliability. However, their high computational and memory costs during inference pose significant challenges for wide practical deployment. To overcome this issue, we propose credal ensemble distillation (CED), a novel framework that compresses a DE into a single model, CREDIT, for classification tasks. Instead of a single softmax probability distribution, CREDIT predicts class-wise probability intervals that define a credal set, a convex set of probability distributions, for uncertainty quantification. Empirical results on out-of-distribution detection benchmarks demonstrate that CED achieves superior or comparable uncertainty estimation compared to several existing baselines, while substantially reducing inference overhead compared to DE.

</details>


### [20] [Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture](https://arxiv.org/abs/2511.13780)
*Nihal Mehta*

Main category: cs.LG

TL;DR: 本文从分布语义学角度对自注意力机制进行数学解释，证明其源于将语料库级共现统计投影到序列上下文中。


<details>
  <summary>Details</summary>
Motivation: 为自注意力机制提供数学理论基础，证明Transformer架构的代数形式源于投影原理而非任意设计选择。

Method: 从GloVe嵌入的共现矩阵出发，展示投影如何自然捕获上下文影响，query-key-value机制作为建模方向关系的自然非对称扩展。

Result: 证明位置编码和多头注意力都是同一投影原理的结构化改进，自注意力机制从语料级共现统计投影到序列上下文中自然产生。

Conclusion: Transformer架构的特定代数形式遵循这些投影原理，而非任意的设计选择，为自注意力提供了数学理论基础。

Abstract: This paper presents a mathematical interpretation of self-attention by connecting it to distributional semantics principles. We show that self-attention emerges from projecting corpus-level co-occurrence statistics into sequence context. Starting from the co-occurrence matrix underlying GloVe embeddings, we demonstrate how the projection naturally captures contextual influence, with the query-key-value mechanism arising as the natural asymmetric extension for modeling directional relationships. Positional encodings and multi-head attention then follow as structured refinements of this same projection principle. Our analysis demonstrates that the Transformer architecture's particular algebraic form follows from these projection principles rather than being an arbitrary design choice.

</details>


### [21] [ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design](https://arxiv.org/abs/2511.13809)
*Emanuel Covaci,Fabian Galis,Radu Balan,Daniela Zaharie,Darian Onchis*

Main category: cs.LG

TL;DR: 本文提出了一种新的可微分全局可解释性方法，通过在模型训练过程中直接集成特征重要性估计来实现。该方法使用ScoresActivation函数作为特征排序机制，使模型能够以可微分和端到端可训练的方式根据特征对预测性能的贡献进行优先级排序。


<details>
  <summary>Details</summary>
Motivation: 当前的事后解释方法与模型训练过程脱节，限制了其忠实性和实用性。为了解决这个问题，需要一种将特征重要性估计直接集成到模型训练中的方法，以弥合模型准确性和可解释性之间的差距。

Method: 提出ScoresActivation函数作为特征排序机制，嵌入到学习流程中。这种方法使模型能够在训练过程中以可微分和端到端可训练的方式优先考虑对预测性能贡献最大的特征。

Result: 在基准数据集上的评估显示，该方法产生了全局忠实、稳定的特征排序，与SHAP值和真实特征重要性一致，同时保持高预测性能。特征评分速度比经典SHAP方法快150倍，训练仅需2秒。在10个特征（5个相关）和16个特征（5个相关，11个无关）的情况下，分类准确率分别提高了11.24%和29.33%。

Conclusion: 这项工作弥合了模型准确性和可解释性之间的差距，为固有可解释的机器学习提供了一个可扩展的框架。

Abstract: Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.

</details>


### [22] [AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection](https://arxiv.org/abs/2511.13880)
*Saleh Momeni,Changnan Xiao,Bing Liu*

Main category: cs.LG

TL;DR: AnaCP是一种新颖的类增量学习方法，通过分析对比投影在保持分析分类器效率的同时实现增量特征适应，无需基于梯度的训练，从而避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习方法存在灾难性遗忘问题，而现有基于预训练模型的方法虽然高效，但无法持续适应特征表示，导致性能次优。

Method: 提出AnaCP方法，结合分析分类器的效率和对比投影技术，实现无需梯度训练的特征增量适应。

Result: 实验表明AnaCP不仅优于现有基线方法，而且达到了联合训练的准确率水平，这是类增量学习的理论上界。

Conclusion: AnaCP成功解决了类增量学习中的特征适应问题，在保持效率的同时实现了接近理论上界的性能。

Abstract: This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.

</details>


### [23] [A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation](https://arxiv.org/abs/2511.14057)
*Xianghe Liu,Jiajia Liu,Chuxian Xu,Minghan Wang,Hongbo Peng,Tao Sun,Jiaqi Xu*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的多模态框架，集成可穿戴传感器数据同时进行动作识别和压力估计，用于射箭等精准运动训练优化。


<details>
  <summary>Details</summary>
Motivation: 传统运动分析系统昂贵且侵入性强，限制了在自然训练环境中的使用，需要开发更实用的运动员技术和心理状态监测方法。

Method: 使用自研腕戴设备采集加速度和PPG数据，采用LSTM模型进行动作阶段识别（使用SmoothDiff特征），MLP分类器进行压力水平分类（基于HRV特征）。

Result: 动作识别准确率96.8%，F1分数95.9%；压力估计准确率80%。

Conclusion: 集成运动和生理感知可为运动员技术和心理状态提供有意义的洞察，为开发智能实时反馈系统奠定基础。

Abstract: In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.

</details>


### [24] [Observational Auditing of Label Privacy](https://arxiv.org/abs/2511.14084)
*Iden Kalemaj,Luca Melis,Maxime Boucher,Ilya Mironov,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私审计框架，无需修改训练数据集即可评估隐私保证，解决了现有方法在大规模系统中资源密集的问题。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计方法需要修改训练数据集（如注入异常数据或移除样本），在大规模系统中资源密集且工程开销大，需要一种无需干预数据管道的审计方法。

Method: 利用数据分布的固有随机性，开发观察性审计框架，无需修改原始数据集，将隐私审计扩展到保护属性和标签隐私。

Result: 在Criteo和CIFAR-10数据集上的实验证明该方法能有效审计标签隐私保证。

Conclusion: 这项工作为大规模生产环境中的实用隐私审计开辟了新途径。

Abstract: Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.

</details>


### [25] [MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts](https://arxiv.org/abs/2511.14102)
*Wenfeng Wang,Jiacheng Liu,Xiaofeng Hou,Xinfeng Xia,Peng Tang,Mingxuan Zhang,Chao Li,Minyi Guo*

Main category: cs.LG

TL;DR: MoE-SpeQ通过推测执行和专家卸载的协同设计，使用小型草稿模型预测未来token所需的专家序列，通过预取专家来隐藏I/O延迟，在内存受限设备上实现最高2.34倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型推理时因专家选择的数据依赖性导致的PCIe总线I/O瓶颈问题，使MoE推理在商品硬件上更加可行。

Method: 采用推测执行和专家卸载协同设计，使用小型草稿模型预测未来token的专家需求，运行时编排器预取专家，自适应调节器根据摊销屋顶模型动态调整推测策略。

Result: 在内存受限设备上，对Phi-MoE模型实现了最高2.34倍的速度提升，相比最先进的卸载框架有显著改进。

Conclusion: MoE-SpeQ为资源受限环境中管理数据依赖性内存访问提供了一种新的原则性方法，使MoE推理在商品硬件上更加可行。

Abstract: The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.
  This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.

</details>


### [26] [Soft-Label Training Preserves Epistemic Uncertainty](https://arxiv.org/abs/2511.14117)
*Agamdeep Singh,Ashish Tiwari,Hosein Hasanbeig,Priyanshu Gupta*

Main category: cs.LG

TL;DR: 该论文主张在主观性机器学习任务中，应将标注分布视为真实标签而非聚合为单一标签，通过软标签训练保留认知不确定性，在视觉和NLP任务中显著改善了模型与人类标注的一致性。


<details>
  <summary>Details</summary>
Motivation: 标准实践将多样的人类标注聚合为单一标签，这在模糊数据上存在认知偏差，迫使模型在本质上模糊的情况下表达虚假的置信度，造成模型确定性与人类感知多样性之间的错位。

Method: 采用软标签训练方法，将标注分布视为真实标签进行训练，而不是将多样的人类判断聚合为点估计。

Result: 在视觉和NLP任务中，软标签训练实现了32%更低的KL散度（与人类标注相比）和61%更强的模型与标注熵相关性，同时保持了与硬标签训练相当的准确率。

Conclusion: 标注分布应重新定位为认知不确定性的忠实表示，模型应学习重现这种不确定性，而不是将其作为需要聚合的噪声信号。

Abstract: Many machine learning tasks involve inherent subjectivity, where annotators naturally provide varied labels. Standard practice collapses these label distributions into single labels, aggregating diverse human judgments into point estimates. We argue that this approach is epistemically misaligned for ambiguous data--the annotation distribution itself should be regarded as the ground truth. Training on collapsed single labels forces models to express false confidence on fundamentally ambiguous cases, creating a misalignment between model certainty and the diversity of human perception. We demonstrate empirically that soft-label training, which treats annotation distributions as ground truth, preserves epistemic uncertainty. Across both vision and NLP tasks, soft-label training achieves 32% lower KL divergence from human annotations and 61% stronger correlation between model and annotation entropy, while matching the accuracy of hard-label training. Our work repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.

</details>


### [27] [Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts](https://arxiv.org/abs/2511.14218)
*Xinlei Xiong,Wenbo Hu,Shuxun Zhou,Kaifeng Bi,Lingxi Xie,Ying Liu,Richang Hong,Qi Tian*

Main category: cs.LG

TL;DR: 提出统一的混合贝叶斯深度学习框架，将天气预报不确定性分解为认知不确定性和偶然不确定性，通过变分推理和物理信息随机扰动方案分别学习，在ERA5数据集上验证了方法的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 天气预报面临大气混沌特性的挑战，传统集合预报方法计算密集，而贝叶斯深度学习提供了有前景但往往脱节的替代方案，需要桥接这两种范式。

Method: 通过混合贝叶斯深度学习框架，使用变分推理学习认知不确定性，物理信息随机扰动方案建模流依赖的大气动力学来学习偶然不确定性，建立了BDL与EPS的统一理论框架。

Result: 在ERA5再分析数据集上的实验结果表明，该方法不仅提高了预报准确性，提供了更好校准的不确定性量化，而且在计算效率上优于最先进的概率扩散模型。

Conclusion: 提出的混合贝叶斯深度学习框架成功桥接了传统集合预报和贝叶斯深度学习，为天气预报提供了更准确、更高效的不确定性量化方法。

Abstract: Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25° spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.

</details>


### [28] [EBind: a practical approach to space binding](https://arxiv.org/abs/2511.14229)
*Jim Broadbent,Felix Cohen,Frederik Hvilshøj,Eric Landau,Eren Sasoglu*

Main category: cs.LG

TL;DR: EBind是一种简单、数据驱动且参数高效的方法，用于绑定多模态对比模型的嵌入空间，仅需单个GPU在几小时内训练出最先进模型，而非传统需要的数天。


<details>
  <summary>Details</summary>
Motivation: 简化空间绑定过程，通过专注于单一编码器和高质量数据，实现参数效率和多模态融合。

Method: 使用1.8B参数的多模态模型（图像-文本-视频-音频-3D），结合三个互补数据源：670万全自动多模态五元组、100万人工标注三元组和340万现有标注数据项。

Result: 该模型在13个不同评估中表现优异，超越参数规模大4-17倍的模型，并引入了首个高质量、共识标注的音频与点云零样本分类基准。

Conclusion: EBind方法证明了通过精心策划的数据集和参数效率设计，可以在较小模型规模下实现最先进的性能，并将开源代码、模型权重和数据集。

Abstract: We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.

</details>


### [29] [Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention](https://arxiv.org/abs/2511.14265)
*Rui Zhang,Chao Li,Kezhong Liu,Chen Wang,Bolong Zheng,Hongbo Jiang*

Main category: cs.LG

TL;DR: 提出了一种结合可解释导航意图的统一多模态轨迹预测框架，通过持续意图树和瞬态意图建模，在复杂海事环境中实现更准确和可解释的船舶轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 解决现有船舶多模态轨迹预测方法在场景适用性和可解释性方面的不足，特别是在复杂海事环境中对快速行为变化的短期预测需求。

Method: 构建持续意图树从历史轨迹中提取信息，使用条件变分自编码器建模动态瞬态意图，并采用非局部注意力机制保持全局场景一致性。

Result: 在真实AIS数据集上的实验表明，该方法在多种场景下具有广泛适用性，在ADE和FDE指标上取得显著提升。

Conclusion: 该方法通过显式揭示每个预测轨迹背后的导航意图，不仅提高了预测准确性，还增强了模型的可解释性。

Abstract: Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.

</details>


### [30] [Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation](https://arxiv.org/abs/2511.14406)
*Bastien Vuillod,Pierre-Alain Moellic,Jean-Max Dutertre*

Main category: cs.LG

TL;DR: 本文分析了联邦学习中LoRA参数高效微调技术对后门攻击的影响，发现LoRA秩越低，后门持久性越长，并指出了FL后门攻击评估中的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的大模型适配面临安全威胁，特别是后门攻击，需要研究参数高效微调技术如LoRA对后门攻击的影响。

Method: 通过实验分析LoRA不同秩设置对最先进后门攻击的影响，重点关注后门寿命这一关键特征。

Result: 关键发现：对于最优注入的后门，LoRA秩越低，攻击停止后后门持久性越长。

Conclusion: 研究揭示了FL后门攻击评估中的问题，有助于开发更稳健公平的后门攻击评估方法，提高关键FL系统风险评估的可靠性。

Abstract: Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.

</details>


### [31] [Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters](https://arxiv.org/abs/2511.14452)
*Emanuele Palumbo,Sorawit Saengkyongam,Maria R. Cervera,Jens Behrmann,Andrew C. Miller,Guillermo Sapiro,Christina Heinze-Deml,Antoine Wehenkel*

Main category: cs.LG

TL;DR: 提出了一种结合血流动力学模拟和无标签临床数据的混合方法，直接从PPG信号估计心血管生物标志物，解决了关键心脏生物标志物预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 连续心血管监测对精准健康至关重要，但关键心脏生物标志物如每搏输出量和心输出量需要侵入性测量。PPG作为非侵入性替代方案存在标注数据稀缺的问题。

Method: 使用条件变分自编码器在配对PPG-APW数据上训练，结合在标记模拟APW段上训练的条件密度估计器，形成混合模型。

Result: 实验表明该方法能够检测心输出量和每搏输出量的波动，并在监测这些生物标志物的时间变化方面优于监督基线。

Conclusion: 提出的混合方法能够有效从PPG信号估计心血管生物标志物，为非侵入性心血管监测提供了可行解决方案。

Abstract: Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.

</details>


### [32] [nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers](https://arxiv.org/abs/2511.14465)
*Clément Dumas*

Main category: cs.LG

TL;DR: nnterp是一个轻量级包装器，为Transformer分析提供统一接口，同时保留原始HuggingFace实现，解决了当前工具在一致性和准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer分析工具面临根本权衡：自定义实现确保接口一致但需要手动适配每个架构且存在数值不匹配，而直接使用HuggingFace保持准确行为但缺乏标准化。

Method: 开发nnterp作为NNsight的轻量级包装器，通过自动模块重命名和全面验证测试，为50+模型变体提供统一分析接口。

Result: nnterp使研究人员能够编写一次干预代码并在16个架构家族的50多个模型变体中部署，包含常见可解释性方法的实现并提供注意力概率的直接访问。

Conclusion: nnterp在机械可解释性工具中弥合了正确性和可用性之间的差距。

Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.

</details>


### [33] [Notes on Kernel Methods in Machine Learning](https://arxiv.org/abs/2511.14485)
*Diego Armando Pérez-Rosero,Danna Valentina Salazar-Dubois,Juan Camilo Lugo-Rojas,Andrés Marino Álvarez-Meza,Germán Castellanos-Dominguez*

Main category: cs.LG

TL;DR: 本文提供了关于核方法及其在机器学习中几何基础的自包含介绍，涵盖希尔伯特空间构造、正定核、再生核希尔伯特空间等核心理论，并探讨了它们在统计估计和概率测度表示中的作用。


<details>
  <summary>Details</summary>
Motivation: 为机器学习中的核方法提供系统的几何理论基础，将经典统计概念与希尔伯特空间几何联系起来，并为高斯过程、核贝叶斯推断等高级主题奠定基础。

Method: 从希尔伯特空间构造出发，系统发展正定核、再生核希尔伯特空间和希尔伯特-施密特算子理论，通过希尔伯特空间几何重新审视协方差、回归和信息度量等经典概念。

Result: 建立了核密度估计、分布核嵌入和最大均值差异的理论框架，为统计估计和概率测度表示提供了统一的几何视角。

Conclusion: 该导论为理解核方法在机器学习中的应用奠定了坚实的理论基础，并为后续学习高斯过程、核贝叶斯推断等功能分析方法提供了必要的数学准备。

Abstract: These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.

</details>


### [34] [CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design](https://arxiv.org/abs/2511.14510)
*Jiawei Yi,Ping Gong,Youhui Bai,Jiaqi Ruan,Shengnan Wang,Pengcheng Wang,Haibo Wang,Weiguang Wang,Xia Zhu,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: CLO是一个通过算法-系统协同设计实现的CPU轻量级KVCache卸载系统，解决了现有系统在CPU瓶颈、PCIe带宽利用和GPU运行时气泡方面的问题，显著提升了LLM推理的解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着百万token LLM的发展，推理系统的可扩展性面临挑战，其中KVCache主导了内存使用和数据传输开销。现有卸载系统虽然将KVCache迁移到CPU内存并采用top-k注意力来减少数据传输量，但忽视了CPU瓶颈的三个关键方面：细粒度动态缓存管理开销、PCIe带宽利用率低以及GPU运行时气泡。

Method: CLO采用算法-系统协同设计，包括：(1)粗粒度的头级近似GPU缓存策略，缓存管理成本可忽略；(2)数据预取和GPU持久缓存的无缝结合以降低传输开销；(3)零拷贝传输引擎充分利用PCIe带宽，以及GPU中心的同步方法消除GPU停顿。

Result: 在两个广泛使用的LLM上的评估表明，CLO在保持与最先进系统相当准确度的同时，显著最小化CPU开销，充分利用PCIe带宽，将解码吞吐量提高了9.3%-66.6%。

Conclusion: 算法-系统协同设计对于现代GPU平台上内存受限的LLM推理至关重要。CLO已开源在GitHub上。

Abstract: The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.

</details>


### [35] [Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare](https://arxiv.org/abs/2511.14619)
*Marco Locatelli,Arjen Hommersom,Roberto Clemens Cerioli,Daniela Besozzi,Fabio Stella*

Main category: cs.LG

TL;DR: 提出Fuzzy MAP EM算法，通过将专家知识融入期望最大化框架来解决有限数据下部分可观测马尔可夫决策过程的参数学习问题。


<details>
  <summary>Details</summary>
Motivation: 在有限数据条件下学习POMDP参数具有挑战性，需要利用专家知识来指导学习过程。

Method: 将专家定义的模糊模型生成的模糊伪计数融入EM框架，将问题重新表述为最大后验估计。

Result: 在合成医疗模拟中，该方法在低数据和高噪声条件下始终优于标准EM算法；在重症肌无力案例研究中成功恢复了临床一致的POMDP。

Conclusion: Fuzzy MAP EM算法是医疗保健领域数据高效建模的实用工具，能够有效利用专家知识改善有限数据环境下的学习效果。

Abstract: Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.

</details>


### [36] [Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting](https://arxiv.org/abs/2511.14632)
*Yuchen Luo,Xinyu Li,Liuhua Peng,Mingming Gong*

Main category: cs.LG

TL;DR: Adapformer是一种基于Transformer的多变量时间序列预测框架，通过自适应通道管理结合了通道独立和通道依赖方法的优势，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多变量时间序列预测方法存在通道独立方法无法利用通道间交互信息，以及通道依赖方法包含过多无关信息导致过拟合的问题。

Method: 采用双阶段编码器-解码器架构，包括自适应通道增强器(ACE)用于丰富嵌入过程，以及自适应通道预测器(ACF)用于优化预测结果。ACE选择性整合关键依赖关系，ACF专注于最相关的协变量以减少噪声和冗余。

Result: 在多个数据集上的严格测试表明，Adapformer在预测准确性和计算效率方面均优于现有模型，达到了最先进的性能水平。

Conclusion: Adapformer通过有效的通道管理策略，成功解决了多变量时间序列预测中的依赖关系建模挑战，在准确性和效率方面都表现出色。

Abstract: In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.

</details>


### [37] [Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction](https://arxiv.org/abs/2511.14544)
*Jaume Ros,Alessio Arleo,Fernando Paulovich*

Main category: cs.LG

TL;DR: 本文提出了Warping Index (WI)这一新的维度缩减投影质量度量指标，专注于测量投影中空区域的正确保持，以避免视觉分析中的误导。


<details>
  <summary>Details</summary>
Motivation: 现有的投影质量度量主要关注全局或局部结构保持，但忽略了视觉失真、异常值和伪影对视觉分析的影响，可能导致用户得出误导性结论。

Method: 基于空区域正确保持对忠实视觉表示至关重要的假设，开发了Warping Index (WI)度量指标来量化维度缩减投影的质量。

Result: 提出了一个新的投影质量度量方法，专门关注投影中空区域的保持情况，以解决现有度量方法在视觉失真方面的不足。

Conclusion: Warping Index (WI)为维度缩减投影质量评估提供了新的视角，特别强调空区域保持对视觉分析准确性的重要性。

Abstract: Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.

</details>

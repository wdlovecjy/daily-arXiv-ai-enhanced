<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 2]
- [cs.LG](#cs.LG) [Total: 24]
- [eess.SP](#eess.SP) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Prototype Selection Using Topological Data Analysis](https://arxiv.org/abs/2511.04873)
*Jordan Eckert,Elvan Ceyhan,Henry Schenck*

Main category: stat.ML

TL;DR: 提出基于拓扑数据分析的框架TPS，用于从大数据集中选择代表性子集，在保持或提升分类性能的同时显著减少数据规模。


<details>
  <summary>Details</summary>
Motivation: 利用拓扑原理来捕捉数据结构和关系，解决大数据集中选择代表性原型的问题。

Method: 基于拓扑数据分析的框架TPS，通过拓扑原理选择代表性子集，并与现有原型选择方法进行比较。

Result: 在模拟和真实数据中，TPS显著保持或改善分类性能，同时大幅减少数据规模。

Conclusion: TPS在原型学习的算法和几何方面都有贡献，为并行化、可解释和高效分类提供了实用工具。

Abstract: Recently, there has been an explosion in statistical learning literature to
represent data using topological principles to capture structure and
relationships. We propose a topological data analysis (TDA)-based framework,
named Topological Prototype Selector (TPS), for selecting representative
subsets (prototypes) from large datasets. We demonstrate the effectiveness of
TPS on simulated data under different data intrinsic characteristics, and
compare TPS against other currently used prototype selection methods in real
data settings. In all simulated and real data settings, TPS significantly
preserves or improves classification performance while substantially reducing
data size. These contributions advance both algorithmic and geometric aspects
of prototype learning and offer practical tools for parallelized,
interpretable, and efficient classification.

</details>


### [2] [A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights](https://arxiv.org/abs/2511.05159)
*Shubhayan Pan,Saptarshi Chakraborty,Debolina Paul,Kushal Bose,Swagatam Das*

Main category: stat.ML

TL;DR: 提出了一种凸聚类方法的核化扩展，通过将数据点映射到再生核希尔伯特空间来处理线性不可分和非凸结构数据，提供了理论保证并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统凸聚类方法在处理线性不可分或非凸结构数据时可能失效，需要扩展其能力以应对复杂数据分布。

Method: 通过特征映射将数据点投影到再生核希尔伯特空间，在该变换空间中进行凸聚类，产生有限维向量空间中的嵌入。

Result: 提供了完整的理论基础，证明了算法收敛性并建立了估计的有限样本界，在合成和真实数据集上的实验显示优于现有聚类技术。

Conclusion: 这项工作在聚类领域取得了显著进展，为非线性和非凸数据场景提供了有效的聚类解决方案。

Abstract: Convex clustering is a well-regarded clustering method, resembling the
similar centroid-based approach of Lloyd's $k$-means, without requiring a
predefined cluster count. It starts with each data point as its centroid and
iteratively merges them. Despite its advantages, this method can fail when
dealing with data exhibiting linearly non-separable or non-convex structures.
To mitigate the limitations, we propose a kernelized extension of the convex
clustering method. This approach projects the data points into a Reproducing
Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in
this transformed space. This kernelization not only allows for better handling
of complex data distributions but also produces an embedding in a
finite-dimensional vector space. We provide a comprehensive theoretical
underpinnings for our kernelized approach, proving algorithmic convergence and
establishing finite sample bounds for our estimates. The effectiveness of our
method is demonstrated through extensive experiments on both synthetic and
real-world datasets, showing superior performance compared to state-of-the-art
clustering techniques. This work marks a significant advancement in the field,
offering an effective solution for clustering in non-linear and non-convex data
scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: 本文提出了一种传感器引导的正则化GLISp扩展方法，通过将可测量的描述符集成到偏好学习循环中，结合主观反馈与定量传感器信息，在车辆悬架调校等任务中实现了更快的收敛速度和更优的最终解。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法（如Preferential Bayesian Optimization和GLISp）将系统视为黑盒，忽略了信息丰富的传感器测量数据，限制了学习效率。

Method: 引入传感器引导的正则化GLISp扩展，通过物理信息假设函数和最小二乘正则化项将可测量描述符集成到偏好学习循环中，注入灰盒结构。

Result: 在分析基准和人类在环车辆悬架调校任务中的数值评估显示，相比基线GLISp，该方法具有更快的收敛速度和更优的最终解。

Conclusion: 该方法成功地将主观偏好反馈与定量传感器信息相结合，在保持偏好搜索灵活性的同时提高了学习效率和性能。

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [4] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: 该论文探讨了将表示学习与因果推断相结合的必要性，特别是在处理多模态数据时，以解决因果任务中的挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模数据收集和表示学习在预测任务中很成功，但在因果任务（如预测干预效果）中表现不佳，因此需要结合表示学习和因果推断。

Method: 提出了一个统计和计算框架，利用多模态数据（观察性和扰动性数据）进行因果发现、学习因果变量和设计最优扰动。

Result: 该框架旨在更有效地利用观察性和扰动性数据，以及多模态系统视图来学习因果变量。

Conclusion: 结合表示学习和因果推断，特别是在多模态数据的背景下，对于解决生物医学中的基本问题至关重要。

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [5] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: 提出了一种可扩展的ROC-SVM变体，通过不完全U统计量显著降低计算复杂度，并扩展到非线性分类，在保持AUC性能的同时大幅减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 原始ROC-SVM虽然能直接最大化AUC，但在类别不平衡情况下计算成本过高（O(n²)），限制了实际应用。

Method: 使用不完全U统计量降低计算复杂度，并通过低秩核近似扩展到非线性分类，在再生核希尔伯特空间中进行高效训练。

Result: 理论分析建立了误差界证明近似合理性，在合成和真实数据集上的实证结果表明，该方法在保持与原始ROC-SVM相当AUC性能的同时，训练时间大幅减少。

Conclusion: 所提出的方法成功解决了ROC-SVM的计算瓶颈问题，为类别不平衡分类提供了高效实用的解决方案。

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [6] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: PuzzleMoE是一种无需训练的MoE模型压缩方法，通过稀疏专家合并和位打包编码技术，实现高达50%的压缩率，同时保持模型精度并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型因存储所有专家参数导致的高内存开销问题，特别是在专家数量增加时。现有方法在高压缩比下往往存在性能下降的问题。

Method: 1. 稀疏专家合并：通过识别权重冗余和专业化，使用双掩码捕获共享和专家特定参数
2. 位打包编码：重用未充分利用的指数位，避免存储二进制掩码和符号的开销

Result: 压缩MoE模型达50%，在各种任务上保持精度。在MMLU任务上50%压缩比下比现有方法提升16.7%，推理速度提升1.28倍。

Conclusion: PuzzleMoE通过创新的稀疏专家合并和位打包编码，实现了高效的MoE模型压缩，在保持精度的同时显著减少内存占用并加速推理。

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [7] [Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models](https://arxiv.org/abs/2511.05460)
*Sarkar Snigdha Sarathi Das,Palash Goyal,Mihir Parmar,Yiwen Song,Long T. Le,Lesly Miculicich,Jinsung Yoon,Rui Zhang,Hamid Palangi,Tomas Pfister*

Main category: cs.LG

TL;DR: 本文提出了Synapse框架，用于仲裁多个预训练时间序列基础模型(TSFMs)的输出，通过动态分配权重和自适应采样来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的TSFMs在不同预测任务、领域和时域上表现差异很大，但如何有效利用这种互补性进行仲裁仍是一个未充分探索的研究领域。

Method: 提出Synapse仲裁框架，动态利用TSFM池，根据上下文相关性能分配和调整预测权重，通过自适应采样构建稳健的预测分布。

Result: 实验结果表明，Synapse在时间序列预测中始终优于其他流行的集成技术以及单个TSFMs。

Conclusion: Synapse框架能够有效利用不同TSFMs的互补专长，在时间序列预测中展现出卓越的性能。

Abstract: Pre-trained Time Series Foundational Models (TSFMs) represent a significant
advance, capable of forecasting diverse time series with complex
characteristics, including varied seasonalities, trends, and long-range
dependencies. Despite their primary goal of universal time series forecasting,
their efficacy is far from uniform; divergent training protocols and data
sources cause individual TSFMs to exhibit highly variable performance across
different forecasting tasks, domains, and horizons. Leveraging this
complementary expertise by arbitrating existing TSFM outputs presents a
compelling strategy, yet this remains a largely unexplored area of research. In
this paper, we conduct a thorough examination of how different TSFMs exhibit
specialized performance profiles across various forecasting settings, and how
we can effectively leverage this behavior in arbitration between different time
series models. We specifically analyze how factors such as model selection and
forecast horizon distribution can influence the efficacy of arbitration
strategies. Based on this analysis, we propose Synapse, a novel arbitration
framework for TSFMs. Synapse is designed to dynamically leverage a pool of
TSFMs, assign and adjust predictive weights based on their relative,
context-dependent performance, and construct a robust forecast distribution by
adaptively sampling from the output quantiles of constituent models.
Experimental results demonstrate that Synapse consistently outperforms other
popular ensembling techniques as well as individual TSFMs, demonstrating
Synapse's efficacy in time series forecasting.

</details>


### [8] [On Flow Matching KL Divergence](https://arxiv.org/abs/2511.05480)
*Maojiang Su,Jerry Yao-Chieh Hu,Sophia Pi,Han Liu*

Main category: cs.LG

TL;DR: 本文推导了流匹配分布近似的KL散度的确定性非渐近上界，证明了当L2流匹配损失有界时，估计分布与真实数据分布的KL散度也有界，从而在总变差距离下实现了流匹配变换器的统计收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究流匹配方法的统计效率，将其与扩散模型在总变差距离下的性能进行比较，证明流匹配在估计平滑分布时能达到近乎极小极大最优效率。

Method: 通过推导流匹配分布近似的KL散度的确定性非渐近上界，建立L2流匹配损失与KL散度之间的定量关系，其中常数仅依赖于数据和速度场的正则性。

Result: 当L2流匹配损失有界于ε²时，真实数据分布与估计分布之间的KL散度有界于A₁ε + A₂ε²，其中A₁和A₂为常数。该结果支持流匹配变换器在总变差距离下的统计收敛率。

Conclusion: 流匹配在总变差距离下具有与扩散模型相当的统计效率，在估计平滑分布时能达到近乎极小极大最优效率，数值研究验证了理论结果。

Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler
(KL) divergence of the flow-matching distribution approximation. In particular,
if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL
divergence between the true data distribution and the estimated distribution is
bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$
depend only on the regularities of the data and velocity fields. Consequently,
this bound implies statistical convergence rates of Flow Matching Transformers
under the Total Variation (TV) distance. We show that, flow matching achieves
nearly minimax-optimal efficiency in estimating smooth distributions. Our
results make the statistical efficiency of flow matching comparable to that of
diffusion models under the TV distance. Numerical studies on synthetic and
learned velocities corroborate our theory.

</details>


### [9] [A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates](https://arxiv.org/abs/2511.04909)
*Paula Rodriguez-Diaz,Kirk Bansak Elisabeth Paulson*

Main category: cs.LG

TL;DR: 提出了Dual-Guided Loss (DGL)，一种用于决策导向学习的简单可扩展方法，通过利用下游问题的对偶变量来指导学习，减少对求解器的依赖，同时保持决策对齐。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多决策都是在不确定性下通过使用预测数量解决优化问题做出的。现有的决策导向学习方法要么需要频繁调用求解器，要么依赖特定任务的替代方法，都面临扩展性挑战。

Method: 利用下游问题的对偶变量来塑造学习，引入DGL目标函数。该方法通过周期性求解下游问题，在刷新间隔期间使用对偶调整的目标进行训练，采用简单的可微分替代损失函数。

Result: DGL在两种问题类别上匹配或超越了最先进的DFL方法，同时使用了更少的求解器调用和显著减少的训练时间。

Conclusion: DGL提供了一种可扩展的决策导向学习方法，随着刷新频率降低，训练成本趋近标准监督学习，同时保持强大的决策对齐能力。

Abstract: Many real-world decisions are made under uncertainty by solving optimization
problems using predicted quantities. This predict-then-optimize paradigm has
motivated decision-focused learning, which trains models with awareness of how
the optimizer uses predictions, improving the performance of downstream
decisions. Despite its promise, scaling is challenging: state-of-the-art
methods either differentiate through a solver or rely on task-specific
surrogates, both of which require frequent and expensive calls to an optimizer,
often a combinatorial one. In this paper, we leverage dual variables from the
downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a
simple, scalable objective that preserves decision alignment while reducing
solver dependence. We construct DGL specifically for combinatorial selection
problems with natural one-of-many constraints, such as matching, knapsack, and
shortest path. Our approach (a) decouples optimization from gradient updates by
solving the downstream problem only periodically; (b) between refreshes, trains
on dual-adjusted targets using simple differentiable surrogate losses; and (c)
as refreshes become less frequent, drives training cost toward standard
supervised learning while retaining strong decision alignment. We prove that
DGL has asymptotically diminishing decision regret, analyze runtime complexity,
and show on two problem classes that DGL matches or exceeds state-of-the-art
DFL methods while using far fewer solver calls and substantially less training
time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.

</details>


### [10] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 本文探讨了自动编码器在数据流形上的拓扑限制和能力，以及其在具有不变流形的动力系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究自动编码器在数据流形上的拓扑特性，理解其在表示学习和动力系统建模中的潜在能力与限制。

Method: 通过分析自动编码器的连续映射对（编码器E和解码器D），研究其在数据流形M上的拓扑性质，特别是D∘E与恒等映射的逼近程度。

Result: 揭示了自动编码器在搜索过程中的各种拓扑限制和能力，并描述了其在具有不变流形的动力系统中的编码能力。

Conclusion: 自动编码器的拓扑结构对其在数据流形上的表示能力和动力系统建模具有重要影响，需要充分考虑这些拓扑特性。

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [11] [BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records](https://arxiv.org/abs/2511.04998)
*Daniel S. Lee,Mayra S. Haedo-Cruz,Chen Jiang,Oshin Miranda,LiRong Wang*

Main category: cs.LG

TL;DR: 提出了BiPETE模型，结合旋转位置嵌入和正弦嵌入来处理电子健康记录中的时间依赖性问题，用于预测酒精和物质使用障碍风险，在两个精神健康队列中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的深度学习模型在电子健康记录疾病风险预测中显示出潜力，但由于不规则就诊间隔和缺乏统一结构，建模时间依赖性仍然是一个关键挑战。

Method: 提出Bi-Positional Embedding Transformer Encoder (BiPETE)模型，整合旋转位置嵌入编码相对就诊时间，正弦嵌入保留就诊顺序，在两个精神健康队列（抑郁障碍和创伤后应激障碍）上训练预测ASUD风险。

Result: BiPETE在抑郁和PTSD队列中分别将精确率-召回率曲线下面积提高了34%和50%，消融研究证实了双重位置编码策略的有效性。

Conclusion: 该研究提出了一个实用且可解释的电子健康记录疾病风险预测框架，能够实现强大的性能，为风险评估过程提供更深入的理解和降低潜在风险的有价值线索。

Abstract: Transformer-based deep learning models have shown promise for disease risk
prediction using electronic health records(EHRs), but modeling temporal
dependencies remains a key challenge due to irregular visit intervals and lack
of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder
or BiPETE for single-disease prediction, which integrates rotary positional
embeddings to encode relative visit timing and sinusoidal embeddings to
preserve visit order. Without relying on large-scale pretraining, BiPETE is
trained on EHR data from two mental health cohorts-depressive disorder and
post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and
substance use disorders (ASUD). BiPETE outperforms baseline models, improving
the area under the precision-recall curve (AUPRC) by 34% and 50% in the
depression and PTSD cohorts, respectively. An ablation study further confirms
the effectiveness of the dual positional encoding strategy. We apply the
Integrated Gradients method to interpret model predictions, identifying key
clinical features associated with ASUD risk and protection, such as abnormal
inflammatory, hematologic, and metabolic markers, as well as specific
medications and comorbidities. Overall, these key clinical features identified
by the attribution methods contribute to a deeper understanding of the risk
assessment process and offer valuable clues for mitigating potential risks. In
summary, our study presents a practical and interpretable framework for disease
risk prediction using EHR data, which can achieve strong performance.

</details>


### [12] [Multi-agent Coordination via Flow Matching](https://arxiv.org/abs/2511.05005)
*Dongsu Lee,Daehee Lee,Amy Zhang*

Main category: cs.LG

TL;DR: MAC-Flow是一个简单而表达力强的多智能体协调框架，通过流式表示学习联合行为，并将其蒸馏为去中心化的一步策略，在保持协调能力的同时实现快速执行。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有效协调的两个要求（丰富的联合行为表示和实时高效行动能力）之间存在权衡：基于去噪扩散的方法能捕捉复杂协调但计算缓慢，而基于高斯策略的方法快速但处理多智能体交互时脆弱。

Method: 首先学习基于流的联合行为表示，然后将其蒸馏为去中心化的一步策略，这些策略在保持协调的同时支持快速执行。

Result: 在4个基准测试（包括12个环境和34个数据集）中，MAC-Flow缓解了性能与计算成本之间的权衡，相比基于扩散的MARL方法推理速度提升约14.5倍，同时保持良好性能，推理速度与先前基于高斯策略的离线MARL方法相当。

Conclusion: MAC-Flow成功解决了多智能体协调中表示能力与计算效率的权衡问题，提供了一种既表达力强又计算高效的解决方案。

Abstract: This work presents MAC-Flow, a simple yet expressive framework for
multi-agent coordination. We argue that requirements of effective coordination
are twofold: (i) a rich representation of the diverse joint behaviors present
in offline data and (ii) the ability to act efficiently in real time. However,
prior approaches often sacrifice one for the other, i.e., denoising
diffusion-based solutions capture complex coordination but are computationally
slow, while Gaussian policy-based solutions are fast but brittle in handling
multi-agent interaction. MAC-Flow addresses this trade-off by first learning a
flow-based representation of joint behaviors, and then distilling it into
decentralized one-step policies that preserve coordination while enabling fast
execution. Across four different benchmarks, including $12$ environments and
$34$ datasets, MAC-Flow alleviates the trade-off between performance and
computational cost, specifically achieving about $\boldsymbol{\times14.5}$
faster inference compared to diffusion-based MARL methods, while maintaining
good performance. At the same time, its inference speed is similar to that of
prior Gaussian policy-based offline multi-agent reinforcement learning (MARL)
methods.

</details>


### [13] [Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches](https://arxiv.org/abs/2511.04845)
*Jingchen Bi,Rodrigo Mesa-Arango*

Main category: cs.LG

TL;DR: 使用机器学习模型分析美国消费者对具有创新运输证书食品的偏好，重点关注运输安全、能源等证书的影响。


<details>
  <summary>Details</summary>
Motivation: 基于先前研究发现运输因素在消费者食品购买选择中具有显著影响，需要进一步识别消费者重视的具体运输属性。

Method: 采用机器学习模型分析消费者偏好实验数据，提出五种创新运输证书：运输模式、物联网、安全措施、能源来源和必须到达日期。

Result: 研究发现消费者对运输领域的安全和能源证书有显著偏好，同时价格、产品类型、证书和决策者因素都会影响购买选择。

Conclusion: 研究为改进食品供应链系统提供了数据驱动的建议，强调运输安全和能源相关证书的重要性。

Abstract: This paper utilizes a machine learning model to estimate the consumer's
behavior for food products with innovative transportation certificates in the
U.S. Building on previous research that examined demand for food products with
supply chain traceability using stated preference analysis, transportation
factors were identified as significant in consumer food purchasing choices.
Consequently, a second experiment was conducted to pinpoint the specific
transportation attributes valued by consumers. A machine learning model was
applied, and five innovative certificates related to transportation were
proposed: Transportation Mode, Internet of Things (IoT), Safety measures,
Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also
incorporated product-specific and decision-maker factors for control purposes.
The findings reveal a notable inclination toward safety and energy certificates
within the transportation domain of the U.S. food supply chain. Additionally,
the study examined the influence of price, product type, certificates, and
decision-maker factors on purchasing choices. Ultimately, the study offers
data-driven recommendations for improving food supply chain systems.

</details>


### [14] [ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids](https://arxiv.org/abs/2511.05420)
*Emad Efatinasab,Nahal Azadi,Davide Dalle Pezze,Gian Antonio Susto,Chuadhry Mujeeb Ahmed,Mirco Rampazzo*

Main category: cs.LG

TL;DR: 提出了一种用于智能电网故障预测的持续学习框架ProDER，通过原型特征正则化、logit蒸馏和原型引导重放，在四种现实评估场景中实现了最佳性能，准确率下降仅0.045（故障类型）和0.015（故障区域）。


<details>
  <summary>Details</summary>
Motivation: 现有AI故障预测模型在适应新故障类型和操作区域时可靠性不足，需要能够随环境演化的持续学习解决方案。

Method: 设计基于原型重放的持续学习方法ProDER，结合原型特征正则化、logit蒸馏和原型引导重放记忆机制。

Result: ProDER在测试的持续学习技术中表现最佳，故障类型预测准确率下降仅0.045，故障区域预测下降0.015。

Conclusion: 证明了持续学习在智能电网可扩展、现实世界故障预测中的实用性。

Abstract: As smart grids evolve to meet growing energy demands and modern operational
challenges, the ability to accurately predict faults becomes increasingly
critical. However, existing AI-based fault prediction models struggle to ensure
reliability in evolving environments where they are required to adapt to new
fault types and operational zones. In this paper, we propose a continual
learning (CL) framework in the smart grid context to evolve the model together
with the environment. We design four realistic evaluation scenarios grounded in
class-incremental and domain-incremental learning to emulate evolving grid
conditions. We further introduce Prototype-based Dark Experience Replay
(ProDER), a unified replay-based approach that integrates prototype-based
feature regularization, logit distillation, and a prototype-guided replay
memory. ProDER achieves the best performance among tested CL techniques, with
only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone
prediction. These results demonstrate the practicality of CL for scalable,
real-world fault prediction in smart grids.

</details>


### [15] [FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting](https://arxiv.org/abs/2511.04865)
*Esha Sharma,Lauren Davis,Julie Ivy,Min Chi*

Main category: cs.LG

TL;DR: FoodRL是一个基于强化学习的元学习框架，用于预测食品银行的捐赠波动，通过动态加权多个预测模型来提高准确性，特别是在自然灾害等干扰时期表现优异。


<details>
  <summary>Details</summary>
Motivation: 食品银行在缓解粮食不安全方面至关重要，但传统预测模型难以处理高度波动的实物捐赠数据，特别是在季节性变化和自然灾害（如飓风、野火）导致的波动和概念漂移情况下。

Method: 提出FoodRL框架，使用强化学习进行元学习，基于近期表现和上下文信息对不同的预测模型进行聚类和动态加权。

Result: 在两个结构不同的美国食品银行（西海岸受野火影响的大型区域食品银行和东海岸持续受飓风影响的州级食品银行）的多年度数据上评估，FoodRL始终优于基线方法，特别是在干扰或衰退时期。

Conclusion: FoodRL通过提供更可靠和自适应的预测，每年可促进相当于170万份额外餐食的食品重新分配，展示了其在人道主义供应链中自适应集成学习的巨大潜力和社会影响力。

Abstract: Food banks are crucial for alleviating food insecurity, but their
effectiveness hinges on accurately forecasting highly volatile in-kind
donations to ensure equitable and efficient resource distribution. Traditional
forecasting models often fail to maintain consistent accuracy due to
unpredictable fluctuations and concept drift driven by seasonal variations and
natural disasters such as hurricanes in the Southeastern U.S. and wildfires in
the West Coast. To address these challenges, we propose FoodRL, a novel
reinforcement learning (RL) based metalearning framework that clusters and
dynamically weights diverse forecasting models based on recent performance and
contextual information. Evaluated on multi-year data from two structurally
distinct U.S. food banks-one large regional West Coast food bank affected by
wildfires and another state-level East Coast food bank consistently impacted by
hurricanes, FoodRL consistently outperforms baseline methods, particularly
during periods of disruption or decline. By delivering more reliable and
adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent
to 1.7 million additional meals annually, demonstrating its significant
potential for social impact as well as adaptive ensemble learning for
humanitarian supply chains.

</details>


### [16] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 提出加速路径修补(APP)方法，通过对比性注意力头剪枝技术大幅减少电路发现的计算成本，相比传统路径修补方法提速59.63%-93.27%，同时保持电路性能。


<details>
  <summary>Details</summary>
Motivation: 当前电路发现方法如路径修补计算成本高，限制了在小模型上的深度电路分析，需要更高效的电路发现方法。

Method: 提出加速路径修补(APP)方法，首先使用对比性FLAP剪枝算法减少搜索空间，然后对剩余注意力头应用传统路径修补。

Result: APP平均减少56%的搜索空间，计算速度提升59.63%-93.27%，发现的电路与原始路径修补电路有显著重叠且性能相似。

Conclusion: APP方法在保持电路性能的同时大幅降低了电路发现的计算成本，为机制可解释性研究提供了更高效的解决方案。

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


### [17] [Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning](https://arxiv.org/abs/2511.04883)
*Di Chen,Jia Li,Michael Zhang*

Main category: cs.LG

TL;DR: 研究表明，在混合自动驾驶交通系统中，即使自动驾驶车辆出于自利动机，也能通过深度强化学习实现集体理性，为所有驾驶主体带来整体利益。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车即将商业化，混合交通系统将包含自动驾驶车辆和人类驾驶车辆。现有研究表明自动驾驶车辆可以通过系统级目标优化整体交通性能，但尚不清楚当所有驾驶主体（包括人类和自动驾驶车辆）都出于自利动机时，这种效益是否仍然存在。

Method: 采用深度强化学习训练自利的交通主体，使用简单的奖励设计，不直接包含系统级目标。基于博弈论和行为经济学的集体理性概念，在微观动态环境中验证集体理性的出现机制。

Result: 在各种场景中，集体理性持续出现，表明这一特性具有鲁棒性。研究还提出了解释集体理性在微观动态环境中出现的机制，并通过仿真证据进行了验证。

Conclusion: 这项研究表明，可以利用先进的学习方法（如联邦学习）在混合自动驾驶系统中实现自利驾驶主体之间的集体合作，为未来智能交通系统设计提供了重要启示。

Abstract: Autonomous vehicles (AVs) are expected to be commercially available in the
near future, leading to mixed autonomy traffic consisting of both AVs and
human-driven vehicles (HVs). Although numerous studies have shown that AVs can
be deployed to benefit the overall traffic system performance by incorporating
system-level goals into their decision making, it is not clear whether the
benefits still exist when agents act out of self-interest -- a trait common to
all driving agents, both human and autonomous. This study aims to understand
whether self-interested AVs can bring benefits to all driving agents in mixed
autonomy traffic systems. The research is centered on the concept of collective
rationality (CR). This concept, originating from game theory and behavioral
economics, means that driving agents may cooperate collectively even when
pursuing individual interests. Our recent research has proven the existence of
CR in an analytical game-theoretical model and empirically in mixed
human-driven traffic. In this paper, we demonstrate that CR can be attained
among driving agents trained using deep reinforcement learning (DRL) with a
simple reward design. We examine the extent to which self-interested traffic
agents can achieve CR without directly incorporating system-level objectives.
Results show that CR consistently emerges in various scenarios, which indicates
the robustness of this property. We also postulate a mechanism to explain the
emergence of CR in the microscopic and dynamic environment and verify it based
on simulation evidence. This research suggests the possibility of leveraging
advanced learning methods (such as federated learning) to achieve collective
cooperation among self-interested driving agents in mixed-autonomy systems.

</details>


### [18] [Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application](https://arxiv.org/abs/2511.04918)
*A. Ganapathi Rao,Sathish Krishna Anumula,Aditya Kumar Singh,Renukhadevi M,Y. Jeevan Nagendra Kumar,Tammineni Rama Tulasi*

Main category: cs.LG

TL;DR: 本文研究机器学习算法与传统统计模型的整合方式，探讨现代ML算法如何丰富传统模型，展示混合模型在预测准确性、鲁棒性和可解释性方面的显著改进。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习与统计模型的新型整合方式，改变数据分析、预测分析和决策制定的传统方法。

Method: 研究ML与统计模型的连接方式，展示现代ML算法如何改进传统模型的性能、规模、灵活性和鲁棒性。

Result: 混合模型在预测准确性、鲁棒性和可解释性方面表现出显著改进。

Conclusion: 机器学习与传统统计模型的整合为数据分析领域带来了革命性的进步，混合模型在多个关键指标上都有显著提升。

Abstract: It involves the completely novel ways of integrating ML algorithms with
traditional statistical modelling that has changed the way we analyze data, do
predictive analytics or make decisions in the fields of the data. In this
paper, we study some ML and statistical model connections to understand ways in
which some modern ML algorithms help 'enrich' conventional models; we
demonstrate how new algorithms improve performance, scale, flexibility and
robustness of the traditional models. It shows that the hybrid models are of
great improvement in predictive accuracy, robustness, and interpretability

</details>


### [19] [Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2511.04971)
*Esha Chowdhury*

Main category: cs.LG

TL;DR: 本研究提出了一种使用机器学习和混合深度学习方法的糖尿病患者心血管疾病风险预测模型，在BRFSS数据集上实现了最高0.9050的准确率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病患病率不断上升且与心脏病密切相关，需要开发高效的心血管疾病风险预测模型来改善医疗决策。

Method: 使用BRFSS数据集，进行数据预处理（去重、处理缺失值、识别特征类别）和主成分分析特征提取。实现多种机器学习模型（决策树、随机森林、KNN、SVM、AdaBoost、XGBoost）和深度学习模型（ANN、DNN、RNN、CNN、LSTM、BiLSTM、GRU）以及混合模型（CNN与LSTM、BiLSTM、GRU的组合）。

Result: XGBoost和LSTM模型均达到最高准确率0.9050，部分模型实现了完美的召回率（1.00）。

Conclusion: 机器学习和深度学习模型在预测糖尿病患者心血管疾病风险方面非常有效，能够自动化和增强临床决策，高准确率和F1分数表明这些模型有潜力改善个性化风险管理和预防策略。

Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for
healthcare institutions. This study addresses the growing prevalence of
diabetes and its strong link to heart disease by proposing an efficient CVD
risk prediction model for diabetic patients using machine learning (ML) and
hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by
removing duplicates, handling missing values, identifying categorical and
numerical features, and applying Principal Component Analysis (PCA) for feature
extraction. Several ML models, including Decision Trees (DT), Random Forest
(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and
XGBoost, were implemented, with XGBoost achieving the highest accuracy of
0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep
Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and
Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,
BiLSTM, and GRU, were also explored. Some of these models achieved perfect
recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.
Our research highlights the effectiveness of ML and DL models in predicting CVD
risk among diabetic patients, automating and enhancing clinical
decision-making. High accuracy and F1 scores demonstrate these models'
potential to improve personalized risk management and preventive strategies.

</details>


### [20] [Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk](https://arxiv.org/abs/2511.04980)
*Rongbin Ye,Jiaqi Chen*

Main category: cs.LG

TL;DR: 本文研究了金融行业中机器学习模型的可解释性问题，提出了一个五维框架来评估模型可解释性，并证明了使用SHAP和LIME等现代可解释性技术可以在保持高性能的同时满足监管要求。


<details>
  <summary>Details</summary>
Motivation: 金融行业面临在先进机器学习模型的预测能力与监管机构要求的可解释性之间取得平衡的挑战，需要填补"黑盒"模型与可解释性框架之间的应用空白。

Method: 应用LIME和SHAP等可解释性框架于不同模型，并提出了一个包含内在可解释性、全局解释、局部解释、一致性和复杂性的五维评估框架。

Result: 研究表明，具有更好预测能力的更复杂模型可以通过SHAP和LIME达到相同的可解释性水平，证明了在受监管金融环境中使用高性能ML模型的可行性。

Conclusion: 通过利用现代可解释性技术，可以在模型性能与可解释性之间实现平衡，为受监管金融环境应用复杂ML模型提供了结构化评估方法。

Abstract: The financial industry faces a significant challenge modeling and risk
portfolios: balancing the predictability of advanced machine learning models,
neural network models, and explainability required by regulatory entities (such
as Office of the Comptroller of the Currency, Consumer Financial Protection
Bureau). This paper intends to fill the gap in the application between these
"black box" models and explainability frameworks, such as LIME and SHAP.
Authors elaborate on the application of these frameworks on different models
and demonstrates the more complex models with better prediction powers could be
applied and reach the same level of the explainability, using SHAP and LIME.
Beyond the comparison and discussion of performances, this paper proposes a
novel five dimensional framework evaluating Inherent Interpretability, Global
Explanations, Local Explanations, Consistency, and Complexity to offer a
nuanced method for assessing and comparing model explainability beyond simple
accuracy metrics. This research demonstrates the feasibility of employing
sophisticated, high performing ML models in regulated financial environments by
utilizing modern explainability techniques and provides a structured approach
to evaluate the crucial trade offs between model performance and
interpretability.

</details>


### [21] [Deep Progressive Training: scaling up depth capacity of zero/one-layer models](https://arxiv.org/abs/2511.04981)
*Zhiqi Bu*

Main category: cs.LG

TL;DR: 本文研究了深度模型训练中的渐进式训练策略，通过优化理论和特征学习视角分析深度扩展，提出零/一层渐进训练方法，在GPT2上可节省约80%计算成本或加速5倍，同时保持几乎相同的性能。


<details>
  <summary>Details</summary>
Motivation: 模型深度在深度学习中具有双重性：更深的模型能获得更高准确率但需要更高计算成本。为了高效训练大规模模型，需要寻找在减少计算开销的同时不牺牲性能的方法。

Method: 通过优化理论和特征学习研究深度扩展，提出零/一层渐进训练策略，包括新层初始化、超参数迁移、学习率调度和模型扩展时机等关键技术。

Result: 在GPT2模型上的实验表明，零/一层渐进训练可节省约80%计算成本或实现约5倍加速，同时达到与完全训练的60层70亿参数模型几乎相同的损失值。

Conclusion: 渐进式训练是平衡计算效率和模型性能的有效策略，特别是在大规模模型训练中，零/一层渐进训练提供了计算与损失之间的最优权衡。

Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve
higher accuracy but require higher computational cost. To efficiently train
models at scale, an effective strategy is the progressive training, which
scales up model capacity during training, hence significantly reducing
computation with little to none performance degradation. In this work, we study
the depth expansion of large models through the lens of optimization theory and
feature learning, offering insights on the initialization of new layers,
hyperparameter transfer, learning rate schedule, and timing of model expansion.
Specifically, we propose zero/one-layer progressive training for the optimal
tradeoff between computation and loss. For example, zero/one-layer progressive
training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate
$\approx 5\times$ while achieving almost the same loss, compared to to a fully
trained 60-layer model with 7B parameters.

</details>


### [22] [QuAnTS: Question Answering on Time Series](https://arxiv.org/abs/2511.05124)
*Felix Divo,Maurice Kraus,Anh Q. Nguyen,Hao Xue,Imran Razzak,Flora D. Salim,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: 提出了一个新颖的时间序列问答数据集QuAnTS，专注于人体运动骨架轨迹数据，填补了时间序列问答研究领域的空白，并建立了基准模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 文本能够直观地提供信息，可以补充数值时间序列的密度，从而改善与时间序列模型的交互，增强可访问性和决策能力。当前问答研究主要集中在视觉和文本领域，时间序列问答研究严重不足。

Method: 创建了大规模的时间序列问答数据集QuAnTS，包含关于人体运动骨架轨迹的各种问答对，并通过广泛实验验证数据集的完整性和质量，同时评估现有和新提出的基线模型。

Result: 验证了QuAnTS数据集结构良好且全面，为时间序列问答研究奠定了基础，并提供了人类表现作为模型实用性的重要参考基准。

Conclusion: 希望通过QuAnTS数据集促进未来通过文本与时间序列模型交互的研究，实现更好的决策制定和更透明的系统。

Abstract: Text offers intuitive access to information. This can, in particular,
complement the density of numerical time series, thereby allowing improved
interactions with time series models to enhance accessibility and
decision-making. While the creation of question-answering datasets and models
has recently seen remarkable growth, most research focuses on question
answering (QA) on vision and text, with time series receiving minute attention.
To bridge this gap, we propose a challenging novel time series QA (TSQA)
dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we
pose a wide variety of questions and answers about human motion in the form of
tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is
well-formed and comprehensive through extensive experiments. Thoroughly
evaluating existing and newly proposed baselines then lays the groundwork for a
deeper exploration of TSQA using QuAnTS. Additionally, we provide human
performances as a key reference for gauging the practical usability of such
models. We hope to encourage future research on interacting with time series
models through text, enabling better decision-making and more transparent
systems.

</details>


### [23] [Attention and Compression is all you need for Controllably Efficient Language Models](https://arxiv.org/abs/2511.05313)
*Jatin Prakash,Aahlad Puli,Rajesh Ranganath*

Main category: cs.LG

TL;DR: CAT（压缩与注意力变换器）是一种高效Transformer架构，通过压缩序列块和密集注意力机制，在保持质量的同时显著降低计算和内存成本，支持测试时自适应调整质量-计算权衡。


<details>
  <summary>Details</summary>
Motivation: 现有高效Transformer方法（如稀疏注意力、滑动窗口等）在减少计算和内存时往往牺牲了上下文召回性能，且需要复杂的启发式选择或混合架构设计，无法灵活适应不同应用需求。

Method: 使用两个简单组件：密集注意力和压缩机制。通过将序列分块压缩后解码，从压缩后的序列长度进行注意力计算，从而减少计算和内存需求。支持多块大小联合训练，实现测试时自适应调整。

Result: 在语言建模、上下文召回和长上下文理解任务中，单一自适应CAT模型在不同计算-内存预算下均优于现有高效基线，包括混合架构。与密集Transformer相比，CAT在保持相同语言建模性能的同时，速度提升1.4-3倍，总内存使用降低2-9倍。

Conclusion: CAT提供了一种简单而有效的解决方案，通过压缩和密集注意力实现了质量与效率的良好平衡，支持测试时灵活调整，无需重新训练即可适应不同应用需求。

Abstract: The quadratic cost of attention in transformers motivated the development of
efficient approaches: namely sparse and sliding window attention, convolutions
and linear attention. Although these approaches result in impressive reductions
in compute and memory, they often trade-off with quality, specifically
in-context recall performance. Moreover, apriori fixing this quality-compute
tradeoff means being suboptimal from the get-go: some downstream applications
require more memory for in-context recall, while others require lower latency
and memory. Further, these approaches rely on heuristic choices that
artificially restrict attention, or require handcrafted and complex recurrent
state update rules, or they must be carefully composed with attention at
specific layers to form a hybrid architecture that complicates the design
process, especially at scale. To address above issues, we propose Compress &
Attend Transformer (CAT), a conceptually simple architecture employing two
simple ingredients only: dense attention and compression. CAT decodes chunks of
tokens by attending to compressed chunks of the sequence so far. Compression
results in decoding from a reduced sequence length that yields compute and
memory savings, while choosing a particular chunk size trades-off quality for
efficiency. Moreover, CAT can be trained with multiple chunk sizes at once,
unlocking control of quality-compute trade-offs directly at test-time without
any retraining, all in a single adaptive architecture. In exhaustive
evaluations on common language modeling tasks, in-context recall, and
long-context understanding, a single adaptive CAT model outperforms existing
efficient baselines, including hybrid architectures, across different
compute-memory budgets. Further, a single CAT matches dense transformer in
language modeling across model scales while being 1.4-3x faster and requiring
2-9x lower total memory usage.

</details>


### [24] [Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval](https://arxiv.org/abs/2511.05325)
*Janet Jenq,Hongda Shen*

Main category: cs.LG

TL;DR: 该论文提出一种新颖方法，通过在商品图像上直接渲染相关文本内容（如标题、描述）来执行视觉-文本压缩，从而增强图像-文本对齐并提升多模态商品检索性能。


<details>
  <summary>Details</summary>
Motivation: 多模态商品检索系统依赖视觉和文本信号的结合，但像CLIP这样的视觉语言模型容易受到排版攻击的影响，其中嵌入图像的误导性或无关文本会扭曲模型预测。

Method: 提出一种反转排版攻击逻辑的方法，将相关文本内容直接渲染到商品图像上，进行视觉-文本压缩，从而加强图像-文本对齐。

Result: 在三个垂直电商数据集（运动鞋、手提包和交易卡）上使用六种最先进的视觉基础模型进行评估，实验显示在单模态和多模态检索准确性方面均获得一致改进。

Conclusion: 视觉渲染商品元数据是电商应用中零样本多模态检索的一种简单而有效的增强方法。

Abstract: Multimodal product retrieval systems in e-commerce platforms rely on
effectively combining visual and textual signals to improve search relevance
and user experience. However, vision-language models such as CLIP are
vulnerable to typographic attacks, where misleading or irrelevant text embedded
in images skews model predictions. In this work, we propose a novel method that
reverses the logic of typographic attacks by rendering relevant textual content
(e.g., titles, descriptions) directly onto product images to perform
vision-text compression, thereby strengthening image-text alignment and
boosting multimodal product retrieval performance. We evaluate our method on
three vertical-specific e-commerce datasets (sneakers, handbags, and trading
cards) using six state-of-the-art vision foundation models. Our experiments
demonstrate consistent improvements in unimodal and multimodal retrieval
accuracy across categories and model families. Our findings suggest that
visually rendering product metadata is a simple yet effective enhancement for
zero-shot multimodal retrieval in e-commerce applications.

</details>


### [25] [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](https://arxiv.org/abs/2511.05330)
*Jan-Hendrik Ewering,Robin E. Herrmann,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: cs.LG

TL;DR: 该论文提出了一种从输入-输出数据学习非保守哈密顿系统的方法，使用高斯过程回归结合哈密顿动力学，无需速度或动量测量数据，并提供了完全贝叶斯框架来估计隐藏状态和超参数。


<details>
  <summary>Details</summary>
Motivation: 在基于学习的方法中嵌入能量守恒等非限制性先验知识，从有限数据构建物理一致的模型，特别是用于模型预测控制。现有方法依赖速度或动量数据，但这些在实践中很少可用。

Method: 使用非保守哈密顿高斯过程进行动力学学习，采用降秩GP近似降低计算复杂度，提供完全贝叶斯方案估计隐藏状态概率密度、GP超参数和结构超参数（如阻尼系数）。

Result: 在非线性仿真案例研究中评估了所提方法，并与依赖动量测量的最先进方法进行比较。

Conclusion: 该方法能够从输入-输出数据学习哈密顿系统动力学，无需速度或动量测量，计算效率高，为物理一致的模型构建提供了实用解决方案。

Abstract: Embedding non-restrictive prior knowledge, such as energy conservation laws,
in learning-based approaches is a key motive to construct physically consistent
models from limited data, relevant for, e.g., model-based control. Recent work
incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to
obtain uncertainty-quantifying models that adhere to the underlying physical
principles. However, these works rely on velocity or momentum data, which is
rarely available in practice. In this paper, we consider dynamics learning with
non-conservative Hamiltonian GPs, and address the more realistic problem
setting of learning from input-output data. We provide a fully Bayesian scheme
for estimating probability densities of unknown hidden states, of GP
hyperparameters, as well as of structural hyperparameters, such as damping
coefficients. Considering the computational complexity of GPs, we take
advantage of a reduced-rank GP approximation and leverage its properties for
computationally efficient prediction and training. The proposed method is
evaluated in a nonlinear simulation case study and compared to a
state-of-the-art approach that relies on momentum measurements.

</details>


### [26] [SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning](https://arxiv.org/abs/2511.05462)
*Xiaodong Wang,Jing Huang,Kevin J Liang*

Main category: cs.LG

TL;DR: 本文通过将无监督聚类方法与经典统计混合模型建立联系，提出了SiamMM模型，在自监督学习基准测试中达到最先进性能，并发现学习到的聚类与真实标签高度相似，揭示了潜在的标签错误。


<details>
  <summary>Details</summary>
Motivation: 虽然基于聚类的方法在自监督和无监督学习中表现出色，但应用方式往往具有启发性，最优方法尚不明确。本文旨在建立无监督聚类方法与统计混合模型的理论联系。

Method: 通过将无监督聚类方法与经典混合模型建立理论框架，开发了名为SiamMM的新模型，改进了现有的聚类方法。

Result: SiamMM模型在各种自监督学习基准测试中达到了最先进的性能，学习到的聚类与未见过的真实标签高度相似，并发现了潜在的标签错误实例。

Conclusion: 通过将无监督聚类方法与统计混合模型相结合，可以显著提升聚类方法的性能，SiamMM模型在自监督学习中表现出色，同时揭示了数据集中可能存在的标签质量问题。

Abstract: Recent studies have demonstrated the effectiveness of clustering-based
approaches for self-supervised and unsupervised learning. However, the
application of clustering is often heuristic, and the optimal methodology
remains unclear. In this work, we establish connections between these
unsupervised clustering methods and classical mixture models from statistics.
Through this framework, we demonstrate significant enhancements to these
clustering methods, leading to the development of a novel model named SiamMM.
Our method attains state-of-the-art performance across various self-supervised
learning benchmarks. Inspection of the learned clusters reveals a strong
resemblance to unseen ground truth labels, uncovering potential instances of
mislabeling.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [27] [OPF-Based Optimal Power System Network Restoration Considering Frequency Dynamics](https://arxiv.org/abs/2511.04777)
*Dawn Virginillo,Asja Derviškadić,Mario Paolone*

Main category: eess.SP

TL;DR: 本文提出并解决了包含频率动态的最优电力系统恢复问题，验证了静态约束的全局最优性，并在IEEE 9-Bus模型上证明静态最优切换序列会违反动态约束，强调了动态考虑在PSR规划中的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于全球电网停电和系统分裂事件频发，以及能源转型带来的系统复杂性增加，需要重新评估现有的电力系统恢复计划，特别是在低惯量孤岛条件下确保系统的动态稳定性。

Method: 提出包含频率动态的最优PSR问题公式，使用暴力树搜索方法验证静态版本中切换约束的全局最优性，并将动态问题公式应用于IEEE 9-Bus模型。

Result: 静态公式的最优切换序列会违反动态约束，表明动态考虑在PSR规划中的必要性。

Conclusion: 动态稳定性考虑在电力系统恢复规划中至关重要，仅基于静态优化的恢复计划可能无法满足系统的动态性能要求。

Abstract: Due to recent blackout and system split incidents in power grids worldwide,
as well as increased system complexity in view of the energy transition, there
has been increasing interest in re-evaluating existing Power System Restoration
(PSR) plans. In restoration scenarios, due to low island inertia, it is
necessary to ensure not only the static, but also the dynamic stability of the
system. In this paper, we pose and solve a formulation of the optimal PSR
problem including frequency dynamics. We validate the switching constraints for
global optimality within a static version of the formulation using a
brute-force tree search method. We apply the dynamic problem formulation to the
IEEE 9-Bus model, and show that the optimal switching sequence using the static
formulation would violate dynamic constraints, illustrating the importance of
dynamic considerations in PSR planning.

</details>


### [28] [Joint Power Allocation and Radiation Optimization in NOMA-Assisted Pinching Antenna Systems](https://arxiv.org/abs/2511.04861)
*Nikoloz Vashakidze,Chadi Assi,Mohamed Elhattab,Ali Ghrayeb,Maurice J. Khabbaz*

Main category: eess.SP

TL;DR: 本文研究了在采用非正交多址接入(NOMA)的Pinching天线系统(PASS)中，联合优化发射功率分配和辐射系数的问题，通过分解为两个子问题并使用交替优化方法，显著提高了系统总速率。


<details>
  <summary>Details</summary>
Motivation: 利用PASS的灵活信道调整能力和NOMA的功率分配适应性，解决下行链路中的总速率最大化问题，克服传统功率分配方案的局限性。

Method: 将主问题分解为发射功率分配子问题和PA辐射功率分配子问题，前者使用闭式解，后者采用逐次凸近似(SCA)，并通过交替优化(AO)方法迭代求解。

Result: 数值结果表明，与广泛采用的等功率分配PASS方案相比，所提出的方法显著提高了系统总速率。

Conclusion: 提出的联合优化方法在NOMA辅助的PASS系统中有效提升了性能，并考虑了不同复杂度的解码顺序策略以适应实际应用需求。

Abstract: This paper explores a joint optimization of transmit power allocation and
radiation coefficients in a downlink Pinching Antenna SyStem (PASS) employing
Non-Orthogonal Multiple Access (NOMA). By leveraging the PASS-enabled flexible
channel adjustment and NOMA's power allocation adaptability, a sum rate
maximization problem is formulated with the objective of simultaneously
optimizing base station (BS)'s transmit power coefficients and pinching antenna
(PA)'s radiation powers. Due to its non-convexity and complexity, the
formulated optimization problem is challenging to solve directly. Hence, we
decompose the main problem into two sub-problems, namely transmit power
allocation sub-problem and PA radiation power allocation sub-problem. In the
first sub-problem, closed-form solutions are derived for the BS's power
allocation among NOMA users. Meanwhile, in the second sub-problem, we optimize
the PA's radiation power utilizing successive convex approximation (SCA). These
two sub-problems are solved alternatively using Alternating Optimization (AO)
until convergence. It should be noted that decoding order plays a significant
role in NOMA-assisted PASS. Hence, two variations of decoding order are
considered, namely: i) a high-complexity exhaustive search approach, and, ii) a
low-complexity alternative that utilizes pre-determined channel information.
Numerical results show that our proposed approach substantially improves the
system's sum-rate compared to widely adopted equal power allocation PASS
schemes.

</details>


### [29] [Two-timescale Beamforming Optimization for Downlink Multi-user Holographic MIMO Surfaces](https://arxiv.org/abs/2511.04908)
*Haochen Wu,Yuanbin Chen,Yang Ming,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 提出了一种两时间尺度优化方案，通过统计CSI优化波束模式，利用瞬时CSI设计预编码矩阵，以降低HMIMOS系统的计算复杂度和信令开销。


<details>
  <summary>Details</summary>
Motivation: HMIMOS虽然能实现精确波束成形，但大量辐射元件导致传统基于瞬时CSI的波束成形方案计算复杂度和信令开销过高。

Method: 使用两时间尺度优化：基于统计CSI通过CSSCA算法优化BS和UE的波束模式，然后利用瞬时CSI设计预编码矩阵。

Result: 仿真结果表明所提方法相比其他基线方案具有更好的性能。

Conclusion: 所提出的两时间尺度优化方案能有效降低HMIMOS系统的计算复杂度和信令开销，同时保证系统性能。

Abstract: Benefiting from the rapid development of metamaterials and metasurfaces, the
holographic multiple-input and multiple-output surface (HMIMOS) has been
regarded as a promising solution for future wireless networks recently. By
densely packing numerous radiation elements together, HMIMOS is capable of
realizing accurate beamforming with low hardware complexity. However, enormous
radiation elements on the HMIMOS lead to high computational complexity and
signaling overhead when applying traditional beamforming schemes relying on
instantaneous channel state information (CSI). To tackle this problem, we
propose a two-timescale optimization scheme to minimize the required
transmission power under the constraint of all users' quality-of-service (QoS).
Specifically, the beampatterns at the base station (BS) and the user equippment
(UE) are optimized over the slowly changing statistical CSI based on the
constrained stochastic successive convex approximation (CSSCA) algorithm. Then,
the instantaneous CSI is utilized to design the precoding matrix in order to
ensure the system performance without significant increase in computational
cost, due to the small number of feeds on the HMIMOS. Simulation results
demonstrate the effectiveness of our proposed method compared to other
baselines.

</details>


### [30] [Near-optimal Reconfigurable Intelligent Surface Configuration: Blind Beamforming with Sensing](https://arxiv.org/abs/2511.05132)
*Son Dinh-Van,Nam Phuong Tran,Matthew D. Higgins*

Main category: eess.SP

TL;DR: 提出了一种名为BORN的盲波束赋形算法，仅使用接收信号强度(RSS)配置可重构智能表面(RIS)，无需信道状态信息或几何模型。该算法通过感知阶段估计二次模型，优化阶段获得RIS配置，在样本复杂度O(N log₂(N))下实现近最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有盲波束赋形方法依赖多数投票机制，而BORN利用接收信噪比(SNR)的固有二次结构，旨在实现更优性能，特别是在弱背景信道场景下。

Method: BORN算法分为两个阶段：感知阶段从RSS测量中估计二次模型，优化阶段使用估计的二次模型获得RIS配置。算法利用Rademacher特征分布下的二次模型可学习性。

Result: 理论分析表明BORN仅需O(N log₂(N))样本即可实现近最优性能。仿真和实地测试显示，BORN显著优于现有盲波束赋形算法，在非视距(NLOS)等弱背景信道场景下表现尤为突出。

Conclusion: BORN算法通过利用SNR的二次结构，在无需CSI的情况下实现了高效的RIS配置，为盲波束赋形提供了新的理论保证和实际性能提升。

Abstract: Blind beamforming has emerged as a promising approach to configure
reconfigurable intelligent surfaces (RISs) without relying on channel state
information (CSI) or geometric models, making it directly compatible with
commodity hardware. In this paper, we propose a new blind beamforming
algorithm, so-called Blind Optimal RIS Beamforming with Sensing
(\textsc{BORN}), that operates using only received signal strength (RSS). In
contrast to existing methods that rely on majority-voting mechanisms,
\textsc{BORN} exploits the intrinsic quadratic structure of the received
signal-to-noise ratio (SNR). The algorithm proceeds in two stages:
\emph{sensing}, where a quadratic model is estimated from RSS measurements, and
\emph{optimization}, where the RIS configuration is obtained using the
estimated quadratic model. Our novelties are twofold. Firstly, we show for the
first time, that \textsc{BORN} can achieve provable near-optimal performance
using only $O(N \log_2(N))$ samples, where $N$ is the number of RIS elements.
As a by-product of our analysis, we show that quadratic models are learnable
under Rademacher feature distributions when the second-order coefficient matrix
is low-rank. This result, to our knowledge, has not been established in prior
matrix sensing literature. Secondly, extensive simulations and real-world field
tests demonstrate that \textsc{BORN} achieves near-optimal performance,
substantially outperforming state-of-the-art blind beamforming algorithms,
particularly in scenarios with a weak background channel such as
non-line-of-sight (NLOS).

</details>


### [31] [Look Before Switch: Sensing-Assisted Handover in 5G NR V2I Networks](https://arxiv.org/abs/2511.05195)
*Yunxin Li,Fan Liu,Haoqiu Xiong,Zhenkun Wang,Narengerile,Christos Masouros*

Main category: eess.SP

TL;DR: 本文提出了一种基于集成感知与通信(ISAC)的感知辅助切换框架，通过距离和概率两种算法实现精确波束成形和主动切换决策，显著减少信令开销和切换中断时间。


<details>
  <summary>Details</summary>
Motivation: 解决5G NR车联网(V2I)通信中高移动性场景的挑战，传统切换方法需要上行反馈和波束扫描，导致信令开销大和中断时间长。

Method: 开发了两种感知使能的切换触发算法：基于距离的方案利用估计的空间定位，基于概率的方法使用交互多模型扩展卡尔曼滤波(IMM-EKF)跟踪预测车辆机动；提出了感知辅助的NR帧结构和协议设计。

Result: 链路级仿真显示，该框架将平均切换中断时间减少50%以上，实现更低的切换率和更好的通信性能。

Conclusion: ISAC支持的感知辅助切换框架能有效提升车联网在高移动性场景下的通信可靠性和效率。

Abstract: Integrated Sensing and Communication (ISAC) has emerged as a promising
solution in addressing the challenges of high-mobility scenarios in 5G NR
Vehicle-to-Infrastructure (V2I) communications. This paper proposes a novel
sensing-assisted handover framework that leverages ISAC capabilities to enable
precise beamforming and proactive handover decisions. Two sensing-enabled
handover triggering algorithms are developed: a distance-based scheme that
utilizes estimated spatial positioning, and a probability-based approach that
predicts vehicle maneuvers using interacting multiple model extended Kalman
filter (IMM-EKF) tracking. The proposed methods eliminate the need for uplink
feedback and beam sweeping, thus significantly reducing signaling overhead and
handover interruption time. A sensing-assisted NR frame structure and
corresponding protocol design are also introduced to support rapid
synchronization and access under vehicular mobility. Extensive link-level
simulations using real-world map data demonstrate that the proposed framework
reduces the average handover interruption time by over 50%, achieves lower
handover rates, and enhances overall communication performance.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [32] [On the Estimation of Climate Normals and Anomalies](https://arxiv.org/abs/2511.05071)
*Tommaso Proietti,Alessandro Giovannelli*

Main category: stat.AP

TL;DR: 本文提出了一种基于局部三角回归的正则化实时滤波器，用于在气候变化背景下优化气候常态估计的偏差-方差权衡，并引入了一类季节性核函数来增强气候常态估计的局部化。


<details>
  <summary>Details</summary>
Motivation: 在非平稳的气候变化背景下，当前基于30年简单算术平均的标准气候常态估计方法受到质疑。需要开发能够更好处理气候变化趋势的新方法来准确量化气候时间序列的年际变化分量。

Method: 提出基于局部三角回归的正则化实时滤波器，优化偏差-方差权衡；引入季节性核函数类来增强气候常态估计的局部化。

Result: 该方法应用于厄尔尼诺3.4区域的海表温度序列以及赤道和热带太平洋区域的纬向风和信风强度序列，证明了所提方法的适用性。

Conclusion: 所提出的正则化实时滤波器和季节性核函数方法在气候变化背景下能够更准确地估计气候常态，为厄尔尼诺-南方涛动现象的评估和预测提供了改进的工具。

Abstract: The quantification of the interannual component of variability in
climatological time series is essential for the assessment and prediction of
the El Ni\~{n}o - Southern Oscillation phenomenon. This is achieved by
estimating the deviation of a climate variable (e.g., temperature, pressure,
precipitation, or wind strength) from its normal conditions, defined by its
baseline level and seasonal patterns. Climate normals are currently estimated
by simple arithmetic averages calculated over the most recent 30-year period
ending in a year divisible by 10. The suitability of the standard methodology
has been questioned in the context of a changing climate, characterized by
nonstationary conditions. The literature has focused on the choice of the
bandwidth and the ability to account for trends induced by climate change. The
paper contributes to the literature by proposing a regularized real time filter
based on local trigonometric regression, optimizing the estimation
bias-variance trade-off in the presence of climate change, and by introducing a
class of seasonal kernels enhancing the localization of the estimates of
climate normals. Application to sea surface temperature series in the \nino 3.4
region and zonal and trade winds strength in the equatorial and tropical
Pacific region, illustrates the relevance of our proposal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯学习的拒绝选项预测器，能够在训练数据不足导致认知不确定性高的区域拒绝预测，通过最小化期望遗憾来优化预测性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，预测模型不仅需要准确预测，还需要量化不确定性。传统拒绝选项方法只关注偶然不确定性，但在数据有限的实际场景中，认知不确定性不可忽视。

Method: 基于贝叶斯学习重新定义最优预测器，通过最小化期望遗憾（学习模型与完全了解数据分布的贝叶斯最优预测器之间的性能差距）来构建预测器，当输入对应的遗憾超过指定拒绝成本时拒绝预测。

Result: 提出了第一个原则性框架，能够学习识别训练数据不足以做出可靠决策的输入的预测器。

Conclusion: 该框架为数据有限场景下的不确定性量化提供了理论基础，使模型能够在认知不确定性高的区域安全地拒绝预测。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>

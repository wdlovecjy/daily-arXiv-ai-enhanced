{"id": "2510.18071", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.18071", "abs": "https://arxiv.org/abs/2510.18071", "authors": ["Yixin Fang", "Weili He"], "title": "Arbitrated Indirect Treatment Comparisons", "comment": null, "summary": "Matching-adjusted indirect comparison (MAIC) has been increasingly employed\nin health technology assessments (HTA). By reweighting subjects from a trial\nwith individual participant data (IPD) to match the covariate summary\nstatistics of another trial with only aggregate data (AgD), MAIC facilitates\nthe estimation of a treatment effect defined with respect to the AgD trial\npopulation. This manuscript introduces a new class of methods, termed\narbitrated indirect treatment comparisons, designed to address the ``MAIC\nparadox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox\narises when different sponsors, analyzing the same data, reach conflicting\nconclusions regarding which treatment is more effective. The underlying issue\nis that each sponsor implicitly targets a different population. To resolve this\ninconsistency, the proposed methods focus on estimating treatment effects in a\ncommon target population, specifically chosen to be the overlap population.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ef2\u88c1\u95f4\u63a5\u6cbb\u7597\u6bd4\u8f83\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3MAIC\u6096\u8bba\u95ee\u9898\uff0c\u901a\u8fc7\u4f30\u8ba1\u5171\u540c\u76ee\u6807\u4eba\u7fa4\uff08\u91cd\u53e0\u4eba\u7fa4\uff09\u7684\u6cbb\u7597\u6548\u679c\u6765\u6d88\u9664\u4e0d\u540c\u8d5e\u52a9\u5546\u5206\u6790\u540c\u4e00\u6570\u636e\u65f6\u5f97\u51fa\u77db\u76fe\u7ed3\u8bba\u7684\u95ee\u9898\u3002", "motivation": "MAIC\u65b9\u6cd5\u5728\u5065\u5eb7\u6280\u672f\u8bc4\u4f30\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b58\u5728MAIC\u6096\u8bba\u95ee\u9898\uff0c\u5373\u4e0d\u540c\u8d5e\u52a9\u5546\u5206\u6790\u76f8\u540c\u6570\u636e\u65f6\u4f1a\u5f97\u51fa\u76f8\u4e92\u77db\u76fe\u7684\u6cbb\u7597\u6548\u679c\u7ed3\u8bba\uff0c\u8fd9\u662f\u56e0\u4e3a\u6bcf\u4e2a\u8d5e\u52a9\u5546\u9690\u5f0f\u5730\u9488\u5bf9\u4e0d\u540c\u4eba\u7fa4\u3002", "method": "\u63d0\u51fa\u4ef2\u88c1\u95f4\u63a5\u6cbb\u7597\u6bd4\u8f83\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u4e2a\u4f53\u53c2\u4e0e\u8005\u6570\u636e\u8bd5\u9a8c\u7684\u53d7\u8bd5\u8005\uff0c\u4f7f\u5176\u4e0e\u4ec5\u5177\u6709\u6c47\u603b\u6570\u636e\u7684\u8bd5\u9a8c\u7684\u534f\u53d8\u91cf\u6c47\u603b\u7edf\u8ba1\u91cf\u5339\u914d\uff0c\u91cd\u70b9\u4f30\u8ba1\u5171\u540c\u76ee\u6807\u4eba\u7fa4\uff08\u91cd\u53e0\u4eba\u7fa4\uff09\u7684\u6cbb\u7597\u6548\u679c\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3MAIC\u6096\u8bba\uff0c\u786e\u4fdd\u4e0d\u540c\u5206\u6790\u8005\u4f7f\u7528\u76f8\u540c\u6570\u636e\u65f6\u5f97\u5230\u4e00\u81f4\u7684\u7ed3\u8bba\uff0c\u901a\u8fc7\u660e\u786e\u5b9a\u4e49\u5171\u540c\u76ee\u6807\u4eba\u7fa4\u6765\u6d88\u9664\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u4ef2\u88c1\u95f4\u63a5\u6cbb\u7597\u6bd4\u8f83\u65b9\u6cd5\u4e3a\u89e3\u51b3MAIC\u6096\u8bba\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u805a\u7126\u4e8e\u91cd\u53e0\u4eba\u7fa4\u7684\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\uff0c\u786e\u4fdd\u4e86\u5206\u6790\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.18032", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.18032", "abs": "https://arxiv.org/abs/2510.18032", "authors": ["Zhenyu Bi", "Meng Lu", "Yang Li", "Swastik Roy", "Weijie Guan", "Morteza Ziyadi", "Xuan Wang"], "title": "OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning", "comment": "8 pages for main content", "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities in\nmathematical and scientific tasks. To enhance complex reasoning, multi-agent\nsystems have been proposed to harness the collective intelligence of LLM\nagents. However, existing collaboration structures are either predefined or\nrely on majority voting or round-table debates, which can suppress correct but\nless dominant agent contributions. Recent approaches model multi-agent systems\nas graph networks but optimize purely for agent performance, neglecting the\nquality of interactions. We hypothesize that effective agent communication is\ncrucial for multi-agent reasoning and that debating quality plays a significant\nrole. To address this, we propose $\\ours$, a multi-agent verbal reinforcement\nlearning algorithm that dynamically constructs and refines multi-agent\ncollaboration structures. Our method defines action spaces and a feedback\nmechanism that evaluates communication robustness and coherence throughout the\ndebate. The final decision is achieved through a majority vote over all the\nagents. We assess $\\ours$ on various reasoning tasks, including mathematical\nreasoning, creative writing, scientific reasoning, and numerical sorting.\nResults demonstrate that our approach significantly outperforms single-agent\nprompting methods and state-of-the-art multi-agent frameworks on diverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u53e3\u5934\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u548c\u4f18\u5316\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7ed3\u6784\u6765\u589e\u5f3a\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u521b\u610f\u5199\u4f5c\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8981\u4e48\u91c7\u7528\u9884\u5b9a\u4e49\u7ed3\u6784\uff0c\u8981\u4e48\u4f9d\u8d56\u591a\u6570\u6295\u7968\u6216\u5706\u684c\u8fa9\u8bba\uff0c\u8fd9\u4f1a\u538b\u5236\u6b63\u786e\u4f46\u975e\u4e3b\u5bfc\u7684\u667a\u80fd\u4f53\u8d21\u732e\u3002\u4f5c\u8005\u5047\u8bbe\u6709\u6548\u7684\u667a\u80fd\u4f53\u901a\u4fe1\u5bf9\u591a\u667a\u80fd\u4f53\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u8fa9\u8bba\u8d28\u91cf\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u53e3\u5934\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\u548c\u53cd\u9988\u673a\u5236\uff0c\u8bc4\u4f30\u6574\u4e2a\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u7684\u901a\u4fe1\u9c81\u68d2\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u6700\u7ec8\u901a\u8fc7\u6240\u6709\u667a\u80fd\u4f53\u7684\u591a\u6570\u6295\u7968\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u521b\u610f\u5199\u4f5c\u3001\u79d1\u5b66\u63a8\u7406\u548c\u6570\u503c\u6392\u5e8f\u7b49\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u63d0\u793a\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7ed3\u6784\u548c\u5f3a\u8c03\u901a\u4fe1\u8d28\u91cf\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6709\u6548\u667a\u80fd\u4f53\u901a\u4fe1\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.18040", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.18040", "abs": "https://arxiv.org/abs/2510.18040", "authors": ["Alexander Boldachev"], "title": "Subject-Event Ontology Without Global Time: Foundations and Execution Semantics", "comment": "32 pages", "summary": "A formalization of a subject-event ontology is proposed for modeling complex\ndynamic systems without reliance on global time. Key principles: (1) event as\nan act of fixation - a subject discerns and fixes changes according to models\n(conceptual templates) available to them; (2) causal order via happens-before -\nthe order of events is defined by explicit dependencies, not timestamps; (3)\nmaking the ontology executable via a declarative dataflow mechanism, ensuring\ndeterminism; (4) models as epistemic filters - a subject can only fix what\nfalls under its known concepts and properties; (5) presumption of truth - the\ndeclarative content of an event is available for computation from the moment of\nfixation, without external verification. The formalization includes nine axioms\n(A1-A9), ensuring the correctness of executable ontologies: monotonicity of\nhistory (I1), acyclicity of causality (I2), traceability (I3). Special\nattention is given to the model-based approach (A9): event validation via\nschemas, actor authorization, automatic construction of causal chains (W3)\nwithout global time. Practical applicability is demonstrated on the boldsea\nsystem - a workflow engine for executable ontologies, where the theoretical\nconstructs are implemented in BSL (Boldsea Semantic Language). The\nformalization is applicable to distributed systems, microservice architectures,\nDLT platforms, and multiperspectivity scenarios (conflicting facts from\ndifferent subjects).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u4f53-\u4e8b\u4ef6\u7684\u672c\u4f53\u8bba\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u5168\u5c40\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u5efa\u6a21\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e8b\u4ef6\u56fa\u5b9a\u3001\u56e0\u679c\u987a\u5e8f\u3001\u58f0\u660e\u5f0f\u6570\u636e\u6d41\u7b49\u673a\u5236\u786e\u4fdd\u786e\u5b9a\u6027\uff0c\u5e76\u5728boldsea\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6267\u884c\u672c\u4f53\u3002", "motivation": "\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e0d\u4f9d\u8d56\u5168\u5c40\u65f6\u95f4\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u67b6\u6784\u3001DLT\u5e73\u53f0\u7b49\u573a\u666f\u4e2d\u7684\u591a\u89c6\u89d2\u548c\u51b2\u7a81\u4e8b\u5b9e\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e5d\u4e2a\u516c\u7406(A1-A9)\u7684\u5f62\u5f0f\u5316\u672c\u4f53\u8bba\uff0c\u5305\u62ec\u4e8b\u4ef6\u4f5c\u4e3a\u56fa\u5b9a\u884c\u4e3a\u3001\u56e0\u679c\u987a\u5e8f\u901a\u8fc7happens-before\u5b9a\u4e49\u3001\u58f0\u660e\u5f0f\u6570\u636e\u6d41\u673a\u5236\u3001\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u8fc7\u6ee4\u5668\u7b49\u6838\u5fc3\u539f\u5219\u3002", "result": "\u5f00\u53d1\u4e86boldsea\u7cfb\u7edf\u4f5c\u4e3a\u53ef\u6267\u884c\u672c\u4f53\u7684\u5de5\u4f5c\u6d41\u5f15\u64ce\uff0c\u5728BSL\u8bed\u8a00\u4e2d\u5b9e\u73b0\u4e86\u7406\u8bba\u6784\u9020\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u67b6\u6784\u3001DLT\u5e73\u53f0\u548c\u591a\u89c6\u89d2\u573a\u666f\uff0c\u80fd\u591f\u5904\u7406\u6765\u81ea\u4e0d\u540c\u4e3b\u4f53\u7684\u51b2\u7a81\u4e8b\u5b9e\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u6846\u67b6\u3002"}}
{"id": "2510.18161", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.18161", "abs": "https://arxiv.org/abs/2510.18161", "authors": ["Hamsa Bastani", "Osbert Bastani", "Bryce McLaughlin"], "title": "Beating the Winner's Curse via Inference-Aware Policy Optimization", "comment": null, "summary": "There has been a surge of recent interest in automatically learning policies\nto target treatment decisions based on rich individual covariates. A common\napproach is to train a machine learning model to predict counterfactual\noutcomes, and then select the policy that optimizes the predicted objective\nvalue. In addition, practitioners also want confidence that the learned policy\nhas better performance than the incumbent policy according to downstream policy\nevaluation. However, due to the winner's curse-an issue where the policy\noptimization procedure exploits prediction errors rather than finding actual\nimprovements-predicted performance improvements are often not substantiated by\ndownstream policy optimization. To address this challenge, we propose a novel\nstrategy called inference-aware policy optimization, which modifies policy\noptimization to account for how the policy will be evaluated downstream.\nSpecifically, it optimizes not only for the estimated objective value, but also\nfor the chances that the policy will be statistically significantly better than\nthe observational policy used to collect data. We mathematically characterize\nthe Pareto frontier of policies according to the tradeoff of these two goals.\nBased on our characterization, we design a policy optimization algorithm that\nuses machine learning to predict counterfactual outcomes, and then plugs in\nthese predictions to estimate the Pareto frontier; then, the decision-maker can\nselect the policy that optimizes their desired tradeoff, after which policy\nevaluation can be performed on the test set as usual. Finally, we perform\nsimulations to illustrate the effectiveness of our methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u63a8\u7406\u611f\u77e5\u7b56\u7565\u4f18\u5316\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u4f30\u8ba1\u76ee\u6807\u503c\u548c\u7b56\u7565\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\u7684\u6982\u7387\uff0c\u6765\u89e3\u51b3\u8d62\u5bb6\u8bc5\u5492\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u53cd\u4e8b\u5b9e\u7ed3\u679c\u6765\u4f18\u5316\u6cbb\u7597\u51b3\u7b56\u7b56\u7565\u7684\u65b9\u6cd5\u5b58\u5728\u8d62\u5bb6\u8bc5\u5492\u95ee\u9898\uff0c\u5373\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4f1a\u5229\u7528\u9884\u6d4b\u8bef\u5dee\u800c\u975e\u771f\u6b63\u6539\u8fdb\uff0c\u5bfc\u81f4\u9884\u6d4b\u7684\u6027\u80fd\u6539\u8fdb\u65e0\u6cd5\u5728\u4e0b\u6e38\u7b56\u7565\u8bc4\u4f30\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u4f18\u5316\u4f30\u8ba1\u76ee\u6807\u503c\uff0c\u8fd8\u4f18\u5316\u7b56\u7565\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\u4f18\u4e8e\u89c2\u5bdf\u7b56\u7565\u7684\u6982\u7387\u3002\u901a\u8fc7\u6570\u5b66\u8868\u5f81\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u8bbe\u8ba1\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u53cd\u4e8b\u5b9e\u7ed3\u679c\u5e76\u4f30\u8ba1\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u51b3\u7b56\u8005\u53ef\u4ee5\u6839\u636e\u671f\u671b\u7684\u6743\u8861\u9009\u62e9\u7b56\u7565\uff0c\u7136\u540e\u6309\u5e38\u89c4\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u7b56\u7565\u8bc4\u4f30\u3002", "conclusion": "\u63a8\u7406\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8d62\u5bb6\u8bc5\u5492\u95ee\u9898\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u4e0b\u6e38\u8bc4\u4f30\u4e2d\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.17809", "categories": ["eess.SP", "cs.LG", "I.2"], "pdf": "https://arxiv.org/pdf/2510.17809", "abs": "https://arxiv.org/abs/2510.17809", "authors": ["Massimo Capurso", "Luciano Afferrante"], "title": "In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning", "comment": "20 pages, 17 figures, 3 tables, 33 references", "summary": "In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH)\nrequirements demand high-precision finishing operations such as power honing.\nConventional quality control strategies rely on post-process inspections and\nStatistical Process Control (SPC), which fail to capture transient machining\nanomalies and cannot ensure real-time defect detection. This study proposes a\nnovel, data-driven framework for in-process monitoring of gear power honing\nusing vibration signal analysis and machine learning. Our proposed methodology\ninvolves continuous data acquisition via accelerometers, followed by\ntime-frequency signal analysis. We investigate and compare the efficacy of\nthree subspace learning methods for features extraction: (1) Principal\nComponent Analysis (PCA) for dimensionality reduction; (2) a two-stage\nframework combining PCA with Linear Discriminant Analysis (LDA) for enhanced\nclass separation; and (3) Uncorrelated Multilinear Discriminant Analysis with\nRegularization (R-UMLDA), adapted for tensor data, which enforces feature\ndecorrelation and includes regularization for small sample sizes. These\nextracted features are then fed into a Support Vector Machine (SVM) classifier\nto predict four distinct gear quality categories, established through rigorous\ngeometrical inspections and test bench results of assembled gearboxes. The\nmodels are trained and validated on an experimental dataset collected in an\nindustrial context during gear power-honing operations, with gears classified\ninto four different quality categories. The proposed framework achieves high\nclassification accuracy (up to 100%) in an industrial setting. The approach\noffers interpretable spectral features that correlate with process dynamics,\nenabling practical integration into real-time monitoring and predictive\nmaintenance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u4fe1\u53f7\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u9f7f\u8f6e\u5f3a\u529b\u73e9\u78e8\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u65f6\u76d1\u63a7\uff0c\u901a\u8fc7\u4e09\u79cd\u5b50\u7a7a\u95f4\u5b66\u4e60\u65b9\u6cd5\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408SVM\u5206\u7c7b\u5668\u5b9e\u73b0\u9ad8\u8fbe100%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u4ee3\u9f7f\u8f6e\u5236\u9020\u5bf9NVH\u6027\u80fd\u8981\u6c42\u4e25\u683c\uff0c\u4f20\u7edf\u8d28\u91cf\u63a7\u5236\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u77ac\u65f6\u52a0\u5de5\u5f02\u5e38\u548c\u5b9e\u73b0\u5b9e\u65f6\u7f3a\u9677\u68c0\u6d4b\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8fc7\u7a0b\u76d1\u63a7\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u52a0\u901f\u5ea6\u8ba1\u8fde\u7eed\u91c7\u96c6\u632f\u52a8\u6570\u636e\uff0c\u8fdb\u884c\u65f6\u9891\u4fe1\u53f7\u5206\u6790\uff0c\u6bd4\u8f83\u4e09\u79cd\u5b50\u7a7a\u95f4\u5b66\u4e60\u65b9\u6cd5\uff08PCA\u3001PCA+LDA\u3001R-UMLDA\uff09\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u7136\u540e\u7528SVM\u5206\u7c7b\u5668\u9884\u6d4b\u56db\u4e2a\u9f7f\u8f6e\u8d28\u91cf\u7c7b\u522b\u3002", "result": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6536\u96c6\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff08\u6700\u9ad8\u8fbe100%\uff09\uff0c\u63d0\u4f9b\u4e86\u4e0e\u5de5\u827a\u52a8\u529b\u5b66\u76f8\u5173\u7684\u53ef\u89e3\u91ca\u9891\u8c31\u7279\u5f81\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u9f7f\u8f6e\u5f3a\u529b\u73e9\u78e8\u8fc7\u7a0b\u7684\u5b9e\u65f6\u76d1\u63a7\uff0c\u4e3a\u96c6\u6210\u5230\u5b9e\u65f6\u76d1\u63a7\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18215", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18215", "abs": "https://arxiv.org/abs/2510.18215", "authors": ["Haixiang Lan", "Luofeng Liao", "Adam N. Elmachtoub", "Christian Kroer", "Henry Lam", "Haofeng Zhang"], "title": "The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective", "comment": null, "summary": "Data-driven stochastic optimization is ubiquitous in machine learning and\noperational decision-making problems. Sample average approximation (SAA) and\nmodel-based approaches such as estimate-then-optimize (ETO) or integrated\nestimation-optimization (IEO) are all popular, with model-based approaches\nbeing able to circumvent some of the issues with SAA in complex\ncontext-dependent problems. Yet the relative performance of these methods is\npoorly understood, with most results confined to the dichotomous cases of the\nmodel-based approach being either well-specified or misspecified. We develop\nthe first results that allow for a more granular analysis of the relative\nperformance of these methods under a local misspecification setting, which\nmodels the scenario where the model-based approach is nearly well-specified. By\nleveraging tools from contiguity theory in statistics, we show that there is a\nbias-variance tradeoff between SAA, IEO, and ETO under local misspecification,\nand that the relative importance of the bias and the variance depends on the\ndegree of local misspecification. Moreover, we derive explicit expressions for\nthe decision bias, which allows us to characterize (un)impactful\nmisspecification directions, and provide further geometric understanding of the\nvariance.", "AI": {"tldr": "\u672c\u6587\u5728\u5c40\u90e8\u8bef\u8bbe\u6761\u4ef6\u4e0b\u5206\u6790\u4e86\u6837\u672c\u5e73\u5747\u8fd1\u4f3c(SAA)\u3001\u4f30\u8ba1-\u4f18\u5316(ETO)\u548c\u96c6\u6210\u4f30\u8ba1-\u4f18\u5316(IEO)\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u5bf9\u6570\u636e\u9a71\u52a8\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u76f8\u5bf9\u6027\u80fd\u7684\u7406\u89e3\u6709\u9650\uff0c\u5927\u591a\u6570\u7ed3\u679c\u5c40\u9650\u4e8e\u6a21\u578b\u5b8c\u5168\u6b63\u786e\u6216\u5b8c\u5168\u8bef\u8bbe\u7684\u4e8c\u5143\u60c5\u51b5\u3002\u672c\u6587\u65e8\u5728\u5728\u5c40\u90e8\u8bef\u8bbe\u573a\u666f\u4e0b\u8fdb\u884c\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u3002", "method": "\u5229\u7528\u7edf\u8ba1\u5b66\u4e2d\u7684\u8fde\u7eed\u6027\u7406\u8bba\u5de5\u5177\uff0c\u5728\u5c40\u90e8\u8bef\u8bbe\u6761\u4ef6\u4e0b\u5206\u6790SAA\u3001IEO\u548cETO\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u63a8\u5bfc\u51b3\u7b56\u504f\u5dee\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u53d1\u73b0\u5728\u5c40\u90e8\u8bef\u8bbe\u6761\u4ef6\u4e0b\uff0c\u4e09\u79cd\u65b9\u6cd5\u5b58\u5728\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u5173\u7cfb\uff0c\u504f\u5dee\u548c\u65b9\u5dee\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u53d6\u51b3\u4e8e\u5c40\u90e8\u8bef\u8bbe\u7684\u7a0b\u5ea6\u3002\u63a8\u5bfc\u51fa\u4e86\u51b3\u7b56\u504f\u5dee\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u80fd\u591f\u523b\u753b(\u975e)\u6709\u5f71\u54cd\u7684\u8bef\u8bbe\u65b9\u5411\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5bf9\u6570\u636e\u9a71\u52a8\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u5728\u5c40\u90e8\u8bef\u8bbe\u6761\u4ef6\u4e0b\u76f8\u5bf9\u6027\u80fd\u7684\u51e0\u4f55\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u673a\u5236\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2510.17810", "categories": ["eess.SP", "cs.LG", "nlin.CD", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.17810", "abs": "https://arxiv.org/abs/2510.17810", "authors": ["Camilo Quiceno Quintero", "Sandip Varkey George"], "title": "Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification", "comment": "Version submitted to NODYCON 2025", "summary": "The complex dynamics of the heart are reflected in its electrical activity,\ncaptured through electrocardiograms (ECGs). In this study we use nonlinear time\nseries analysis to understand how ECG complexity varies with cardiac pathology.\nUsing the large PTB-XL dataset, we extracted nonlinear measures from lead II\nECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations\nand mutual information. Significant differences between diseased and healthy\nindividuals were found in almost all measures between healthy and diseased\nclasses, and between 5 diagnostic superclasses ($p<.001$). Moreover,\nincorporating these complexity quantifiers into machine learning models\nsubstantially improved classification accuracy measured using area under the\nROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90\n(including cross-time series metrics).", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5206\u6790ECG\u590d\u6742\u5ea6\u4e0e\u5fc3\u810f\u75c5\u7406\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u975e\u7ebf\u6027\u6307\u6807\u80fd\u663e\u8457\u533a\u5206\u5065\u5eb7\u548c\u60a3\u75c5\u4e2a\u4f53\uff0c\u5e76\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5fc3\u810f\u7684\u590d\u6742\u52a8\u6001\u53cd\u6620\u5728\u5176\u7535\u6d3b\u52a8\uff08ECG\uff09\u4e2d\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7406\u89e3ECG\u590d\u6742\u5ea6\u5982\u4f55\u968f\u5fc3\u810f\u75c5\u7406\u53d8\u5316\uff0c\u4e3a\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528PTB-XL\u6570\u636e\u96c6\uff0c\u4ece\u5bfc\u8054II ECG\u63d0\u53d6\u975e\u7ebf\u6027\u6307\u6807\uff0c\u5e76\u4f7f\u7528Spearman\u76f8\u5173\u6027\u548c\u4e92\u4fe1\u606f\u8ba1\u7b97\u8de8\u5bfc\u8054\uff08II\u3001V2\u3001AVL\uff09\u6307\u6807\u3002", "result": "\u51e0\u4e4e\u6240\u6709\u975e\u7ebf\u6027\u6307\u6807\u5728\u5065\u5eb7\u548c\u60a3\u75c5\u4e2a\u4f53\u95f4\u5747\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08p<.001\uff09\uff0c\u5c06\u590d\u6742\u5ea6\u6307\u6807\u52a0\u5165\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f7fAUC\u4ece0.86\u63d0\u5347\u81f30.87\uff08\u4ec5\u975e\u7ebf\u6027\u6307\u6807\uff09\u548c0.90\uff08\u5305\u62ec\u8de8\u65f6\u95f4\u5e8f\u5217\u6307\u6807\uff09\u3002", "conclusion": "\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u80fd\u6709\u6548\u91cf\u5316ECG\u590d\u6742\u5ea6\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u5fc3\u810f\u75be\u75c5\u5206\u7c7b\u6027\u80fd\uff0c\u4e3aECG\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u5de5\u5177\u3002"}}
{"id": "2510.18259", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18259", "abs": "https://arxiv.org/abs/2510.18259", "authors": ["Dechen Zhang", "Junwei Su", "Difan Zou"], "title": "Learning under Quantization for High-Dimensional Linear Regression", "comment": null, "summary": "The use of low-bit quantization has emerged as an indispensable technique for\nenabling the efficient training of large-scale models. Despite its widespread\nempirical success, a rigorous theoretical understanding of its impact on\nlearning performance remains notably absent, even in the simplest linear\nregression setting. We present the first systematic theoretical study of this\nfundamental question, analyzing finite-step stochastic gradient descent (SGD)\nfor high-dimensional linear regression under a comprehensive range of\nquantization targets: data, labels, parameters, activations, and gradients. Our\nnovel analytical framework establishes precise algorithm-dependent and\ndata-dependent excess risk bounds that characterize how different quantization\naffects learning: parameter, activation, and gradient quantization amplify\nnoise during training; data quantization distorts the data spectrum; and data\nand label quantization introduce additional approximation and quantized error.\nCrucially, we prove that for multiplicative quantization (with input-dependent\nquantization step), this spectral distortion can be eliminated, and for\nadditive quantization (with constant quantization step), a beneficial scaling\neffect with batch size emerges. Furthermore, for common polynomial-decay data\nspectra, we quantitatively compare the risks of multiplicative and additive\nquantization, drawing a parallel to the comparison between FP and integer\nquantization methods. Our theory provides a powerful lens to characterize how\nquantization shapes the learning dynamics of optimization algorithms, paving\nthe way to further explore learning theory under practical hardware\nconstraints.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u4ece\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u5bf9\u9ad8\u7ef4\u7ebf\u6027\u56de\u5f52\u4e2d\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u6570\u636e\u3001\u6807\u7b7e\u3001\u53c2\u6570\u3001\u6fc0\u6d3b\u548c\u68af\u5ea6\u4e94\u79cd\u91cf\u5316\u7c7b\u578b\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u4e0d\u540c\u5f71\u54cd\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u4f4e\u6bd4\u7279\u91cf\u5316\u5728\u5927\u89c4\u6a21\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3\u4e2d\u5e7f\u6cdb\u5e94\u7528\u4e14\u7ecf\u9a8c\u4e0a\u6210\u529f\uff0c\u4f46\u5176\u5bf9\u5b66\u4e60\u6027\u80fd\u5f71\u54cd\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u7f3a\u4e4f\uff0c\u5373\u4f7f\u5728\u6700\u7b80\u5355\u7684\u7ebf\u6027\u56de\u5f52\u8bbe\u7f6e\u4e2d\u4e5f\u662f\u5982\u6b64\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u6790\u6846\u67b6\uff0c\u7814\u7a76\u9ad8\u7ef4\u7ebf\u6027\u56de\u5f52\u4e2d\u6709\u9650\u6b65\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5728\u4e94\u79cd\u91cf\u5316\u76ee\u6807\u4e0b\u7684\u8868\u73b0\uff1a\u6570\u636e\u3001\u6807\u7b7e\u3001\u53c2\u6570\u3001\u6fc0\u6d3b\u548c\u68af\u5ea6\u91cf\u5316\uff0c\u5206\u6790\u7b97\u6cd5\u4f9d\u8d56\u548c\u6570\u636e\u4f9d\u8d56\u7684\u8fc7\u91cf\u98ce\u9669\u754c\u9650\u3002", "result": "\u53d1\u73b0\u53c2\u6570\u3001\u6fc0\u6d3b\u548c\u68af\u5ea6\u91cf\u5316\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u653e\u5927\u566a\u58f0\uff1b\u6570\u636e\u91cf\u5316\u626d\u66f2\u6570\u636e\u8c31\uff1b\u6570\u636e\u548c\u6807\u7b7e\u91cf\u5316\u5f15\u5165\u989d\u5916\u8fd1\u4f3c\u548c\u91cf\u5316\u8bef\u5dee\u3002\u5bf9\u4e8e\u4e58\u6cd5\u91cf\u5316\u53ef\u4ee5\u6d88\u9664\u8c31\u5931\u771f\uff0c\u5bf9\u4e8e\u52a0\u6cd5\u91cf\u5316\u5219\u51fa\u73b0\u6279\u6b21\u5927\u5c0f\u7684\u6709\u76ca\u7f29\u653e\u6548\u5e94\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u7406\u89e3\u91cf\u5316\u5982\u4f55\u5851\u9020\u4f18\u5316\u7b97\u6cd5\u7684\u5b66\u4e60\u52a8\u6001\u63d0\u4f9b\u4e86\u5f3a\u5927\u89c6\u89d2\uff0c\u4e3a\u5728\u5b9e\u7528\u786c\u4ef6\u7ea6\u675f\u4e0b\u8fdb\u4e00\u6b65\u63a2\u7d22\u5b66\u4e60\u7406\u8bba\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.17811", "categories": ["eess.SP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.17811", "abs": "https://arxiv.org/abs/2510.17811", "authors": ["Zhixing Wang", "Renzhi Yuan", "Haifeng Yao", "Chuang Yang", "Mugen Peng"], "title": "Channel Modeling of Satellite-to-Underwater Laser Communication Links: An Analytical-Monte Carlo Hybrid Approach", "comment": null, "summary": "Channel modeling for satellite-to-underwater laser communication (StULC)\nlinks remains challenging due to long distances and the diversity of the\nchannel constituents. The StULC channel is typically segmented into three\nisolated channels: the atmospheric channel, the air-water interface channel,\nand the underwater channel. Previous studies involving StULC channel modeling\neither focused on separated channels or neglected the combined effects of\nparticles and turbulence on laser propagation. In this paper, we established a\ncomprehensive StULC channel model by an analytical-Monte Carlo hybrid approach,\ntaking into account the effects of both particles and turbulence. We first\nobtained the intensity distribution of the transmitted laser beam after passing\nthrough the turbulent atmosphere based on the extended Huygens-Fresnel\nprinciple. Then we derived a closed-form probability density function of the\nphoton propagating direction after passing through the air-water interface,\nwhich greatly simplified the modeling of StULC links. At last, we employed a\nMonte Carlo method to model the underwater links and obtained the power\ndistribution at the receiving plane. Based on the proposed StULC channel model,\nwe analyzed the bit error rate and the outage probability under different\nenvironmental conditions. Numerical results demonstrated that, the influence of\nunderwater particle concentration on the communication performance is much\npronounced than those of both the atmospheric turbulence and the underwater\nturbulence. Notably, increasing the wind speed at the air-water interface does\nnot significantly worsen the communication performance of the StULC links.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u536b\u661f\u5230\u6c34\u4e0b\u6fc0\u5149\u901a\u4fe1\u7684\u5b8c\u6574\u4fe1\u9053\u6a21\u578b\uff0c\u91c7\u7528\u5206\u6790-\u8499\u7279\u5361\u6d1b\u6df7\u5408\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u7c92\u5b50\u548c\u6e4d\u6d41\u6548\u5e94\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u8bef\u7801\u7387\u548c\u4e2d\u65ad\u6982\u7387\u3002", "motivation": "\u536b\u661f\u5230\u6c34\u4e0b\u6fc0\u5149\u901a\u4fe1\u4fe1\u9053\u5efa\u6a21\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5173\u6ce8\u5206\u79bb\u4fe1\u9053\uff0c\u8981\u4e48\u5ffd\u7565\u7c92\u5b50\u548c\u6e4d\u6d41\u5bf9\u6fc0\u5149\u4f20\u64ad\u7684\u7efc\u5408\u5f71\u54cd\uff0c\u9700\u8981\u5efa\u7acb\u7efc\u5408\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u7684\u5b8c\u6574\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5206\u6790-\u8499\u7279\u5361\u6d1b\u6df7\u5408\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6269\u5c55\u60e0\u66f4\u65af-\u83f2\u6d85\u8033\u539f\u7406\u83b7\u5f97\u6e4d\u6d41\u5927\u6c14\u4e2d\u4f20\u8f93\u6fc0\u5149\u675f\u7684\u5f3a\u5ea6\u5206\u5e03\uff1b\u63a8\u5bfc\u7a7a\u6c14-\u6c34\u754c\u9762\u540e\u5149\u5b50\u4f20\u64ad\u65b9\u5411\u7684\u95ed\u5f0f\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff1b\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u6a21\u62df\u6c34\u4e0b\u94fe\u8def\u5e76\u83b7\u5f97\u63a5\u6536\u5e73\u9762\u529f\u7387\u5206\u5e03\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6c34\u4e0b\u7c92\u5b50\u6d53\u5ea6\u5bf9\u901a\u4fe1\u6027\u80fd\u7684\u5f71\u54cd\u8fdc\u5927\u4e8e\u5927\u6c14\u6e4d\u6d41\u548c\u6c34\u4e0b\u6e4d\u6d41\uff1b\u589e\u52a0\u7a7a\u6c14-\u6c34\u754c\u9762\u7684\u98ce\u901f\u4e0d\u4f1a\u663e\u8457\u6076\u5316\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u5efa\u7acb\u4e86\u7efc\u5408\u8003\u8651\u7c92\u5b50\u548c\u6e4d\u6d41\u6548\u5e94\u7684\u536b\u661f\u5230\u6c34\u4e0b\u6fc0\u5149\u901a\u4fe1\u5b8c\u6574\u4fe1\u9053\u6a21\u578b\uff0c\u53d1\u73b0\u6c34\u4e0b\u7c92\u5b50\u6d53\u5ea6\u662f\u5f71\u54cd\u901a\u4fe1\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u754c\u9762\u98ce\u901f\u5f71\u54cd\u8f83\u5c0f\u3002"}}
{"id": "2510.17816", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17816", "abs": "https://arxiv.org/abs/2510.17816", "authors": ["Xin Li", "Jingzhi Hu", "Yinghui He", "Hongbo Wang", "Jin Gan", "Jun Luo"], "title": "Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing", "comment": null, "summary": "Wi-Fi-based human activity recognition (HAR) provides substantial convenience\nand has emerged as a thriving research field, yet the coarse spatial resolution\ninherent to Wi-Fi significantly hinders its ability to distinguish multiple\nsubjects. By exploiting the near-field domination effect, establishing a\ndedicated sensing link for each subject through their personal Wi-Fi device\noffers a promising solution for multi-person HAR under native traffic. However,\ndue to the subject-specific characteristics and irregular patterns of\nnear-field signals, HAR neural network models require fine-tuning (FT) for\ncross-domain adaptation, which becomes particularly challenging with certain\ncategories unavailable. In this paper, we propose WiAnchor, a novel training\nframework for efficient cross-domain adaptation in the presence of incomplete\nactivity categories. This framework processes Wi-Fi signals embedded with\nirregular time information in three steps: during pre-training, we enlarge\ninter-class feature margins to enhance the separability of activities; in the\nFT stage, we innovate an anchor matching mechanism for cross-domain adaptation,\nfiltering subject-specific interference informed by incomplete activity\ncategories, rather than attempting to extract complete features from them;\nfinally, the recognition of input samples is further improved based on their\nfeature-level similarity with anchors. We construct a comprehensive dataset to\nthoroughly evaluate WiAnchor, achieving over 90% cross-domain accuracy with\nabsent activity categories.", "AI": {"tldr": "WiAnchor\u662f\u4e00\u4e2a\u7528\u4e8eWi-Fi\u591a\u7528\u6237\u6d3b\u52a8\u8bc6\u522b\u7684\u8de8\u57df\u9002\u5e94\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u951a\u70b9\u5339\u914d\u673a\u5236\u5728\u4e0d\u5b8c\u6574\u6d3b\u52a8\u7c7b\u522b\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\u3002", "motivation": "Wi-Fi\u6d3b\u52a8\u8bc6\u522b\u5728\u591a\u7528\u6237\u573a\u666f\u4e0b\u56e0\u7a7a\u95f4\u5206\u8fa8\u7387\u9650\u5236\u800c\u96be\u4ee5\u533a\u5206\u4e0d\u540c\u7528\u6237\uff0c\u4e14\u8fd1\u573a\u4fe1\u53f7\u5177\u6709\u7528\u6237\u7279\u5b9a\u7279\u5f81\u548c\u6a21\u5f0f\u4e0d\u89c4\u5219\u6027\uff0c\u5bfc\u81f4\u8de8\u57df\u9002\u5e94\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u67d0\u4e9b\u6d3b\u52a8\u7c7b\u522b\u4e0d\u53ef\u7528\u65f6\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u8bad\u7ec3\u6846\u67b6\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u6269\u5927\u7c7b\u95f4\u7279\u5f81\u8fb9\u754c\u589e\u5f3a\u6d3b\u52a8\u53ef\u5206\u6027\uff1b\u5fae\u8c03\u9636\u6bb5\u5f15\u5165\u951a\u70b9\u5339\u914d\u673a\u5236\u8fdb\u884c\u8de8\u57df\u9002\u5e94\uff0c\u57fa\u4e8e\u4e0d\u5b8c\u6574\u6d3b\u52a8\u7c7b\u522b\u8fc7\u6ee4\u7528\u6237\u7279\u5b9a\u5e72\u6270\uff1b\u6700\u540e\u57fa\u4e8e\u8f93\u5165\u6837\u672c\u4e0e\u951a\u70b9\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u8fdb\u4e00\u6b65\u6539\u8fdb\u8bc6\u522b\u3002", "result": "\u6784\u5efa\u4e86\u5168\u9762\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u6d3b\u52a8\u7c7b\u522b\u7f3a\u5931\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u8de8\u57df\u51c6\u786e\u7387\u3002", "conclusion": "WiAnchor\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86Wi-Fi\u591a\u7528\u6237\u6d3b\u52a8\u8bc6\u522b\u4e2d\u7684\u8de8\u57df\u9002\u5e94\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5b8c\u6574\u6d3b\u52a8\u7c7b\u522b\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002"}}
{"id": "2510.17818", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17818", "abs": "https://arxiv.org/abs/2510.17818", "authors": ["Salar Nouri"], "title": "Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach", "comment": null, "summary": "This paper tackles the challenging problem of gridless two-dimensional (2D)\ndirection-of-arrival (DOA) estimation for a uniform circular array (UCA) from a\nsingle snapshot of data. Conventional gridless methods often fail in this\nscenario due to prohibitive computational costs or a lack of robustness. We\npropose a novel framework that overcomes these limitations by jointly\nestimating a manifold transformation matrix and the source azimuth-elevation\npairs within a single, unified optimization problem. This problem is solved\nefficiently using an inexact Augmented Lagrangian Method (iALM), which\ncompletely circumvents the need for semidefinite programming. By unifying the\nobjectives of data fidelity and transformation robustness, our approach is\nuniquely suited for the demanding single-snapshot case. Simulation results\nconfirm that the proposed iALM framework provides robust and high-resolution,\ngridless 2D-DOA estimates, establishing its efficacy for challenging array\nsignal processing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5747\u5300\u5706\u9635\u5217\u5355\u5feb\u7167\u6570\u636e\u7684\u65e0\u7f51\u683c\u4e8c\u7ef4\u6ce2\u8fbe\u65b9\u5411\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u6d41\u5f62\u53d8\u6362\u77e9\u9635\u548c\u6e90\u65b9\u4f4d-\u4ef0\u89d2\u5bf9\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65e0\u7f51\u683c\u65b9\u6cd5\u5728\u5355\u5feb\u7167\u573a\u666f\u4e0b\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u6216\u7f3a\u4e4f\u9c81\u68d2\u6027\u800c\u5931\u6548\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u65b0\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u6d41\u5f62\u53d8\u6362\u77e9\u9635\u548c\u6e90\u65b9\u4f4d-\u4ef0\u89d2\u5bf9\uff0c\u6784\u5efa\u7edf\u4e00\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u4e0d\u7cbe\u786e\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u9ad8\u6548\u6c42\u89e3\uff0c\u5b8c\u5168\u907f\u514d\u4e86\u534a\u5b9a\u89c4\u5212\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684iALM\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u9c81\u68d2\u4e14\u9ad8\u5206\u8fa8\u7387\u7684\u65e0\u7f51\u683c\u4e8c\u7ef4\u6ce2\u8fbe\u65b9\u5411\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u5355\u5feb\u7167\u9635\u5217\u4fe1\u53f7\u5904\u7406\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17896", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17896", "abs": "https://arxiv.org/abs/2510.17896", "authors": ["Tao Bu", "Qiangang Wang", "Bowen Zeng", "Hanwen Sun", "Yunpeng Huang", "Chun Cao", "Jingwei Xu"], "title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism", "comment": "56 pages", "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess, yet their standard attention mechanism incurs quadratic computation\nand memory costs with respect to sequence length, posing a major bottleneck for\nlong-context training. Prior work tackles this challenge along two directions:\n(1) kernel-level optimizations, which accelerate dense and sparse attention\noperators; and (2) module-level strategies, often referred to as distributed\nattention or context parallel training, which scale attention across multiple\ndevices. However, systematic evaluation still remains limited: operator-level\ncomparisons are often incomplete, while context parallel strategies are\ntypically framework-specific, with unclear performance analysis across\ncontexts. To address these gaps, we propose a unified benchmark that integrates\nrepresentative attention kernels and context parallel mechanisms with a modular\nand extensible interface for evaluation. The benchmark evaluates methods along\ntwo critical dimensions: (1) attention mask patterns, which strongly affect\nefficiency, scalability, and usability, and (2) sequence length and distributed\nscale, which determine performance under extreme long-context training. Through\ncomprehensive experiments on the cluster of up to 96 GPUs, our benchmark\nenables reproducible comparisons, highlights method-specific trade-offs, and\nprovides practical guidance for designing and deploying attention mechanisms in\nlong-context LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587LLM\u8bad\u7ec3\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u5185\u6838\u7ea7\u4f18\u5316\u548c\u6a21\u5757\u7ea7\u5e76\u884c\u7b56\u7565\u3002", "motivation": "Transformer\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u95ee\u9898\uff0c\u6210\u4e3a\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u64cd\u4f5c\u7ea7\u6bd4\u8f83\u4e0d\u5b8c\u6574\uff0c\u4e0a\u4e0b\u6587\u5e76\u884c\u7b56\u7565\u901a\u5e38\u662f\u6846\u67b6\u7279\u5b9a\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u96c6\u6210\u4ee3\u8868\u6027\u6ce8\u610f\u529b\u5185\u6838\u548c\u4e0a\u4e0b\u6587\u5e76\u884c\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u53ef\u6269\u5c55\u63a5\u53e3\u8fdb\u884c\u8bc4\u4f30\u3002\u8bc4\u4f30\u7ef4\u5ea6\u5305\u62ec\u6ce8\u610f\u529b\u63a9\u7801\u6a21\u5f0f\u548c\u5e8f\u5217\u957f\u5ea6/\u5206\u5e03\u5f0f\u89c4\u6a21\u3002", "result": "\u5728\u6700\u591a96\u4e2aGPU\u7684\u96c6\u7fa4\u4e0a\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\uff0c\u7a81\u51fa\u4e86\u65b9\u6cd5\u7279\u5b9a\u7684\u6743\u8861\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u8bad\u7ec3\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u6ce8\u610f\u529b\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u957f\u4e0a\u4e0b\u6587LLM\u8bad\u7ec3\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.17821", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17821", "abs": "https://arxiv.org/abs/2510.17821", "authors": ["Long Lin", "Pablo Peiro-Corbacho", "Pablo \u00c1vila", "Alejandro Carta-Bergaz", "\u00c1ngel Arenal", "Gonzalo R. R\u00edos-Mu\u00f1oz", "Carlos Sevilla-Salcedo"], "title": "CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms", "comment": null, "summary": "Intracavitary atrial electrograms (EGMs) provide high-resolution insights\ninto cardiac electrophysiology but are often contaminated by noise and remain\nhigh-dimensional, limiting real-time analysis. We introduce CLARAE\n(CLArity-preserving Reconstruction AutoEncoder), a one-dimensional\nencoder--decoder designed for atrial EGMs, which achieves both high-fidelity\nreconstruction and a compact 64-dimensional latent representation. CLARAE is\ndesigned to preserve waveform morphology, mitigate reconstruction artifacts,\nand produce interpretable embeddings through three principles: downsampling\nwith pooling, a hybrid interpolation--convolution upsampling path, and a\nbounded latent space.\n  We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29\npatients across three rhythm types (AF, SR300, SR600). Performance was\nbenchmarked against six state-of-the-art autoencoders using reconstruction\nmetrics, rhythm classification, and robustness across signal-to-noise ratios\nfrom -5 to 15 dB. In downstream rhythm classification, CLARAE achieved\nF1-scores above 0.97 for all rhythm types, and its latent space showed clear\nclustering by rhythm. In denoising tasks, it consistently ranked among the top\nperformers for both unipolar and bipolar signals.\n  In order to promote reproducibility and enhance accessibility, we offer an\ninteractive web-based application. This platform enables users to explore\npre-trained CLARAE models, visualize the reconstructions, and compute metrics\nin real time. Overall, CLARAE combines robust denoising with compact,\ndiscriminative representations, offering a practical foundation for clinical\nworkflows such as rhythm discrimination, signal quality assessment, and\nreal-time mapping.", "AI": {"tldr": "CLARAE\u662f\u4e00\u79cd\u7528\u4e8e\u5fc3\u623f\u5185\u7535\u56fe\u76841D\u81ea\u7f16\u7801\u5668\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u7d27\u51d1\u768464\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u5728\u566a\u58f0\u6291\u5236\u548c\u8282\u5f8b\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5fc3\u623f\u5185\u7535\u56fe\u5e38\u53d7\u566a\u58f0\u6c61\u67d3\u4e14\u7ef4\u5ea6\u8f83\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5206\u6790\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u964d\u566a\u53c8\u80fd\u63d0\u4f9b\u7d27\u51d1\u8868\u793a\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u6c60\u5316\u4e0b\u91c7\u6837\u3001\u6df7\u5408\u63d2\u503c-\u5377\u79ef\u4e0a\u91c7\u6837\u8def\u5f84\u548c\u6709\u754c\u6f5c\u5728\u7a7a\u95f4\u4e09\u4e2a\u539f\u5219\u6765\u4fdd\u6301\u6ce2\u5f62\u5f62\u6001\u5e76\u51cf\u5c11\u91cd\u5efa\u4f2a\u5f71\u3002", "result": "\u5728495,731\u4e2a\u7535\u56fe\u6bb5\u4e0a\u6d4b\u8bd5\uff0cCLARAE\u5728\u6240\u6709\u8282\u5f8b\u7c7b\u578b\u4e0aF1\u5206\u6570\u8d85\u8fc70.97\uff0c\u6f5c\u5728\u7a7a\u95f4\u663e\u793a\u6e05\u6670\u7684\u8282\u5f8b\u805a\u7c7b\uff0c\u5728\u53bb\u566a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CLARAE\u7ed3\u5408\u4e86\u9c81\u68d2\u7684\u53bb\u566a\u80fd\u529b\u548c\u7d27\u51d1\u7684\u5224\u522b\u6027\u8868\u793a\uff0c\u4e3a\u8282\u5f8b\u9274\u522b\u3001\u4fe1\u53f7\u8d28\u91cf\u8bc4\u4f30\u548c\u5b9e\u65f6\u6620\u5c04\u7b49\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2510.18154", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18154", "abs": "https://arxiv.org/abs/2510.18154", "authors": ["Antonio-Gabriel Chac\u00f3n Menke", "Phan Xuan Tan", "Eiji Kamioka"], "title": "Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety", "comment": null, "summary": "Recent work has highlighted the importance of monitoring chain-of-thought\nreasoning for AI safety; however, current approaches that analyze textual\nreasoning steps can miss subtle harmful patterns and may be circumvented by\nmodels that hide unsafe reasoning. We present a sentence-level labeled dataset\nthat enables activation-based monitoring of safety behaviors during LLM\nreasoning. Our dataset contains reasoning sequences with sentence-level\nannotations of safety behaviors such as expression of safety concerns or\nspeculation on user intent, which we use to extract steering vectors for\ndetecting and influencing these behaviors within model activations. The dataset\nfills a key gap in safety research: while existing datasets label reasoning\nholistically, effective application of steering vectors for safety monitoring\ncould be improved by identifying precisely when specific behaviors occur within\nreasoning chains. We demonstrate the dataset's utility by extracting\nrepresentations that both detect and steer safety behaviors in model\nactivations, showcasing the potential of activation-level techniques for\nimproving safety oversight on reasoning.\n  Content Warning: This paper discusses AI safety in the context of harmful\nprompts and may contain references to potentially harmful content.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53e5\u5b50\u7ea7\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u6fc0\u6d3b\u72b6\u6001\u76d1\u63a7\u5b89\u5168\u884c\u4e3a\uff0c\u901a\u8fc7\u63d0\u53d6\u5f15\u5bfc\u5411\u91cf\u6765\u68c0\u6d4b\u548c\u5f71\u54cd\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u5b89\u5168\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u63a8\u7406\u6b65\u9aa4\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u5fae\u5999\u7684\u6709\u5bb3\u6a21\u5f0f\uff0c\u4e14\u53ef\u80fd\u88ab\u9690\u85cf\u4e0d\u5b89\u5168\u63a8\u7406\u7684\u6a21\u578b\u89c4\u907f\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u76d1\u63a7\u6280\u672f\u3002", "method": "\u6784\u5efa\u5305\u542b\u63a8\u7406\u5e8f\u5217\u548c\u53e5\u5b50\u7ea7\u5b89\u5168\u884c\u4e3a\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u63d0\u53d6\u5f15\u5bfc\u5411\u91cf\u7528\u4e8e\u5728\u6a21\u578b\u6fc0\u6d3b\u4e2d\u68c0\u6d4b\u548c\u5f71\u54cd\u5b89\u5168\u884c\u4e3a\u3002", "result": "\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\uff0c\u63d0\u53d6\u7684\u8868\u5f81\u80fd\u591f\u68c0\u6d4b\u548c\u5f15\u5bfc\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u5b89\u5168\u884c\u4e3a\u3002", "conclusion": "\u6fc0\u6d3b\u7ea7\u6280\u672f\u6709\u6f5c\u529b\u6539\u5584\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u76d1\u7763\uff0c\u586b\u8865\u4e86\u5b89\u5168\u7814\u7a76\u4e2d\u7cbe\u786e\u8bc6\u522b\u63a8\u7406\u94fe\u4e2d\u7279\u5b9a\u884c\u4e3a\u53d1\u751f\u65f6\u673a\u7684\u91cd\u8981\u7a7a\u767d\u3002"}}
{"id": "2510.18130", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.18130", "abs": "https://arxiv.org/abs/2510.18130", "authors": ["Jan Quan", "Johan Suykens", "Panagiotis Patrinos"], "title": "Rethinking PCA Through Duality", "comment": "NeurIPS 2025 poster", "summary": "Motivated by the recently shown connection between self-attention and\n(kernel) principal component analysis (PCA), we revisit the fundamentals of\nPCA. Using the difference-of-convex (DC) framework, we present several novel\nformulations and provide new theoretical insights. In particular, we show the\nkernelizability and out-of-sample applicability for a PCA-like family of\nproblems. Moreover, we uncover that simultaneous iteration, which is connected\nto the classical QR algorithm, is an instance of the difference-of-convex\nalgorithm (DCA), offering an optimization perspective on this longstanding\nmethod. Further, we describe new algorithms for PCA and empirically compare\nthem with state-of-the-art methods. Lastly, we introduce a kernelizable dual\nformulation for a robust variant of PCA that minimizes the $l_1$ deviation of\nthe reconstruction errors.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u4e3b\u6210\u5206\u5206\u6790(PCA)\u7684\u8054\u7cfb\uff0c\u91cd\u65b0\u5ba1\u89c6PCA\u57fa\u7840\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u65b0\u516c\u5f0f\u548c\u7406\u8bba\u89c1\u89e3\uff0c\u5305\u62ec\u6838\u5316\u80fd\u529b\u3001\u6837\u672c\u5916\u5e94\u7528\u3001\u9c81\u68d2PCA\u53d8\u4f53\u7b49\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7b97\u6cd5\u3002", "motivation": "\u53d7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u6838\u4e3b\u6210\u5206\u5206\u6790(PCA)\u4e4b\u95f4\u65b0\u53d1\u73b0\u7684\u8054\u7cfb\u542f\u53d1\uff0c\u91cd\u65b0\u5ba1\u89c6PCA\u7684\u57fa\u672c\u539f\u7406\uff0c\u63a2\u7d22\u5176\u7406\u8bba\u6269\u5c55\u548c\u7b97\u6cd5\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u51f8\u5dee(DC)\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u591a\u4e2aPCA\u7684\u65b0\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u6838\u5316\u7248\u672c\u3001\u6837\u672c\u5916\u5e94\u7528\u7b97\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8el1\u8303\u6570\u91cd\u6784\u8bef\u5dee\u7684\u9c81\u68d2PCA\u53d8\u4f53\u3002", "result": "\u63ed\u793a\u4e86\u540c\u65f6\u8fed\u4ee3(\u4e0e\u7ecf\u5178QR\u7b97\u6cd5\u76f8\u5173)\u662f\u51f8\u5dee\u7b97\u6cd5(DCA)\u7684\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u63d0\u4f9b\u4e86\u5bf9\u8be5\u957f\u671f\u4f7f\u7528\u65b9\u6cd5\u7684\u4f18\u5316\u89c6\u89d2\uff1b\u5f00\u53d1\u4e86\u65b0\u7684PCA\u7b97\u6cd5\u5e76\u5728\u5b9e\u8bc1\u4e2d\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "conclusion": "\u901a\u8fc7DC\u6846\u67b6\u4e3aPCA\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u548c\u7b97\u6cd5\u5b9e\u73b0\uff0c\u6269\u5c55\u4e86PCA\u7684\u6838\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u7ecf\u5178\u6570\u503c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4f18\u5316\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2510.17914", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17914", "abs": "https://arxiv.org/abs/2510.17914", "authors": ["Rikard Vinge", "Isabelle Wittmann", "Jannik Schneider", "Michael Marszalek", "Luis Gilch", "Thomas Brunschwiler", "Conrad M Albrecht"], "title": "NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation", "comment": null, "summary": "We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)\nneural compression and representation learning in the context of Earth\nObservation (EO). Our approach builds on fixed-size embeddings that act as\ncompact, task-agnostic representations applicable to a broad range of\ndownstream tasks. NeuCo-Bench comprises three core components: (i) an\nevaluation pipeline built around reusable embeddings, (ii) a new challenge mode\nwith a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)\na scoring system that balances accuracy and stability. To support\nreproducibility, we release SSL4EO-S12-downstream, a curated multispectral,\nmultitemporal EO dataset. We present initial results from a public challenge at\nthe 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art\nfoundation models. NeuCo-Bench provides a first step towards community-driven,\nstandardized evaluation of neural embeddings for EO and beyond.", "AI": {"tldr": "NeuCo-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5730\u7403\u89c2\u6d4b\u9886\u57df\u795e\u7ecf\u538b\u7f29\u548c\u8868\u793a\u5b66\u4e60\u7684\u65b0\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b\u53ef\u91cd\u7528\u5d4c\u5165\u8bc4\u4f30\u6d41\u7a0b\u3001\u9690\u85cf\u4efb\u52a1\u6392\u884c\u699c\u548c\u5e73\u8861\u7cbe\u5ea6\u4e0e\u7a33\u5b9a\u6027\u7684\u8bc4\u5206\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u5730\u7403\u89c2\u6d4b\u9886\u57df\u63d0\u4f9b\u793e\u533a\u9a71\u52a8\u7684\u6807\u51c6\u5316\u795e\u7ecf\u5d4c\u5165\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u9884\u8bad\u7ec3\u504f\u5dee\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u56fa\u5b9a\u5927\u5c0f\u5d4c\u5165\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u91cd\u7528\u5d4c\u5165\u8bc4\u4f30\u6d41\u7a0b\u3001\u9690\u85cf\u4efb\u52a1\u6311\u6218\u6a21\u5f0f\u548c\u5e73\u8861\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u53d1\u5e03SSL4EO-S12-downstream\u6570\u636e\u96c6\u652f\u6301\u53ef\u590d\u73b0\u6027\u3002", "result": "\u57282025\u5e74CVPR EARTHVISION\u7814\u8ba8\u4f1a\u4e0a\u8fdb\u884c\u4e86\u516c\u5f00\u6311\u6218\u8d5b\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "NeuCo-Bench\u4e3a\u5730\u7403\u89c2\u6d4b\u53ca\u5176\u4ed6\u9886\u57df\u7684\u795e\u7ecf\u5d4c\u5165\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u793e\u533a\u9a71\u52a8\u7684\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2510.18250", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18250", "abs": "https://arxiv.org/abs/2510.18250", "authors": ["Xiaohan Qin", "Xiaoxing Wang", "Ning Liao", "Cancheng Zhang", "Xiangdong Zhang", "Mingquan Feng", "Jingzhi Wang", "Junchi Yan"], "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning", "comment": null, "summary": "Data quality plays a critical role in enhancing supervised fine-tuning (SFT)\nfor large language models (LLMs), and token-level data selection has emerged as\na promising direction for its fine-grained nature. Despite their strong\nempirical performance, existing token-level selection methods share two key\nlimitations: (1) requiring training or accessing an additional reference model,\nand (2) relying solely on loss information for token selection, which cannot\nwell preserve semantically important tokens that are not favored by loss-based\nmetrics. To address these challenges, we propose ssToken, a Self-modulated and\nSemantic-aware Token Selection approach. ssToken leverages readily accessible\nhistory models to compute the per-token loss difference with the current model,\nwhich serves as a self-modulated signal that enables the model to adaptively\nselect tokens along its optimization trajectory, rather than relying on excess\nloss from an offline-trained reference model as in prior works. We further\nintroduce a semantic-aware, attention-based token importance estimation metric,\northogonal to loss-based selection and providing complementary semantic\ninformation for more effective filtering. Extensive experiments across\ndifferent model families and scales demonstrate that both self-modulated\nselection and semantic-aware selection alone outperform full-data fine-tuning,\nwhile their integration--ssToken--achieves synergistic gains and further\nsurpasses prior token-level selection methods, delivering performance\nimprovements while maintaining training efficiency.", "AI": {"tldr": "\u63d0\u51fassToken\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u8c03\u5236\u548c\u8bed\u4e49\u611f\u77e5\u7684token\u9009\u62e9\u6765\u89e3\u51b3\u73b0\u6709token\u7ea7\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u989d\u5916\u53c2\u8003\u6a21\u578b\u4e14\u80fd\u4fdd\u7559\u8bed\u4e49\u91cd\u8981token\u3002", "motivation": "\u73b0\u6709token\u7ea7\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u6216\u8bbf\u95ee\u989d\u5916\u53c2\u8003\u6a21\u578b\uff0c\u4e14\u4ec5\u4f9d\u8d56\u635f\u5931\u4fe1\u606f\u8fdb\u884ctoken\u9009\u62e9\uff0c\u65e0\u6cd5\u5f88\u597d\u4fdd\u7559\u8bed\u4e49\u91cd\u8981\u4f46\u635f\u5931\u6307\u6807\u4e0d\u504f\u597d\u7684token\u3002", "method": "ssToken\u5229\u7528\u5386\u53f2\u6a21\u578b\u8ba1\u7b97\u5f53\u524d\u6a21\u578b\u7684\u6bcftoken\u635f\u5931\u5dee\u5f02\u4f5c\u4e3a\u81ea\u8c03\u5236\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bed\u4e49\u611f\u77e5token\u91cd\u8981\u6027\u4f30\u8ba1\u6307\u6807\uff0c\u63d0\u4f9b\u8865\u5145\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u81ea\u8c03\u5236\u9009\u62e9\u548c\u8bed\u4e49\u611f\u77e5\u9009\u62e9\u5355\u72ec\u4f7f\u7528\u5747\u4f18\u4e8e\u5168\u6570\u636e\u5fae\u8c03\uff0c\u4e24\u8005\u7ed3\u5408\u7684ssToken\u5b9e\u73b0\u534f\u540c\u589e\u76ca\uff0c\u8d85\u8d8a\u73b0\u6709token\u7ea7\u9009\u62e9\u65b9\u6cd5\u3002", "conclusion": "ssToken\u5728\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u6027\u80fd\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u81ea\u8c03\u5236\u548c\u8bed\u4e49\u611f\u77e5token\u9009\u62e9\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.18713", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.18713", "abs": "https://arxiv.org/abs/2510.18713", "authors": ["Joongkyu Lee", "Seouh-won Yi", "Min-hwan Oh"], "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options", "comment": "Accepted at NeurIPS 2025", "summary": "We study online preference-based reinforcement learning (PbRL) with the goal\nof improving sample efficiency. While a growing body of theoretical work has\nemerged-motivated by PbRL's recent empirical success, particularly in aligning\nlarge language models (LLMs)-most existing studies focus only on pairwise\ncomparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,\nThekumparampil et al., 2024) have explored using multiple comparisons and\nranking feedback, but their performance guarantees fail to improve-and can even\ndeteriorate-as the feedback length increases, despite the richer information\navailable. To address this gap, we adopt the Plackett-Luce (PL) model for\nranking feedback over action subsets and propose M-AUPO, an algorithm that\nselects multiple actions by maximizing the average uncertainty within the\noffered subset. We prove that M-AUPO achieves a suboptimality gap of\n$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}}\n\\right)$, where $T$ is the total number of rounds, $d$ is the feature\ndimension, and $|S_t|$ is the size of the subset at round $t$. This result\nshows that larger subsets directly lead to improved performance and, notably,\nthe bound avoids the exponential dependence on the unknown parameter's norm,\nwhich was a fundamental limitation in most previous works. Moreover, we\nestablish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}}\n\\right)$, where $K$ is the maximum subset size. To the best of our knowledge,\nthis is the first theoretical result in PbRL with ranking feedback that\nexplicitly shows improved sample efficiency as a function of the subset size.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Plackett-Luce\u6392\u540d\u53cd\u9988\u6a21\u578b\u7684\u65b0\u7b97\u6cd5M-AUPO\uff0c\u901a\u8fc7\u6700\u5927\u5316\u63d0\u4f9b\u5b50\u96c6\u5185\u7684\u5e73\u5747\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u591a\u4e2a\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u7406\u8bba\u5de5\u4f5c\u5927\u591a\u53ea\u5173\u6ce8\u6210\u5bf9\u6bd4\u8f83\uff0c\u5c11\u6570\u63a2\u7d22\u591a\u6bd4\u8f83\u548c\u6392\u540d\u53cd\u9988\u7684\u7814\u7a76\u5728\u53cd\u9988\u957f\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u4fdd\u8bc1\u65e0\u6cd5\u6539\u5584\u751a\u81f3\u6076\u5316\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u6392\u540d\u53cd\u9988\u4fe1\u606f\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "\u91c7\u7528Plackett-Luce\u6a21\u578b\u5bf9\u52a8\u4f5c\u5b50\u96c6\u8fdb\u884c\u6392\u540d\u53cd\u9988\uff0c\u63d0\u51faM-AUPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u63d0\u4f9b\u5b50\u96c6\u5185\u7684\u5e73\u5747\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u591a\u4e2a\u52a8\u4f5c\u3002", "result": "M-AUPO\u5b9e\u73b0\u4e86$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$\u7684\u6b21\u4f18\u6027\u5dee\u8ddd\uff0c\u8868\u660e\u66f4\u5927\u7684\u5b50\u96c6\u76f4\u63a5\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u907f\u514d\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u5bf9\u672a\u77e5\u53c2\u6570\u8303\u6570\u7684\u6307\u6570\u4f9d\u8d56\u3002", "conclusion": "\u8fd9\u662fPbRL\u4e2d\u6392\u540d\u53cd\u9988\u9886\u57df\u7684\u9996\u4e2a\u7406\u8bba\u7ed3\u679c\uff0c\u660e\u786e\u663e\u793a\u4e86\u6837\u672c\u6548\u7387\u968f\u5b50\u96c6\u5927\u5c0f\u7684\u589e\u52a0\u800c\u6539\u5584\uff0c\u5efa\u7acb\u4e86\u8fd1\u4e4e\u5339\u914d\u7684\u4e0b\u754c$\\Omega \\left( \\frac{d}{K \\sqrt{T}} \\right)$\u3002"}}
{"id": "2510.18254", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18254", "abs": "https://arxiv.org/abs/2510.18254", "authors": ["Sion Weatherhead", "Flora Salim", "Aaron Belbasis"], "title": "Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning", "comment": null, "summary": "Humans do not just find mistakes after the fact -- we often catch them\nmid-stream because 'reflection' is tied to the goal and its constraints.\nToday's large language models produce reasoning tokens and 'reflective' text,\nbut is it functionally equivalent with human reflective reasoning? Prior work\non closed-ended tasks -- with clear, external 'correctness' signals -- can make\n'reflection' look effective while masking limits in self-correction. We\ntherefore test eight frontier models on a simple, real-world task that is\nopen-ended yet rule-constrained, with auditable success criteria: to produce\nvalid scientific test items, then revise after considering their own critique.\nFirst-pass performance is poor (often zero valid items out of 4 required; mean\n$\\approx$ 1), and reflection yields only modest gains (also $\\approx$ 1).\nCrucially, the second attempt frequently repeats the same violation of\nconstraint, indicating 'corrective gains' arise largely from chance production\nof a valid item rather than error detection and principled,\nconstraint-sensitive repair. Performance before and after reflection\ndeteriorates as open-endedness increases, and models marketed for 'reasoning'\nshow no advantage. Our results suggest that current LLM 'reflection' lacks\nfunctional evidence of the active, goal-driven monitoring that helps humans\nrespect constraints even on a first pass. Until such mechanisms are\ninstantiated in the model itself, reliable performance requires external\nstructure that enforces constraints.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6d4b\u8bd5\u4e868\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4f46\u89c4\u5219\u7ea6\u675f\u7684\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u53cd\u601d\u53ea\u80fd\u5e26\u6765\u6709\u9650\u7684\u6539\u8fdb\uff0c\u4e14\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u7684\u7ea6\u675f\u8fdd\u89c4\uff0c\u8868\u660e\u7f3a\u4e4f\u771f\u6b63\u7684\u4eba\u7c7b\u5f0f\u76ee\u6807\u9a71\u52a8\u76d1\u63a7\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684'\u53cd\u601d'\u80fd\u529b\u662f\u5426\u5728\u529f\u80fd\u4e0a\u7b49\u540c\u4e8e\u4eba\u7c7b\u7684\u53cd\u601d\u63a8\u7406\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u4f46\u89c4\u5219\u7ea6\u675f\u7684\u4efb\u52a1\u4e2d\uff0c\u800c\u975e\u4ec5\u5728\u6709\u660e\u786e\u6b63\u786e\u6027\u4fe1\u53f7\u7684\u5c01\u95ed\u4efb\u52a1\u4e2d\u3002", "method": "\u6d4b\u8bd58\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u4e00\u4e2a\u7b80\u5355\u4f46\u73b0\u5b9e\u7684\u4efb\u52a1\u4e0a\uff1a\u751f\u6210\u6709\u6548\u7684\u79d1\u5b66\u6d4b\u8bd5\u9879\u76ee\uff0c\u7136\u540e\u57fa\u4e8e\u81ea\u6211\u6279\u8bc4\u8fdb\u884c\u4fee\u8ba2\u3002\u4efb\u52a1\u5177\u6709\u53ef\u5ba1\u8ba1\u7684\u6210\u529f\u6807\u51c6\u3002", "result": "\u9996\u6b21\u5c1d\u8bd5\u6027\u80fd\u8f83\u5dee\uff08\u5e73\u5747\u7ea61\u4e2a\u6709\u6548\u9879\u76ee\uff09\uff0c\u53cd\u601d\u540e\u4ec5\u6709\u9002\u5ea6\u6539\u8fdb\uff08\u4e5f\u7ea61\u4e2a\uff09\u3002\u5173\u952e\u53d1\u73b0\u662f\u7b2c\u4e8c\u6b21\u5c1d\u8bd5\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u7684\u7ea6\u675f\u8fdd\u89c4\uff0c\u8868\u660e'\u7ea0\u6b63\u6536\u76ca'\u4e3b\u8981\u6765\u81ea\u5076\u7136\u751f\u6210\u6709\u6548\u9879\u76ee\u800c\u975e\u771f\u6b63\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u539f\u5219\u6027\u4fee\u590d\u3002", "conclusion": "\u5f53\u524dLLM\u7684'\u53cd\u601d'\u7f3a\u4e4f\u529f\u80fd\u6027\u8bc1\u636e\u8868\u660e\u5b58\u5728\u4e3b\u52a8\u3001\u76ee\u6807\u9a71\u52a8\u7684\u76d1\u63a7\u673a\u5236\uff0c\u8fd9\u79cd\u673a\u5236\u80fd\u5e2e\u52a9\u4eba\u7c7b\u5373\u4f7f\u5728\u9996\u6b21\u5c1d\u8bd5\u65f6\u4e5f\u5c0a\u91cd\u7ea6\u675f\u3002\u5728\u6a21\u578b\u672c\u8eab\u5b9e\u4f8b\u5316\u6b64\u7c7b\u673a\u5236\u4e4b\u524d\uff0c\u53ef\u9760\u6027\u80fd\u9700\u8981\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u7684\u5916\u90e8\u7ed3\u6784\u3002"}}
{"id": "2510.18783", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.18783", "abs": "https://arxiv.org/abs/2510.18783", "authors": ["Jan Sobotka", "Petr \u0160im\u00e1nek", "Pavel Kord\u00edk"], "title": "Enhancing Fractional Gradient Descent with Learned Optimizers", "comment": null, "summary": "Fractional Gradient Descent (FGD) offers a novel and promising way to\naccelerate optimization by incorporating fractional calculus into machine\nlearning. Although FGD has shown encouraging initial results across various\noptimization tasks, it faces significant challenges with convergence behavior\nand hyperparameter selection. Moreover, the impact of its hyperparameters is\nnot fully understood, and scheduling them is particularly difficult in\nnon-convex settings such as neural network training. To address these issues,\nwe propose a novel approach called Learning to Optimize Caputo Fractional\nGradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the\nhyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule\noutperforms CFGD with static hyperparameters found through an extensive search\nand, in some tasks, achieves performance comparable to a fully black-box\nmeta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for\nresearchers to identify high-performing hyperparameters and gain insights on\nhow to leverage the history-dependence of the fractional differential in\noptimization.", "AI": {"tldr": "\u63d0\u51faL2O-CFGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u52a8\u6001\u8c03\u6574Caputo\u5206\u6570\u68af\u5ea6\u4e0b\u964d\u7684\u8d85\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u5206\u6570\u68af\u5ea6\u4e0b\u964d\u5728\u6536\u655b\u884c\u4e3a\u548c\u8d85\u53c2\u6570\u9009\u62e9\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u5206\u6570\u68af\u5ea6\u4e0b\u964d\u867d\u7136\u80fd\u52a0\u901f\u4f18\u5316\uff0c\u4f46\u5728\u6536\u655b\u884c\u4e3a\u548c\u8d85\u53c2\u6570\u9009\u62e9\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u975e\u51f8\u8bbe\u7f6e\u4e0b\u8d85\u53c2\u6570\u8c03\u5ea6\u56f0\u96be\u3002", "method": "\u63d0\u51faL2O-CFGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u52a8\u6001\u8c03\u6574Caputo\u5206\u6570\u68af\u5ea6\u4e0b\u964d\u7684\u8d85\u53c2\u6570\uff0c\u751f\u6210\u5143\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5143\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\u4f18\u4e8e\u901a\u8fc7\u5e7f\u6cdb\u641c\u7d22\u627e\u5230\u7684\u9759\u6001\u8d85\u53c2\u6570CFGD\uff0c\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u6027\u80fd\u53ef\u4e0e\u5b8c\u5168\u9ed1\u76d2\u5143\u5b66\u4e60\u4f18\u5316\u5668\u76f8\u5ab2\u7f8e\u3002", "conclusion": "L2O-CFGD\u53ef\u4f5c\u4e3a\u5f3a\u5927\u5de5\u5177\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc6\u522b\u9ad8\u6027\u80fd\u8d85\u53c2\u6570\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u5206\u6570\u5fae\u5206\u7684\u5386\u53f2\u4f9d\u8d56\u6027\u5728\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.18342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18342", "abs": "https://arxiv.org/abs/2510.18342", "authors": ["Peng Tang", "Xiaoxiao Yan", "Xiaobin Hu", "Yuning Cui", "Donghao Luo", "Jiangning Zhang", "Pengcheng Xu", "Jinlong Peng", "Qingdong He", "Feiyue Huang", "Song Xue", "Tobias Lasser"], "title": "ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection", "comment": "Under Review", "summary": "Multi-class unsupervised anomaly detection (MUAD) has garnered growing\nresearch interest, as it seeks to develop a unified model for anomaly detection\nacross multiple classes, i.e., eliminating the need to train separate models\nfor distinct objects and thereby saving substantial computational resources.\nUnder the MUAD setting, while advanced Transformer-based architectures have\nbrought significant performance improvements, identity shortcuts persist: they\ndirectly copy inputs to outputs, narrowing the gap in reconstruction errors\nbetween normal and abnormal cases, and thereby making the two harder to\ndistinguish. Therefore, we propose ShortcutBreaker, a novel unified\nfeature-reconstruction framework for MUAD tasks, featuring two key innovations\nto address the issue of shortcuts. First, drawing on matrix rank inequality, we\ndesign a low-rank noisy bottleneck (LRNB) to project highdimensional features\ninto a low-rank latent space, and theoretically demonstrate its capacity to\nprevent trivial identity reproduction. Second, leveraging ViTs global modeling\ncapability instead of merely focusing on local features, we incorporate a\nglobal perturbation attention to prevent information shortcuts in the decoders.\nExtensive experiments are performed on four widely used anomaly detection\nbenchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)\nand one medical dataset (Universal Medical). The proposed method achieves a\nremarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four\ndatasets, respectively, consistently outperforming previous MUAD methods across\ndifferent scenarios.", "AI": {"tldr": "\u63d0\u51faShortcutBreaker\u6846\u67b6\u89e3\u51b3\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u79e9\u566a\u58f0\u74f6\u9888\u548c\u5168\u5c40\u6270\u52a8\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u8f93\u5165\u76f4\u63a5\u590d\u5236\u5230\u8f93\u51fa\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u9700\u8981\u7edf\u4e00\u6a21\u578b\u68c0\u6d4b\u591a\u4e2a\u7c7b\u522b\u7684\u5f02\u5e38\uff0c\u4f46\u73b0\u6709Transformer\u67b6\u6784\u5b58\u5728\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u5373\u76f4\u63a5\u590d\u5236\u8f93\u5165\u5230\u8f93\u51fa\uff0c\u5bfc\u81f4\u6b63\u5e38\u548c\u5f02\u5e38\u6837\u672c\u7684\u91cd\u6784\u8bef\u5dee\u5dee\u5f02\u7f29\u5c0f\uff0c\u96be\u4ee5\u533a\u5206\u3002", "method": "\u63d0\u51faShortcutBreaker\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u77e9\u9635\u79e9\u4e0d\u7b49\u5f0f\u8bbe\u8ba1\u4f4e\u79e9\u566a\u58f0\u74f6\u9888\uff0c\u5c06\u9ad8\u7ef4\u7279\u5f81\u6295\u5f71\u5230\u4f4e\u79e9\u6f5c\u5728\u7a7a\u95f4\u9632\u6b62\u5e73\u51e1\u8eab\u4efd\u590d\u5236\uff1b2\uff09\u5229\u7528ViT\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u5f15\u5165\u5168\u5c40\u6270\u52a8\u6ce8\u610f\u529b\uff0c\u9632\u6b62\u89e3\u7801\u5668\u4e2d\u7684\u4fe1\u606f\u6377\u5f84\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u4e09\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6MVTec-AD\u3001ViSA\u3001Real-IAD\u548c\u4e00\u4e2a\u533b\u5b66\u6570\u636e\u96c6Universal Medical\uff09\u4e0a\u5206\u522b\u8fbe\u523099.8%\u300198.9%\u300190.6%\u548c87.8%\u7684\u56fe\u50cf\u7ea7AUROC\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709MUAD\u65b9\u6cd5\u3002", "conclusion": "ShortcutBreaker\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u8eab\u4efd\u6377\u5f84\u95ee\u9898\uff0c\u5728\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18760", "categories": ["eess.SP", "cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.18760", "abs": "https://arxiv.org/abs/2510.18760", "authors": ["Mouna Gharbi", "Silvia Villa", "Emilie Chouzenoux", "Jean-Christophe Pesquet", "Laurent Duval"], "title": "Analyse comparative d'algorithmes de restauration en architecture d\u00e9pli\u00e9e pour des signaux chromatographiques parcimonieux", "comment": "4 pages, in French, GRETSI Symposium on Signal and Image Processing,\n  Strasbourg, France, August 2025", "summary": "Data restoration from degraded observations, of sparsity hypotheses, is an\nactive field of study. Traditional iterative optimization methods are now\ncomplemented by deep learning techniques. The development of unfolded methods\nbenefits from both families. We carry out a comparative study of three\narchitectures on parameterized chromatographic signal databases, highlighting\nthe performance of these approaches, especially when employing metrics adapted\nto physico-chemical peak signal characterization.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cd\u5c55\u5f00\u5f0f\u67b6\u6784\u5728\u53c2\u6570\u5316\u8272\u8c31\u4fe1\u53f7\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u7269\u7406\u5316\u5b66\u5cf0\u4fe1\u53f7\u8868\u5f81\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u6062\u590d\u662f\u7814\u7a76\u7a00\u758f\u5047\u8bbe\u4e0b\u9000\u5316\u89c2\u6d4b\u6570\u636e\u91cd\u5efa\u7684\u6d3b\u8dc3\u9886\u57df\uff0c\u4f20\u7edf\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u76f8\u4e92\u8865\u5145\uff0c\u5c55\u5f00\u5f0f\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u5728\u53c2\u6570\u5316\u8272\u8c31\u4fe1\u53f7\u6570\u636e\u5e93\u4e0a\u5bf9\u4e09\u79cd\u5c55\u5f00\u5f0f\u67b6\u6784\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\uff0c\u4f7f\u7528\u9002\u5e94\u7269\u7406\u5316\u5b66\u5cf0\u4fe1\u53f7\u8868\u5f81\u7684\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7a81\u51fa\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u91c7\u7528\u9002\u5e94\u7269\u7406\u5316\u5b66\u5cf0\u4fe1\u53f7\u8868\u5f81\u7684\u6307\u6807\u65f6\u3002", "conclusion": "\u5c55\u5f00\u5f0f\u65b9\u6cd5\u7ed3\u5408\u4e86\u4f20\u7edf\u4f18\u5316\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u5728\u8272\u8c31\u4fe1\u53f7\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u7269\u7406\u5316\u5b66\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u65f6\u3002"}}
{"id": "2510.18442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18442", "abs": "https://arxiv.org/abs/2510.18442", "authors": ["Ziwei Deng", "Mian Deng", "Chenjing Liang", "Zeming Gao", "Chennan Ma", "Chenxing Lin", "Haipeng Zhang", "Songzhu Mei", "Cheng Wang", "Siqi Shen"], "title": "PlanU: Large Language Model Decision Making through Planning under Uncertainty", "comment": "38 pages, 19 figures, NeurIPS 2025 Accepted", "summary": "Large Language Models (LLMs) are increasingly being explored across a range\nof decision-making tasks. However, LLMs sometimes struggle with decision-making\ntasks under uncertainty that are relatively easy for humans, such as planning\nactions in stochastic environments. The adoption of LLMs for decision-making is\nimpeded by uncertainty challenges, such as LLM uncertainty and environmental\nuncertainty. LLM uncertainty arises from the stochastic sampling process\ninherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM\nuncertainty through multiple reasoning chains or search trees. However, these\napproaches overlook environmental uncertainty, which leads to poor performance\nin environments with stochastic state transitions. Some recent LDM approaches\ndeal with uncertainty by forecasting the probability of unknown variables.\nHowever, they are not designed for multi-step decision-making tasks that\nrequire interaction with the environment. To address uncertainty in LLM\ndecision-making, we introduce PlanU, an LLM-based planning method that captures\nuncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of\neach node in the MCTS as a quantile distribution, which uses a set of quantiles\nto represent the return distribution. To balance exploration and exploitation\nduring tree search, PlanU introduces an Upper Confidence Bounds with Curiosity\n(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive\nexperiments, we demonstrate the effectiveness of PlanU in LLM-based\ndecision-making tasks under uncertainty.", "AI": {"tldr": "PlanU\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6355\u83b7\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u5206\u4f4d\u6570\u5206\u5e03\u5efa\u6a21\u56de\u62a5\uff0c\u5e76\u5f15\u5165UCC\u8bc4\u5206\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "LLM\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u968f\u673a\u73af\u5883\u4e2d\u89c4\u5212\u884c\u52a8\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5904\u7406LLM\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5728\u968f\u673a\u72b6\u6001\u8f6c\u6362\u73af\u5883\u4e2d\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u5c06\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u5230\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e2d\uff0c\u4f7f\u7528\u5206\u4f4d\u6570\u5206\u5e03\u8868\u793a\u6bcf\u4e2a\u8282\u70b9\u7684\u56de\u62a5\u5206\u5e03\uff0c\u5e76\u5f15\u5165UCC\u8bc4\u5206\u6765\u4f30\u8ba1MCTS\u8282\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PlanU\u5728\u57fa\u4e8eLLM\u7684\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PlanU\u6210\u529f\u89e3\u51b3\u4e86LLM\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u968f\u673a\u73af\u5883\u4e2d\u7684\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u3002"}}
{"id": "2510.18074", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.18074", "abs": "https://arxiv.org/abs/2510.18074", "authors": ["Nadir Farhi"], "title": "R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning", "comment": "27 pages", "summary": "In this work, we address the problem of determining reliable policies in\nreinforcement learning (RL), with a focus on optimization under uncertainty and\nthe need for performance guarantees. While classical RL algorithms aim at\nmaximizing the expected return, many real-world applications - such as routing,\nresource allocation, or sequential decision-making under risk - require\nstrategies that ensure not only high average performance but also a guaranteed\nprobability of success. To this end, we propose a novel formulation in which\nthe objective is to maximize the probability that the cumulative return exceeds\na prescribed threshold. We demonstrate that this reliable RL problem can be\nreformulated, via a state-augmented representation, into a standard RL problem,\nthereby allowing the use of existing RL and deep RL algorithms without the need\nfor entirely new algorithmic frameworks. Theoretical results establish the\nequivalence of the two formulations and show that reliable strategies can be\nderived by appropriately adapting well-known methods such as Q-learning or\nDueling Double DQN. To illustrate the practical relevance of the approach, we\nconsider the problem of reliable routing, where the goal is not to minimize the\nexpected travel time but rather to maximize the probability of reaching the\ndestination within a given time budget. Numerical experiments confirm that the\nproposed formulation leads to policies that effectively balance efficiency and\nreliability, highlighting the potential of reliable RL for applications in\nstochastic and safety-critical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u9760\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u76ee\u6807\u4e0d\u662f\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\uff0c\u800c\u662f\u6700\u5927\u5316\u7d2f\u79ef\u56de\u62a5\u8d85\u8fc7\u9884\u8bbe\u9608\u503c\u7684\u6982\u7387\uff0c\u9002\u7528\u4e8e\u9700\u8981\u6027\u80fd\u4fdd\u8bc1\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e13\u6ce8\u4e8e\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\uff0c\u4f46\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\uff08\u5982\u8def\u7531\u3001\u8d44\u6e90\u5206\u914d\u3001\u98ce\u9669\u51b3\u7b56\uff09\u9700\u8981\u786e\u4fdd\u9ad8\u5e73\u5747\u6027\u80fd\u548c\u6210\u529f\u6982\u7387\u4fdd\u8bc1\u7684\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u72b6\u6001\u589e\u5f3a\u8868\u793a\u5c06\u53ef\u9760RL\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6807\u51c6RL\u95ee\u9898\uff0c\u5141\u8bb8\u4f7f\u7528\u73b0\u6709RL\u548c\u6df1\u5ea6RL\u7b97\u6cd5\uff0c\u65e0\u9700\u5168\u65b0\u7b97\u6cd5\u6846\u67b6\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8bc1\u660e\u4e24\u79cd\u8868\u8ff0\u7684\u7b49\u4ef7\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u53ef\u9760\u8def\u7531\u95ee\u9898\u4e2d\u80fd\u6709\u6548\u5e73\u8861\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u53ef\u9760\u5f3a\u5316\u5b66\u4e60\u5728\u968f\u673a\u548c\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u73b0\u6709\u65b9\u6cd5\u63a8\u5bfc\u51fa\u53ef\u9760\u7b56\u7565\u3002"}}
{"id": "2510.18470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18470", "abs": "https://arxiv.org/abs/2510.18470", "authors": ["Shaobo Wang", "Yongliang Miao", "Yuancheng Liu", "and Qianli Ma", "Ning Liao", "Linfeng Zhang"], "title": "CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs", "comment": "14 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, but scaling their performance often relies on massive reasoning\ndatasets that are computationally expensive to train on. Existing data\nselection methods aim to curate smaller, high-quality subsets but often rely on\ncostly external models or opaque heuristics. In this work, we shift the focus\nfrom external heuristics to the model's internal mechanisms. We find that\ncomplex reasoning tasks consistently activate a sparse, specialized subset of\nattention heads, forming core reasoning circuits. Building on this insight, we\npropose CircuitSeer, a novel data selection method that quantifies the\nreasoning complexity of data by measuring its influence on these crucial\ncircuits. Extensive experiments on 4 models and 9 datasets demonstrate\nCircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of\ndata selected by our method achieves a 1.4-point gain in average Pass@1 over\ntraining on the full dataset, highlighting its efficiency and effectiveness.", "AI": {"tldr": "CircuitSeer\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6838\u5fc3\u63a8\u7406\u7535\u8def\u6765\u91cf\u5316\u6570\u636e\u590d\u6742\u5ea6\uff0c\u4ec5\u4f7f\u752810%\u6570\u636e\u5c31\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u6a21\u578b\u6216\u4e0d\u900f\u660e\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u6269\u5c55\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53d1\u73b0\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4f1a\u6fc0\u6d3b\u7a00\u758f\u7684\u4e13\u7528\u6ce8\u610f\u529b\u5934\uff0c\u5f62\u6210\u6838\u5fc3\u63a8\u7406\u7535\u8def\u3002CircuitSeer\u901a\u8fc7\u6d4b\u91cf\u6570\u636e\u5bf9\u8fd9\u4e9b\u5173\u952e\u7535\u8def\u7684\u5f71\u54cd\u6765\u91cf\u5316\u63a8\u7406\u590d\u6742\u5ea6", "result": "\u57284\u4e2a\u6a21\u578b\u548c9\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCircuitSeer\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u9009\u62e9\u768410%\u6570\u636e\u5fae\u8c03Qwen2.5-Math-7B\uff0c\u5e73\u5747Pass@1\u6bd4\u5b8c\u6574\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u9ad8\u4e861.4\u4e2a\u767e\u5206\u70b9", "conclusion": "\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5CircuitSeer\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u6210\u672c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2510.18103", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "68T07, 92C50", "I.2.6; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.18103", "abs": "https://arxiv.org/abs/2510.18103", "authors": ["Nursultan Mamatov", "Philipp Kellmeyer"], "title": "Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV", "comment": "38 pages, 5 figures, 2 tables, 3 appendices", "summary": "Accurate early prediction of in-hospital mortality in intensive care units\n(ICUs) is essential for timely clinical intervention and efficient resource\nallocation. This study develops and evaluates machine learning models that\nintegrate both structured clinical data and unstructured textual information,\nspecifically discharge summaries and radiology reports, from the MIMIC-IV\ndatabase. We used LASSO and XGBoost for feature selection, followed by a\nmultivariate logistic regression trained on the top features identified by both\nmodels. Incorporating textual features using TF-IDF and BERT embeddings\nsignificantly improved predictive performance. The final logistic regression\nmodel, which combined structured and textual input, achieved an AUC of 0.918,\ncompared to 0.753 when using structured data alone, a relative improvement 22%.\nThe analysis of the decision curve demonstrated a superior standardized net\nbenefit in a wide range of threshold probabilities (0.2-0.8), confirming the\nclinical utility of the model. These results underscore the added prognostic\nvalue of unstructured clinical notes and support their integration into\ninterpretable feature-driven risk prediction models for ICU patients.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u7ed3\u5408\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u4fe1\u606f\uff08\u51fa\u9662\u5c0f\u7ed3\u548c\u653e\u5c04\u5b66\u62a5\u544a\uff09\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8eICU\u4f4f\u9662\u6b7b\u4ea1\u7387\u7684\u65e9\u671f\u9884\u6d4b\u3002\u901a\u8fc7LASSO\u548cXGBoost\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u7ed3\u5408TF-IDF\u548cBERT\u5d4c\u5165\u5904\u7406\u6587\u672c\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "ICU\u4f4f\u9662\u6b7b\u4ea1\u7387\u7684\u51c6\u786e\u65e9\u671f\u9884\u6d4b\u5bf9\u4e8e\u53ca\u65f6\u4e34\u5e8a\u5e72\u9884\u548c\u9ad8\u6548\u8d44\u6e90\u914d\u7f6e\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u975e\u7ed3\u6784\u5316\u6587\u672c\u4fe1\u606f\u4e2d\u7684\u5b9d\u8d35\u4e34\u5e8a\u6d1e\u5bdf\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u5e93\uff0c\u7ed3\u5408LASSO\u548cXGBoost\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u91c7\u7528TF-IDF\u548cBERT\u5d4c\u5165\u5904\u7406\u6587\u672c\u7279\u5f81\uff0c\u6784\u5efa\u591a\u5143\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3002", "result": "\u7ed3\u5408\u7ed3\u6784\u5316\u548c\u6587\u672c\u8f93\u5165\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578bAUC\u8fbe\u52300.918\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u7ed3\u6784\u5316\u6570\u636e\u76840.753\uff0c\u76f8\u5bf9\u63d0\u534722%\u3002\u51b3\u7b56\u66f2\u7ebf\u5206\u6790\u663e\u793a\u5728\u5e7f\u6cdb\u9608\u503c\u6982\u7387\u8303\u56f4\u5185\u5177\u6709\u4f18\u8d8a\u7684\u6807\u51c6\u5316\u51c0\u6548\u76ca\u3002", "conclusion": "\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u8bb0\u5f55\u5177\u6709\u989d\u5916\u7684\u9884\u540e\u4ef7\u503c\uff0c\u5e94\u5c06\u5176\u6574\u5408\u5230\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u9a71\u52a8\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u4ee5\u6539\u5584ICU\u60a3\u8005\u7684\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2510.18121", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18121", "abs": "https://arxiv.org/abs/2510.18121", "authors": ["Yonghao Zhuang", "Junda Chen", "Bo Pang", "Yi Gu", "Yibo Zhu", "Yimin Jiang", "Ion Stoica", "Eric Xing", "Hao Zhang"], "title": "Efficient Long-context Language Model Training by Core Attention Disaggregation", "comment": null, "summary": "We present core attention disaggregation (CAD), a technique that improves\nlong-context large language model training by decoupling the core attention\ncomputation, softmax(QK^T)V, from the rest of the model and executing it on a\nseparate pool of devices. In existing systems, core attention is colocated with\nother layers; at long context lengths, its quadratic compute growth compared to\nthe near-linear growth of other components causes load imbalance and stragglers\nacross data and pipeline parallel groups. CAD is enabled by two observations.\nFirst, core attention is stateless: it has no trainable parameters and only\nminimal transient data, so balancing reduces to scheduling compute-bound tasks.\nSecond, it is composable: modern attention kernels retain high efficiency when\nprocessing fused batches of token-level shards with arbitrary lengths. CAD\npartitions core attention into token-level tasks and dispatches them to\ndedicated attention servers, which dynamically rebatch tasks to equalize\ncompute without sacrificing kernel efficiency. We implement CAD in a system\ncalled DistCA, which uses a ping-pong execution scheme to fully overlap\ncommunication with computation and in-place execution on attention servers to\nreduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,\nDistCA improves end-to-end training throughput by up to 1.35x, eliminates data\nand pipeline parallel stragglers, and achieves near-perfect compute and memory\nbalance.", "AI": {"tldr": "\u63d0\u51fa\u6838\u5fc3\u6ce8\u610f\u529b\u89e3\u8026\uff08CAD\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u6838\u5fc3\u6ce8\u610f\u529b\u8ba1\u7b97\u4e0e\u5176\u4ed6\u6a21\u578b\u5c42\u5206\u79bb\u5e76\u5728\u4e13\u7528\u8bbe\u5907\u4e0a\u6267\u884c\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4e2d\u6838\u5fc3\u6ce8\u610f\u529b\u4e0e\u5176\u4ed6\u5c42\u5171\u5b58\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5176\u4e8c\u6b21\u8ba1\u7b97\u589e\u957f\u5bfc\u81f4\u6570\u636e\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u7ec4\u4e4b\u95f4\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u6ede\u540e\u95ee\u9898\u3002", "method": "\u5c06\u6838\u5fc3\u6ce8\u610f\u529b\u5206\u89e3\u4e3atoken\u7ea7\u4efb\u52a1\u5e76\u5206\u53d1\u5230\u4e13\u7528\u6ce8\u610f\u529b\u670d\u52a1\u5668\uff0c\u52a8\u6001\u91cd\u65b0\u6279\u5904\u7406\u4efb\u52a1\u4ee5\u5747\u8861\u8ba1\u7b97\u800c\u4e0d\u727a\u7272\u5185\u6838\u6548\u7387\uff0c\u4f7f\u7528\u4e52\u4e53\u6267\u884c\u65b9\u6848\u5b8c\u5168\u91cd\u53e0\u901a\u4fe1\u4e0e\u8ba1\u7b97\u3002", "result": "\u5728512\u4e2aH200 GPU\u548c512k token\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cDistCA\u7cfb\u7edf\u5c06\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.35\u500d\uff0c\u6d88\u9664\u6570\u636e\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u6ede\u540e\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5e73\u8861\u3002", "conclusion": "CAD\u6280\u672f\u901a\u8fc7\u89e3\u8026\u6838\u5fc3\u6ce8\u610f\u529b\u8ba1\u7b97\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18554", "abs": "https://arxiv.org/abs/2510.18554", "authors": ["Federico Barbero", "Xiangming Gu", "Christopher A. Choquette-Choo", "Chawin Sitawarin", "Matthew Jagielski", "Itay Yona", "Petar Veli\u010dkovi\u0107", "Ilia Shumailov", "Jamie Hayes"], "title": "Extracting alignment data in open models", "comment": null, "summary": "In this work, we show that it is possible to extract significant amounts of\nalignment training data from a post-trained model -- useful to steer the model\nto improve certain capabilities such as long-context reasoning, safety,\ninstruction following, and maths. While the majority of related work on\nmemorisation has focused on measuring success of training data extraction\nthrough string matching, we argue that embedding models are better suited for\nour specific goals. Distances measured through a high quality embedding model\ncan identify semantic similarities between strings that a different metric such\nas edit distance will struggle to capture. In fact, in our investigation,\napproximate string matching would have severely undercounted (by a conservative\nestimate of $10\\times$) the amount of data that can be extracted due to trivial\nartifacts that deflate the metric. Interestingly, we find that models readily\nregurgitate training data that was used in post-training phases such as SFT or\nRL. We show that this data can be then used to train a base model, recovering a\nmeaningful amount of the original performance. We believe our work exposes a\npossibly overlooked risk towards extracting alignment data. Finally, our work\nopens up an interesting discussion on the downstream effects of distillation\npractices: since models seem to be regurgitating aspects of their training set,\ndistillation can therefore be thought of as indirectly training on the model's\noriginal dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u4ece\u540e\u8bad\u7ec3\u6a21\u578b\u4e2d\u63d0\u53d6\u5927\u91cf\u5bf9\u9f50\u8bad\u7ec3\u6570\u636e\uff0c\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u5b89\u5168\u6027\u3001\u6307\u4ee4\u9075\u5faa\u548c\u6570\u5b66\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5bb9\u6613\u91cd\u73b0SFT\u6216RL\u7b49\u540e\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u7684\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u53ef\u7528\u4e8e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u6062\u590d\u76f8\u5f53\u7a0b\u5ea6\u7684\u539f\u59cb\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u4ece\u540e\u8bad\u7ec3\u6a21\u578b\u4e2d\u63d0\u53d6\u5bf9\u9f50\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u80fd\u6027\uff0c\u63ed\u793a\u53ef\u80fd\u88ab\u5ffd\u89c6\u7684\u5bf9\u9f50\u6570\u636e\u63d0\u53d6\u98ce\u9669\uff0c\u5e76\u8ba8\u8bba\u84b8\u998f\u5b9e\u8df5\u7684\u4e0b\u6e38\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u9ad8\u8d28\u91cf\u5d4c\u5165\u6a21\u578b\u6765\u8861\u91cf\u5b57\u7b26\u4e32\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5b57\u7b26\u4e32\u5339\u914d\u65b9\u6cd5\u3002\u901a\u8fc7\u5d4c\u5165\u8ddd\u79bb\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u514b\u670d\u4e86\u7f16\u8f91\u8ddd\u79bb\u7b49\u4f20\u7edf\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd1\u4f3c\u5b57\u7b26\u4e32\u5339\u914d\u4f1a\u4e25\u91cd\u4f4e\u4f30\u53ef\u63d0\u53d6\u7684\u6570\u636e\u91cf\uff08\u4fdd\u5b88\u4f30\u8ba1\u4f4e\u4f3010\u500d\uff09\u3002\u6a21\u578b\u5bb9\u6613\u91cd\u73b0\u540e\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u7684\u6570\u636e\uff0c\u8fd9\u4e9b\u63d0\u53d6\u7684\u6570\u636e\u53ef\u7528\u4e8e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u6062\u590d\u6709\u610f\u4e49\u7684\u539f\u59cb\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u66b4\u9732\u4e86\u63d0\u53d6\u5bf9\u9f50\u6570\u636e\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5e76\u5f00\u542f\u4e86\u5173\u4e8e\u84b8\u998f\u5b9e\u8df5\u4e0b\u6e38\u5f71\u54cd\u7684\u8ba8\u8bba\uff1a\u7531\u4e8e\u6a21\u578b\u4f3c\u4e4e\u5728\u91cd\u73b0\u5176\u8bad\u7ec3\u96c6\u7684\u67d0\u4e9b\u65b9\u9762\uff0c\u84b8\u998f\u53ef\u4ee5\u88ab\u89c6\u4e3a\u95f4\u63a5\u5728\u6a21\u578b\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002"}}
{"id": "2510.18631", "categories": ["cs.AI", "cs.LO", "03B60"], "pdf": "https://arxiv.org/pdf/2510.18631", "abs": "https://arxiv.org/abs/2510.18631", "authors": ["Carlo Proietti", "Antonio Yuste-Ginel"], "title": "Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises", "comment": null, "summary": "Modelling qualitative uncertainty in formal argumentation is essential both\nfor practical applications and theoretical understanding. Yet, most of the\nexisting works focus on \\textit{abstract} models for arguing with uncertainty.\nFollowing a recent trend in the literature, we tackle the open question of\nstudying plausible instantiations of these abstract models. To do so, we ground\nthe uncertainty of arguments in their components, structured within rules and\npremises. Our main technical contributions are: i) the introduction of a notion\nof expressivity that can handle abstract and structured formalisms, and ii) the\npresentation of both negative and positive expressivity results, comparing the\nexpressivity of abstract and structured models of argumentation with\nuncertainty. These results affect incomplete abstract argumentation frameworks,\nand their extension with dependencies, on the abstract side, and ASPIC+, on the\nstructured side.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f62\u5f0f\u8bba\u8bc1\u4e2d\u5b9a\u6027\u4e0d\u786e\u5b9a\u6027\u7684\u5efa\u6a21\uff0c\u6bd4\u8f83\u4e86\u62bd\u8c61\u6a21\u578b\u548c\u7ed3\u6784\u5316\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u5904\u7406\u8fd9\u4e24\u79cd\u5f62\u5f0f\u4e3b\u4e49\u7684\u65b0\u8868\u8fbe\u6027\u6982\u5ff5\uff0c\u5e76\u7ed9\u51fa\u4e86\u8d1f\u9762\u548c\u6b63\u9762\u7684\u8868\u8fbe\u6027\u7ed3\u679c\u3002", "motivation": "\u5728\u5f62\u5f0f\u8bba\u8bc1\u4e2d\u5efa\u6a21\u5b9a\u6027\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u548c\u7406\u8bba\u7406\u89e3\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5173\u6ce8\u62bd\u8c61\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u4e9b\u62bd\u8c61\u6a21\u578b\u7684\u5408\u7406\u5b9e\u4f8b\u5316\uff0c\u5c06\u8bba\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\uff08\u89c4\u5219\u548c\u524d\u63d0\uff09\u3002", "method": "\u901a\u8fc7\u5c06\u8bba\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\uff08\u89c4\u5219\u548c\u524d\u63d0\uff09\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u62bd\u8c61\u548c\u7ed3\u6784\u5316\u5f62\u5f0f\u4e3b\u4e49\u7684\u8868\u8fbe\u6027\u6982\u5ff5\uff0c\u6bd4\u8f83\u4e86\u62bd\u8c61\u6a21\u578b\uff08\u5305\u62ec\u4e0d\u5b8c\u6574\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u53ca\u5176\u4f9d\u8d56\u6269\u5c55\uff09\u548c\u7ed3\u6784\u5316\u6a21\u578b\uff08ASPIC+\uff09\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u8d1f\u9762\u548c\u6b63\u9762\u7684\u8868\u8fbe\u6027\u7ed3\u679c\uff0c\u8868\u660e\u62bd\u8c61\u6a21\u578b\u548c\u7ed3\u6784\u5316\u6a21\u578b\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u4e9b\u7ed3\u679c\u5f71\u54cd\u4e86\u4e0d\u5b8c\u6574\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u53ca\u5176\u4f9d\u8d56\u6269\u5c55\u3002", "conclusion": "\u901a\u8fc7\u6bd4\u8f83\u62bd\u8c61\u548c\u7ed3\u6784\u5316\u8bba\u8bc1\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u5f62\u5f0f\u8bba\u8bc1\u4e2d\u5b9a\u6027\u4e0d\u786e\u5b9a\u6027\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4e0d\u540c\u5efa\u6a21\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2510.18751", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18751", "abs": "https://arxiv.org/abs/2510.18751", "authors": ["Patterson Hsieh", "Jerry Yeh", "Mao-Chi He", "Wen-Han Hsieh", "Elvis Hsieh"], "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation", "comment": null, "summary": "Climate change is intensifying the occurrence of harmful algal bloom (HAB),\nparticularly cyanobacteria, which threaten aquatic ecosystems and human health\nthrough oxygen depletion, toxin release, and disruption of marine biodiversity.\nTraditional monitoring approaches, such as manual water sampling, remain\nlabor-intensive and limited in spatial and temporal coverage. Recent advances\nin vision-language models (VLMs) for remote sensing have shown potential for\nscalable AI-driven solutions, yet challenges remain in reasoning over imagery\nand quantifying bloom severity. In this work, we introduce ALGae Observation\nand Segmentation (ALGOS), a segmentation-and-reasoning system for HAB\nmonitoring that combines remote sensing image understanding with severity\nestimation. Our approach integrates GeoSAM-assisted human evaluation for\nhigh-quality segmentation mask curation and fine-tunes vision language model on\nseverity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)\nfrom NASA. Experiments demonstrate that ALGOS achieves robust performance on\nboth segmentation and severity-level estimation, paving the way toward\npractical and automated cyanobacterial monitoring systems.", "AI": {"tldr": "ALGOS\u662f\u4e00\u4e2a\u7528\u4e8e\u6709\u5bb3\u85fb\u534e\u76d1\u6d4b\u7684\u5206\u5272\u548c\u63a8\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u9065\u611f\u56fe\u50cf\u7406\u89e3\u548c\u4e25\u91cd\u7a0b\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7GeoSAM\u8f85\u52a9\u4eba\u5de5\u8bc4\u4f30\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u5206\u5272\u548c\u4e25\u91cd\u7a0b\u5ea6\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u7a33\u5065\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6709\u5bb3\u85fb\u534e\u7684\u53d1\u751f\uff0c\u7279\u522b\u662f\u84dd\u85fb\uff0c\u901a\u8fc7\u8017\u6c27\u3001\u91ca\u653e\u6bd2\u7d20\u548c\u7834\u574f\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u5a01\u80c1\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u5065\u5eb7\u3002\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u65f6\u7a7a\u8986\u76d6\u6709\u9650\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u9065\u611f\u56fe\u50cf\u7406\u89e3\u4e0e\u4e25\u91cd\u7a0b\u5ea6\u4f30\u8ba1\uff0c\u96c6\u6210GeoSAM\u8f85\u52a9\u4eba\u5de5\u8bc4\u4f30\u8fdb\u884c\u9ad8\u8d28\u91cf\u5206\u5272\u63a9\u7801\u7ba1\u7406\uff0c\u5e76\u4f7f\u7528NASA\u7684\u84dd\u85fb\u805a\u5408\u4eba\u5de5\u6807\u7b7e\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eALGOS\u5728\u5206\u5272\u548c\u4e25\u91cd\u7a0b\u5ea6\u7ea7\u522b\u4f30\u8ba1\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "ALGOS\u4e3a\u5b9e\u7528\u548c\u81ea\u52a8\u5316\u7684\u84dd\u85fb\u76d1\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.18238", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18238", "abs": "https://arxiv.org/abs/2510.18238", "authors": ["Bryan Wilder", "Angela Zhou"], "title": "Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards", "comment": "Accepted at NeurIPS 2025", "summary": "There has been increasing research interest in AI/ML for social impact, and\ncorrespondingly more publication venues have refined review criteria for\npractice-driven AI/ML research. However, these review guidelines tend to most\nconcretely recognize projects that simultaneously achieve deployment and novel\nML methodological innovation. We argue that this introduces incentives for\nresearchers that undermine the sustainability of a broader research ecosystem\nof social impact, which benefits from projects that make contributions on\nsingle front (applied or methodological) that may better meet project partner\nneeds. Our position is that researchers and reviewers in machine learning for\nsocial impact must simultaneously adopt: 1) a more expansive conception of\nsocial impacts beyond deployment and 2) more rigorous evaluations of the impact\nof deployed systems.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20AI/ML\u793e\u4f1a\u5f71\u54cd\u7814\u7a76\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u5f71\u54cd\u5b9a\u4e49\u548c\u66f4\u4e25\u683c\u7684\u90e8\u7f72\u7cfb\u7edf\u8bc4\u4f30\uff0c\u800c\u4e0d\u662f\u4ec5\u5173\u6ce8\u540c\u65f6\u5b9e\u73b0\u90e8\u7f72\u548c\u65b9\u6cd5\u521b\u65b0\u7684\u9879\u76ee\u3002", "motivation": "\u5f53\u524dAI/ML\u793e\u4f1a\u5f71\u54cd\u7814\u7a76\u7684\u8bc4\u5ba1\u6807\u51c6\u8fc7\u5ea6\u5f3a\u8c03\u540c\u65f6\u5b9e\u73b0\u90e8\u7f72\u548c\u65b9\u6cd5\u521b\u65b0\u7684\u9879\u76ee\uff0c\u8fd9\u524a\u5f31\u4e86\u66f4\u5e7f\u6cdb\u7814\u7a76\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\u3002", "method": "\u63d0\u51fa\u7acb\u573a\u89c2\u70b9\uff0c\u4e3b\u5f20\u7814\u7a76\u4eba\u5458\u548c\u8bc4\u5ba1\u8005\u5e94\u91c7\u7eb3\uff1a1\uff09\u8d85\u8d8a\u90e8\u7f72\u7684\u66f4\u5e7f\u6cdb\u793e\u4f1a\u5f71\u54cd\u6982\u5ff5\uff1b2\uff09\u5bf9\u90e8\u7f72\u7cfb\u7edf\u5f71\u54cd\u8fdb\u884c\u66f4\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u5f53\u524d\u8bc4\u5ba1\u6807\u51c6\u5e26\u6765\u7684\u6fc0\u52b1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "AI/ML\u793e\u4f1a\u5f71\u54cd\u7814\u7a76\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u6210\u529f\u6807\u51c6\uff0c\u4ee5\u652f\u6301\u66f4\u53ef\u6301\u7eed\u7684\u7814\u7a76\u751f\u6001\u7cfb\u7edf\uff0c\u66f4\u597d\u5730\u6ee1\u8db3\u9879\u76ee\u5408\u4f5c\u4f19\u4f34\u7684\u9700\u6c42\u3002"}}
{"id": "2510.18803", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18803", "abs": "https://arxiv.org/abs/2510.18803", "authors": ["Shirin Tavakoli Kafiabad", "Andrea Schiffauerova", "Ashkan Ebadi"], "title": "Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location", "comment": "35 pages", "summary": "Optimizing national scientific investment requires a clear understanding of\nevolving research trends and the demographic and geographical forces shaping\nthem, particularly in light of commitments to equity, diversity, and inclusion.\nThis study addresses this need by analyzing 18 years (2005-2022) of research\nproposals funded by the Natural Sciences and Engineering Research Council of\nCanada (NSERC). We conducted a comprehensive comparative evaluation of three\ntopic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic\nModelling (STM), and BERTopic. We also introduced a novel algorithm, named\nCOFFEE, designed to enable robust covariate effect estimation for BERTopic.\nThis advancement addresses a significant gap, as BERTopic lacks a native\nfunction for covariate analysis, unlike the probabilistic STM. Our findings\nhighlight that while all models effectively delineate core scientific domains,\nBERTopic outperformed by consistently identifying more granular, coherent, and\nemergent themes, such as the rapid expansion of artificial intelligence.\nAdditionally, the covariate analysis, powered by COFFEE, confirmed distinct\nprovincial research specializations and revealed consistent gender-based\nthematic patterns across various scientific disciplines. These insights offer a\nrobust empirical foundation for funding organizations to formulate more\nequitable and impactful funding strategies, thereby enhancing the effectiveness\nof the scientific ecosystem.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff08LDA\u3001STM\u3001BERTopic\uff09\u5728\u5206\u6790\u52a0\u62ff\u5927NSERC 18\u5e74\u7814\u7a76\u63d0\u6848\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f00\u53d1\u4e86COFFEE\u7b97\u6cd5\u6765\u589e\u5f3aBERTopic\u7684\u534f\u53d8\u91cf\u5206\u6790\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793aBERTopic\u5728\u8bc6\u522b\u7ec6\u7c92\u5ea6\u3001\u8fde\u8d2f\u6027\u548c\u65b0\u5174\u4e3b\u9898\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u534f\u53d8\u91cf\u5206\u6790\u63ed\u793a\u4e86\u7701\u7ea7\u7814\u7a76\u4e13\u4e1a\u5316\u548c\u6027\u522b\u4e3b\u9898\u6a21\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u56fd\u5bb6\u79d1\u5b66\u6295\u8d44\uff0c\u9700\u8981\u4e86\u89e3\u7814\u7a76\u8d8b\u52bf\u7684\u6f14\u53d8\u4ee5\u53ca\u5851\u9020\u8fd9\u4e9b\u8d8b\u52bf\u7684\u4eba\u53e3\u548c\u5730\u7406\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5728\u516c\u5e73\u3001\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\u627f\u8bfa\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u5206\u6790\u4e862005-2022\u5e74\u52a0\u62ff\u5927\u81ea\u7136\u79d1\u5b66\u4e0e\u5de5\u7a0b\u7814\u7a76\u59d4\u5458\u4f1a\u8d44\u52a9\u7684\u7814\u7a76\u63d0\u6848\uff0c\u6bd4\u8f83\u4e86LDA\u3001STM\u548cBERTopic\u4e09\u79cd\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86COFFEE\u7b97\u6cd5\u7528\u4e8eBERTopic\u7684\u534f\u53d8\u91cf\u6548\u5e94\u4f30\u8ba1\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u80fd\u6709\u6548\u8bc6\u522b\u6838\u5fc3\u79d1\u5b66\u9886\u57df\uff0c\u4f46BERTopic\u5728\u8bc6\u522b\u7ec6\u7c92\u5ea6\u3001\u8fde\u8d2f\u6027\u548c\u65b0\u5174\u4e3b\u9898\uff08\u5982\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u6269\u5f20\uff09\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002COFFEE\u534f\u53d8\u91cf\u5206\u6790\u786e\u8ba4\u4e86\u7701\u7ea7\u7814\u7a76\u4e13\u4e1a\u5316\u548c\u8de8\u5b66\u79d1\u4e00\u81f4\u7684\u6027\u522b\u4e3b\u9898\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u4e3a\u8d44\u52a9\u7ec4\u7ec7\u5236\u5b9a\u66f4\u516c\u5e73\u3001\u66f4\u6709\u5f71\u54cd\u529b\u7684\u8d44\u52a9\u7b56\u7565\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u4ece\u800c\u63d0\u5347\u79d1\u5b66\u751f\u6001\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.18245", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18245", "abs": "https://arxiv.org/abs/2510.18245", "authors": ["Song Bian", "Tao Yu", "Shivaram Venkataraman", "Youngsuk Park"], "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs", "comment": "27 pages, 17 figures", "summary": "Scaling the number of parameters and the size of training data has proven to\nbe an effective strategy for improving large language model (LLM) performance.\nYet, as these models grow increasingly powerful and widely deployed, the cost\nof inference has become a pressing concern. Despite its importance, the\ntrade-off between model accuracy and inference efficiency remains\nunderexplored. In this work, we examine how key architectural factors, hidden\nsize, the allocation of parameters between MLP and attention (mlp-to-attention\nratio), and grouped-query attention (GQA), influence both inference cost and\naccuracy. We introduce a conditional scaling law that augments the Chinchilla\nframework with architectural information, along with a search framework for\nidentifying architectures that are simultaneously inference-efficient and\naccurate. To validate our approach, we train more than 200 models spanning 80M\nto 3B parameters and 8B to 100B training tokens, and fit the proposed\nconditional scaling law. Our results show that the conditional scaling law\nreliably predicts optimal architectural choices and that the resulting models\noutperform existing open-source baselines. Under the same training budget,\noptimized architectures achieve up to 2.1% higher accuracy and 42% greater\ninference throughput compared to LLaMA-3.2.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u56e0\u7d20\uff08\u9690\u85cf\u5c42\u5927\u5c0f\u3001MLP\u4e0e\u6ce8\u610f\u529b\u53c2\u6570\u5206\u914d\u6bd4\u4f8b\u3001\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff09\u5bf9\u63a8\u7406\u6210\u672c\u548c\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u6761\u4ef6\u7f29\u653e\u5b9a\u5f8b\u548c\u641c\u7d22\u6846\u67b6\uff0c\u8bad\u7ec3\u4e86200\u591a\u4e2a\u6a21\u578b\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u548c\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u589e\u957f\uff0c\u63a8\u7406\u6210\u672c\u6210\u4e3a\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u6761\u4ef6\u7f29\u653e\u5b9a\u5f8b\uff0c\u5728Chinchilla\u6846\u67b6\u57fa\u7840\u4e0a\u52a0\u5165\u67b6\u6784\u4fe1\u606f\uff0c\u63d0\u51fa\u641c\u7d22\u6846\u67b6\u6765\u8bc6\u522b\u540c\u65f6\u5177\u5907\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u67b6\u6784\uff0c\u8bad\u7ec3\u4e8680M\u52303B\u53c2\u6570\u7684200\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6761\u4ef6\u7f29\u653e\u5b9a\u5f8b\u80fd\u53ef\u9760\u9884\u6d4b\u6700\u4f18\u67b6\u6784\u9009\u62e9\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u76f8\u540c\u8bad\u7ec3\u9884\u7b97\u4e0b\u6bd4LLaMA-3.2\u51c6\u786e\u7387\u63d0\u9ad82.1%\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u534742%\u3002", "conclusion": "\u901a\u8fc7\u6761\u4ef6\u7f29\u653e\u5b9a\u5f8b\u548c\u67b6\u6784\u4f18\u5316\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.18315", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18315", "abs": "https://arxiv.org/abs/2510.18315", "authors": ["Brady Bhalla", "Honglu Fan", "Nancy Chen", "Tony Yue YU"], "title": "Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task", "comment": null, "summary": "We investigate how embedding dimension affects the emergence of an internal\n\"world model\" in a transformer trained with reinforcement learning to perform\nbubble-sort-style adjacent swaps. Models achieve high accuracy even with very\nsmall embedding dimensions, but larger dimensions yield more faithful,\nconsistent, and robust internal representations. In particular, higher\nembedding dimensions strengthen the formation of structured internal\nrepresentation and lead to better interpretability. After hundreds of\nexperiments, we observe two consistent mechanisms: (1) the last row of the\nattention weight matrix monotonically encodes the global ordering of tokens;\nand (2) the selected transposition aligns with the largest adjacent difference\nof these encoded values. Our results provide quantitative evidence that\ntransformers build structured internal world models and that model size\nimproves representation quality in addition to end performance. We release our\nmetrics and analyses, which can be used to probe similar algorithmic tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5d4c\u5165\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cd\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0b\u6267\u884c\u5192\u6ce1\u6392\u5e8f\u76f8\u90bb\u4ea4\u6362\u7684transformer\u5185\u90e8\"\u4e16\u754c\u6a21\u578b\"\u7684\u5f62\u6210\u3002\u7814\u7a76\u53d1\u73b0\u66f4\u5927\u7684\u5d4c\u5165\u7ef4\u5ea6\u80fd\u4ea7\u751f\u66f4\u5fe0\u5b9e\u3001\u4e00\u81f4\u548c\u9c81\u68d2\u7684\u5185\u90e8\u8868\u793a\uff0c\u5e76\u589e\u5f3a\u7ed3\u6784\u5316\u5185\u90e8\u8868\u793a\u7684\u5f62\u6210\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u5d4c\u5165\u7ef4\u5ea6\u5bf9transformer\u5185\u90e8\u4e16\u754c\u6a21\u578b\u5f62\u6210\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u6a21\u578b\u5927\u5c0f\u5982\u4f55\u6539\u5584\u8868\u793a\u8d28\u91cf\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u7ec8\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3transformer\u6267\u884c\u5192\u6ce1\u6392\u5e8f\u76f8\u90bb\u4ea4\u6362\uff0c\u901a\u8fc7\u6570\u767e\u6b21\u5b9e\u9a8c\u89c2\u5bdf\u4e0d\u540c\u5d4c\u5165\u7ef4\u5ea6\u4e0b\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e00\u81f4\u673a\u5236\uff1a(1)\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635\u7684\u6700\u540e\u4e00\u884c\u5355\u8c03\u7f16\u7801token\u7684\u5168\u5c40\u6392\u5e8f\uff1b(2)\u9009\u62e9\u7684\u4ea4\u6362\u64cd\u4f5c\u4e0e\u8fd9\u4e9b\u7f16\u7801\u503c\u7684\u6700\u5927\u76f8\u90bb\u5dee\u5f02\u5bf9\u9f50\u3002\u9ad8\u5d4c\u5165\u7ef4\u5ea6\u6a21\u578b\u4ea7\u751f\u66f4\u7ed3\u6784\u5316\u3001\u4e00\u81f4\u548c\u9c81\u68d2\u7684\u5185\u90e8\u8868\u793a\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc1\u636e\u8868\u660etransformer\u6784\u5efa\u4e86\u7ed3\u6784\u5316\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u6a21\u578b\u5927\u5c0f\u4e0d\u4ec5\u6539\u5584\u6700\u7ec8\u6027\u80fd\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8868\u793a\u8d28\u91cf\u3002"}}
{"id": "2510.18340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18340", "abs": "https://arxiv.org/abs/2510.18340", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs", "comment": null, "summary": "The classical policy gradient method is the theoretical and conceptual\nfoundation of modern policy-based reinforcement learning (RL) algorithms. Most\nrigorous analyses of such methods, particularly those establishing convergence\nguarantees, assume a discount factor $\\gamma < 1$. In contrast, however, a\nrecent line of work on policy-based RL for large language models uses the\nundiscounted total-reward setting with $\\gamma = 1$, rendering much of the\nexisting theory inapplicable. In this paper, we provide analyses of the policy\ngradient method for undiscounted expected total-reward infinite-horizon MDPs\nbased on two key insights: (i) the classification of the MDP states into\nrecurrent and transient states is invariant over the set of policies that\nassign strictly positive probability to every action (as is typical in deep RL\nmodels employing a softmax output layer) and (ii) the classical state\nvisitation measure (which may be ill-defined when $\\gamma = 1$) can be replaced\nwith a new object that we call the transient visitation measure.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u6298\u6263\u56e0\u5b50\u03b3=1\u7684\u65e0\u6298\u6263\u603b\u5956\u52b1\u65e0\u9650\u65f6\u57dfMDP\u4e2d\uff0c\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u7406\u8bba\u5206\u6790\uff0c\u901a\u8fc7\u5f15\u5165\u77ac\u6001\u8bbf\u95ee\u6d4b\u5ea6\u6765\u66ff\u4ee3\u7ecf\u5178\u7684\u72b6\u6001\u8bbf\u95ee\u6d4b\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u7406\u8bba\u5206\u6790\u4e2d\u901a\u5e38\u5047\u8bbe\u6298\u6263\u56e0\u5b50\u03b3<1\uff0c\u4f46\u6700\u8fd1\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u03b3=1\u7684\u65e0\u6298\u6263\u8bbe\u7f6e\uff0c\u8fd9\u4f7f\u5f97\u73b0\u6709\u7406\u8bba\u4e0d\u9002\u7528\uff0c\u9700\u8981\u5efa\u7acb\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a(1) MDP\u72b6\u6001\u5728\u91c7\u7528\u4e25\u683c\u6b63\u6982\u7387\u7b56\u7565\u65f6\uff0c\u5176\u5faa\u73af\u72b6\u6001\u548c\u77ac\u6001\u72b6\u6001\u7684\u5206\u7c7b\u662f\u4e0d\u53d8\u7684\uff1b(2) \u7528\u65b0\u7684\u77ac\u6001\u8bbf\u95ee\u6d4b\u5ea6\u66ff\u4ee3\u7ecf\u5178\u7684\u72b6\u6001\u8bbf\u95ee\u6d4b\u5ea6\u3002", "result": "\u4e3a\u65e0\u6298\u6263\u603b\u5956\u52b1\u65e0\u9650\u65f6\u57dfMDP\u4e2d\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u03b3=1\u65f6\u7ecf\u5178\u72b6\u6001\u8bbf\u95ee\u6d4b\u5ea6\u53ef\u80fd\u672a\u5b9a\u4e49\u7684\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u77ac\u6001\u8bbf\u95ee\u6d4b\u5ea6\u548c\u72b6\u6001\u5206\u7c7b\u4e0d\u53d8\u6027\u4e3a\u65e0\u6298\u6263\u8bbe\u7f6e\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u5728\u03b3=1\u60c5\u51b5\u4e0b\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.18363", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18363", "abs": "https://arxiv.org/abs/2510.18363", "authors": ["Zhen Zhang", "Bingsheng He"], "title": "Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming", "comment": "Accepted by NeurIPS 2025", "summary": "Unsupervised Graph Domain Adaptation has become a promising paradigm for\ntransferring knowledge from a fully labeled source graph to an unlabeled target\ngraph. Existing graph domain adaptation models primarily focus on the\nclosed-set setting, where the source and target domains share the same label\nspaces. However, this assumption might not be practical in the real-world\nscenarios, as the target domain might include classes that are not present in\nthe source domain. In this paper, we investigate the problem of unsupervised\nopen-set graph domain adaptation, where the goal is to not only correctly\nclassify target nodes into the known classes, but also recognize previously\nunseen node types into the unknown class. Towards this end, we propose a novel\nframework called GraphRTA, which conducts reprogramming on both the graph and\nmodel sides. Specifically, we reprogram the graph by modifying target graph\nstructure and node features, which facilitates better separation of known and\nunknown classes. Meanwhile, we also perform model reprogramming by pruning\ndomain-specific parameters to reduce bias towards the source graph while\npreserving parameters that capture transferable patterns across graphs.\nAdditionally, we extend the classifier with an extra dimension for the unknown\nclass, thus eliminating the need of manually specified threshold in open-set\nrecognition. Comprehensive experiments on several public datasets demonstrate\nthat our proposed model can achieve satisfied performance compared with recent\nstate-of-the-art baselines. Our source codes and datasets are publicly\navailable at https://github.com/cszhangzhen/GraphRTA.", "AI": {"tldr": "\u63d0\u51faGraphRTA\u6846\u67b6\uff0c\u89e3\u51b3\u65e0\u76d1\u7763\u5f00\u653e\u96c6\u56fe\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u91cd\u7f16\u7a0b\u548c\u6a21\u578b\u91cd\u7f16\u7a0b\u6765\u8bc6\u522b\u76ee\u6807\u56fe\u4e2d\u7684\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u3002", "motivation": "\u73b0\u6709\u56fe\u57df\u81ea\u9002\u5e94\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u95ed\u96c6\u8bbe\u7f6e\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u76ee\u6807\u57df\u53ef\u80fd\u5305\u542b\u6e90\u57df\u4e2d\u4e0d\u5b58\u5728\u7684\u7c7b\u522b\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5f00\u653e\u96c6\u56fe\u57df\u81ea\u9002\u5e94\u95ee\u9898\u3002", "method": "GraphRTA\u6846\u67b6\u5305\u542b\u56fe\u91cd\u7f16\u7a0b\uff08\u4fee\u6539\u76ee\u6807\u56fe\u7ed3\u6784\u548c\u8282\u70b9\u7279\u5f81\uff09\u548c\u6a21\u578b\u91cd\u7f16\u7a0b\uff08\u526a\u679d\u57df\u7279\u5b9a\u53c2\u6570\uff09\uff0c\u5e76\u6269\u5c55\u5206\u7c7b\u5668\u7ef4\u5ea6\u4ee5\u8bc6\u522b\u672a\u77e5\u7c7b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u6ee1\u610f\u7684\u6027\u80fd\u3002", "conclusion": "GraphRTA\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f00\u653e\u96c6\u56fe\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u65e0\u9700\u624b\u52a8\u6307\u5b9a\u9608\u503c\u5373\u53ef\u8bc6\u522b\u672a\u77e5\u7c7b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.18478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18478", "abs": "https://arxiv.org/abs/2510.18478", "authors": ["Daniel Bethell", "Simos Gerasimou", "Radu Calinescu", "Calum Imrie"], "title": "Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation", "comment": null, "summary": "Ensuring the safe exploration of reinforcement learning (RL) agents is\ncritical for deployment in real-world systems. Yet existing approaches struggle\nto strike the right balance: methods that tightly enforce safety often cripple\ntask performance, while those that prioritize reward leave safety constraints\nfrequently violated, producing diffuse cost landscapes that flatten gradients\nand stall policy improvement. We introduce the Uncertain Safety Critic (USC), a\nnovel approach that integrates uncertainty-aware modulation and refinement into\ncritic training. By concentrating conservatism in uncertain and costly regions\nwhile preserving sharp gradients in safe areas, USC enables policies to achieve\neffective reward-safety trade-offs. Extensive experiments show that USC reduces\nsafety violations by approximately 40% while maintaining competitive or higher\nrewards, and reduces the error between predicted and true cost gradients by\napproximately 83%, breaking the prevailing trade-off between safety and\nperformance and paving the way for scalable safe RL.", "AI": {"tldr": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u5b89\u5168\u8bc4\u8bba\u5bb6(USC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u786e\u5b9a\u548c\u9ad8\u6210\u672c\u533a\u57df\u96c6\u4e2d\u4fdd\u5b88\u6027\uff0c\u5728\u5b89\u5168\u533a\u57df\u4fdd\u6301\u9510\u5229\u68af\u5ea6\uff0c\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u63a2\u7d22\u4e0e\u4efb\u52a1\u6027\u80fd\u7684\u6709\u6548\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u7ea6\u675f\u548c\u4efb\u52a1\u6027\u80fd\uff1a\u4e25\u683c\u7684\u5b89\u5168\u7ea6\u675f\u4f1a\u524a\u5f31\u4efb\u52a1\u6027\u80fd\uff0c\u800c\u4f18\u5148\u8003\u8651\u5956\u52b1\u7684\u65b9\u6cd5\u5219\u9891\u7e41\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\uff0c\u5bfc\u81f4\u6210\u672c\u68af\u5ea6\u5e73\u5766\u5316\u5e76\u963b\u788d\u7b56\u7565\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u4e0d\u786e\u5b9a\u5b89\u5168\u8bc4\u8bba\u5bb6(USC)\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8c03\u5236\u548c\u7cbe\u70bc\u96c6\u6210\u5230\u8bc4\u8bba\u5bb6\u8bad\u7ec3\u4e2d\uff0c\u5728\u4e0d\u786e\u5b9a\u548c\u9ad8\u6210\u672c\u533a\u57df\u96c6\u4e2d\u4fdd\u5b88\u6027\uff0c\u540c\u65f6\u5728\u5b89\u5168\u533a\u57df\u4fdd\u6301\u9510\u5229\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUSC\u5c06\u5b89\u5168\u8fdd\u89c4\u51cf\u5c11\u7ea640%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u6216\u66f4\u9ad8\u7684\u5956\u52b1\uff0c\u5e76\u5c06\u9884\u6d4b\u6210\u672c\u68af\u5ea6\u4e0e\u771f\u5b9e\u6210\u672c\u68af\u5ea6\u4e4b\u95f4\u7684\u8bef\u5dee\u51cf\u5c11\u7ea683%\u3002", "conclusion": "USC\u6253\u7834\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u666e\u904d\u6743\u8861\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.18499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18499", "abs": "https://arxiv.org/abs/2510.18499", "authors": ["Hyewon Lee", "Junghyun Oh", "Minkyung Song", "Soyoung Park", "Seunghoon Han"], "title": "Alibaba International E-commerce Product Search Competition DILAB Team Technical Report", "comment": "CIKM Alibaba E-commerce Search Challenge 2025", "summary": "This study presents the multilingual e-commerce search system developed by\nthe DILAB team, which achieved 5th place on the final leaderboard with a\ncompetitive overall score of 0.8819, demonstrating stable and high-performing\nresults across evaluation metrics. To address challenges in multilingual\nquery-item understanding, we designed a multi-stage pipeline integrating data\nrefinement, lightweight preprocessing, and adaptive modeling. The data\nrefinement stage enhanced dataset consistency and category coverage, while\nlanguage tagging and noise filtering improved input quality. In the modeling\nphase, multiple architectures and fine-tuning strategies were explored, and\nhyperparameters optimized using curated validation sets to balance performance\nacross query-category (QC) and query-item (QI) tasks. The proposed framework\nexhibited robustness and adaptability across languages and domains,\nhighlighting the effectiveness of systematic data curation and iterative\nevaluation for multilingual search systems. The source code is available at\nhttps://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.", "AI": {"tldr": "DILAB\u56e2\u961f\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u7535\u5546\u641c\u7d22\u7cfb\u7edf\u5728\u6700\u7ec8\u6392\u884c\u699c\u4e0a\u83b7\u5f97\u7b2c5\u540d\uff0c\u603b\u4f53\u5f97\u52060.8819\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u5305\u62ec\u6570\u636e\u7cbe\u70bc\u3001\u8f7b\u91cf\u7ea7\u9884\u5904\u7406\u548c\u81ea\u9002\u5e94\u5efa\u6a21\uff0c\u5728\u67e5\u8be2\u7c7b\u522b\u548c\u67e5\u8be2\u7269\u54c1\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u7a33\u5b9a\u4e14\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u67e5\u8be2-\u7269\u54c1\u7406\u89e3\u4e2d\u7684\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u4e2a\u5728\u591a\u8bed\u8a00\u548c\u8de8\u9886\u57df\u73af\u5883\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u7684\u7535\u5546\u641c\u7d22\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u6570\u636e\u7cbe\u70bc\u9636\u6bb5\u589e\u5f3a\u6570\u636e\u96c6\u4e00\u81f4\u6027\u548c\u7c7b\u522b\u8986\u76d6\uff1b\u8bed\u8a00\u6807\u8bb0\u548c\u566a\u58f0\u8fc7\u6ee4\u63d0\u9ad8\u8f93\u5165\u8d28\u91cf\uff1b\u5efa\u6a21\u9636\u6bb5\u63a2\u7d22\u591a\u79cd\u67b6\u6784\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u4f7f\u7528\u7cbe\u9009\u9a8c\u8bc1\u96c6\u4f18\u5316\u8d85\u53c2\u6570\u4ee5\u5e73\u8861\u67e5\u8be2\u7c7b\u522b\u548c\u67e5\u8be2\u7269\u54c1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "result": "\u7cfb\u7edf\u5728\u6700\u7ec8\u6392\u884c\u699c\u4e0a\u83b7\u5f97\u7b2c5\u540d\uff0c\u603b\u4f53\u5f97\u52060.8819\uff0c\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u7a33\u5b9a\u4e14\u9ad8\u6027\u80fd\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u8bed\u8a00\u548c\u8de8\u9886\u57df\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u7a81\u663e\u4e86\u7cfb\u7edf\u5316\u6570\u636e\u7ba1\u7406\u548c\u8fed\u4ee3\u8bc4\u4f30\u5bf9\u591a\u8bed\u8a00\u641c\u7d22\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.18541", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18541", "abs": "https://arxiv.org/abs/2510.18541", "authors": ["Giovanni De Muri", "Mark Vero", "Robin Staab", "Martin Vechev"], "title": "Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation", "comment": null, "summary": "LLMs are often used by downstream users as teacher models for knowledge\ndistillation, compressing their capabilities into memory-efficient models.\nHowever, as these teacher models may stem from untrusted parties, distillation\ncan raise unexpected security risks. In this paper, we investigate the security\nimplications of knowledge distillation from backdoored teacher models. First,\nwe show that prior backdoors mostly do not transfer onto student models. Our\nkey insight is that this is because existing LLM backdooring methods choose\ntrigger tokens that rarely occur in usual contexts. We argue that this\nunderestimates the security risks of knowledge distillation and introduce a new\nbackdooring technique, T-MTB, that enables the construction and study of\ntransferable backdoors. T-MTB carefully constructs a composite backdoor\ntrigger, made up of several specific tokens that often occur individually in\nanticipated distillation datasets. As such, the poisoned teacher remains\nstealthy, while during distillation the individual presence of these tokens\nprovides enough signal for the backdoor to transfer onto the student. Using\nT-MTB, we demonstrate and extensively study the security risks of transferable\nbackdoors across two attack scenarios, jailbreaking and content modulation, and\nacross four model families of LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4ece\u540e\u95e8\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u5b89\u5168\u98ce\u9669\uff0c\u53d1\u73b0\u73b0\u6709LLM\u540e\u95e8\u65b9\u6cd5\u7531\u4e8e\u4f7f\u7528\u7f55\u89c1\u89e6\u53d1\u8bcd\u800c\u65e0\u6cd5\u4f20\u9012\u5230\u5b66\u751f\u6a21\u578b\uff0c\u56e0\u6b64\u63d0\u51faT-MTB\u6280\u672f\u6784\u5efa\u53ef\u8f6c\u79fb\u7684\u540e\u95e8\u3002", "motivation": "\u7531\u4e8e\u6559\u5e08\u6a21\u578b\u53ef\u80fd\u6765\u81ea\u4e0d\u53ef\u4fe1\u65b9\uff0c\u77e5\u8bc6\u84b8\u998f\u53ef\u80fd\u5e26\u6765\u610f\u60f3\u4e0d\u5230\u7684\u5b89\u5168\u98ce\u9669\uff0c\u7279\u522b\u662f\u540e\u95e8\u653b\u51fb\u7684\u4f20\u9012\u95ee\u9898\u3002", "method": "\u63d0\u51faT-MTB\u540e\u95e8\u6280\u672f\uff0c\u6784\u5efa\u7531\u591a\u4e2a\u7279\u5b9a\u4ee4\u724c\u7ec4\u6210\u7684\u590d\u5408\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u8fd9\u4e9b\u4ee4\u724c\u5728\u9884\u671f\u7684\u84b8\u998f\u6570\u636e\u96c6\u4e2d\u5355\u72ec\u51fa\u73b0\u9891\u7387\u8f83\u9ad8\uff0c\u4ece\u800c\u4fdd\u6301\u6559\u5e08\u6a21\u578b\u7684\u9690\u853d\u6027\u540c\u65f6\u786e\u4fdd\u540e\u95e8\u4f20\u9012\u3002", "result": "\u4f7f\u7528T-MTB\u5728\u4e24\u4e2a\u653b\u51fb\u573a\u666f\uff08\u8d8a\u72f1\u548c\u5185\u5bb9\u8c03\u8282\uff09\u548c\u56db\u4e2aLLM\u6a21\u578b\u5bb6\u65cf\u4e2d\u5c55\u793a\u548c\u5e7f\u6cdb\u7814\u7a76\u4e86\u53ef\u8f6c\u79fb\u540e\u95e8\u7684\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u786e\u5b9e\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0cT-MTB\u6280\u672f\u80fd\u591f\u6709\u6548\u6784\u5efa\u53ef\u8f6c\u79fb\u7684\u540e\u95e8\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4f4e\u4f30\u4e86\u77e5\u8bc6\u84b8\u998f\u5b89\u5168\u5a01\u80c1\u7684\u95ee\u9898\u3002"}}
{"id": "2510.18559", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18559", "abs": "https://arxiv.org/abs/2510.18559", "authors": ["Loc Phuc Truong Nguyen", "Hung Thanh Do"], "title": "RAISE: A Unified Framework for Responsible AI Scoring and Evaluation", "comment": "Accepted at the 26th International Conference on Principles and\n  Practice of Multi-Agent Systems", "summary": "As AI systems enter high-stakes domains, evaluation must extend beyond\npredictive accuracy to include explainability, fairness, robustness, and\nsustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a\nunified framework that quantifies model performance across these four\ndimensions and aggregates them into a single, holistic Responsibility Score. We\nevaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular\nResNet, and a Feature Tokenizer Transformer, on structured datasets from\nfinance, healthcare, and socioeconomics. Our findings reveal critical\ntrade-offs: the MLP demonstrated strong sustainability and robustness, the\nTransformer excelled in explainability and fairness at a very high\nenvironmental cost, and the Tabular ResNet offered a balanced profile. These\nresults underscore that no single model dominates across all responsibility\ncriteria, highlighting the necessity of multi-dimensional evaluation for\nresponsible model selection. Our implementation is available at:\nhttps://github.com/raise-framework/raise.", "AI": {"tldr": "RAISE\u6846\u67b6\u662f\u4e00\u4e2a\u7edf\u4e00\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6301\u7eed\u6027\u56db\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u7efc\u5408\u8bc4\u5206\u5e2e\u52a9\u9009\u62e9\u8d1f\u8d23\u4efb\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u7ef4\u5ea6\u8868\u73b0\u5404\u5f02\uff0c\u9700\u8981\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u8fdb\u5165\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u8bc4\u4f30\u9700\u8981\u8d85\u8d8a\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6269\u5c55\u5230\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6301\u7eed\u6027\u7b49\u7ef4\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86RAISE\u6846\u67b6\uff0c\u91cf\u5316\u6a21\u578b\u5728\u56db\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u5e76\u805a\u5408\u6210\u7efc\u5408\u8d23\u4efb\u8bc4\u5206\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u91d1\u878d\u3001\u533b\u7597\u548c\u793e\u4f1a\u7ecf\u6d4e\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "MLP\u5728\u53ef\u6301\u7eed\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0cTransformer\u5728\u53ef\u89e3\u91ca\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4f46\u73af\u5883\u6210\u672c\u5f88\u9ad8\uff0cTabular ResNet\u63d0\u4f9b\u4e86\u5e73\u8861\u7684\u6027\u80fd\u3002", "conclusion": "\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u8d23\u4efb\u6807\u51c6\u4e0a\u90fd\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5f3a\u8c03\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5bf9\u4e8e\u8d1f\u8d23\u4efb\u6a21\u578b\u9009\u62e9\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.18611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18611", "abs": "https://arxiv.org/abs/2510.18611", "authors": ["Fayad Ali Banna", "Antoine Caradot", "Eduardo Brandao", "Jean-Philippe Colombier", "R\u00e9mi Emonet", "Marc Sebban"], "title": "Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data", "comment": "56 pages, 12 figures, 39 tables", "summary": "Identifying from observation data the governing differential equations of a\nphysical dynamics is a key challenge in machine learning. Although approaches\nbased on SINDy have shown great promise in this area, they still fail to\naddress a whole class of real world problems where the data is sparsely sampled\nin time. In this article, we introduce Unrolled-SINDy, a simple methodology\nthat leverages an unrolling scheme to improve the stability of explicit methods\nfor PDE discovery. By decorrelating the numerical time step size from the\nsampling rate of the available data, our approach enables the recovery of\nequation parameters that would not be the minimizers of the original SINDy\noptimization problem due to large local truncation errors. Our method can be\nexploited either through an iterative closed-form approach or by a gradient\ndescent scheme. Experiments show the versatility of our method. On both\ntraditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with\ndifferent numerical schemes (Euler, RK4), our proposed unrolling scheme allows\nto tackle problems not accessible to non-unrolled methods.", "AI": {"tldr": "Unrolled-SINDy\u662f\u4e00\u79cd\u6539\u8fdbSINDy\u65b9\u6cd5\u7a33\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u6570\u503c\u65f6\u95f4\u6b65\u957f\u4e0e\u6570\u636e\u91c7\u6837\u7387\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u65f6\u95f4\u91c7\u6837\u6570\u636e\u4e0b\u7684\u7269\u7406\u52a8\u529b\u5b66\u5fae\u5206\u65b9\u7a0b\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SINDy\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u7a00\u758f\u91c7\u6837\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7531\u4e8e\u5c40\u90e8\u622a\u65ad\u8bef\u5dee\u8f83\u5927\uff0c\u65e0\u6cd5\u6b63\u786e\u6062\u590d\u65b9\u7a0b\u53c2\u6570\u3002", "method": "\u91c7\u7528\u5c55\u5f00\u65b9\u6848\uff0c\u901a\u8fc7\u8fed\u4ee3\u95ed\u5f0f\u65b9\u6cd5\u6216\u68af\u5ea6\u4e0b\u964d\u65b9\u6848\uff0c\u5c06\u6570\u503c\u65f6\u95f4\u6b65\u957f\u4e0e\u6570\u636e\u91c7\u6837\u7387\u89e3\u8026\uff0c\u63d0\u9ad8\u663e\u5f0f\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u5728\u4f20\u7edfSINDy\u548c\u5148\u8fdb\u7684\u566a\u58f0\u9c81\u68d2iNeuralSINDy\u4e0a\uff0c\u914d\u5408\u4e0d\u540c\u6570\u503c\u65b9\u6848\uff08\u6b27\u62c9\u6cd5\u3001RK4\uff09\uff0c\u90fd\u80fd\u89e3\u51b3\u975e\u5c55\u5f00\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u95ee\u9898\u3002", "conclusion": "Unrolled-SINDy\u901a\u8fc7\u5c55\u5f00\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u65f6\u95f4\u91c7\u6837\u6570\u636e\u4e0b\u7684\u5fae\u5206\u65b9\u7a0b\u8bc6\u522b\u95ee\u9898\uff0c\u6269\u5c55\u4e86SINDy\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2510.18634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18634", "abs": "https://arxiv.org/abs/2510.18634", "authors": ["Satwik Bhattamishra", "Phil Blunsom", "Varun Kanade"], "title": "Hardness of Learning Regular Languages in the Next Symbol Prediction Setting", "comment": "7 pages", "summary": "We study the learnability of languages in the Next Symbol Prediction (NSP)\nsetting, where a learner receives only positive examples from a language\ntogether with, for every prefix, (i) whether the prefix itself is in the\nlanguage and (ii) which next symbols can lead to an accepting string. This\nsetting has been used in prior works to empirically analyze neural sequence\nmodels, and additionally, we observe that efficient algorithms for the NSP\nsetting can be used to learn the (truncated) support of language models. We\nformalize the setting so as to make it amenable to PAC-learning analysis. While\nthe setting provides a much richer set of labels than the conventional\nclassification setting, we show that learning concept classes such as DFAs and\nBoolean formulas remains computationally hard. The proof is via a construction\nthat makes almost all additional labels uninformative, yielding a reduction\nfrom the conventional learning problem to learning with NSP labels. Under\ncryptographic assumptions, the reduction implies that the problem of learning\nDFAs is computationally hard in the NSP setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u4e0b\u4e00\u4e2a\u7b26\u53f7\u9884\u6d4b(NSP)\u8bbe\u7f6e\u4e0b\u8bed\u8a00\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u8bc1\u660e\u5373\u4f7f\u5728\u8fd9\u79cd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u6807\u7b7e\u4fe1\u606f\u7684\u73af\u5883\u4e2d\uff0c\u5b66\u4e60DFA\u548c\u5e03\u5c14\u516c\u5f0f\u7b49\u6982\u5ff5\u7c7b\u4ecd\u7136\u662f\u8ba1\u7b97\u56f0\u96be\u7684\u3002", "motivation": "NSP\u8bbe\u7f6e\u5df2\u88ab\u7528\u4e8e\u5b9e\u8bc1\u5206\u6790\u795e\u7ecf\u5e8f\u5217\u6a21\u578b\uff0c\u4e14\u8be5\u8bbe\u7f6e\u4e2d\u7684\u9ad8\u6548\u7b97\u6cd5\u53ef\u7528\u4e8e\u5b66\u4e60\u8bed\u8a00\u6a21\u578b\u7684\u622a\u65ad\u652f\u6301\u3002\u4f5c\u8005\u5e0c\u671b\u5c06\u8fd9\u4e00\u8bbe\u7f6e\u5f62\u5f0f\u5316\u4ee5\u8fdb\u884cPAC\u5b66\u4e60\u5206\u6790\uff0c\u63a2\u7d22\u5728\u8fd9\u79cd\u66f4\u4e30\u5bcc\u6807\u7b7e\u4fe1\u606f\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6784\u9020\u4f7f\u51e0\u4e4e\u6240\u6709\u989d\u5916\u6807\u7b7e\u53d8\u5f97\u65e0\u4fe1\u606f\uff0c\u4ece\u800c\u5c06\u4f20\u7edf\u5b66\u4e60\u95ee\u9898\u5f52\u7ea6\u5230NSP\u6807\u7b7e\u5b66\u4e60\u95ee\u9898\u3002\u5728\u5bc6\u7801\u5b66\u5047\u8bbe\u4e0b\uff0c\u8fd9\u79cd\u5f52\u7ea6\u8868\u660e\u5728NSP\u8bbe\u7f6e\u4e2d\u5b66\u4e60DFA\u662f\u8ba1\u7b97\u56f0\u96be\u7684\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1NSP\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u5206\u7c7b\u8bbe\u7f6e\u66f4\u4e30\u5bcc\u7684\u6807\u7b7e\u96c6\uff0c\u4f46\u5b66\u4e60DFA\u548c\u5e03\u5c14\u516c\u5f0f\u7b49\u6982\u5ff5\u7c7b\u4ecd\u7136\u4fdd\u6301\u8ba1\u7b97\u56f0\u96be\u3002", "conclusion": "\u5373\u4f7f\u5728\u5b66\u4e60\u8005\u83b7\u5f97\u66f4\u4e30\u5bcc\u6807\u7b7e\u4fe1\u606f\u7684NSP\u8bbe\u7f6e\u4e2d\uff0c\u5b66\u4e60\u67d0\u4e9b\u6982\u5ff5\u7c7b\uff08\u5982DFA\u548c\u5e03\u5c14\u516c\u5f0f\uff09\u7684\u8ba1\u7b97\u56f0\u96be\u6027\u4ecd\u7136\u5b58\u5728\uff0c\u8fd9\u901a\u8fc7\u4ece\u4f20\u7edf\u5b66\u4e60\u95ee\u9898\u5230NSP\u6807\u7b7e\u5b66\u4e60\u7684\u5f52\u7ea6\u5f97\u5230\u8bc1\u660e\u3002"}}
{"id": "2510.18648", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18648", "abs": "https://arxiv.org/abs/2510.18648", "authors": ["Miro Miranda", "Marcela Charfuelan", "Matias Valdenegro Toro", "Andreas Dengel"], "title": "Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction", "comment": null, "summary": "Water is essential for agricultural productivity. Assessing water shortages\nand reduced yield potential is a critical factor in decision-making for\nensuring agricultural productivity and food security. Crop simulation models,\nwhich align with physical processes, offer intrinsic explainability but often\nperform poorly. Conversely, machine learning models for crop yield modeling are\npowerful and scalable, yet they commonly operate as black boxes and lack\nadherence to the physical principles of crop growth. This study bridges this\ngap by coupling the advantages of both worlds. We postulate that the crop yield\nis inherently defined by the water availability. Therefore, we formulate crop\nyield as a function of temporal water scarcity and predict both the crop\ndrought stress and the sensitivity to water scarcity at fine-scale resolution.\nSequentially modeling the crop yield response to water enables accurate yield\nprediction. To enforce physical consistency, a novel physics-informed loss\nfunction is proposed. We leverage multispectral satellite imagery,\nmeteorological data, and fine-scale yield data. Further, to account for the\nuncertainty within the model, we build upon a deep ensemble approach. Our\nmethod surpasses state-of-the-art models like LSTM and Transformers in crop\nyield prediction with a coefficient of determination ($R^2$-score) of up to\n0.82 while offering high explainability. This method offers decision support\nfor industry, policymakers, and farmers in building a more resilient\nagriculture in times of changing climate conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f5c\u7269\u6a21\u62df\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f5c\u7269\u4ea7\u91cf\u5efa\u6a21\u4e3a\u65f6\u95f4\u6027\u6c34\u5206\u7a00\u7f3a\u7684\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u53ef\u89e3\u91ca\u7684\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u3002", "motivation": "\u4f5c\u7269\u6a21\u62df\u6a21\u578b\u5177\u6709\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4f46\u6027\u80fd\u8f83\u5dee\uff0c\u800c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u5f3a\u5927\u4f46\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u4e3a\u519c\u4e1a\u51b3\u7b56\u63d0\u4f9b\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u4ea7\u91cf\u9884\u6d4b\u5de5\u5177\u3002", "method": "\u5c06\u4f5c\u7269\u4ea7\u91cf\u5efa\u6a21\u4e3a\u65f6\u95f4\u6027\u6c34\u5206\u7a00\u7f3a\u7684\u51fd\u6570\uff0c\u9884\u6d4b\u4f5c\u7269\u5e72\u65f1\u80c1\u8feb\u548c\u5bf9\u6c34\u5206\u7a00\u7f3a\u7684\u654f\u611f\u6027\uff1b\u63d0\u51fa\u65b0\u9896\u7684\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\uff1b\u5229\u7528\u591a\u5149\u8c31\u536b\u661f\u5f71\u50cf\u3001\u6c14\u8c61\u6570\u636e\u548c\u7cbe\u7ec6\u5c3a\u5ea6\u4ea7\u91cf\u6570\u636e\uff1b\u91c7\u7528\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u65b9\u9762\u8d85\u8d8a\u4e86LSTM\u548cTransformer\u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u51b3\u5b9a\u7cfb\u6570\uff08R\u00b2-score\uff09\u9ad8\u8fbe0.82\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u884c\u4e1a\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u519c\u6c11\u5728\u6c14\u5019\u53d8\u5316\u65f6\u671f\u6784\u5efa\u66f4\u5177\u97e7\u6027\u7684\u519c\u4e1a\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u7269\u7406\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f3a\u5927\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2510.18680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18680", "abs": "https://arxiv.org/abs/2510.18680", "authors": ["Philippe Formont", "Maxime Darrin", "Banafsheh Karimian", "Jackie CK Cheung", "Eric Granger", "Ismail Ben Ayed", "Mohammadhadi Shateri", "Pablo Piantanida"], "title": "Learning Task-Agnostic Representations through Multi-Teacher Distillation", "comment": "NeurIPS-2025", "summary": "Casting complex inputs into tractable representations is a critical step\nacross various fields. Diverse embedding models emerge from differences in\narchitectures, loss functions, input modalities and datasets, each capturing\nunique aspects of the input. Multi-teacher distillation leverages this\ndiversity to enrich representations but often remains tailored to specific\ntasks. In this paper, we introduce a task-agnostic framework based on a\n``majority vote\" objective function. We demonstrate that this function is\nbounded by the mutual information between student and teachers' embeddings,\nleading to a task-agnostic distillation loss that eliminates dependence on\ntask-specific labels or prior knowledge. Our evaluations across text, vision\nmodels, and molecular modeling show that our method effectively leverages\nteacher diversity, resulting in representations enabling better performance for\na wide range of downstream tasks such as classification, clustering, or\nregression. Additionally, we train and release state-of-the-art embedding\nmodels, enhancing downstream performance in various modalities.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\"\u591a\u6570\u6295\u7968\"\u76ee\u6807\u51fd\u6570\u7684\u4efb\u52a1\u65e0\u5173\u591a\u6559\u5e08\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5b66\u751f\u4e0e\u6559\u5e08\u5d4c\u5165\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u6587\u672c\u3001\u89c6\u89c9\u548c\u5206\u5b50\u5efa\u6a21\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6559\u5e08\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\u5728\u67b6\u6784\u3001\u635f\u5931\u51fd\u6570\u3001\u8f93\u5165\u6a21\u6001\u548c\u6570\u636e\u96c6\u4e0a\u7684\u591a\u6837\u6027\u6765\u4e30\u5bcc\u8868\u793a\u5b66\u4e60\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\"\u591a\u6570\u6295\u7968\"\u76ee\u6807\u51fd\u6570\u7684\u4efb\u52a1\u65e0\u5173\u84b8\u998f\u6846\u67b6\uff0c\u8be5\u51fd\u6570\u53d7\u5b66\u751f\u4e0e\u6559\u5e08\u5d4c\u5165\u95f4\u4e92\u4fe1\u606f\u7684\u4e0a\u754c\u7ea6\u675f\uff0c\u4ece\u800c\u63a8\u5bfc\u51fa\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u7684\u84b8\u998f\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u6587\u672c\u3001\u89c6\u89c9\u6a21\u578b\u548c\u5206\u5b50\u5efa\u6a21\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u6559\u5e08\u591a\u6837\u6027\uff0c\u4ea7\u751f\u7684\u8868\u793a\u5728\u5206\u7c7b\u3001\u805a\u7c7b\u548c\u56de\u5f52\u7b49\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u53d1\u5e03\u4e86\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u65e0\u5173\u591a\u6559\u5e08\u84b8\u998f\u6846\u67b6\u901a\u8fc7\"\u591a\u6570\u6295\u7968\"\u76ee\u6807\u6210\u529f\u5229\u7528\u6559\u5e08\u591a\u6837\u6027\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u8868\u793a\uff0c\u5728\u5404\u79cd\u6a21\u6001\u4e2d\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2510.18687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18687", "abs": "https://arxiv.org/abs/2510.18687", "authors": ["Chenbei Lu", "Zaiwei Chen", "Tongxin Li", "Chenye Wu", "Adam Wierman"], "title": "Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach", "comment": null, "summary": "Traditional reinforcement learning (RL) assumes the agents make decisions\nbased on Markov decision processes (MDPs) with one-step transition models. In\nmany real-world applications, such as energy management and stock investment,\nagents can access multi-step predictions of future states, which provide\nadditional advantages for decision making. However, multi-step predictions are\ninherently high-dimensional: naively embedding these predictions into an MDP\nleads to an exponential blow-up in state space and the curse of dimensionality.\nMoreover, existing RL theory provides few tools to analyze prediction-augmented\nMDPs, as it typically works on one-step transition kernels and cannot\naccommodate multi-step predictions with errors or partial action-coverage. We\naddress these challenges with three key innovations: First, we propose the\n\\emph{Bayesian value function} to characterize the optimal prediction-aware\npolicy tractably. Second, we develop a novel \\emph{Bellman-Jensen Gap} analysis\non the Bayesian value function, which enables characterizing the value of\nimperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with\nOnline Adaptation), a two-stage model-based RL algorithm that separates offline\nBayesian value learning from lightweight online adaptation to real-time\npredictions. We prove that BOLA remains sample-efficient even under imperfect\npredictions. We validate our theory and algorithm on synthetic MDPs and a\nreal-world wind energy storage control problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u591a\u6b65\u9884\u6d4b\u589e\u5f3aMDP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4ef7\u503c\u51fd\u6570\u548cBellman-Jensen Gap\u5206\u6790\u6765\u89e3\u51b3\u4f20\u7edfRL\u5728\u591a\u6b65\u9884\u6d4b\u573a\u666f\u4e0b\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86BOLA\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u57fa\u4e8e\u5355\u6b65\u8f6c\u79fb\u6a21\u578b\uff0c\u4f46\u5728\u80fd\u6e90\u7ba1\u7406\u548c\u80a1\u7968\u6295\u8d44\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u83b7\u5f97\u591a\u6b65\u672a\u6765\u72b6\u6001\u9884\u6d4b\uff0c\u8fd9\u4e9b\u9884\u6d4b\u4e3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u989d\u5916\u4f18\u52bf\u3002\u7136\u800c\uff0c\u591a\u6b65\u9884\u6d4b\u5177\u6709\u9ad8\u7ef4\u7279\u6027\uff0c\u76f4\u63a5\u5d4c\u5165MDP\u4f1a\u5bfc\u81f4\u72b6\u6001\u7a7a\u95f4\u6307\u6570\u7206\u70b8\u548c\u7ef4\u5ea6\u707e\u96be\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4ef7\u503c\u51fd\u6570\u6765\u8868\u5f81\u6700\u4f18\u9884\u6d4b\u611f\u77e5\u7b56\u7565\uff1b\u5f00\u53d1\u65b0\u9896\u7684Bellman-Jensen Gap\u5206\u6790\u6765\u8868\u5f81\u4e0d\u5b8c\u7f8e\u9884\u6d4b\u7684\u4ef7\u503c\uff1b\u5f15\u5165BOLA\uff08\u8d1d\u53f6\u65af\u79bb\u7ebf\u5b66\u4e60\u4e0e\u5728\u7ebf\u9002\u5e94\uff09\u7b97\u6cd5\uff0c\u5c06\u79bb\u7ebf\u8d1d\u53f6\u65af\u4ef7\u503c\u5b66\u4e60\u4e0e\u8f7b\u91cf\u7ea7\u5728\u7ebf\u9002\u5e94\u5206\u79bb\u3002", "result": "\u8bc1\u660e\u4e86BOLA\u5373\u4f7f\u5728\u9884\u6d4b\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u6837\u672c\u6548\u7387\uff1b\u5728\u5408\u6210MDP\u548c\u771f\u5b9e\u4e16\u754c\u98ce\u80fd\u5b58\u50a8\u63a7\u5236\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6b65\u9884\u6d4b\u589e\u5f3aMDP\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u5229\u7528\u591a\u6b65\u9884\u6d4b\u8fdb\u884c\u51b3\u7b56\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2510.18784", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18784", "abs": "https://arxiv.org/abs/2510.18784", "authors": ["Soroush Tabesh", "Mher Safaryan", "Dan Alistarh"], "title": "CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training", "comment": null, "summary": "Despite significant work on low-bit quantization-aware training (QAT), there\nis still a large accuracy gap between such techniques and native training. To\naddress this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new\nQAT method that augments the straight-through estimator (STE) gradient with a\ncurvature-aware correction designed to counteract the loss increase induced by\nquantization. CAGE is derived from a multi-objective view of QAT that balances\nloss minimization with adherence to quantization constraints, yielding a\nprincipled correction term that depends on local curvature information. On the\ntheoretical side, we introduce the notion of Pareto-optimal solutions for\nquantized optimization, and establish that CAGE yields strong convergence\nguarantees in the smooth non-convex setting. In terms of implementation, our\napproach is optimizer-agnostic, but we provide a highly-efficient\nimplementation that leverages Adam statistics. When pre-training Llama-style\nmodels of up to 800M-parameters, CAGE recovers over 10% of the\nquantization-induced loss increase in the W4A4 regime over outlier-mitigation\nmethods. These results indicate that curvature-aware gradient corrections can\nbridge the remaining performance gap beyond current outlier-handling methods.", "AI": {"tldr": "CAGE\u662f\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f2\u7387\u611f\u77e5\u68af\u5ea6\u4f30\u8ba1\u6765\u5f25\u8865\u91cf\u5316\u5e26\u6765\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u5728Llama\u98ce\u683c\u6a21\u578b\u4e0a\u80fd\u6062\u590d\u8d85\u8fc710%\u7684\u91cf\u5316\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u7684\u4f4e\u4f4d\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u4e0e\u539f\u751f\u8bad\u7ec3\u4e4b\u95f4\u4ecd\u5b58\u5728\u8f83\u5927\u7684\u7cbe\u5ea6\u5dee\u8ddd\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5f15\u5165CAGE\u65b9\u6cd5\uff0c\u5728\u76f4\u901a\u4f30\u8ba1\u5668\u68af\u5ea6\u57fa\u7840\u4e0a\u589e\u52a0\u66f2\u7387\u611f\u77e5\u4fee\u6b63\u9879\uff0c\u57fa\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u89c6\u89d2\u5e73\u8861\u635f\u5931\u6700\u5c0f\u5316\u548c\u91cf\u5316\u7ea6\u675f\uff0c\u5229\u7528\u5c40\u90e8\u66f2\u7387\u4fe1\u606f\u8fdb\u884c\u68af\u5ea6\u4fee\u6b63\u3002", "result": "\u5728800M\u53c2\u6570\u7684Llama\u98ce\u683c\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0cCAGE\u5728W4A4\u91cf\u5316\u6a21\u5f0f\u4e0b\u76f8\u6bd4\u5f02\u5e38\u503c\u5904\u7406\u65b9\u6cd5\u80fd\u6062\u590d\u8d85\u8fc710%\u7684\u91cf\u5316\u635f\u5931\u589e\u52a0\u3002", "conclusion": "\u66f2\u7387\u611f\u77e5\u68af\u5ea6\u4fee\u6b63\u80fd\u591f\u5f25\u8865\u5f53\u524d\u5f02\u5e38\u503c\u5904\u7406\u65b9\u6cd5\u4e4b\u5916\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u91cf\u5316\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18786", "abs": "https://arxiv.org/abs/2510.18786", "authors": ["Federica Granese", "Serena Villata", "Charles Bouveyron"], "title": "Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams", "comment": "Under review", "summary": "Online topic models are unsupervised algorithms to identify latent topics in\ndata streams that continuously evolve over time. Although these methods\nnaturally align with real-world scenarios, they have received considerably less\nattention from the community compared to their offline counterparts, due to\nspecific additional challenges. To tackle these issues, we present SB-SETM, an\ninnovative model extending the Embedded Topic Model (ETM) to process data\nstreams by merging models formed on successive partial document batches. To\nthis end, SB-SETM (i) leverages a truncated stick-breaking construction for the\ntopic-per-document distribution, enabling the model to automatically infer from\nthe data the appropriate number of active topics at each timestep; and (ii)\nintroduces a merging strategy for topic embeddings based on a continuous\nformulation of optimal transport adapted to the high dimensionality of the\nlatent topic space. Numerical experiments show SB-SETM outperforming baselines\non simulated scenarios. We extensively test it on a real-world corpus of news\narticles covering the Russian-Ukrainian war throughout 2022-2023.", "AI": {"tldr": "SB-SETM\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5728\u7ebf\u4e3b\u9898\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u5e76\u8fde\u7eed\u6587\u6863\u6279\u6b21\u4e0a\u7684\u6a21\u578b\u6765\u5904\u7406\u6570\u636e\u6d41\uff0c\u80fd\u591f\u81ea\u52a8\u63a8\u65ad\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u6d3b\u52a8\u4e3b\u9898\u6570\u91cf\uff0c\u5e76\u5728\u4fc4\u4e4c\u6218\u4e89\u65b0\u95fb\u8bed\u6599\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u7ebf\u4e3b\u9898\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u968f\u65f6\u95f4\u4e0d\u65ad\u6f14\u5316\u7684\u6570\u636e\u6d41\u4e2d\u7684\u6f5c\u5728\u4e3b\u9898\uff0c\u4e0e\u73b0\u5b9e\u573a\u666f\u81ea\u7136\u5951\u5408\uff0c\u4f46\u7531\u4e8e\u9762\u4e34\u7279\u5b9a\u989d\u5916\u6311\u6218\uff0c\u76f8\u6bd4\u79bb\u7ebf\u6a21\u578b\u53d7\u5230\u7684\u5173\u6ce8\u8f83\u5c11\u3002", "method": "SB-SETM\u6269\u5c55\u4e86\u5d4c\u5165\u5f0f\u4e3b\u9898\u6a21\u578b\uff08ETM\uff09\uff0c\u901a\u8fc7\u622a\u65adstick-breaking\u6784\u9020\u5904\u7406\u6587\u6863-\u4e3b\u9898\u5206\u5e03\uff0c\u81ea\u52a8\u63a8\u65ad\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u6d3b\u52a8\u4e3b\u9898\u6570\u91cf\uff1b\u5e76\u5f15\u5165\u57fa\u4e8e\u8fde\u7eed\u6700\u4f18\u4f20\u8f93\u7684\u5408\u5e76\u7b56\u7565\u6765\u5904\u7406\u9ad8\u7ef4\u6f5c\u5728\u4e3b\u9898\u7a7a\u95f4\u4e2d\u7684\u4e3b\u9898\u5d4c\u5165\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793aSB-SETM\u5728\u6a21\u62df\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u57282022-2023\u5e74\u4fc4\u4e4c\u6218\u4e89\u65b0\u95fb\u6587\u7ae0\u7684\u771f\u5b9e\u8bed\u6599\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\u3002", "conclusion": "SB-SETM\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u5408\u5e76\u7b56\u7565\u548c\u81ea\u52a8\u4e3b\u9898\u6570\u91cf\u63a8\u65ad\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u4e3b\u9898\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5728\u771f\u5b9e\u6570\u636e\u6d41\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.18810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18810", "abs": "https://arxiv.org/abs/2510.18810", "authors": ["Weiqiu You", "Siqi Zeng", "Yao-Hung Hubert Tsai", "Makoto Yamada", "Han Zhao"], "title": "When LRP Diverges from Leave-One-Out in Transformers", "comment": "BlackboxNLP @ EMNLP 2025", "summary": "Leave-One-Out (LOO) provides an intuitive measure of feature importance but\nis computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)\noffers a potentially efficient alternative, its axiomatic soundness in modern\nTransformers remains largely under-examined. In this work, we first show that\nthe bilinear propagation rules used in recent advances of AttnLRP violate the\nimplementation invariance axiom. We prove this analytically and confirm it\nempirically in linear attention layers. Second, we also revisit CP-LRP as a\ndiagnostic baseline and find that bypassing relevance propagation through the\nsoftmax layer -- backpropagating relevance only through the value matrices --\nsignificantly improves alignment with LOO, particularly in middle-to-late\nTransformer layers. Overall, our results suggest that (i) bilinear\nfactorization sensitivity and (ii) softmax propagation error potentially\njointly undermine LRP's ability to approximate LOO in Transformers.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728Transformer\u6a21\u578b\u4e2d\uff0c\u5c42\u95f4\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\u65b9\u6cd5\u5728\u903c\u8fd1\u7559\u4e00\u6cd5\uff08LOO\uff09\u7279\u5f81\u91cd\u8981\u6027\u65f6\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u53cc\u7ebf\u6027\u4f20\u64ad\u89c4\u5219\u8fdd\u53cd\u5b9e\u73b0\u4e0d\u53d8\u6027\u516c\u7406\uff0c\u4e14\u7ed5\u8fc7softmax\u5c42\u4f20\u64ad\u76f8\u5173\u6027\u53ef\u6539\u5584\u4e0eLOO\u7684\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u7814\u7a76LRP\u65b9\u6cd5\u5728Transformer\u4e2d\u903c\u8fd1LOO\u7279\u5f81\u91cd\u8981\u6027\u7684\u6709\u6548\u6027\uff0c\u56e0\u4e3aLOO\u8ba1\u7b97\u6210\u672c\u9ad8\u800cLRP\u53ef\u80fd\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\uff0c\u4f46\u5176\u5728\u73b0\u4ee3Transformer\u4e2d\u7684\u516c\u7406\u5408\u7406\u6027\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u9996\u5148\u5206\u6790AttnLRP\u4e2d\u53cc\u7ebf\u6027\u4f20\u64ad\u89c4\u5219\u8fdd\u53cd\u5b9e\u73b0\u4e0d\u53d8\u6027\u516c\u7406\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u5c42\u5b9e\u9a8c\u9a8c\u8bc1\uff1b\u5176\u6b21\u91cd\u65b0\u8bc4\u4f30CP-LRP\u4f5c\u4e3a\u8bca\u65ad\u57fa\u7ebf\uff0c\u7814\u7a76\u7ed5\u8fc7softmax\u5c42\u4f20\u64ad\u76f8\u5173\u6027\u7684\u6548\u679c\u3002", "result": "\u53cc\u7ebf\u6027\u4f20\u64ad\u89c4\u5219\u786e\u5b9e\u8fdd\u53cd\u5b9e\u73b0\u4e0d\u53d8\u6027\u516c\u7406\uff1b\u7ed5\u8fc7softmax\u5c42\u3001\u4ec5\u901a\u8fc7\u503c\u77e9\u9635\u53cd\u5411\u4f20\u64ad\u76f8\u5173\u6027\u663e\u8457\u6539\u5584\u4e86\u4e0eLOO\u7684\u5bf9\u9f50\u5ea6\uff0c\u7279\u522b\u662f\u5728Transformer\u7684\u4e2d\u540e\u671f\u5c42\u3002", "conclusion": "\u53cc\u7ebf\u6027\u56e0\u5b50\u5316\u654f\u611f\u6027\u548csoftmax\u4f20\u64ad\u8bef\u5dee\u53ef\u80fd\u5171\u540c\u524a\u5f31LRP\u5728Transformer\u4e2d\u903c\u8fd1LOO\u7684\u80fd\u529b\uff0c\u5efa\u8bae\u5728\u76f8\u5173\u65b9\u6cd5\u8bbe\u8ba1\u4e2d\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u3002"}}

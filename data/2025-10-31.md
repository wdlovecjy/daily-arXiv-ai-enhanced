<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.AI](#cs.AI) [Total: 14]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.LG](#cs.LG) [Total: 34]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Tractable Algorithms for Changepoint Detection in Player Performance Metrics](https://arxiv.org/abs/2510.25961)
*Amanda Glazer*

Main category: stat.AP

TL;DR: 本文提出了检测棒球运动员表现变化的可处理方法，应用于2023-2024赛季MLB数据，包括基于统计原理的性能指标可靠性基准和结合似然方法与分割样本推断的变点检测算法。


<details>
  <summary>Details</summary>
Motivation: 开发系统方法来检测棒球运动员表现指标的显著变化，为球员表现评估提供统计上可靠的检测工具。

Method: 首先建立性能指标的统计可靠性基准，然后提出结合似然方法和分割样本推断的变点检测算法，使用非参数检验或适合数据分布的检验，并包含偏移参数以指定最小检测变化幅度。

Result: 方法在91%的真实案例中检测到有意义的变化，某些指标超过60%的变化发生在赛季中，并在击球员板纪律指标和投手球速变化检测中验证了有效性。

Conclusion: 该框架不仅适用于棒球数据分析，还可广泛应用于任何涉及个体表现随时间监测的场景。

Abstract: We present tractable methods for detecting changes in player performance
metrics and apply these methods to Major League Baseball (MLB) batting and
pitching data from the 2023 and 2024 seasons. First, we derive principled
benchmarks for when performance metrics can be considered statistically
reliable, assuming no underlying change, using distributional assumptions and
standard concentration inequalities. We then propose a changepoint detection
algorithm that combines a likelihood-based approach with split-sample inference
to control false positives, using either nonparametric tests or tests
appropriate to the underlying data distribution. These tests incorporate a
shift parameter, allowing users to specify the minimum magnitude of change to
detect. We demonstrate the utility of this approach across several baseball
applications: detecting changes in batter plate discipline metrics (e.g., chase
and whiff rate), identifying velocity changes in pitcher fastballs, and
validating velocity changepoints against a curated ground-truth dataset of
pitchers who transitioned from relief to starting roles. Our method flags
meaningful changes in 91% of these `ground-truth' cases and reveals that, for
some metrics, more than 60% of detected changes occur in-season. While
developed for baseball, the proposed framework is broadly applicable to any
setting involving monitoring of individual performance over time.

</details>


### [2] [Variational System Identification of Aircraft](https://arxiv.org/abs/2510.26496)
*Dimas Abreu Archanjo Dutra*

Main category: stat.AP

TL;DR: 本文提出了一种新的变分系统辨识方法，作为传统滤波误差方法的替代方案，用于估计受过程和测量噪声影响的动态系统参数，并在真实飞行测试数据中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统滤波误差方法在动态系统参数估计中存在Riccati方程求解困难和预测器不稳定等问题，需要开发更稳健的辨识方法。

Method: 采用变分系统辨识方法，这是一种基于最大似然估计的新公式，避免了Riccati方程的求解，并能处理不稳定预测器的情况。

Result: 在真实飞行测试数据应用中，该方法展现出比滤波误差方法更好的收敛特性，即使所有参数和决策变量使用零初始猜测也能达到最优解。

Conclusion: 变分系统辨识方法在实践应用中具有明显优势，本文还提供了该方法的理论基础和实际使用建议。

Abstract: Variational system identification is a new formulation of maximum likelihood
for estimation of parameters of dynamical systems subject to process and
measurement noise, such as aircraft flying in turbulence. This formulation is
an alternative to the filter-error method that circumvents the solution of a
Riccati equation and does not have problems with unstable predictors. In this
paper, variational system identification is demonstrated for estimating
aircraft parameters from real flight-test data. The results show that, in real
applications of practical interest, it has better convergence properties than
the filter-error method, reaching the optimum even when null initial guesses
are used for all parameters and decision variables. This paper also presents
the theory behind the method and practical recommendations for its use.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [3] [6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting](https://arxiv.org/abs/2510.26166)
*Juncong Zhou,Chao Hu,Guanlin Wu,Zixiang Ren,Han Hu,Juyong Zhang,Rui Zhang,Jie Xu*

Main category: eess.SP

TL;DR: 提出了一种名为双向无线高斯散射（BiWGS）的六维信道知识图谱框架，能够建模动态发射器和接收器在3D空间中的无线信道，相比传统方法在6D信道功率增益图构建方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统2D/3D信道知识图谱方法假设基站配置固定，无法有效处理动态发射器和接收器位置变化的情况，需要开发能够建模六维空间信道特性的新方法。

Method: 使用高斯椭球体表示无线环境中的虚拟散射簇和环境障碍物，通过学习双向散射模式和复杂衰减特性来捕获无线环境的电磁传输特性，从而准确建模不同收发器配置下的信号传输。

Result: 实验结果显示，BiWGS在构建6D信道功率增益图方面显著优于经典多层感知器（MLP），在3D信道知识图谱构建方面实现了与最先进的无线辐射场高斯散射（WRF-GS）相当的空间频谱预测精度。

Conclusion: BiWGS能够在不牺牲保真度的情况下成功实现6D信道知识图谱构建的维度扩展，验证了该方法在动态收发器配置下建模无线信道的能力。

Abstract: This paper investigates the construction of channel knowledge map (CKM) from
sparse channel measurements. Dif ferent from conventional
two-/three-dimensional (2D/3D) CKM approaches assuming fixed base station
configurations, we present a six-dimensional (6D) CKM framework named
bidirectional wireless Gaussian splatting (BiWGS), which is capable of mod
eling wireless channels across dynamic transmitter (Tx) and receiver (Rx)
positions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual
scatterer clusters and environmental obstacles in the wireless environment. By
properly learning the bidirectional scattering patterns and complex attenuation
profiles based on channel measurements, these ellipsoids inherently cap ture
the electromagnetic transmission characteristics of wireless environments,
thereby accurately modeling signal transmission under varying transceiver
configurations. Experiment results show that BiWGS significantly outperforms
classic multi-layer perception (MLP) for the construction of 6D channel power
gain map with varying Tx-Rx positions, and achieves spatial spectrum prediction
accuracy comparable to the state-of-the art wireless radiation field Gaussian
splatting (WRF-GS) for 3D CKM construction. This validates the capability of
the proposed BiWGS in accomplishing dimensional expansion of 6D CKM
construction, without compromising fidelity.

</details>


### [4] [Optimal transmit field distribution for partially obstructed continuous radiating surfaces in near-field communication systems](https://arxiv.org/abs/2510.26262)
*Francesco Verde,Donatella Darsena,Marco Di Renzo,Vincenzo Galdi*

Main category: eess.SP

TL;DR: 本文提出了一种在遮挡环境中进行近场通信的孔径场最优合成方法，基于刀边衍射物理模型，将问题表述为希尔伯特空间中的最大化问题，得到匹配滤波器形式的优化解。


<details>
  <summary>Details</summary>
Motivation: 解决在遮挡环境中近场通信的能量聚焦问题，建立物理模型与信号处理方法的联系，支持硬件实现。

Method: 使用基于刀边衍射的物理一致模型，将问题表述为希尔伯特空间中的最大化问题，得到匹配衍射核的滤波器作为最优解。

Result: 获得了匹配滤波器形式的最优解，能够有效聚焦能量，支持连续孔径硬件实现如超表面或透镜天线。

Conclusion: 该框架将物理建模、信号处理和硬件设计相结合，为遮挡近场信道中的高效能量聚焦提供了有效方法。

Abstract: This paper deals with the optimal synthesis of aperture fields for
(radiating) near-field communications in obstructed environments. A physically
consistent model based on knife-edge diffraction is used to formulate the
problem as a maximization in Hilbert space. The optimal solution is obtained as
a matched filter that ``matches" the shape of a diffraction-induced kernel,
thus linking wave propagation with signal processing methods. The framework
supports hardware implementation using continuous apertures such as
metasurfaces or lens antennas. This approach bridges physically grounded
modeling, signal processing, and hardware design for efficient energy focusing
in near-field obstructed channels.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 本文探索将SHAP解释性AI技术应用于国际象棋分析，通过将棋子视为特征并进行系统性消融，计算每个棋子的贡献度来解释引擎评估结果。


<details>
  <summary>Details</summary>
Motivation: 当前国际象棋引擎提供精确但不透明的评估（通常以百分兵分数表示），这些输出掩盖了单个棋子或模式的潜在贡献。

Method: 将棋子作为特征，采用SHAP技术进行系统性消融，计算每个棋子的加性贡献，以局部忠实且人类可解释的方式解释引擎输出。

Result: 开发了一种能够将引擎评估归因于棋盘上特定棋子的方法，为可视化、人类训练和引擎比较开辟了新可能性。

Conclusion: 该方法将经典国际象棋教学理念（通过心理移除棋子评估局面）与现代可解释AI技术相结合，并发布了配套代码和数据以促进可解释国际象棋AI的未来研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [6] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 3.8B参数的Humans-Junior模型在FACTS Grounding基准测试中与GPT-4o表现相当（在±5%等效范围内），同时云服务成本降低约19倍，自托管/边缘部署可接近零边际成本。


<details>
  <summary>Details</summary>
Motivation: 开发成本效益高的小型语言模型，在保持与大型模型相当性能的同时显著降低推理成本。

Method: 结合最小化的"外骨骼推理"支架和行为微调，教导协议合规性而非领域知识，两者协同作用显著提升性能。

Result: 在Q1-Q500测试中，GPT-4o得分73.5%，Humans-Junior得分72.7%，配对差异0.8个百分点，统计等效性在±5%范围内成立。前沿模型仅通过提示工程可获得额外提升。

Conclusion: 小型模型通过精心设计的推理支架和行为微调，可以在特定任务上达到与大型模型相当的性能，同时大幅降低成本。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [7] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出了注意力感知逆规划问题，旨在从人类行为中推断认知偏差，特别是注意力偏差。通过结合深度强化学习和计算认知建模，在真实驾驶场景中展示了该方法可扩展性。


<details>
  <summary>Details</summary>
Motivation: 人类的目标导向行为受到认知偏差影响，与人类交互的自主系统需要意识到这一点。特别是注意力偏差会系统性地影响日常任务执行，如驾驶。

Method: 结合深度强化学习与计算认知建模，构建注意力感知逆规划方法，从行为数据中推断注意力策略。

Result: 在Waymo开放数据集的真实驾驶场景中成功推断RL智能体的注意力策略，证明了注意力感知逆规划在估计认知偏差方面的可扩展性。

Conclusion: 注意力感知逆规划与标准逆强化学习有系统性差异，能够有效推断认知偏差，为理解人类行为提供了新视角。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [8] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS是一个基于Lean4的大学物理问题推理框架，包含LeanPhysBench基准测试和PhysLib物理库，在现有模型上表现不佳但PhysLib能显著提升性能


<details>
  <summary>Details</summary>
Motivation: 为大学物理问题提供正式的推理框架和基准测试，填补Lean4在物理领域基准测试的空白

Method: 构建包含200个手工制作和同行评审的物理问题的LeanPhysBench基准测试，开发社区驱动的PhysLib物理库包含基本单位系统和定理

Result: DeepSeek-Prover-V2-7B仅达到16%准确率，Claude-Sonnet-4达到35%，PhysLib平均提升模型性能11.75%

Conclusion: LeanPhysBench具有挑战性，PhysLib有效提升模型性能，这是首个在Lean4中提供的物理基准测试研究

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [9] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的大语言模型推理经济学框架，将LLM推理视为计算驱动的智能生产活动，分析了边际成本、规模经济效应和输出质量，并基于WiNEval-3.0数据构建了首个LLM推理生产前沿。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理成本已成为决定其商业可行性和广泛应用的关键因素，需要建立经济分析框架来指导模型部署决策。

Method: 采用量化经济学框架，将LLM推理过程视为计算驱动的智能生产活动，基于WiNEval-3.0实证数据构建LLM推理生产前沿。

Result: 揭示了三个原则：边际成本递减、规模收益递减以及最优成本效益区域，为模型部署提供了经济基础。

Conclusion: 该研究不仅为模型部署决策提供了经济基础，还为未来基于市场的AI推理资源定价和优化奠定了实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [10] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: QASU是一个用于评估大型语言模型处理问卷数据能力的基准，通过测试六种序列化格式和多种提示策略，发现选择合适的格式和提示组合可以将准确率提高8.8个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前调查问卷数据与LLM的集成存在空白，现有工具主要为人工作业设计，缺乏基于证据的指导来优化问卷数据在LLM中的表示方式。

Method: 引入QASU基准，评估六种结构技能（包括答案查找、受访者计数和多跳推理），测试六种序列化格式和多种提示策略，通过自增强提示添加轻量级结构提示。

Result: 实验表明，选择有效的格式和提示组合相比次优格式可提高8.8%的准确率；对特定任务，通过自增强提示添加结构提示可平均提高3-4%的准确率。

Conclusion: QASU基准通过系统隔离格式和提示效果，为基于LLM的问卷分析研究和实际应用提供了简单而多功能的基础。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [11] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个用于LLM强化微调的贝叶斯在线任务选择框架，通过自适应维护任务难度后验估计，结合显式和隐式证据，使用Thompson采样平衡探索与利用，提高数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化微调方法在任务选择上存在效率低下、成本高、适应性差或证据不完整的问题，需要一种更有效的动态任务选择方案。

Method: 基于贝叶斯推断，自适应维护任务难度后验估计，联合使用直接评估的显式证据和从未选择任务推断的隐式证据，采用Thompson采样策略，并通过超轻量插值插件实现隐式证据估计。

Result: 在多个领域和不同规模的LLM上，BOTS相比基准方法和消融实验，持续提高了数据效率和性能表现。

Conclusion: BOTS为RFT中的动态任务选择提供了一个实用且可扩展的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [12] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 该研究探讨了AI数学家系统作为研究伙伴在数学研究中的应用，通过人类与AI的协同推理解决同质化理论中的挑战性问题，展示了人类直觉与机器计算的互补性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在数学推理方面取得了显著进展，但在数学研究实践中的应用仍然有限。本研究旨在探索AI如何作为研究伙伴而非单纯的问题解决者参与数学发现过程。

Method: 通过分析AI数学家的自主推理轨迹，结合针对性的人类干预来结构化发现过程，包括迭代分解问题为可处理的子目标、选择适当的分析方法以及验证中间结果。

Result: 该方法产生了一个完整且可验证的证明，展示了人类与AI协同推理如何提高证明的可靠性、透明性和可解释性，同时保持人类对形式严谨性和正确性的监督。

Conclusion: 系统化的人类-AI协同推理能够推进数学发现的前沿，这种协作范式结合了人类直觉和机器计算的优势，为数学研究提供了新的可能性。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [13] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出了一种基于任务项内在属性的基准测试子集选择方法Scales++，通过认知需求选择数据，在仅使用0.5%数据子集的情况下，能以2.9%的平均绝对误差预测完整基准测试分数，显著降低了前期选择成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型性能选择基准测试项的方法存在高前期成本、无法处理新基准测试（冷启动问题）以及依赖未来模型与现有模型失败模式相似的脆弱假设等局限性。

Method: 提出项目中心的高效基准测试方法Scales++，基于基准测试样本的认知需求进行数据选择，而不是依赖模型特定的失败模式。

Result: Scales++将前期选择成本降低了18倍以上，同时保持了有竞争力的预测保真度。在Open LLM排行榜上，仅使用0.5%数据子集就能以2.9%的平均绝对误差预测完整基准测试分数。

Conclusion: 项目中心方法能够在不显著降低保真度的情况下实现更高效的模型评估，同时提供更好的冷启动性能和更可解释的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [14] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 本文提出医学稀疏自编码器（MedSAEs）应用于MedCLIP的潜在空间，通过相关性指标、熵分析和MedGEMMA自动神经元命名来量化可解释性，在CheXpert数据集上验证了MedSAE神经元比原始MedCLIP特征具有更高的单义性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要既准确又可解释的模型，本文旨在推进医学视觉中的机制可解释性研究。

Method: 将医学稀疏自编码器（MedSAEs）应用于MedCLIP视觉语言模型的潜在空间，提出结合相关性指标、熵分析和MedGEMMA自动神经元命名的评估框架。

Result: 在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征实现了更高的单义性和可解释性。

Conclusion: 该研究在高效医疗AI和透明度之间架起了桥梁，为临床可靠表示提供了可扩展的解决方案。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [15] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Thought Hijacking的越狱攻击方法，通过在有害请求前添加无害的推理序列来绕过大型推理模型的安全防护，攻击成功率高达94%-100%。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型通过增加推理时间计算可以提高任务性能，但研究发现这种扩展推理反而可能削弱安全性，因为同样的推理能力可以被用来绕过安全防护机制。

Method: 使用Chain-of-Thought Hijacking攻击方法，在有害请求前填充长序列的无害谜题推理，通过稀释安全检查信号来绕过模型的安全防护。

Result: 在HarmBench测试中，该方法对Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet的攻击成功率分别达到99%、94%、100%和94%，远超之前的越狱方法。

Conclusion: 研究表明，最可解释的推理形式——显式链式思维，当与最终答案提示结合时，本身可能成为越狱攻击的载体，揭示了推理模型安全性的新脆弱点。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [16] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文探讨了上下文工程的概念，将其定义为使机器更好地理解人类情境和目的的方法，并追溯了该领域从1990年代至今的演变历程。


<details>
  <summary>Details</summary>
Motivation: 受马克思关于"人的本质是社会关系总和"的启发，作者认为在人工智能时代，上下文不再局限于人际互动，还包括人机互动。核心问题是：机器如何更好地理解我们的情境和目的？

Method: 通过历史分析，将上下文工程的发展划分为不同阶段：从早期人机交互框架到当今的智能体驱动交互范式，再到未来可能的人类级或超人类智能。

Result: 提出了上下文工程的系统定义，勾勒了其历史和概念图景，并探讨了实践中的关键设计考虑因素。

Conclusion: 本文为上下文工程提供了概念基础，并描绘了其未来前景，是推动AI系统中系统化上下文工程更广泛社区努力的基石。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [17] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型存在文本偏好问题，其根源在于模型内部注意力机制中视觉键向量与文本键向量的分布不匹配，导致视觉信息在注意力计算中被低估。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理视觉语言数据时表现出明显的文本偏好，限制了其基于视觉证据进行有效推理的能力。不同于先前研究将这种文本偏见归因于数据不平衡或指令调优等外部因素，本研究认为偏见源于模型内部架构。

Method: 从LLaVA和Qwen2.5-VL模型中提取键向量，使用t-SNE进行定性分析和Jensen-Shannon散度进行定量分析，研究视觉键向量和文本键向量的分布结构。

Result: 视觉键向量和文本键向量在注意力空间中占据明显不同的子空间，模态间差异在统计上显著，超过模态内变异的几个数量级。

Conclusion: 文本偏见源于注意力键空间内的内在错位，而不仅仅是外部数据因素。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [18] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 该论文研究了一种最小控制接口，让智能体在自主行动或请求人类干预之间选择，同时人类在信任或监督之间选择。通过将其建模为马尔可夫潜在游戏，提供了对齐保证：在人类价值函数的结构假设下，智能体增加自主性的决策不会损害人类价值。


<details>
  <summary>Details</summary>
Motivation: 随着智能体能力不断增强，如何在保持底层系统不变的情况下维持有意义的人类控制成为一个核心安全问题。研究旨在提供一种透明控制层，使智能体学会在风险时请求干预，在安全时自主行动。

Method: 将人机交互建模为双玩家马尔可夫游戏，特别关注马尔可夫潜在游戏情况。通过独立学习，智能体和人类发现各自的最优监督角色，智能体学会在不确定时请求帮助，人类学会何时进行监督。

Result: 网格世界模拟显示，通过独立学习，智能体和人类能够发现最优监督角色，形成新兴协作，避免训练后引入的安全违规。智能体学会在不确定时请求帮助，人类学会适时监督。

Conclusion: 该方法为部署后使未对齐模型更安全提供了实用方法，在满足特定条件时，智能体改善自身结果不会损害人类价值，实现了特定形式的内在对齐。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [19] [Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms](https://arxiv.org/abs/2510.25811)
*William Réveillard,Richard Combes*

Main category: stat.ML

TL;DR: 本文提出了一种计算Graves-Lai优化问题解的可行算法，用于解决具有最多m个模态的随机多臂老虎机问题，并实现了渐近最优算法。


<details>
  <summary>Details</summary>
Motivation: 解决具有多模态期望奖励函数的随机多臂老虎机问题，现有方法在计算上不可行，需要开发计算可行的算法。

Method: 提出第一个已知的计算可行的算法来计算Graves-Lai优化问题的解，从而能够实现该老虎机问题的渐近最优算法。

Result: 成功开发了计算Graves-Lai优化问题解的算法，并实现了渐近最优的多臂老虎机算法，代码已公开可用。

Conclusion: 该研究为多模态多臂老虎机问题提供了首个计算可行的渐近最优解决方案，具有重要的理论和实践意义。

Abstract: We consider a stochastic multi-armed bandit problem with i.i.d. rewards where
the expected reward function is multimodal with at most m modes. We propose the
first known computationally tractable algorithm for computing the solution to
the Graves-Lai optimization problem, which in turn enables the implementation
of asymptotically optimal algorithms for this bandit problem. The code for the
proposed algorithms is publicly available at
https://github.com/wilrev/MultimodalBandits

</details>


### [20] [Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation](https://arxiv.org/abs/2510.26026)
*Feichen Gan,Youcun Lu,Yingying Zhang,Yukun Liu*

Main category: stat.ML

TL;DR: 提出了一种统一的共形预测框架，用于无限时域策略评估，在在线和离线策略设置下构建无分布的回报预测区间。


<details>
  <summary>Details</summary>
Motivation: 在高风险设置中，可靠的强化学习不确定性量化至关重要。

Method: 将分布强化学习与共形校准相结合，提出基于截断滚动的模块化伪回报构建和时间感知校准策略，使用经验回放和加权子采样。

Result: 在合成环境和基准环境（如Mountain Car）中的实验表明，该方法显著提高了覆盖率和可靠性。

Conclusion: 该方法能够解决未观测回报、时间依赖性和分布偏移等挑战，即使在策略偏移下也能实现不确定性量化。

Abstract: Reliable uncertainty quantification is crucial for reinforcement learning
(RL) in high-stakes settings. We propose a unified conformal prediction
framework for infinite-horizon policy evaluation that constructs
distribution-free prediction intervals {for returns} in both on-policy and
off-policy settings. Our method integrates distributional RL with conformal
calibration, addressing challenges such as unobserved returns, temporal
dependencies, and distributional shifts. We propose a modular pseudo-return
construction based on truncated rollouts and a time-aware calibration strategy
using experience replay and weighted subsampling. These innovations mitigate
model bias and restore approximate exchangeability, enabling uncertainty
quantification even under policy shifts. Our theoretical analysis provides
coverage guarantees that account for model misspecification and importance
weight estimation. Empirical results, including experiments in synthetic and
benchmark environments like Mountain Car, show that our method significantly
improves coverage and reliability over standard distributional RL baselines.

</details>


### [21] [Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning](https://arxiv.org/abs/2510.26121)
*Mara Daniels,Liam Hodgkinson,Michael Mahoney*

Main category: stat.ML

TL;DR: 本文提出了物理信息对数证据（PILE）评分，作为物理信息机器学习（PIML）模型的单一不确定性感知评估指标，解决了传统多目标方法在模型质量评估上的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 传统PIML方法同时包含数据和物理约束的多目标特性导致模型质量评估存在模糊性，这与认知不确定性的理解不足相关，即使统计指标显示良好拟合也可能出现意外失败模式。

Method: 在高斯过程回归框架内引入PILE评分，绕过测试损失的模糊性，为PIML模型的超参数提供选择原则。该方法可应用于核带宽、最小二乘正则化权重甚至核函数选择。

Result: PILE最小化能为各种模型参数提供优秀选择，包括核带宽、正则化权重和核函数选择。在数据获取前，PILE的'无数据'特殊情况还能识别与给定PDE'良好适应'的先验核选择。

Conclusion: PILE评分可作为PIML的统一评估指标，未来可扩展到更广泛的PIML应用中，为物理信息机器学习提供更可靠的模型选择和评估框架。

Abstract: Physics-informed machine learning (PIML) integrates prior physical
information, often in the form of differential equation constraints, into the
process of fitting machine learning models to physical data. Popular PIML
approaches, including neural operators, physics-informed neural networks,
neural ordinary differential equations, and neural discrete equilibria, are
typically fit to objectives that simultaneously include both data and physical
constraints. However, the multi-objective nature of this approach creates
ambiguity in the measurement of model quality. This is related to a poor
understanding of epistemic uncertainty, and it can lead to surprising failure
modes, even when existing statistical metrics suggest strong fits. Working
within a Gaussian process regression framework, we introduce the
Physics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test
losses, the PILE score is a single, uncertainty-aware metric that provides a
selection principle for hyperparameters of a PIML model. We show that PILE
minimization yields excellent choices for a wide variety of model parameters,
including kernel bandwidth, least squares regularization weights, and even
kernel function selection. We also show that, even prior to data acquisition, a
special 'data-free' case of the PILE score identifies a priori kernel choices
that are 'well-adapted' to a given PDE. Beyond the kernel setting, we
anticipate that the PILE score can be extended to PIML at large, and we outline
approaches to do so.

</details>


### [22] [Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning](https://arxiv.org/abs/2510.26723)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 本文揭示了经验福利最大化(EWM)和插件方法在策略学习中的等价性，提出了一种避免NP-hard组合优化步骤的凸化训练方法。


<details>
  <summary>Details</summary>
Motivation: 策略学习中的EWM方法和插件方法看似不同，但缺乏统一的理论框架来理解它们之间的关系。

Method: 通过证明EWM与最小二乘法在策略类重新参数化下的精确等价性，提出基于此等价性的正则化方法。

Result: 两种方法在共同条件下具有相同的理论保证，提出的新方法实现了凸化且计算高效的训练过程。

Conclusion: EWM和插件方法本质上是相同的优化问题，可以相互转换，这为策略学习提供了更有效的计算框架。

Abstract: The goal of policy learning is to train a policy function that recommends a
treatment given covariates to maximize population welfare. There are two major
approaches in policy learning: the empirical welfare maximization (EWM)
approach and the plug-in approach. The EWM approach is analogous to a
classification problem, where one first builds an estimator of the population
welfare, which is a functional of policy functions, and then trains a policy by
maximizing the estimated welfare. In contrast, the plug-in approach is based on
regression, where one first estimates the conditional average treatment effect
(CATE) and then recommends the treatment with the highest estimated outcome.
This study bridges the gap between the two approaches by showing that both are
based on essentially the same optimization problem. In particular, we prove an
exact equivalence between EWM and least squares over a reparameterization of
the policy class. As a consequence, the two approaches are interchangeable in
several respects and share the same theoretical guarantees under common
conditions. Leveraging this equivalence, we propose a novel regularization
method for policy learning. Our findings yield a convex and computationally
efficient training procedure that avoids the NP-hard combinatorial step
typically required in EWM.

</details>


### [23] [A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression](https://arxiv.org/abs/2510.26783)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 该论文提出了一个统一的因果推断理论，整合了Riesz回归、协变量平衡、密度比估计、目标最大似然估计和匹配估计器在平均处理效应估计中的应用。


<details>
  <summary>Details</summary>
Motivation: 动机是建立一个统一的理论框架，将因果推断中的多种方法联系起来，揭示它们之间的内在关系和等价性。

Method: 方法是通过理论分析，展示Riesz回归、协变量平衡、密度比估计、目标最大似然估计和匹配估计器在平均处理效应估计中的等价性和对偶关系。

Result: 结果表明：Riesz回归在ATE背景下与密度比估计本质等价；匹配估计器是密度比估计的特例；密度比估计与协变量平衡存在对偶关系；目标最大似然估计用于构造使主要偏差项为零的回归函数估计器。

Conclusion: 结论是这些看似不同的因果推断方法在理论上是统一的，平衡权重（Riesz表示器、偏差校正项、聪明协变量）和结果回归函数在ATE估计中起着关键作用。

Abstract: This note introduces a unified theory for causal inference that integrates
Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted
maximum likelihood estimation (TMLE), and the matching estimator in average
treatment effect (ATE) estimation. In ATE estimation, the balancing weights and
the regression functions of the outcome play important roles, where the
balancing weights are referred to as the Riesz representer, bias-correction
term, and clever covariates, depending on the context. Riesz regression,
covariate balancing, DRE, and the matching estimator are methods for estimating
the balancing weights, where Riesz regression is essentially equivalent to DRE
in the ATE context, the matching estimator is a special case of DRE, and DRE is
in a dual relationship with covariate balancing. TMLE is a method for
constructing regression function estimators such that the leading bias term
becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density
Ratio Estimation and Riesz Regression.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: 本文系统综述了Kolmogorov-Arnold Networks (KANs)的理论基础、架构变体和实践策略，提供了KANs与MLPs的对比分析、基函数选择指南以及改进技术路线图。


<details>
  <summary>Details</summary>
Motivation: KANs作为传统多层感知器(MLPs)的有前景替代方案，具有更好的表达能力和可解释性。本文旨在系统梳理快速发展的KAN研究领域，超越简单的性能比较，提供结构化综述。

Method: 收集和分类大量开源实现，建立KANs与MLPs的概念桥梁，分析不同基函数选择（B样条、切比雪夫多项式、ReLU组合等）的权衡，并分类整理改进技术。

Result: 建立了KANs与MLPs的形式等价性，突出了KAN公式的优越参数效率，提供了清晰的改进路线图，包括提高精度、效率和正则化的技术。

Conclusion: 本文提供了实用的"选择你的KAN"指南，帮助实践者选择合适的架构，并识别了当前研究空白。相关GitHub仓库作为KAN研究的结构化参考。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [25] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [26] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: MemEIC是一种用于大型视觉语言模型（LVLMs）的持续组合知识编辑方法，通过混合外部-内部编辑器和双LoRA适配器实现视觉和文本知识的组合编辑，显著提升多模态问题处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术通常只关注单一模态（视觉或语言），忽视了LVLMs固有的多模态特性和知识更新的连续性，导致编辑效果不理想。

Method: 采用混合外部-内部编辑器，包含用于跨模态证据检索的双外部记忆和用于解耦参数更新的双LoRA适配器，以及受大脑启发的知识连接器进行组合推理。

Result: 实验表明MemEIC在复杂多模态问题上显著提升性能，并能有效保留先前的编辑结果。

Conclusion: MemEIC为LVLMs中的持续组合知识编辑设立了新基准，解决了多模态知识编辑的挑战。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [27] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 提出一种基于图拓扑的主动学习方法，通过平衡Forman曲率选择代表性初始标签来指导探索，并动态触发从探索到利用的转换，同时引入局部图重连策略提升标签传播效果。


<details>
  <summary>Details</summary>
Motivation: 解决在标签稀缺情况下主动学习中探索与利用的核心挑战，替代手动调整的启发式方法，提高低标签率下的分类性能。

Method: 使用平衡Forman曲率进行核心集构建来选择初始标签，包含数据驱动的停止准则；利用BFC动态触发探索到利用的转换；引入局部图重连策略整合多尺度信息。

Result: 在基准分类任务上的实验表明，该方法在低标签率下持续优于现有的基于图的半监督基线方法。

Conclusion: 所提出的图拓扑方法有效解决了主动学习中的探索-利用平衡问题，在标签稀缺情况下显著提升了分类性能。

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [28] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 本文提出在主动学习过程中使用任务驱动的表示方法，通过定期更新表示来改善对混乱数据池的处理效果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的主动学习方法在处理混乱、未整理的数据池时效果有限，因为它们依赖固定的无监督表示，无法捕捉与目标任务相关的信息。

Method: 提出了两种学习任务驱动表示的策略：直接学习半监督表示，以及对初始无监督表示进行监督微调。这些表示在主动学习过程中定期更新。

Result: 两种策略都显著优于使用无监督或预训练表示的方法，在实证性能上有明显提升。

Conclusion: 在主动学习中使用任务驱动的动态表示更新策略能够有效处理混乱数据池，提高学习效果。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [29] [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745)
*Shahriar Noroozizadeh,Vaishnavh Nagarajan,Elan Rosenfeld,Sanjiv Kumar*

Main category: cs.LG

TL;DR: 该论文对比了序列建模中参数化内存的关联视图与几何视图，发现Transformer模型会自发合成原子事实的几何结构，将复杂的推理任务简化为易于学习的几何任务，这种几何结构源于谱偏差而非典型的架构或优化压力。


<details>
  <summary>Details</summary>
Motivation: 研究序列建模中参数化内存的本质，挑战传统的关联视图（将内存视为实体共现的暴力查找），探索几何视图如何解释模型对非共现实体间全局关系的编码能力。

Method: 通过分离出可分析的Transformer推理实例，对比关联视图与几何视图的差异；分析Node2Vec连接，揭示几何结构源于谱偏差；探讨几何结构形成的机制。

Result: 发现Transformer模型能够自发合成原子事实的几何结构，将复杂的ℓ重组合推理任务简化为单步几何任务；证明这种优雅几何结构的学习不需要比暴力关联查找更简洁，且源于自然产生的谱偏差。

Conclusion: 参数化内存的几何视图提供了新的研究视角，鼓励在知识获取、容量、发现和遗忘等领域重新审视默认直觉，并指出了使Transformer内存更几何化的改进空间。

Abstract: In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.

</details>


### [30] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: MLT是一种可逆的确定性技术，用于将高基数分类标识符编码为紧凑数值向量，通过模运算和可逆线性变换保持双射映射，具有维度控制和计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统哈希或独热编码方法在高基数分类标识符编码中存在信息损失或维度爆炸问题，需要一种既能保持可逆性又具有计算效率的编码方法。

Method: 利用有限域上的模运算和可逆线性变换，构建可逆的确定性编码过程，支持对维度进行显式控制。

Result: 在MovieLens 20M数据集上的实验表明，MLT在预测性能上与监督嵌入方法相当，但参数数量和训练成本显著降低。

Conclusion: MLT提供了一种高效、可逆且可扩展的高基数分类标识符编码方案，在保持性能的同时大幅减少了计算资源需求。

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [31] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: 本研究评估了三种地理基础模型嵌入源在马拉维预测15项常规健康项目指标的表现，发现多源GeoFM模型在87%的指标上优于传统地理统计方法，对人口密度、新HIV病例和儿童疫苗接种等指标预测效果最佳。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家的常规健康数据存在报告延迟和覆盖不完整的问题，需要探索新的数据源和分析方法来提高数据可靠性。

Method: 使用XGBoost模型，基于552个健康服务区域的数据（2021年1月-2023年5月），评估Google人口动态基础模型、Google AlphaEarth卫星图像和手机通话记录三种GeoFM嵌入源的预测性能，并与传统地理插值方法比较。

Result: 基于嵌入的方法在15个指标中的13个（87%）优于基线地理统计方法。多源GeoFM模型表现最稳健，在人口密度、新HIV病例和儿童疫苗接种等指标上取得良好预测效果，但对结核病和营养不良病例等数据可用性低的目标预测效果较差。

Conclusion: GeoFM嵌入为LMIC环境中的特定健康和人口结果提供了适度的预测改进，多源GeoFM整合是补充和加强受限常规健康信息系统的有效且有价值的工具。

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [32] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [33] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 知识蒸馏会削弱模型的去偏能力，但通过高质量数据增强、迭代知识蒸馏和教师权重初始化等方法可以改善去偏方法的可蒸馏性。


<details>
  <summary>Details</summary>
Motivation: 研究知识蒸馏对模型去偏能力转移的影响，探索知识蒸馏如何影响模型对分布外数据的鲁棒性。

Method: 在自然语言推理和图像分类任务上进行广泛实验，分析知识蒸馏后模型去偏能力的变化，并通过注意力模式和电路分析内部机制。

Result: 发现知识蒸馏会削弱模型去偏能力，去偏模型训练不受教师知识注入的益处，不同偏见的鲁棒性存在显著差异。

Conclusion: 提出了三种改进去偏方法可蒸馏性的解决方案，为理解知识蒸馏工作原理和设计更好的去偏方法提供了见解。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [34] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型的多阶段漏洞二分法方法，相比传统基于补丁的方法，能够更好地识别引入漏洞的提交，准确率提升超过38%。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的漏洞二分法存在多个限制：假设漏洞引入提交和补丁提交修改相同函数、仅依赖代码变更而忽略提交消息中的信息、基于简单启发式规则而缺乏逻辑分析。LLM能够理解文本和代码，有望突破这些限制。

Method: 设计了一个全面的多阶段流水线，利用LLM：(1)充分利用补丁信息，(2)在上下文中比较多个候选提交，(3)通过一系列筛选步骤逐步缩小候选范围。

Result: 评估显示该方法比最先进解决方案的准确率提升超过38%，多阶段流水线相比基线LLM二分法方法准确率提升60%。

Conclusion: 基于LLM的多阶段流水线方法能够有效克服传统漏洞二分法的限制，显著提高漏洞引入提交识别的准确性。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [35] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: SAFE是一个用于评估地球预测模型分层性能的开源工具包，通过按国家、地区、收入水平和土地覆盖等属性对预测结果进行分层分析，揭示模型在不同地理和社会经济条件下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型评估方法基于测试集上的平均损失，这种方法在天气和气候预测中忽略了人类发展和地理分布的不均匀性，无法反映模型在不同区域的实际表现差异。

Method: 开发SAFE工具包，整合多种数据域，按地理网格点的不同属性（领土、全球子区域、收入、土地覆盖）进行分层，分析每个属性层级的模型性能。

Result: 通过对多个最先进的AI天气预测模型进行基准测试，发现所有模型在不同属性上都存在预测技能的不均衡性，建立了基于不同提前时间和气候变量的模型预测公平性基准。

Conclusion: SAFE工具包首次实现了超越全局平均指标的模型评估，能够识别模型表现最佳和最差的区域，以及最公平的模型，为改进天气预报模型的公平性提供了重要工具。

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [36] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 研究发现，在工业时间序列异常检测中，简单的随机森林+XGBoost集成模型在分段数据上的表现优于复杂的特征工程和混合架构方法。


<details>
  <summary>Details</summary>
Motivation: 研究复杂特征工程和混合模型架构在多元工业时间序列异常检测中的有效性，特别关注蒸汽轮机系统。

Method: 评估了变化点导出的统计特征、基于聚类的子结构表示和混合学习策略对检测性能的影响，并与简单的随机森林+XGBoost集成模型在分段数据上的表现进行比较。

Result: 复杂方法表现不佳，而简单集成模型实现了AUC-ROC 0.976、F1分数0.41，并在定义时间窗口内达到100%的早期检测率。

Conclusion: 在高度不平衡和时间不确定性数据场景中，模型简单性结合优化分段可以超越更复杂的架构，提供更好的鲁棒性、可解释性和操作实用性。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [37] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了协作公共资源分配（CPRA）问题，考虑容量约束和时空动态，并开发了基于博弈论的时空强化学习框架（GSTRL）来解决这一NP难问题。


<details>
  <summary>Details</summary>
Motivation: 现有公共资源分配方法独立优化单个资源移动，未考虑容量约束，需要更实用的解决方案。

Method: 将CPRA问题建模为势博弈，证明势函数与最优目标无间隙，并设计GSTRL框架捕捉系统时空动态。

Result: 在两个真实世界数据集上的实验显示GSTRL具有优越性能。

Conclusion: GSTRL为CPRA问题提供了理论基础和实用解决方案，能有效处理容量约束和时空动态。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [38] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: 提出ACC-SGD-IE方法，通过跨训练传播留一扰动和累积影响状态，改进了标准SGD-IE在数据影响估计中的准确性，特别是在长训练周期中。


<details>
  <summary>Details</summary>
Motivation: 现代以数据为中心的AI需要精确的样本级影响估计，但标准SGD-IE方法忽略了跨周期复合效应，导致对关键样本的错误排序。

Method: ACC-SGD-IE是一种轨迹感知估计器，在训练过程中传播留一扰动，并在每一步更新累积影响状态。

Result: 在平滑强凸设置中实现几何误差收缩，在平滑非凸机制中收紧误差界限；更大批次进一步减少常数。实证显示在多个数据集上产生更准确的影响估计，特别是在长周期中。

Conclusion: ACC-SGD-IE在下游数据清洗中更可靠地标记噪声样本，使用其清洗数据训练的模型优于使用SGD-IE清洗的模型。

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [39] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 使用机器学习技术分析健康索赔数据，识别导致再入院的关键因素，以降低可预防的医院再入院率。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是支付方、提供者和政策制定者的国家优先事项，旨在提高医疗质量并降低成本。再入院率被用作评估医院医疗质量的基准。

Method: 使用逻辑回归、随机森林和支持向量机等机器学习技术分析健康索赔数据，并采用主成分分析进行降维处理，基于AUC指标比较和评估模型性能。

Result: 随机森林模型表现最佳，其次是逻辑回归和支持向量机模型。这些模型能够识别导致再入院的关键因素。

Conclusion: 这些模型可用于识别导致再入院的关键因素，帮助确定需要重点关注的患者，从而降低再入院风险，最终降低医疗成本并提高医疗质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [40] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: 提出了一种新的测试时对齐方法AISP，通过在预对数层应用高斯扰动来最大化期望奖励，优于最佳n采样和其他基于奖励的测试时对齐方法。


<details>
  <summary>Details</summary>
Motivation: 由于微调大语言模型需要高昂的计算成本，测试时对齐方法受到关注。

Method: 基于随机控制输入的采样模型预测控制，在预对数层应用高斯扰动，通过重要性采样获得最优均值。

Result: AISP在样本使用数量方面优于最佳n采样，并比其他基于奖励的测试时对齐方法获得更高奖励。

Conclusion: AISP是一种有效的测试时对齐方法，能够在较低计算成本下实现更好的性能。

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [41] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出了一种基于归纳方法的机器遗忘解决方案，通过在模型末尾添加投影-重分布层来实现遗忘，无需访问原始数据集或完整模型，具有模块化、模型无关的特性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在现实部署中面临可扩展性问题，需要完整访问原始数据集和模型，而本文旨在解决这些实际挑战。

Method: 将分类训练视为顺序学习过程，通过反转最后训练序列实现遗忘，在模型末尾添加投影-重分布层作为输出过滤器。

Result: 在多个数据集（CIFAR-10/100、Covertype）上的实验表明，该方法输出与完全重新训练的模型相似，同时大幅降低计算成本。

Conclusion: 该方法在保持输出性能的同时，展示了适用性、可扩展性和系统兼容性，为机器遗忘提供了更实用的解决方案。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [42] [Linear Causal Discovery with Interventional Constraints](https://arxiv.org/abs/2510.26342)
*Zhigao Guo,Feng Dong*

Main category: cs.LG

TL;DR: 本文提出了一种新的因果发现概念——干预约束，它不同于干预数据，而是以因果效应不等式约束的形式编码高层因果知识，确保学习到的模型符合已知的因果影响。


<details>
  <summary>Details</summary>
Motivation: 现有的因果发现方法虽然可以施加结构约束，但仍可能产生错误的因果结论。为了改进因果模型并提升下游任务效果，需要整合因果知识和机制。

Method: 提出干预约束概念，为线性因果模型提出量化总因果效应的度量，并构建为约束优化问题，采用两阶段约束优化方法求解。

Result: 在真实数据集上的评估表明，整合干预约束不仅提高了模型准确性、确保与已有发现的一致性，还能促进发现原本成本高昂的新因果关系。

Conclusion: 干预约束填补了现有方法的空白，通过明确约束变量对之间的总因果效应，使学习到的模型更符合已知因果影响，更具可解释性。

Abstract: Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.

</details>


### [43] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 本文提出改进经典强化学习方法，使其能够在稀疏、随机和非平稳环境中高效运行，特别是在水下污染云搜索等应用中。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在随机和非平稳环境中表现有限，特别是在奖励稀疏的环境下，许多行动只能获得零奖励。本文旨在解决这些挑战，使强化学习能更好地适应复杂环境。

Method: 系统研究大量改进方法，包括分层算法变更、多目标学习，以及集成位置记忆作为外部输出过滤器以防止状态重复访问。采用改进的蒙特卡洛方法。

Result: 改进的蒙特卡洛方法显著优于传统Q学习和两种穷举搜索模式，展示了其在复杂环境中适应强化学习的潜力。

Conclusion: 强化学习方法可以有效适应随机、非平稳和奖励稀疏的环境，为复杂应用场景提供了可行的解决方案。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [44] [CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse](https://arxiv.org/abs/2510.26369)
*Kazuma Kano,Yuki Mori,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.LG

TL;DR: 提出CorVS方法，通过视觉轨迹与传感器测量的对应关系进行人员识别，在真实仓库环境中验证了有效性


<details>
  <summary>Details</summary>
Motivation: 工业场所中工人定位数据对提高生产力至关重要，但仅凭视觉数据识别个体往往不实用，现有方法在真实条件下可能失效

Method: 1. 深度学习模型预测轨迹与传感器测量的对应概率和可靠性；2. 基于预测概率和可靠性随时间匹配轨迹和传感器测量

Result: 开发了真实仓库操作数据集，验证了该方法在现实应用中的有效性

Conclusion: CorVS方法能够克服现有方法的局限性，在真实工业环境中实现可靠的人员识别

Abstract: Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.

</details>


### [45] [Robust Graph Condensation via Classification Complexity Mitigation](https://arxiv.org/abs/2510.26451)
*Jiayi Luo,Qingyun Sun,Beining Yang,Haonan Yuan,Xingcheng Fu,Yanbiao Ma,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种新的流形约束鲁棒图压缩框架MRGC，通过三个图数据流形学习模块，使压缩图位于平滑、低维的流形中，在对抗攻击下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法在原始图被破坏时性能显著下降，而现有鲁棒图学习技术效果有限。研究发现图压缩本质上是内在维度降低过程，但这一特性容易受到对抗扰动的影响。

Method: 采用图数据流形的几何视角，提出MRGC框架，包含三个图数据流形学习模块，引导压缩图位于平滑、低维流形中，最小化类别模糊性。

Result: 大量实验表明，MRGC在各种攻击场景下都表现出鲁棒性。

Conclusion: MRGC框架通过流形约束有效提升了图压缩的鲁棒性，在对抗攻击下保持稳定的性能。

Abstract: Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.

</details>


### [46] [Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters](https://arxiv.org/abs/2510.26501)
*Mustafa Fuad Rifet Ibrahim,Maurice Meijer,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 本文研究在资源受限的可穿戴设备上，通过无监督异常检测(UAD)作为上游过滤机制来提高心电图(ECG)分析的鲁棒性，防止因分布外(OOD)数据导致的错误预测。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备的连续ECG监测对早期心血管疾病检测具有重要价值，但标准深度学习模型在资源受限环境下面对OOD数据时会产生高置信度的错误预测，危及患者安全。现有方法要么忽略计算约束，要么分别处理噪声和未见类别问题。

Method: 在严格资源约束下(最多512k参数)，通过神经架构搜索(NAS)优化六种UAD方法，包括Deep SVDD、重建模型、掩码异常检测、归一化流和扩散模型。在PTB-XL和BUT QDB数据集上评估对OOD心血管疾病类别和噪声污染信号的检测能力。

Result: Deep SVDD在检测性能和效率之间实现了最佳平衡。在实际部署模拟中，将优化的Deep SVDD过滤器与诊断分类器集成，相比仅使用分类器的基线，准确率提高了最多21个百分点。

Conclusion: 优化的UAD过滤器能够保护自动化ECG分析，使可穿戴设备上的连续心血管监测更加安全可靠。

Abstract: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.

</details>


### [47] [Higher-Order Regularization Learning on Hypergraphs](https://arxiv.org/abs/2510.26533)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文扩展了高阶超图学习（HOHL）的理论基础，证明了截断版本HOHL的一致性，推导了在完全监督学习中作为正则化器的收敛速率，并展示了其在主动学习和非几何结构数据集中的强大实证性能。


<details>
  <summary>Details</summary>
Motivation: 先前工作通过几何设置中的渐近一致性分析建立了HOHL的适定性和不适定性，本文旨在扩展这一理论基础并验证HOHL在不同学习场景中的实用性。

Method: 通过证明截断HOHL的一致性，推导HOHL作为正则化器在完全监督学习中的显式收敛速率，并在主动学习和非几何结构数据集上进行实证评估。

Result: 理论分析证明了截断HOHL的一致性，获得了明确的收敛速率；实证结果显示HOHL在主动学习和缺乏底层几何结构的数据集中表现出强大性能。

Conclusion: HOHL在不同学习设置中展现出多功能性和鲁棒性，为超图学习提供了坚实的理论基础和实用的实证验证。

Abstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.

</details>


### [48] [Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices](https://arxiv.org/abs/2510.26557)
*Jan Stenkamp,Nina Herrmann,Benjamin Karic,Stefan Oehmcke,Fabian Gieseke*

Main category: cs.LG

TL;DR: 提出了一种针对提升决策树的压缩方案，通过特征和阈值重用等技术训练紧凑的树集成模型，在保持相同性能的同时实现4-16倍的压缩比。


<details>
  <summary>Details</summary>
Motivation: 在计算受限的物联网设备上部署机器学习模型的需求日益增长，需要轻量级的机器学习模型来支持设备在无持续通信或外部能源供应的情况下自主运行。

Method: 采用特征和阈值重用等技术训练紧凑的提升决策树集成，使用改进的训练过程和替代内存布局来减少内存占用。

Result: 实验评估显示，与LightGBM模型相比，在保持相同性能的情况下实现了4-16倍的压缩比。

Conclusion: 该压缩方案使得物联网设备能够在仅需最小计算能力和能源的情况下自主运行，为远程监控、边缘分析和实时决策等应用开辟了广阔前景。

Abstract: Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.

</details>


### [49] [On the limitation of evaluating machine unlearning using only a single training seed](https://arxiv.org/abs/2510.26714)
*Jamie Lanyon,Axel Finke,Petros Andreou,Georgina Cosma*

Main category: cs.LG

TL;DR: 该论文指出机器遗忘算法评估中常见的做法存在问题：多次从同一训练模型开始运行遗忘算法可能产生非代表性结果，因为某些遗忘方法对模型训练时使用的随机数种子高度敏感。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘算法通常只能近似地移除数据影响，其性能主要依赖经验评估。现有评估实践可能产生误导性结果，需要更准确的评估方法。

Method: 通过分析机器遗忘算法对模型训练随机种子的敏感性，提出评估时应考虑不同训练种子带来的变异性。

Result: 研究发现某些机器遗忘方法对训练随机种子高度敏感，从同一训练模型多次运行可能产生非代表性结果。

Conclusion: 建议在机器遗忘算法的经验比较中，必须反映不同模型训练种子带来的变异性，以获得更准确的评估结果。

Abstract: Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.

</details>


### [50] [Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis](https://arxiv.org/abs/2510.26607)
*Maksim Maslov,Alexander Kugaevskikh,Matthew Ivanov*

Main category: cs.LG

TL;DR: 提出一种结合伯恩斯坦基参数化和Wasserstein距离最小化的新方法，用于分布回归问题，在保持几何准确性的同时提高计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有分布回归方法要么忽略概率空间的几何结构，要么计算成本高昂，需要一种平衡几何精度和计算实用性的解决方案。

Method: 使用伯恩斯坦基参数化概率轨迹，将条件分布建模为高斯分量的加权和，其均值和协方差通过伯恩斯坦多项式构造为输入变量的函数，采用基于自动微分的优化方法训练模型。

Result: 在包含复杂轨迹的合成数据集上，该方法在Wasserstein距离、能量距离和RMSE指标上表现出有竞争力的近似质量，特别是在显著非线性情况下，轨迹平滑度优于或与替代方法相当，且对数据结构变化具有鲁棒性。

Conclusion: 该方法代表了平衡几何精度、计算实用性和可解释性的解决方案，未来研究方向包括扩展到非高斯分布、应用熵正则化加速计算，以及适应高维数据近似曲面和更复杂结构。

Abstract: This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.

</details>


### [51] [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)
*Colin Doumont,Victor Picheny,Viacheslav Borovitskiy,Henry Moss*

Main category: cs.LG

TL;DR: 本文提出了一个基于热核的统一框架来理解组合优化中的贝叶斯优化内核，证明了多个成功组合内核与热核的等价关系，并展示了热核方法在性能上达到或超越复杂算法的效果。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在组合任务中应用广泛，但需要专门的内核来建模组合域。现有组合内核之间的关系不明确，需要建立统一的理论框架来理解这些内核的内在联系。

Method: 开发了基于热核的统一框架，系统地推导热核并表达为简单闭式表达式，通过理论证明和实验验证多个组合内核与热核的关系。

Result: 证明了多个成功组合内核与热核相关或等价；热核对最优值位置不敏感，而某些算法性能会大幅下降；基于热核的简单管道能够达到最先进的结果。

Conclusion: 热核框架为理解组合内核提供了统一视角，热核方法不仅理论上有优势，在实践中也能取得优异性能，为组合贝叶斯优化提供了有效解决方案。

Abstract: Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.

</details>


### [52] [Faithful and Fast Influence Function via Advanced Sampling](https://arxiv.org/abs/2510.26776)
*Jungyeon Koh,Hyeonsu Lyu,Jonggyu Jang,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 本文提出了两种基于特征和logits的先进采样技术，通过选择具有代表性的训练数据子集来改进影响函数估计，在减少计算资源的同时提高了估计准确性。


<details>
  <summary>Details</summary>
Motivation: 影响函数需要计算整个数据集的Hessian矩阵，计算资源消耗大。随机采样方法虽然可行但估计结果不一致，需要更有效的采样策略。

Method: 提出了基于特征和logits的两种采样器，通过考虑特征或logits的随机分布来选择具有代表性的数据子集，从而改进影响函数估计。

Result: 在类别移除实验中，方法减少了30.1%的计算时间和42.2%的内存使用，或将F1分数提高了2.5%。

Conclusion: 提出的采样技术能够有效减少影响函数计算所需的资源，同时提高估计准确性，为黑盒模型训练数据影响分析提供了实用解决方案。

Abstract: How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.

</details>


### [53] [Curly Flow Matching for Learning Non-gradient Field Dynamics](https://arxiv.org/abs/2510.26645)
*Katarina Petrović,Lazar Atanackovic,Viggo Moro,Kacper Kapuśniak,İsmail İlkan Ceylan,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: 本文提出Curly-FM方法，用于学习非梯度场动力学，解决了现有方法无法捕捉周期性行为的问题，并在单细胞RNA、计算流体动力学和洋流等应用中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于最小作用原理的方法只能学习梯度场动力学，无法捕捉现实世界中许多系统（如单细胞RNA中的细胞周期）表现出的非梯度周期性行为。

Method: 通过设计和求解具有非零漂移参考过程的薛定谔桥问题，利用推断的速度和种群快照数据来学习非梯度场动力学。

Result: 在单细胞、计算流体动力学和洋流等轨迹推断问题中，Curly-FM能够学习到与参考过程和种群边际更匹配的轨迹。

Conclusion: Curly-FM将流匹配模型扩展到超越种群建模，能够建模物理系统中已知的周期性行为，解决了现有方法的根本局限性。

Abstract: Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git

</details>


### [54] [Tight Differentially Private PCA via Matrix Coherence](https://arxiv.org/abs/2510.26679)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解和标准扰动机制的简单高效算法，用于在差分隐私下计算矩阵前r个奇异向量的跨度。该算法在密集设置下达到了与最优非私有算法相同的保证，显著优于现有私有算法。


<details>
  <summary>Details</summary>
Motivation: 解决Hardt和Roth提出的关于在差分隐私下计算矩阵前r个奇异向量跨度的问题，改进现有私有算法的性能。

Method: 使用奇异值分解和标准扰动机制，基于秩r相干性和谱间隙σr-σr+1来构建私有秩r近似。

Result: 提出的算法在密集设置下实现了与最优非私有算法相同的单峰PCA保证，且证明了高斯扰动不会增加秩r相干性。

Conclusion: 该方法在差分隐私下有效解决了奇异向量跨度计算问题，并在图问题中展示了应用潜力，特别是在低相干性假设下的最大割和其他约束满足问题。

Abstract: We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.

</details>


### [55] [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](https://arxiv.org/abs/2510.26690)
*Amir Reza Mirzaei,Yuqiao Wen,Yanshuai Cao,Lili Mou*

Main category: cs.LG

TL;DR: LoRAQuant是一种针对LoRA适配器的混合精度后训练量化方法，通过SVD重新参数化适配器，将重要信息集中在特定行列，从而实现重要部分高精度量化、其余部分超低位宽量化，显著降低比特使用量同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多个LoRA适配器同时加载时，虽然单个适配器轻量，但聚合成本在规模上变得显著，需要解决多适配器部署的效率问题。

Method: 使用奇异值分解重新参数化每个适配器，将最重要信息集中在特定行列，然后对重要组件采用高精度量化，其余部分采用超低位宽量化。

Result: 在LLaMA 2-7B、LLaMA 2-13B和Mistral 7B模型上的数学推理、编码和摘要任务实验表明，LoRAQuant使用显著更低的比特数，但达到可比甚至更高的性能。

Conclusion: LoRAQuant能够有效降低多LoRA适配器部署时的存储和计算成本，同时保持模型性能，为大规模个性化LLM应用提供了高效解决方案。

Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.

</details>


### [56] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 研究发现，在大型语言模型的强化学习微调中，BF16浮点精度会引入较大的舍入误差，导致训练和推理策略不一致。简单改用FP16精度即可有效解决这一问题，无需修改模型架构或学习算法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习微调大型语言模型时因数值不匹配导致的不稳定性问题，发现问题的根本原因在于浮点精度本身。

Method: 通过将广泛采用的BF16精度改为FP16精度，这种改变简单易行，只需几行代码修改，完全由现代框架支持。

Result: 使用FP16能带来更稳定的优化、更快的收敛速度，并在多样化任务、算法和框架中表现出更强的性能。

Conclusion: FP16精度在RL微调中比BF16更优，建议重新考虑精度权衡问题。

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


### [57] [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)
*Chuyan Chen,Chenyang Ma,Zhangxin Li,Yutong He,Yanjie Dong,Kun Yuan*

Main category: cs.LG

TL;DR: 提出ARC-Top-K压缩器，通过轻量级梯度草图对齐节点间的稀疏模式，实现无索引All-Reduce操作，在保持Top-K准确性的同时显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式机器学习中通信是主要瓶颈，现有梯度压缩方法存在局限性：Rand-K丢弃结构信息且实际表现差，Top-K保留信息但失去压缩特性且需要昂贵的All-Gather操作。

Method: 设计All-Reduce兼容的Top-K压缩器，使用轻量级梯度草图对齐跨节点的稀疏模式，结合动量误差反馈(EF21M)实现线性加速和更快的收敛速度。

Result: ARC-Top-K在保持Top-K准确性的同时，将训练时间减少高达60.7%，结合了Rand-K的鲁棒性和Top-K的强性能。

Conclusion: ARC-Top-K提供了高效可扩展的解决方案，证明具有压缩特性，在标准假设下比原始EF21M实现更快的收敛速率。

Abstract: Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.

</details>

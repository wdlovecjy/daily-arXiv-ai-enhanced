<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.AI](#cs.AI) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Ambient Backscatter Communication Assisted by Fluid Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2510.24725)
*Masoud Kaveh,Farshad Rostami Ghadi,Riku Jantti,Kai-Kit Wong,F. Javier Lopez-Martinez*

Main category: eess.SP

TL;DR: 该论文研究了将流体可重构智能表面(FRIS)集成到环境反向散射通信(AmBC)系统中，通过动态调整流体元素位置来增强空间适应性，在直接链路受阻时提升通信性能。


<details>
  <summary>Details</summary>
Motivation: 传统RIS具有固定位置元素，在直接标签到阅读器链路弱或被障碍物阻挡时性能受限。FRIS通过流体元素动态调整位置，提供更强的空间适应性来解决这一问题。

Method: 建立AmBC标签通过FRIS与阅读器通信的系统模型，将FRIS元素位置优化建模为非凸问题，并采用粒子群优化(PSO)算法来获得流体元素的近最优配置。

Result: 仿真结果表明，FRIS辅助的AmBC系统在可实现的吞吐量方面显著优于传统的基于RIS的AmBC系统。

Conclusion: FRIS在AmBC系统中展现出优越性能，通过动态位置调整实现了比传统RIS更好的通信效果，为弱链路环境下的反向散射通信提供了有效解决方案。

Abstract: This paper investigates the integration of a fluid reconfigurable intelligent
surface (FRIS) into ambient backscatter communication (AmBC) systems. Unlike
conventional reconfigurable intelligent surfaces (RISs) with fixed position
elements, FRIS employs fluidic elements that can dynamically adjust their
positions, offering enhanced spatial adaptability. We develop a system model
where an AmBC tag communicates with a reader through an FRIS, which is
particularly beneficial in scenarios where the direct tag-to-reader link is
weak or blocked by obstacles. The achievable backscatter rate is analyzed, and
the optimization of FRIS element positions is formulated as a non-convex
problem. To address this, we employ particle swarm optimization (PSO) to obtain
near-optimal configurations of the fluid elements. Simulation results
demonstrate that FRIS-aided AmBC significantly outperforms conventional
RIS-based AmBC systems in terms of achievable throughput.

</details>


### [2] [Modelling Real-Life Cycling Decisions in Real Urban Settings Through Psychophysiology and LLM-Derived Contextual Data](https://arxiv.org/abs/2510.24726)
*Maximiliano Rosadio Z.,Angel Jimenez-Molina,Bastián Henríquez,Paulina Leiva,Ricardo Hurtubia,Ricardo De La Paz Guala,Leandro Gayozo,C. Angelo Guevara*

Main category: eess.SP

TL;DR: 该论文提出了一种创新方法，利用大语言模型从记录的多媒体中提取上下文数据，结合生理数据研究城市骑行环境中不同情境对参与者情绪状态和行为的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于自我报告的情绪测量方法存在粒度低和记忆偏差问题，而生理指标虽能提供连续数据，但需要足够的上下文数据来解释情绪变化的原因，这在真实环境中极难获取。

Method: 从智利圣地亚哥城市骑行案例研究中收集数据，使用大语言模型处理视频中提取的图像序列，获取环境语义描述，将这些离散但密集的上下文数据整合到混合模型中，其中疲劳和唤醒度作为影响观察到的骑行行为的潜在变量。

Result: 研究证实骑行决策受到压力相关情绪的影响，并突显了城市特征和交通条件对骑行者行为的强烈影响。

Conclusion: 该方法成功解决了在真实环境中获取足够上下文数据的挑战，为理解情绪状态与环境因素之间的关系提供了有效途径。

Abstract: Measuring emotional states in transportation contexts is an emerging field.
Methods based on self-reported emotions are limited by their low granularity
and their susceptibility to memory bias. In contrast, methods based on
physiological indicators provide continuous data, enabling researchers to
measure changes in emotional states with high detail and accuracy. Not only are
emotions important in the analysis, but understanding what triggers emotional
changes is equally important. Uncontrolled variables such as traffic
conditions, pedestrian interactions, and infrastructure remain a significant
challenge, as they can have a great impact on emotional states. Explaining the
reasons behind these emotional states requires gathering sufficient and proper
contextual data, which can be extremely difficult in real-world environments.
This paper addresses these challenges by applying an innovative approach,
extracting contextual data (expert annotator level) from recorded multimedia
using large language models (LLMs). In this paper, data are collected from an
urban cycling case study of the City of Santiago, Chile. The applied models
focus on understanding how different environments and traffic situations affect
the emotional states and behaviors of the participants using physiological
data. Sequences of images, extracted from the recorded videos, are processed by
LLMs to obtain semantic descriptions of the environment. These discrete,
although dense and detailed, contextual data are integrated into a hybrid
model, where fatigue and arousal serve as latent variables influencing observed
cycling behaviors (inferred from GPS data) like waiting, accelerating, braking,
etc. The study confirms that cycling decisions are influenced by stress-related
emotions and highlights the strong impact of urban characteristics and traffic
conditions on cyclist behavior.

</details>


### [3] [Aerial RIS-Enhanced Communications: Joint UAV Trajectory, Altitude Control, and Phase Shift Design](https://arxiv.org/abs/2510.24731)
*Bin Li,Dongdong Yang,Lei Liu,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于欧拉角的空中可重构智能表面控制方案，通过联合优化ARIS的高度和轨迹来解决四旋翼无人机飞行过程中的倾斜和高度变化导致的波束失准问题，显著提高了系统总速率。


<details>
  <summary>Details</summary>
Motivation: 相比部署在建筑物立面的地面RIS，安装在四旋翼无人机上的空中RIS具有更好的灵活性和覆盖范围，但无人机飞行过程中的不可避免的倾斜和高度变化会导致严重的波束失准，显著降低ARIS性能。

Method: 提出基于欧拉角的ARIS控制方案，联合优化ARIS高度和轨迹；将问题建模为马尔可夫决策过程，采用带优先经验回放的软演员-评论家算法学习ARIS控制策略；基于优化配置使用注水法和二分法确定最优基站波束成形。

Result: 数值结果表明，所提算法在收敛性和通信性能方面显著优于基准方法，实现了约14.4%的总速率提升；相比固定水平ARIS方案，产生更自适应的轨迹，显著减轻了ARIS倾斜导致的性能下降。

Conclusion: 所提出的方案展示了实际ARIS部署的强大潜力，通过联合优化ARIS配置和基站波束成形，有效解决了无人机飞行动态带来的波束失准问题。

Abstract: Reconfigurable intelligent surface (RIS) has emerged as a pivotal technology
for enhancing wireless networks. Compared to terrestrial RIS deployed on
building facades, aerial RIS (ARIS) mounted on quadrotor unmanned aerial
vehicle (UAV) offers superior flexibility and extended coverage. However, the
inevitable tilt and altitude variations of a quadrotor UAV during flight may
lead to severe beam misalignment, significantly degrading ARIS's performance.
To address this challenge, we propose a Euler angles-based ARIS control scheme
that jointly optimizes the altitude and trajectory of the ARIS by leveraging
the UAV's dynamic model. Considering the constraints on ARIS flight energy
consumption, flight safety, and the transmission power of a base station (BS),
we jointly design the ARIS's altitude, trajectory, phase shifts, and BS
beamforming to maximize the system sum-rate. Due to the continuous control
nature of ARIS flight and the strong coupling among variables, we formulate the
problem as a Markov decision process and adopt a soft actor-critic algorithm
with prioritized experience replay to learn efficient ARIS control policies.
Based on the optimized ARIS configuration, we further employ the water-filling
and bisection method to efficiently determine the optimal BS beamforming.
Numerical results demonstrate that the proposed algorithm significantly
outperforms benchmarks in both convergence and communication performance,
achieving approximately 14.4\% improvement in sum-rate. Moreover, in comparison
to the fixed-horizontal ARIS scheme, the proposed scheme yields more adaptive
trajectories and significantly mitigates performance degradation caused by ARIS
tilting, demonstrating strong potential for practical ARIS deployment.

</details>


### [4] [Cardi-GPT: An Expert ECG-Record Processing Chatbot](https://arxiv.org/abs/2510.24737)
*Koustav Mallick,Neel Singh,Mohammedreza Hajiarbabi*

Main category: eess.SP

TL;DR: Cardi-GPT是一个基于深度学习和自然语言交互的专家系统，用于心电图(ECG)解读和临床沟通，通过16层残差块CNN处理12导联ECG数据，在24种心脏状况上达到0.6194的加权准确率。


<details>
  <summary>Details</summary>
Motivation: 传统ECG解读和临床沟通需要大量专业知识且具有挑战性，需要开发能够简化ECG解读并增强临床沟通的先进系统。

Method: 使用16层残差块CNN处理12导联ECG数据，引入模糊化层将复杂数值输出转换为临床有意义的语言类别，集成聊天机器人界面促进诊断洞察的直观探索。

Result: 在来自4个国家6家医院的多样化数据集上评估，性能优于基线模型，总体响应质量得分达到73%，在覆盖度、基础性和连贯性方面表现优异。

Conclusion: Cardi-GPT通过弥合复杂ECG数据解读与可操作临床洞察之间的差距，代表了心血管医疗领域的变革性创新，有望提高诊断准确性、优化临床工作流程并改善患者预后。

Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial
yet challenging tasks in cardiovascular diagnosis, traditionally requiring
significant expertise and precise clinical communication. This paper introduces
Cardi-GPT, an advanced expert system designed to streamline ECG interpretation
and enhance clinical communication through deep learning and natural language
interaction. Cardi-GPT employs a 16-residual-block convolutional neural network
(CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194
across 24 cardiac conditions. A novel fuzzification layer converts complex
numerical outputs into clinically meaningful linguistic categories, while an
integrated chatbot interface facilitates intuitive exploration of diagnostic
insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across
four countries, demonstrating superior performance compared to baseline models.
Additionally, Cardi-GPT achieved an impressive overall response quality score
of 73\%, assessed using a comprehensive evaluation framework that measures
coverage, grounding, and coherence. By bridging the gap between intricate ECG
data interpretation and actionable clinical insights, Cardi-GPT represents a
transformative innovation in cardiovascular healthcare, promising to improve
diagnostic accuracy, clinical workflows, and patient outcomes across diverse
medical settings.

</details>


### [5] [StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs](https://arxiv.org/abs/2510.24738)
*Tianheng Ling,Chao Qian,Peter Zdankin,Torben Weis,Gregor Schiele*

Main category: eess.SP

TL;DR: StrikeWatch是一个紧凑的腕戴式系统，能够在设备上实时进行步态识别，特别针对检测脚跟与脚前掌着地模式，通过视觉和听觉反馈帮助跑步者自我纠正有害步态。


<details>
  <summary>Details</summary>
Motivation: 跑步有显著健康益处，但不正确的步态模式可能导致损伤，而现有步态分析系统通常笨重且仅限于离线分析。腕戴式可穿戴设备提供了更实用、非侵入性的替代方案，但在这类设备上实现实时步态识别仍面临挑战。

Method: 提出了四种紧凑的深度学习架构（1D-CNN、1D-SepCNN、LSTM和Transformer），并在两个代表性嵌入式FPGA上优化以实现能效推理。使用自定义硬件原型收集户外跑步标记数据集，并通过全自动部署管道评估所有模型。

Result: 评估12名参与者，6位量化的1D-SepCNN在iCE40UP5K上达到最高平均F1分数0.847，每次推理仅消耗0.350μJ，延迟0.140ms。该配置支持在320mAh电池上连续推理13.6天。

Conclusion: 研究揭示了模型复杂度和硬件效率之间的权衡关系，证明了在资源受限的腕戴设备上实现实时步态识别的可行性，为跑步者提供了实用的自我纠正工具。

Abstract: Running offers substantial health benefits, but improper gait patterns can
lead to injuries, particularly without expert feedback. While prior gait
analysis systems based on cameras, insoles, or body-mounted sensors have
demonstrated effectiveness, they are often bulky and limited to offline,
post-run analysis. Wrist-worn wearables offer a more practical and
non-intrusive alternative, yet enabling real-time gait recognition on such
devices remains challenging due to noisy Inertial Measurement Unit (IMU)
signals, limited computing resources, and dependence on cloud connectivity.
This paper introduces StrikeWatch, a compact wrist-worn system that performs
entirely on-device, real-time gait recognition using IMU signals. As a case
study, we target the detection of heel versus forefoot strikes to enable
runners to self-correct harmful gait patterns through visual and auditory
feedback during running. We propose four compact DL architectures (1D-CNN,
1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient
inference on two representative embedded Field-Programmable Gate Arrays
(FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our
custom-built hardware prototype, we collect a labeled dataset from outdoor
running sessions and evaluate all models via a fully automated deployment
pipeline. Our results reveal clear trade-offs between model complexity and
hardware efficiency. Evaluated across 12 participants, 6-bit quantized
1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just
0.350 {\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running
at 20 MHz. This configuration supports up to 13.6 days of continuous inference
on a 320 mAh battery. All datasets and code are available in the GitHub
repository https://github.com/tianheng-ling/StrikeWatch.

</details>


### [6] [A Cylindrical Nanowire Array-Based Flexure-FET Receiver for Molecular Communication](https://arxiv.org/abs/2510.24890)
*Dilara Aktas,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 提出了一种基于圆柱形纳米线阵列的Flexure-FET分子通信接收器，通过分布式机电耦合增强设计灵活性和可扩展性，为IoNT应用提供可调谐的接收器架构。


<details>
  <summary>Details</summary>
Motivation: 分子通信在纳米物联网和体内医疗系统中具有重要应用前景，但需要开发能够检测带电和中性分子、兼具生物相容性和能效的高级接收器架构。

Method: 采用圆柱形纳米线阵列的Flexure-FET分子通信接收器，在悬浮栅配置中实现分布式机电耦合，提供纳米线半径、长度、间距和阵列尺寸等几何自由度。

Result: 建立了端到端分析模型，揭示了几何结构、机电动力学和分子结合过程之间的强相互依赖性，实现了对灵敏度、噪声特性和通信容量的可调控制。

Conclusion: 该设计增强了结构可调性和阵列配置，为未来基于混合物和空间调制的分子通信系统提供了灵活基础，推动了IoNT框架内可扩展和多功能接收器架构的发展。

Abstract: Molecular communication (MC) enables biocompatible and energy-efficient
information transfer through chemical signaling, forming a foundational
paradigm for emerging applications in the Internet of Nano Things (IoNT) and
intrabody healthcare systems. The realization of this vision critically depends
on developing advanced receiver architectures that merge nanoscale
communication and networking techniques with bio-cyber interfaces, ensuring
energy-efficient, reliable, and low-complexity modulation and detection while
maintaining biocompatibility. To address these challenges, the Flexure-FET MC
receiver was introduced as a mechanically transducing design capable of
detecting both charged and neutral molecular species. In this study, we present
a cylindrical nanowire array-based Flexure-FET MC receiver that enhances design
versatility and scalability through distributed electromechanical coupling in a
suspended-gate configuration. The proposed array architecture offers additional
geometric degrees of freedom, including nanowire radius, length, spacing, and
array size, providing a flexible framework that can be tailored to advanced MC
scenarios. An analytical end-to-end model is developed to characterize the
system's electromechanical response, noise behavior, and information-theoretic
performance, including signal-to-noise ratio (SNR) and channel capacity. The
results reveal the strong interdependence between geometry, electromechanical
dynamics, and molecular binding processes, enabling tunable control over
sensitivity, noise characteristics, and communication capacity. The enhanced
structural tunability and array configuration of the proposed design provide a
flexible foundation for future mixture-based and spatially modulated MC
systems, paving the way toward scalable and multifunctional receiver
architectures within the IoNT framework.

</details>


### [7] [Fair Rate Maximization for Multi-user Multi-cell MISO Communication Systems via Novel Transmissive RIS Transceiver](https://arxiv.org/abs/2510.25290)
*Yuan Guo,Wen Chen,Qingqing Wu,Zhendong Li,Kunlun Wang,Hongying Tang,Jun Li*

Main category: eess.SP

TL;DR: 本文提出了一种基于透射式可重构智能表面收发器(TRTC)的多小区MISO下行通信系统，开发了高效低复杂度的优化算法来最大化用户最小速率，显著降低了计算复杂度且无性能损失。


<details>
  <summary>Details</summary>
Motivation: 在多小区MISO下行通信系统中，通过TRTC配置来提升系统性能，但最大最小速率优化问题由于目标函数不可微而难以求解。

Method: 采用分数规划方法将速率函数转化为可处理形式，利用平滑逼近理论近似最大最小目标函数，结合主化最小化技术和最优性条件分析，提出无需数值求解器的解析更新算法。

Result: 数值结果表明所提算法具有收敛性和有效性，能显著降低计算复杂度且无性能损失，TRTC部署方案明显优于基准方案。

Conclusion: 提出的低复杂度优化算法成功解决了TRTC多小区系统中的最大最小速率优化问题，为实际系统部署提供了高效解决方案。

Abstract: This paper explores a multi-cell multiple-input single-output (MISO) downlink
communication system enabled by a unique transmissive reconfigurable
intelligent surface (RIS) transceiver (TRTC) configuration. Within this system
framework, we formulate an optimization problem for the purpose of maximizing
the minimum rate of users for each cell via designing the transmit beamforming
of the TRTC, subject to the power constraints of each TRTC unit. Since the
objective function is non-differentiable, the max-min rate problem is difficult
to solve. In order to tackle this challenging optimization problem, an
efficient low-complexity optimization algorithm is developed. Specifically, the
log-form rate function is transformed into a tractable form by employing the
fractional programming (FP) methodology. Next, the max-min objective function
can be approximated using a differentiable function derived from smooth
approximation theory. Moreover, by applying the majorization-minimization (MM)
technique and examining the optimality conditions, a solution is proposed that
updates all variables analytically without relying on any numerical solvers.
Numerical results are presented to demonstrate the convergence and
effectiveness of the proposed low-complexity algorithm. Additionally, the
algorithm can significantly reduce the computational complexity without
performance loss. Furthermore, the simulation results illustrate the clear
superiority of the deployment of the TRTC over the benchmark schemes.

</details>


### [8] [Millimeter-Wave Radar Sensing of Wombat Respiration](https://arxiv.org/abs/2510.25293)
*Marina Murakami,Ryoko Iwase,Chiemi Iba,Daisuke Ogura,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 本研究使用79-GHz毫米波雷达系统对袋熊进行非接触式呼吸监测，通过自相关函数中的谐波分量求和法估计呼吸间隔，测量误差分别为47.4毫秒(2.44%)和0.81 bpm(2.21%)，验证了该方法在袋熊健康监测中的可行性。


<details>
  <summary>Details</summary>
Motivation: 开发一种非接触式的呼吸监测方法，用于袋熊的健康监测，避免传统接触式监测可能带来的干扰和压力。

Method: 使用79-GHz毫米波雷达系统，通过自相关函数中的谐波分量求和法来估计呼吸间隔，捕捉由呼吸引起的体表准周期性位移。采用两台雷达从不同角度同时测量以评估准确性。

Result: 呼吸间隔和呼吸率的测量误差分别为47.4毫秒(2.44%)和0.81 bpm(2.21%)。研究还发现了两只袋熊之间以及6月与12月之间的呼吸率差异。

Conclusion: 雷达基非接触呼吸监测方法在袋熊健康监测中具有应用潜力，能够准确测量呼吸参数并检测季节性变化。

Abstract: This study demonstrates the feasibility of radar-based non-contact
respiratory monitoring for wombats. Two measurement experiments were conducted
in June and December 2024 using 79-GHz millimeter-wave radar systems to monitor
the respiration of two wombats. To estimate the respiratory interval, we used a
method based on summing harmonic components in the autocorrelation function,
capturing the quasi-periodic displacement of the body surface caused by
respiration. Estimation accuracy was evaluated through simultaneous
measurements from different angles using two radar units. The respiratory
interval and respiratory rate were measured with errors of 47.4 ms (2.44%) and
0.81 bpm (2.21%), respectively. We also discuss the differences in respiratory
rates between the two wombats, as well as seasonal variations between June and
December. The results support the potential application of this method to
non-contact health monitoring of wombats.

</details>


### [9] [Low Probability of Detection Communication Using Noncoherent Grassmannian Signaling](https://arxiv.org/abs/2510.25751)
*Diego Cuevas,Mikel Gutiérrez,Jesús Ibáñez,Ignacio Santamaria*

Main category: eess.SP

TL;DR: 提出基于直接序列扩频和Grassmannian信号的非相干低检测概率通信系统，通过噪声样分布增强隐蔽性，在低信噪比下具有竞争性误码率和低检测概率。


<details>
  <summary>Details</summary>
Motivation: 开发具有更好隐蔽性和性能的非相干通信系统，避免传统相干方案需要导频进行信道估计的问题，提高低检测概率通信的实用性。

Method: 结合直接序列扩频技术和Grassmannian信号设计，利用Grassmannian星座的噪声样分布特性来增强通信的隐蔽性。

Result: 仿真显示Grassmannian信号在低信噪比下提供竞争性误码率，与需要导频的QPSK或QAM相干方案相比，在非预期接收器处具有更低的检测概率。

Conclusion: 非相干Grassmannian信号在低检测概率通信中具有实用性和安全性优势，因其改进的隐蔽性和性能表现。

Abstract: This paper proposes a noncoherent low probability of detection (LPD)
communication system based on direct sequence spread spectrum (DSSS) and
Grassmannian signaling. Grassmannian constellations enhance covertness because
they tend to follow a noise-like distribution. Simulations showed that
Grassmannian signaling provides competitive bit error rates (BER) at low
signal-to-noise ratio (SNR) regimes with low probability of detection at the
unintended receiver compared to coherent schemes that use QPSK or QAM
modulation formats and need pilots to perform channel estimation. The results
suggest the practicality and security benefits of noncoherent Grassmannian
signaling for LPD communications due to their improved covertness and
performance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [10] [Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains](https://arxiv.org/abs/2510.25514)
*Maik Overmars,Jasper Goseling,Richard Boucherie*

Main category: stat.ML

TL;DR: 本文研究了在可逆马尔可夫链中，使用线性函数近似的离策略TD(0)算法的收敛性，证明了在折扣因子满足特定上界条件下，算法能够以概率1收敛到零投影Bellman误差。


<details>
  <summary>Details</summary>
Motivation: 离策略学习与函数近似的结合通常会导致算法发散，现有方法通过重要性采样等修改算法来保证收敛，但增加了复杂性。本文旨在分析标准算法，但限制在可逆马尔可夫链类中，利用领域知识假设可逆性条件。

Method: 将Tsitsiklis和Van Roy[1997]用于同策略情况的随机逼近框架，适配到离策略情况，分析标准离策略TD(0)算法在可逆马尔可夫链中的行为。

Result: 在可逆马尔可夫链条件下，建立了折扣因子的显式上界（基于同策略与离策略过程之间的差异），证明了算法以概率1收敛，并达到零投影Bellman误差。

Conclusion: 对于可逆马尔可夫链，标准离策略TD(0)算法在适当折扣因子条件下能够收敛，无需修改算法，这为实际应用提供了理论保证。

Abstract: We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.

</details>


### [11] [Using latent representations to link disjoint longitudinal data for mixed-effects regression](https://arxiv.org/abs/2510.25531)
*Clemens Schächter,Maren Hackenberg,Michelle Pfaffenlehner,Félix B. Tambe-Ndonfack,Thorsten Schmidt,Astrid Pechmann,Janbernd Kirschner,Jan Hasenauser,Harald Binder*

Main category: stat.ML

TL;DR: 本文提出了一种新方法，通过变分自编码器将不同测量仪器的观测数据映射到共享的潜在空间，然后使用混合效应回归模型在潜在表示中捕捉时间动态和治疗切换效应，解决了罕见疾病研究中因测量仪器变化导致的数据不连续问题。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病治疗选择有限，患者会在新药出现时切换治疗。但由于样本量小且测量仪器可能随时间变化（如适应不同年龄段），导致纵向数据不连续，传统混合效应回归难以应用。

Method: 使用变分自编码器架构将不同仪器的项目值嵌入到共享潜在空间中，然后在潜在表示上应用混合效应回归模型来捕捉时间动态和治疗切换效应，并提出新的统计检验方法。

Result: 该方法成功应用于脊髓性肌萎缩症患者，对齐了不同测量仪器的运动性能项目，量化了治疗切换的效果，证明了在联合潜在表示中建模解决小数据挑战的潜力。

Conclusion: 提出的方法能够处理罕见疾病研究中因测量仪器变化导致的数据不连续问题，为小样本情况下的治疗效果评估提供了有效解决方案。

Abstract: Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.

</details>


### [12] [Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification](https://arxiv.org/abs/2510.25573)
*Christopher T. Franck,Anne R. Driscoll,Zoe Szajnfarber,William H. Woodall*

Main category: stat.ML

TL;DR: 提出一种基于累积和的方法来持续监测机器学习模型的校准状态，能够检测传统过程监控和概念漂移应用中的校准失效问题。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习模型在图像分类方面取得了显著进展，但如何评估和维护这些模型预测的校准性是一个重要问题。目前缺乏持续监测预测模型随时间推移可能出现的校准失效的方法。

Method: 采用累积和（CUSUM）方法结合动态限制，能够检测传统过程监控和概念漂移应用中的校准失效。该方法仅基于概率预测和事件结果，不需要访问机器学习模型的内部结构。

Result: 该方法能够早期检测到影响图像分类性能的操作环境变化，适用于任何需要随时间监测概率预测校准状态的场景。

Conclusion: 提出的累积和图表方法为持续监测机器学习模型校准状态提供了一种有效工具，特别适用于需要长期稳定性能的实际应用场景。

Abstract: Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 该论文研究了视觉语言模型中存在的文化敏感神经元，这些神经元对特定文化背景的输入表现出偏好敏感性。通过CVQA基准测试，作者识别出文化选择性神经元，并通过消融实验验证了它们对文化多样性视觉问答的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现出色，但在处理文化相关输入时仍存在困难。为了理解模型如何处理文化背景信息，研究者希望探究是否存在对特定文化敏感的神经元。

Method: 使用CVQA基准测试识别文化选择性神经元，通过消融实验测试不同识别方法标记的神经元。提出新的基于边际的选择器——对比激活选择(CAS)，并在三个VLMs上对25个文化群体进行实验。

Result: 实验证明存在文化敏感神经元，消融这些神经元会显著降低对应文化问题的性能，而对其他文化影响较小。CAS方法在识别文化敏感神经元方面优于现有的概率和熵基方法。层间分析显示这些神经元倾向于聚集在某些解码器层中。

Conclusion: 研究揭示了多模态表征的内部组织结构，发现了文化敏感神经元的存在及其在模型处理文化信息中的重要作用，为理解VLMs的文化处理机制提供了新视角。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [14] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出了一种基于博弈论的端到端特征选择框架，通过协同博弈评估特征重要性，在保持预测性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 数据量指数增长导致机器学习模型训练计算成本急剧上升，许多特征对模型性能没有正面贡献却消耗大量计算资源。

Method: 基于合作博弈的特征选择框架，将特征建模为玩家，通过评估协同交互和边际贡献确定特征重要性，包含样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 实验结果表明，该方法在保持预测性能的同时实现了显著的计算减少，为大规模机器学习计算挑战提供了高效解决方案。

Conclusion: 该博弈论特征选择框架能够有效解决大规模机器学习中的计算效率问题，源代码已在GitHub上公开。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [15] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将每个去噪步骤视为无条件先验和状态条件策略头之间的顺序假设检验，为扩散策略添加了原则性的风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中具有竞争力，但在采样时通常缺乏统计风险概念的启发式指导。作者希望引入一种具有用户可解释风险预算的风险感知采样规则。

Method: 提出LRT-Diffusion方法，累积对数似然比，并使用逻辑控制器对条件均值进行门控，其阈值τ在H0下校准一次以满足用户指定的Type-I水平α。该方法保持训练过程不变（标准DDPM结构），在推理时添加风险控制。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion在回报-离群分布权衡方面优于强Q引导基线，同时满足期望的α水平。理论分析建立了水平α校准、简洁的稳定性界限和回报比较。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL中的扩散策略添加了原则性的、校准的风险控制，特别在离群误差占主导时优于Q引导方法。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [16] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，用于自动发现语义上有意义的子目标并学习最优选项终止边界，从而提升分层强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在长时程任务中通过时间抽象提高决策可扩展性，但实践中面临自主发现语义子目标和学习最优选项终止边界的挑战。

Method: 集成自监督Transformer变化点检测模块到Option-Critic框架，利用内在信号生成的启发式伪标签训练CPD模块，推断环境动态的潜在变化，并将推断的变化点用于稳定终止函数梯度、预训练选项内策略以及通过选项间发散惩罚增强功能专业化。

Result: 在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速收敛、更高累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验能够在复杂环境中产生更可解释、样本效率更高且更稳健的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [17] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统分析了矩阵白化优化器的性能优势，发现方差自适应是解释性能差距的关键因素，而非仅靠准确的谱归一化。


<details>
  <summary>Details</summary>
Motivation: 研究近期出现的各种近似矩阵白化变换的优化器，旨在解构其关键组件以解释性能差异。

Method: 通过系统解构矩阵白化优化器，进行超参数调优实验，比较不同变体的性能，并分析方差自适应策略的效果。

Result: 所有矩阵白化方法都可靠地优于逐元素优化器如Adam；SOAP显示最大的单步增益，尽管Muon在谱下降方向上更准确；方差自适应版本始终优于符号下降对应版本。

Conclusion: 矩阵白化优化器的性能优势主要来自方差自适应组件，而非仅靠谱归一化；低秩方差估计器可有效降低内存成本且不损失性能。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [18] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出神经随机流（NSFs）及其潜在变体，通过条件归一化流直接学习SDE转移规律，实现任意时间点的一次性采样，相比传统数值方法获得高达两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统随机微分方程建模需要昂贵的数值求解器在任意时间点之间采样，计算成本高。

Method: 使用具有架构约束的条件归一化流直接学习（潜在）SDE转移规律，保持随机流继承的特性。

Result: 在大时间间隔下获得高达两个数量级的加速，同时在合成SDE模拟和真实世界跟踪、视频数据上保持与数值方法相当的分布精度。

Conclusion: NSFs在保持分布精度的同时，显著减少了任意时间点采样的计算成本。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


### [19] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 本研究系统评估了基于放射组学的机器学习模型在五种MRI序列分布偏移下的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下保持F1分数>0.85，而使用所有特征的模型性能下降40%。数据集增强显著改善了不确定性估计质量。


<details>
  <summary>Details</summary>
Motivation: 放射组学机器学习模型在临床决策支持中具有潜力，但容易受到成像协议、定位和分割变化引起的分布偏移影响，需要系统评估其鲁棒性。

Method: 使用16个水果模型，评估了：(1)五种MRI序列的协议变化；(2)分割变化（完整、部分、旋转）；(3)观察者间变异性。训练XGBoost分类器，比较协议不变特征与序列特定特征在域内和域外条件下的表现。

Result: 基于协议不变特征的模型在分布偏移下F1分数保持>0.85，而使用所有特征的模型在协议变化下性能下降40%。数据集增强使期望校准误差降低35%，温度缩放校准效果有限。

Conclusion: 协议感知特征选择和受控模型研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [20] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出了一种基于因果效应估计任务的有向无环混合图距离度量方法，用于在存在未观测混杂的情况下评估因果发现方法的性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现领域缺乏在存在潜在混杂情况下有效评估发现图质量的方法，难以衡量方法进展。

Method: 使用基于固定识别的识别方法和符号验证器，量化图差异对不同处理-结果对因果效应估计量的扭曲程度。

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较。

Conclusion: 提出的图距离度量为在未观测混杂下评估因果发现方法提供了新的有效工具。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [21] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为DWMGrad的新型优化算法，通过动态指导机制动态更新动量和学习率，解决了传统优化算法在处理复杂模型和非凸优化问题时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习研究中，SGD和Adam等优化算法在处理学习效率波动、复杂模型需求和非凸优化问题时存在明显不足，主要源于算法在处理复杂数据结构和模型时的局限性。

Method: 基于传统方法，引入依赖历史数据的动态指导机制来动态更新动量和学习率，使优化器能够灵活调整对历史信息的依赖，适应不同的训练场景。

Result: 通过大量实验验证，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法能够更好地适应变化的环境和任务复杂性，在深度学习优化方面表现出优越性能。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [22] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 本文提出了一个统一的对抗学习双层模型，重点研究了聚类模型中的对抗攻击机制，从数据扰动角度解释攻击效果，并分析了δ-度量在评估攻击效果中的适用性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型结构复杂，对抗攻击机制尚未得到很好解释，攻击效果的衡量方法也不够明确，因此需要深入研究对抗学习机制。

Method: 提出了统一的对抗学习双层模型，从数据扰动角度分析聚类模型中的对抗攻击，并研究了δ-度量在评估攻击效果中的适用性。

Result: 研究发现当数据扰动较小时聚类模型具有鲁棒性，当扰动较大时会导致聚类结果改变从而形成攻击。δ-度量在对抗学习双层模型中可用于评估聚类模型的攻击效果。

Conclusion: 通过提出的双层模型和δ-度量分析，为理解对抗攻击机制和评估攻击效果提供了理论框架，特别适用于聚类模型的对抗学习研究。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [23] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 本文提出了Bulk-Space-Filtration-Accelerator (BSFA)框架，通过差异缩放损失Hessian不同特征子空间中的参数更新分量来加速深度学习训练。BSFA在主导子空间抑制更新以增强稳定性，在bulk子空间放大更新以提升收敛速度，并在LLaMA预训练中实现了约2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明深度学习优化中存在基本二分法：损失Hessian的顶部特征方向（Dom-space）的更新幅度大但对损失减少贡献小，而正交分量（Bulk-space）的更新幅度小但驱动大部分学习进展。本文旨在深入理解这一现象并开发实用加速框架。

Method: 提出BSFA框架，通过差异缩放不同子空间的更新分量来加速训练。引入两个关键创新：1）使用历史更新的PCA进行高效子空间估计；2）按参数块应用估计的块级策略。这些设计使BSFA计算可行且高效。

Result: 在各种任务中展示了BSFA的加速效果，特别是在WikiText-103上预训练LLaMA-72M和在OpenWebText上预训练LLaMA-134M时，相比标准AdamW实现了约2倍的加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，通过差异缩放不同特征子空间的参数更新，在保持训练稳定性的同时显著加速收敛，为大规模模型训练提供了有效解决方案。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [24] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律发现，通过两级架构分别学习PDE的基本符号组件及其组合，并嵌入已知物理定律保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 复杂时空动力学建模是科学中的重大挑战，传统PDE难以从第一性原理推导，纯数据驱动方法物理不一致且数据密集，现有物理信息方法缺乏表示复杂算子的结构能力。

Method: 采用分层架构：第一级学习PDE的基本符号组件，第二级学习其组合；基于自适应傅里叶神经算子构建框架，能够捕捉非局部依赖和高阶算子；通过结构解耦已知和未知项实现可解释发现。

Result: 该框架在正向时空预测和逆向物理定律发现方面都有根本性进展，提高了数据效率并保证物理一致性。

Conclusion: 分层物理嵌入学习框架通过镜像科学发现过程，有效解决了复杂时空动力学建模的挑战，为物理定律发现提供了新途径。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [25] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 本文提出了一个无信息泄漏的信息级联流行度预测框架，通过时间有序数据分割、大规模电商数据集和轻量级模型设计，解决了现有方法中的时间泄漏、特征匮乏和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测方法存在三个关键问题：时间泄漏导致评估不真实、数据集缺乏下游转化信号、图方法计算效率低下。这些问题限制了实际应用价值。

Method: 提出三方面解决方案：1) 时间有序分割策略防止未来信息泄漏；2) 构建Taoke电商数据集，包含丰富的推广者/产品属性和真实购买转化；3) 开发CasTemp轻量框架，使用时间游走、Jaccard邻居选择和GRU编码。

Result: 在无泄漏评估下，CasTemp在四个数据集上达到最先进性能，计算速度提升数个数量级，特别擅长预测第二阶段流行度转化这一关键实际任务。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个层面的挑战，提出的方法实现了真实可靠的级联流行度预测，为实际应用提供了有效解决方案。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [26] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文对随机几何超图上的变分学习进行了渐近一致性分析，提出了高阶超图学习（HOHL）方法，通过骨架图的拉普拉斯幂进行多尺度平滑正则化。


<details>
  <summary>Details</summary>
Motivation: 超图为建模高阶交互提供了自然框架，但在半监督学习中的理论基础仍然有限。本文旨在填补这一空白，提供超图学习的理论保证。

Method: 提出高阶超图学习（HOHL）方法，通过骨架图的拉普拉斯幂进行正则化以实现多尺度平滑。该方法收敛到高阶Sobolev半范数。

Result: 理论分析表明超图学习在适当条件下是适定的，并收敛到加权p-拉普拉斯方程。实证结果显示HOHL在标准基准测试中表现优异。

Conclusion: 本文为超图学习提供了坚实的理论基础，提出的HOHL方法在理论和实证上都表现出色，为高阶交互建模提供了有效工具。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [27] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出一种基于损失函数从初始非凸性向凸性转换假设的两阶段优化算法，通过检测转换点分别使用非凸算法（Adam）和凸算法（共轭梯度法）来提升收敛性和精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习中损失函数经常存在非凸区域，导致广泛使用非凸优化方法如Adam。但局部最小值周围环境通常是凸的，此时二阶方法能保证超线性收敛。

Method: 设计两阶段优化算法：通过观察梯度范数与损失的关系检测凸性转换点，在非凸区域使用Adam，在凸区域使用共轭梯度法。

Result: 计算实验证实这种简单的凸性结构在真实任务中足够频繁，可以被实际利用来显著改善收敛性和准确性。

Conclusion: 利用损失函数从非凸到凸的转换特性设计的混合优化算法能有效提升机器学习任务的优化性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [28] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 本文提出原型神经符号架构，通过在极低数据条件下训练模型学习正确的基本概念而非利用伪相关，从根本上解决神经符号AI中的推理捷径问题。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI容易学习到推理捷径，即利用伪相关而非正确概念来满足符号约束，这影响了模型的安全性和可靠性。

Method: 基于原型学习理论，训练模型在考虑输入与少量标注数据相似度的同时满足背景知识，从而避免推理捷径。

Result: 在rsbench基准测试中，无论是合成任务(MNIST-EvenOdd和Kand-Logic)还是真实高风险任务(BDD-OIA)都显著改善了概念学习效果。

Conclusion: 原型基础化是一种有效且标注高效的策略，为安全可靠的神经符号学习开辟了新途径。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [29] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法预测员工倦怠风险，在三种监督学习算法中，支持向量机(SVM)表现最佳(R²=0.84)，并开发了交互式界面供非技术用户使用。


<details>
  <summary>Details</summary>
Motivation: 倦怠是一种严重影响个人福祉和组织绩效的心理综合征，需要早期检测和预防。

Method: 使用HackerEarth员工倦怠挑战数据集，评估了三种监督学习算法：最近邻(KNN)、随机森林和支持向量机(SVM)，通过30折交叉验证评估模型性能。

Result: SVM模型取得了最高的预测性能(R²=0.84)，在配对t检验中统计显著优于KNN和随机森林。

Conclusion: 研究结果突显了机器学习在支持早期检测倦怠和促进组织环境中数据驱动的心理健康策略方面的潜力。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [30] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了FP和INT量化格式在不同粒度下的性能，发现在8位细粒度量化中MXINT8优于FP格式，但在4位量化中FP格式通常有精度优势。研究挑战了当前硬件趋势，表明细粒度INT格式（特别是MXINT8）为未来AI加速器提供了更好的精度、功耗和效率平衡。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件越来越倾向于使用低精度FP格式处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式之间的权衡，包括在不同粒度（粗粒度和细粒度）下的性能比较，引入对称裁剪方法解决细粒度低比特INT训练中的梯度偏差问题。

Result: 发现关键性能交叉点：FP在粗粒度量化中表现优异，但在细粒度（块级）量化中，8位格式MXINT8在算法精度和硬件效率上都优于FP对应格式；4位格式中FP通常有精度优势，但应用Hadamard旋转等异常值缓解技术后NVINT4可以超越NVFP4。

Conclusion: 挑战当前硬件发展轨迹，证明一刀切的FP方法不是最优的，倡导细粒度INT格式（特别是MXINT8）为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [31] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedLap是一个用于联邦学习的新型框架，专门处理分布在多个客户端上的图结构数据，通过拉普拉斯平滑在谱域中捕获节点间依赖关系，同时确保隐私和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在处理互联子图时存在隐私风险或计算效率低的问题，需要一种既能保护隐私又具有良好可扩展性的解决方案。

Method: 利用拉普拉斯平滑在谱域中获取全局结构信息，有效捕获节点间依赖关系，同时避免交换敏感的节点嵌入。

Result: FedLap在基准数据集上的实验表明，与现有技术相比，它实现了竞争性或更优的性能，并且是第一个具有强隐私保证的子图联邦学习方案。

Conclusion: FedLap通过创新的谱域处理方法，成功解决了联邦学习中图结构数据的隐私保护和可扩展性问题，为互联子图的联邦学习提供了有效的解决方案。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [32] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 本文系统研究了对称矩阵低秩伪逆在噪声扰动下的谱范数误差，提出了基于轮廓积分技术的新颖分析方法，得到了比经典方法更紧致的误差界。


<details>
  <summary>Details</summary>
Motivation: 低秩伪逆在可扩展机器学习、优化和科学计算中被广泛用于近似矩阵逆，但现实世界中的矩阵往往存在噪声，而低秩逆近似的谱范数鲁棒性研究尚不充分。

Method: 使用轮廓积分技术分析非整函数f(z)=1/z，在噪声满足温和假设条件下，推导出考虑特征间隙、谱衰减和噪声与低曲率方向对齐的尖锐非渐近扰动界。

Result: 提出的误差界比经典全逆界的朴素适应方法改进高达√n倍，经验验证表明新界能紧密跟踪真实扰动误差，而基于经典结果的估计往往显著高估。

Conclusion: 研究结果为噪声计算环境中低秩逆近似提供了实用且频谱感知的保证，改进了对低秩伪逆鲁棒性的理论理解。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [33] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一个基于适当评分规则的回归任务不确定性量化框架，特别关注核评分，统一了多种已知度量方法，并提供了设计新度量的原则性方法。


<details>
  <summary>Details</summary>
Motivation: 回归任务在安全关键领域需要适当的不确定性量化，但现有文献主要集中在分类任务上，因此需要专门针对回归任务的不确定性度量方法。

Method: 基于适当评分规则构建了总不确定性、偶然不确定性和认知不确定性的度量家族，特别强调核评分方法，通过核选择来控制度量的行为特性。

Result: 实验证明这些度量在下游任务中有效，并揭示了不同实例化之间的权衡，包括鲁棒性和分布外检测性能。

Conclusion: 该框架为回归任务的不确定性量化提供了统一的理论基础和实践指导，核选择决定了度量的关键行为特性。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [34] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 本文建立了对称矩阵谱范数扰动的新边界，改进了经典的Eckart-Young-Mirsky定理，并应用于差分隐私PCA，解决了文献中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习中需要理解噪声或测量误差如何影响低秩逼近，特别是在谱范数下。差分隐私低秩逼近中需要保持数据矩阵的顶部结构同时确保隐私，但现有工作多分析Frobenius范数误差，可能高估或低估真实子空间失真。

Method: 使用复分析中的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱函数类。在温和的特征值间隙和范数条件下，建立了对称矩阵的高概率谱范数扰动边界。

Result: 我们的边界对‖(A+E)_p - A_p‖给出了尖锐估计，改进因子高达√n。经验结果证实边界在不同扰动机制下紧密跟踪实际谱误差。

Conclusion: 本文的谱范数扰动边界为差分隐私PCA提供了改进的效用保证，解决了文献中的开放问题，分析框架可扩展到更广泛的谱函数类。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [35] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出卷积脉冲GRU（CS-GRU）单元，结合卷积操作保留局部结构，集成脉冲神经元的时间精度和GRU的高效门控机制，在时序和时空数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时容易丢失局部细节，现有方法如SpikGRU无法捕捉事件型时空数据中的细粒度局部依赖关系。

Method: 开发卷积脉冲GRU（CS-GRU）单元，利用卷积操作保持局部结构和依赖关系，同时结合脉冲神经元的时间精度和GRU的门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，比SpikGRU效率高69%。

Conclusion: CS-GRU是一个多功能架构，在保持局部依赖关系的同时实现了高性能和高效率，为时序和时空数据处理提供了有效解决方案。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [36] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于推理树结构的RLVR数据调度方法Re-Schedule，通过r-score度量查询的学习难度，构建从简单到复杂的课程学习计划，在数学推理基准上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据调度方法仅依赖路径指标对查询排序，忽略了查询的推理树结构信息，无法充分利用结构复杂度来优化学习过程。

Method: 引入推理分数(r-score)度量查询的学习难度，基于推理树结构设计Re-Schedule调度算法，构建从高r-score(结构简单)到低r-score(结构复杂)的课程学习计划。

Result: 在6个数学推理基准测试中，Re-Schedule显著提升了平均准确率，最高增益达到3.2%。

Conclusion: 对推理树的结构理解为RLVR数据调度提供了更强大和原则性的基础，验证了基于结构复杂度进行课程学习的有效性。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [37] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究循环结构因果模型中的反事实推理，关注在包含反馈环的真实系统中进行移位-尺度干预下的推理问题


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理框架假设无环结构因果模型，但许多真实系统（如生物系统）包含反馈循环或循环依赖，这违反了无环性假设

Method: 研究循环结构因果模型中的反事实推理，特别关注移位-尺度干预（即软性、策略式变化，重新缩放和/或移动变量的机制）

Result: 论文探讨了在循环系统中进行反事实推理的理论框架和方法

Conclusion: 需要扩展反事实推理框架以处理包含反馈循环的循环结构因果模型

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [38] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文提出了ProFees框架，使用LLM自动化医疗评估与管理(E/M)编码，解决了现实世界中的复杂性问题，在真实数据集上比商业系统提高了36%以上的编码准确率。


<details>
  <summary>Details</summary>
Motivation: 自动化E/M编码可以减轻医生的文档负担，提高计费效率，最终实现更好的患者护理。但现实世界的复杂性使得E/M编码自动化具有挑战性。

Method: 提出了基于LLM的ProFees框架，专门设计用于解决E/M编码中的现实复杂性。

Result: 在专家策划的真实世界数据集上，ProFees比商业CPT E/M编码系统提高了36%以上的编码准确率，比最强的单提示基线提高了近5%。

Conclusion: ProFees框架在解决现实世界E/M编码复杂性方面表现出有效性，能够显著提高编码准确性。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [39] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 提出了Autoregressive State-Tracking Prompting (ASTP)方法，解决LLM在游戏交易系统中遵循程序流程的问题，通过显式状态跟踪和占位符后处理实现高精度的状态合规性和价格计算。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在游戏交易系统中虽然支持动态交互，但无法遵循必要的程序流程（浏览-报价-审核-确认），这损害了玩家信任。需要解决LLM的创意灵活性与游戏交易程序需求之间的核心矛盾。

Method: 引入ASTP方法，通过精心设计的提示强制LLM显式报告预定义的状态标签，结合状态特定的占位符后处理方法确保交易完整性。

Result: 在300个交易对话评估中，状态合规性超过99%，计算精度达到99.3%。在较小模型上使用ASTP与占位符后处理，性能可匹敌更大模型，同时将响应时间从21.2秒减少到2.4秒。

Conclusion: ASTP为商业游戏建立了实用基础，既满足实时性要求，又符合资源限制，实现了LLM在规则治理交易系统中的可靠应用。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [40] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 提出PM4GRPO方法，通过过程挖掘技术增强推理过程的奖励信号，在GRPO后训练中显著提升大推理模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的后训练方法主要关注结果奖励，缺乏对推理过程的关注，限制了多步推理能力的提升

Method: 使用过程挖掘技术计算一致性奖励，衡量策略模型推理过程与预训练教师模型的对齐程度，结合标准答案/格式奖励进行GRPO优化

Result: 在五个基准测试中，PM4GRPO显著优于现有的GRPO后训练方法

Conclusion: 利用过程挖掘实现推理感知的GRPO能有效增强策略模型的推理能力

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [41] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: Agentic Moderation是一个模型无关的多模态系统安全对齐框架，通过专门的智能体防御越狱攻击，相比静态方法提供动态、可解释的上下文感知审核。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法多为静态层，仅提供二元分类（安全/不安全），缺乏上下文感知和可解释性。智能体方法在推理、协作和自适应控制方面表现出强大能力，作者希望将其扩展到安全对齐领域。

Method: 提出Agentic Moderation框架，包含四个协作智能体：Shield（防护）、Responder（响应）、Evaluator（评估）和Reflector（反思），实现动态、可解释的审核。

Result: 在5个数据集和4个大型视觉语言模型上的实验表明，该方法将攻击成功率降低7-19%，保持稳定的不遵循率，并将拒绝率提高4-20%，实现鲁棒、可解释且平衡的安全性能。

Conclusion: Agentic Moderation通过利用智能体架构的灵活性和推理能力，提供了模块化、可扩展和细粒度的安全执行，展示了智能体系统作为自动化安全治理基础的潜力。

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [42] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: EneAD是一个节能的自动驾驶框架，通过自适应感知模块和鲁棒决策模块，在保持感知精度的同时显著降低计算能耗，提高电动汽车的续航里程。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术带来社会经济效益的同时，计算引擎的能耗问题限制了电动汽车的续航里程。感知计算作为最耗能的组件，现有压缩技术往往导致模型尺寸过大或精度显著下降。

Method: 在自适应感知模块中，通过数据管理和调优策略管理多个不同计算消耗的感知模型，动态调整执行帧率；使用贝叶斯优化进行可转移调优；提出轻量级分类模型区分不同场景的感知难度。在鲁棒决策模块中，基于强化学习设计决策模型，并添加正则化项增强对扰动感知结果的稳定性。

Result: 实验证明该框架在能耗和驾驶性能方面具有优越性，可将感知消耗降低1.9倍至3.5倍，从而提高续航里程3.9%至8.5%。

Conclusion: EneAD框架有效解决了自动驾驶中能耗与精度平衡的问题，为节能自动驾驶提供了可行的解决方案。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [43] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出了KVDA-UCT算法，通过放宽状态-动作对价值必须相等的限制，允许价值差异可推断的状态-动作对进行分组，从而显著提高了MCTS的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS抽象算法OGA-UCT要求状态-动作对具有相同的即时奖励，这一严格条件限制了可发现的抽象数量。本文旨在打破价值等价分组的范式，允许价值差异可推断的状态-动作对进行分组。

Method: 提出了已知价值差异抽象框架(KVDA)，通过分析即时奖励来推断价值差异，并基于此框架修改OGA-UCT得到KVDA-UCT算法。

Result: KVDA-UCT在多种确定性环境和参数设置下显著优于OGA-UCT，能够检测到更多的抽象，且不引入额外参数。

Conclusion: KVDA框架通过允许价值差异可推断的状态-动作对分组，有效提高了MCTS的样本效率，KVDA-UCT算法在性能上超越了现有的OGA-UCT方法。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [44] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 提出了CAIR方法，首个用于评估多智能体系统中各智能体对最终输出影响力的方法，通过反事实分析实现任务无关的分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能进行静态结构分析，不适合推理时执行，缺乏评估多智能体系统中各智能体影响力的方法。

Method: 使用反事实分析技术，通过改变智能体的输出来评估其对系统最终输出的影响程度。

Result: 在包含30个用例和230个功能的数据集上评估，CAIR产生一致的排名，优于基线方法，并能有效提升下游任务的效率和相关性。

Conclusion: CAIR是首个能够评估多智能体系统中智能体影响力的方法，具有任务无关性，可离线或在推理时使用。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>

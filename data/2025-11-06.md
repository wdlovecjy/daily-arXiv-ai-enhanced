<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 18]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [eess.SP](#eess.SP) [Total: 7]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Post-2024 U.S. Presidential Election Analysis of Election and Poll Data: Real-life Validation of Prediction via Small Area Estimation and Uncertainty Quantification](https://arxiv.org/abs/2511.03555)
*Zheshi Zheng,Yuanyuan Li,Peter X. K. Song,Jiming Jiang*

Main category: stat.AP

TL;DR: 使用小区域估计方法构建预测模型，对2024年美国总统大选进行后验分析，该模型在44个有民调数据的州中完美预测选举人团结果，并引入错误预测概率来量化预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够准确预测美国总统大选结果并可靠量化预测不确定性的模型，同时分析民调偏差对关键摇摆州的影响。

Method: 采用小区域估计方法构建预测模型，使用选举前一周的民调数据，提出保形推理方法来估计错误预测概率，并进行敏感性分析来评估民调偏差。

Result: 模型在44个有民调数据的州中完美预测了选举人团结果，保形推理方法提供了可靠的预测不确定性量化，敏感性分析显示摇摆州特别容易受到民调偏差的影响。

Conclusion: 基于小区域估计的预测模型在2024年美国总统大选中表现出色，保形推理方法能有效量化预测不确定性，摇摆州的预测结果对民调偏差特别敏感。

Abstract: We carry out a post-election analysis of the 2024 U.S. Presidential Election
(USPE) using a prediction model derived from the Small Area Estimation (SAE)
methodology. With pollster data obtained one week prior to the election day,
retrospectively, our SAE-based prediction model can perfectly predict the
Electoral College election results in all 44 states where polling data were
available. In addition to such desirable prediction accuracy, we introduce the
probability of incorrect prediction (PoIP) to rigorously analyze prediction
uncertainty. Since the standard bootstrap method appears inadequate for
estimating PoIP, we propose a conformal inference method that yields reliable
uncertainty quantification. We further investigate potential pollster biases by
the means of sensitivity analyses and conclude that swing states are
particularly vulnerable to polling bias in the prediction of the 2024 USPE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 提出了一种利用大型语言模型进行人类活动识别系统中数据投毒检测和净化的新框架，采用零样本、单样本和少样本学习范式，减少对大规模标注数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴传感设备在物联网生态系统中的广泛应用，人类活动识别技术面临数据投毒攻击的威胁，传统防御方法需要大量任务特定训练，限制了在动态物联网环境中的适应性。

Method: 使用大型语言模型进行投毒检测和净化，采用角色扮演提示和逐步思考推理策略，让LLM扮演专家角色来情境化评估传感器异常，并推断投毒指标和可能的清洁替代方案。

Result: 对框架进行了广泛评估，量化了检测准确性、净化质量、延迟和通信成本，证明了LLM在提高可穿戴物联网系统安全性和可靠性方面的实用性和有效性。

Conclusion: 该框架展示了大型语言模型在改善可穿戴物联网系统安全性和可靠性方面的实际应用价值，提供了一种适应性强且高效的防御机制。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [3] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文针对决策制定中奖励随时间变化的非平稳环境，开发了ROGUE-TS算法，通过Thompson Sampling和概率裁剪机制平衡个性化推荐与群体效应估计，在微随机化试验中实现低后悔值和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在ROGUE框架下过度强调利用而缺乏充分探索，限制了群体层面效应的估计能力，这在微随机化试验中尤为重要。

Method: 开发ROGUE-TS算法（基于Thompson Sampling），引入概率裁剪程序来平衡个性化与群体学习，量化后悔值与最小探索概率之间的权衡。

Result: 在两个微随机化试验数据集上的验证表明，该方法比现有方法获得更低后悔值，通过裁剪程序保持高统计功效且不显著增加后悔值。

Conclusion: 该框架为设计微随机化试验的研究人员提供了平衡个性化与统计有效性的实用指导，能够可靠检测治疗效果同时考虑个体行为动态。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [4] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 本文提出了一种新的多参考对齐（MRA）算法，通过概率建模和相对位姿作为冗余变量进行边缘化，消除了问题的全局对称性，实现了更直接的解决方案和更好的收敛性。该去中心化方法通过循环一致性避免了集中式方法的立方复杂度缩放，在多种实验设置下都获得了更低的重建误差。


<details>
  <summary>Details</summary>
Motivation: 从分子成像到无线通信，从多个未对齐观测中配准和重建信号对系统性能至关重要。MRA问题出现在许多现实问题中，如冷冻电镜、计算机视觉和无线通信系统。现有方法面临全局对称性和计算复杂度的挑战。

Method: 采用概率方法建模MRA问题，将相对位姿作为冗余变量进行边缘化处理，从而消除问题的全局对称性。提出了去中心化算法，通过循环一致性避免集中式方法的立方复杂度缩放。

Result: 所提出的两种算法在多种实验设置下都实现了更低的重建误差。去中心化方法通过避免立方复杂度缩放实现了显著的计算节省。

Conclusion: 通过概率建模和相对位姿边缘化，新算法成功解决了MRA问题的全局对称性挑战，提供了更直接有效的解决方案，在保持计算效率的同时提升了重建精度。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [5] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 使用Lean 4定理证明器基于Mathlib库形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性。


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是强化学习领域最早且最具影响力的算法，研究其收敛性不仅是早期发展的主要课题，如今也受到越来越多的关注。

Method: 基于Robbins-Siegmund定理的统一框架来形式化验证几乎必然收敛性，该框架可轻松扩展到收敛速率和其他收敛模式。

Result: 成功形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性。

Conclusion: 这项工作为完全形式化收敛强化学习结果迈出了重要一步，相关代码已在GitHub开源。

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [6] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 本研究探索将Vision Transformer（ViT）架构应用于期权数据，通过训练模型从单日的隐含波动率曲面预测资产未来30天的已实现波动率，证明了ViT能够从IV曲面中学习季节性模式和非线性特征。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer架构如Informer在金融时间序列预测中显示出潜力，但将Transformer模型应用于期权数据的研究仍然很少。本研究旨在探索开发用于期权数据的Transformer模型的可行性。

Method: 使用Vision Transformer（ViT）架构，训练模型从单日的隐含波动率曲面（增强日期信息）预测资产未来30天的已实现波动率。

Result: 研究表明ViT能够从隐含波动率曲面中学习季节性模式和非线性特征，显示出模型开发的良好前景。

Conclusion: 将Vision Transformer应用于期权数据是一个有前景的研究方向，能够有效捕捉隐含波动率曲面中的复杂模式。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [7] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 提出MSUCB算法，通过新颖的中位数均值估计器解决在线学习排序中的点击欺诈和操纵问题，在无污染时实现最优对数遗憾，在污染下遗憾仅随总污染量线性增加。


<details>
  <summary>Details</summary>
Motivation: 在线学习排序系统容易受到点击欺诈和其他操纵的影响，这些污染反馈会误导学习过程并降低用户体验。

Method: 提出MSUCB算法，采用中位数均值估计器，在无污染时表现如标准均值，在污染时通过中位数步骤过滤异常值和污染样本。

Result: 在真实世界数据集上的实验表明，该方法始终优于先前方法，对两种最先进方法的遗憾改进分别达到97.35%和91.60%。

Conclusion: MSUCB算法在无污染时实现最优对数遗憾，在污染下遗憾仅随总污染量线性增加，表现出强大的鲁棒性。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [8] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 本文提出改进的CTF-IDF算法和更快的IRLBA降维方法，证明经典机器学习方法在文本分析中比深度学习方法更高效、计算量更小，且碳足迹显著降低，仅需在准确性上做出微小妥协。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法的兴起导致计算资源和能耗需求激增，造成碳足迹严重增加。文本分析领域也受到这种垄断性方法的影响，需要寻找更环保高效的替代方案。

Method: 修改原始TF-IDF算法提出CTF-IDF用于数据预处理，结合更快的IRLBA算法进行降维，构建经典机器学习文本分析流程。

Result: 实验结果显示，与深度学习方法相比，所提方法在时间复杂性上实现多倍降低，模型准确性有所提升，同时计算强度显著减小。

Conclusion: 经典机器学习技术结合CTF-IDF和IRLBA能够在碳足迹方面提供更高效、更快速的文本分析解决方案，仅需在准确性上做出微小妥协。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [9] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文提出了基于增强重叠的理论来分析对比学习的下游性能，并开发了无监督的表征评估指标。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习在各种任务上取得了巨大成功，但其工作机制尚不清晰。现有理论基于条件独立性假设，该假设在实践中往往不成立。

Method: 首先基于条件独立性假设给出最紧致的理论界限，然后放松该假设，提出更实用的增强重叠假设，并推导出下游性能的渐近闭合界限。

Result: 提出的增强重叠理论表明，在激进的数据增强下，类内样本的支持集会更加重叠，因此仅对齐正样本（同一样本的增强视图）就能使对比学习将类内样本聚类在一起。

Conclusion: 从增强重叠的新视角出发，开发了对比学习表征评估的无监督指标，该指标与下游性能高度一致，几乎不需要额外模块。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [10] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 本文提出了周期性技能发现（PSD）框架，用于在无监督强化学习中自动发现周期性行为。该方法通过将状态映射到圆形潜在空间来编码周期性，能够学习具有不同周期的技能，并在复杂机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的无监督技能发现方法往往忽略学习技能的周期性特征，而许多机器人任务（特别是涉及运动的任务）需要在不同时间尺度上执行周期性行为。因此，发现多样化的周期性技能至关重要。

Method: PSD框架训练编码器将状态映射到圆形潜在空间，从而在潜在表示中自然编码周期性。通过捕捉时间距离，PSD能够有效学习具有不同周期的技能。

Result: PSD在复杂机器人任务中成功发现了具有不同周期的技能，即使基于像素观察也能有效工作。这些学习到的技能在下游任务（如跨栏）中表现出高性能。将PSD与现有技能发现方法结合还能产生更多样化的行为。

Conclusion: PSD框架能够有效发现周期性技能，为无监督强化学习提供了新的能力，特别是在需要周期性行为的机器人任务中具有重要应用价值。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [11] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的线性注意力机制，通过熵理论证明和高效近似算法，在保持性能的同时显著降低了计算复杂度和内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制在时间序列建模中应用广泛，但其二次计算复杂度限制了在长序列上的可扩展性。

Method: 基于熵理论，提出线性注意力机制，通过证明对齐概率排名和相似熵值的分布具有结构相似性，开发了线性复杂度的熵近似算法。

Result: 在四个时空数据集上的实验表明，该方法在预测性能上具有竞争力或更优表现，同时显著减少了内存使用和计算时间。

Conclusion: 注意力机制在时空时间序列建模中的有效性可能主要源于获得适度且平衡的权重分布，而非softmax的非线性特性。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [12] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文采用概率U-Net进行气候统计降尺度，结合确定性U-Net主干和变分潜在空间来捕捉随机不确定性，评估了四种训练目标在降水和温度降尺度任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 气候模型由于计算成本高，通常只能提供粗空间分辨率输出，而许多气候变化影响研究需要更精细的尺度，统计降尺度方法可以弥补这一差距。

Method: 使用概率U-Net架构，结合确定性U-Net主干和变分潜在空间来捕捉不确定性，评估了afCRPS和WMSE-MS-SSIM两种训练目标在降水与温度降尺度任务中的表现。

Result: WMSE-MS-SSIM在特定设置下对极端事件表现良好，而afCRPS在跨尺度的空间变异性捕捉方面表现更优。

Conclusion: 不同的训练目标在气候降尺度任务中各有优势，WMSE-MS-SSIM适合极端事件，afCRPS更适合空间变异性分析。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [13] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在严格控制的预训练设置下，当独特数据有限时，扩散语言模型通过更多轮次训练持续超越自回归模型，这种现象被称为Crossover。该现象在不同数据量、模型规模和架构下持续存在。


<details>
  <summary>Details</summary>
Motivation: 研究在数据受限情况下，扩散语言模型与自回归模型的性能对比，探索扩散模型在有限数据下的优势及其原因。

Method: 在严格控制的预训练设置下，比较扩散语言模型和自回归模型在不同数据量、模型规模和训练轮次下的表现，分析影响性能的关键因素。

Result: 1.7B参数的扩散语言模型在10B独特Python token上训练，使用约1.5T token计算预算，超越了匹配设置下的自回归编码器。1B参数扩散模型仅用1B token就达到了HellaSwag >56%和MMLU >33%的准确率。验证交叉熵上升并不表示下游性能下降。

Conclusion: 扩散语言模型在数据受限情况下具有显著优势，归因于任意顺序建模、迭代双向去噪的超密集计算和内置蒙特卡洛增强。这种优势在不同条件下持续存在，为有限数据场景下的语言模型训练提供了新思路。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [14] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: 本文提出了SORTD框架，用于高效枚举稀疏决策树的Rashomon集合（性能相似但结构不同的树），相比现有方法将运行时间减少两个数量级，支持按目标值排序枚举，具有随时性行为。


<details>
  <summary>Details</summary>
Motivation: 稀疏决策树学习提供准确且可解释的预测模型，但单一"最佳"树存在局限。Rashomon集合（性能相似但结构不同的树）可以增强变量重要性分析、丰富解释性，并允许用户根据偏好选择更简单或满足公平性等标准的树，而无需将这些标准硬编码到目标函数中。

Method: 提出SORTD框架，改进可扩展性，按目标值顺序枚举Rashomon集合中的树，提供随时性行为。支持任何可分离和全序目标，并支持使用其他可分离（部分有序）目标对集合进行后评估。

Result: 实验表明，SORTD相比现有技术将运行时间减少高达两个数量级。能够计算任何可分离和全序目标的Rashomon集合，并支持使用其他可分离目标进行后评估。

Conclusion: 这些进展使得在实际应用中探索Rashomon集合变得更加实用，为高风险应用提供了更灵活和可解释的决策树选择方法。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [15] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文提出了一种基于逻辑回归的参数化后验校准方法，通过结构化正则化、鲁棒预处理和高效优化来管理偏差-方差权衡，在多分类校准中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 参数化校准函数虽然理论上适用于二分类和多分类，但更复杂的校准模型会引入过多参数，在有限校准数据下容易过拟合，需要解决偏差-方差权衡问题。

Method: 使用基于逻辑回归的参数化校准函数，结合结构化正则化、鲁棒预处理和高效优化技术来管理模型复杂度。

Result: 通过大量实验证明，该方法在管理偏差-方差权衡方面表现优异，相比现有的基于逻辑回归的校准技术取得了显著提升。

Conclusion: 提出的方法提供了高效易用的开源实现，是温度缩放、向量缩放和矩阵缩放等常见校准方法的有效替代方案。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [16] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 本文提出了一个利用群对称性改进强化学习的理论和算法框架，通过对称感知的核方法显著提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的强化学习环境存在内在对称性，可以利用这些对称性来提高学习效率。

Method: 提出了对称感知的乐观最小二乘值迭代（LSVI）变体，利用不变核来编码奖励和转移动态中的不变性。

Result: 理论分析建立了不变RKHS的最大信息增益和覆盖数的新界限，实证结果在定制Frozen Lake环境和2D布局设计问题上证实了性能提升。

Conclusion: 对称感知强化学习比标准核方法表现显著更好，突出了结构先验在设计更样本高效的强化学习算法中的价值。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [17] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO作为PPO的可扩展替代方案，通过轨迹的组间比较估计优势值而无需学习批评家。研究发现：学习批评家在长时域任务中仍必不可少；GRPO在高折扣因子下表现更好；小规模组优于大规模组。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO作为无批评家策略梯度方法的有效性，探讨学习基线在策略梯度方法中的必要性。

Method: 在经典单任务强化学习环境中系统研究GRPO，通过控制消融实验隔离基线、折扣和组采样等因素的影响。

Result: 1) 除短时域环境外，所有无批评家基线均不如PPO；2) GRPO在高折扣因子下表现更好，但HalfCheetah例外；3) 小规模组优于大规模组。

Conclusion: 揭示了无批评家方法在经典控制任务中的局限性，以及在特定条件下作为学习价值函数替代方案的可行性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [18] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma是一个用于表格预测的LLM模型，通过科学记数法处理数值数据，使用12B Gemma 3模型进行预训练，并采用n-gram检索选择示例，在分类任务上达到SOTA，在回归任务上小样本表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决将预训练LLM应用于表格预测时的两个实际问题：不稳定的数值标记化和有限的上下文大小。

Method: 使用带符号科学记数法规范化数值，在大型真实数据集上继续预训练12B Gemma 3模型，采用目标插补目标，推理时使用紧凑的n-gram检索选择信息性示例。

Result: 在语义丰富的基准测试中，TabGemma在分类任务上建立了新的SOTA，在低数据和高数据机制下都表现优异，并且随着上下文行数增加而单调改进；在回归任务上，小样本时具有竞争力，但随着数据增长落后于传统方法。

Conclusion: 当配备专用数值处理和上下文检索时，LLM可以成为高度语义任务上有效的表格上下文学习器，同时需要在数值建模和长上下文扩展方面进一步改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [19] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ框架通过行为自适应Q学习，利用离线数据中的隐式行为模型在在线微调时提供行为一致性信号，实现从离线到在线强化学习的平稳可靠过渡。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习策略在动态环境中部署时，由于分布偏移和未见状态-动作对上的不可靠值估计而表现不佳，需要解决从离线到在线过渡的可靠性问题。

Method: 引入行为自适应Q学习框架，采用双目标损失函数：(i)在不确定性高时使在线策略向离线行为对齐，(ii)随着在线经验积累逐渐放松约束，通过自适应机制减少分布外估计的误差传播。

Result: 在标准基准测试中，BAQ持续优于先前的离线到在线RL方法，实现了更快的恢复速度、改进的鲁棒性和更高的整体性能。

Conclusion: 隐式行为适应是现实世界策略部署的一个原则性和实用解决方案，能够稳定早期在线更新并加速对新场景的适应。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [20] [Provable Accelerated Bayesian Optimization with Knowledge Transfer](https://arxiv.org/abs/2511.03125)
*Haitao Lin,Boxin Zhao,Mladen Kolar,Chong Liu*

Main category: stat.ML

TL;DR: DeltaBO算法通过构建源任务与目标任务差异函数的不确定性量化方法，在贝叶斯优化中实现知识迁移，显著降低后悔界。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化知识迁移方法要么缺乏理论保证，要么与非迁移设置的后悔界相同，无法充分利用历史知识加速优化过程。

Method: 提出DeltaBO算法，在源任务和目标任务可能属于不同再生核希尔伯特空间的条件下，构建差异函数δ的不确定性量化方法。

Result: 理论证明DeltaBO的后悔界为$\tilde{\mathcal{O}}(\sqrt{T(T/N + \gamma_\delta)})$，其中N为源任务评估次数且通常N≫T，γδ远小于γf。实证研究显示DeltaBO优于基线方法。

Conclusion: DeltaBO通过有效利用源任务知识，在相似任务场景下显著提升了贝叶斯优化的效率，理论和实验均验证了其优越性。

Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task
with historical knowledge transferred from related source tasks. Existing works
on BO with knowledge transfer either do not have theoretical guarantees or
achieve the same regret as BO in the non-transfer setting,
$\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of
evaluations of the target function and $\gamma_f$ denotes its information gain.
In this paper, we propose the DeltaBO algorithm, in which a novel
uncertainty-quantification approach is built on the difference function
$\delta$ between the source and target functions, which are allowed to belong
to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,
we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T
(T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from
source tasks and typically $N \gg T$. In many applications, source and target
tasks are similar, which implies that $\gamma_\delta$ can be much smaller than
$\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks
and synthetic functions show that DeltaBO outperforms other baseline methods
and support our theoretical claims.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: 论文提出PublicAgent多智能体框架，通过专业化分工解决LLM在端到端数据分析工作流中的局限性，包括注意力稀释、推理模式干扰和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库对非专家用户存在可访问性障碍，需要数据集发现、模式映射和统计分析等专业知识。现有LLM在端到端分析工作流中存在根本性限制。

Method: 设计多智能体框架，将工作流分解为意图澄清、数据集发现、分析和报告四个专业化智能体，保持专注注意力并在每个阶段进行验证。

Result: 评估5个模型和50个查询，得出多智能体LLM系统的五个设计原则：专业化价值独立于模型强度、智能体分为通用型和条件型、智能体缓解不同失败模式、架构优势跨任务复杂度持续存在、智能体效果因模型而异需要模型感知设计。

Conclusion: 这些原则指导了何时以及为什么在复杂分析工作流中需要专业化，同时通过自然语言接口实现更广泛的公共数据访问。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [22] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: ScalingEval是一个大规模基准研究，系统比较了36个大型语言模型作为评估者的性能，使用共识驱动的评估协议，发现Claude 3.5 Sonnet决策置信度最高，Gemini 1.5 Pro综合表现最佳，GPT-4o在延迟-准确率-成本权衡中最优。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为评估者在可扩展和可信赖评估流程中的重要性日益增加，需要系统性地比较不同LLM模型的评估性能，为构建评估管道提供指导。

Method: 采用多智能体框架，通过可扩展的多数投票将模式审计和问题代码聚合为真实标签，使用共识驱动的评估协议，无需人工标注即可实现LLM评估者的可重复比较。

Result: 在互补商品推荐基准测试中发现：Claude 3.5 Sonnet决策置信度最高；Gemini 1.5 Pro跨类别综合表现最佳；GPT-4o在延迟-准确率-成本权衡中最优；GPT-OSS 20B在开源模型中领先。结构化领域（电子、体育）共识强，生活类别（服装、食品）存在持续分歧。

Conclusion: ScalingEval建立了可重复的基准和评估协议，为LLM作为评估者提供了关于扩展性、可靠性和模型家族权衡的可操作指导。

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [23] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream是一种KV缓存压缩方法，可在保持模型精度的同时实现4倍内存使用改进，首次在具有静态图和连续批处理的生产推理系统中部署稀疏KV注意力技术。


<details>
  <summary>Details</summary>
Motivation: 随着100B+参数大语言模型和100k+上下文长度的普及，对片上内存支持大KV缓存的需求增加，但现有技术如StreamingLLM和SnapKV在工业部署中应用受限，主要因为框架限制和精度影响不明确。

Method: 开发SnapStream KV缓存压缩方法，在Llama-3.1-8B-Instruct和DeepSeek-R1上探索精度影响，并在SambaNova SN40L加速器上实现16路张量并行部署。

Result: 在128k上下文长度下达到1832 tokens/秒，内存使用改进4倍，在LongBench-v2、AIME24和LiveCodeBench基准测试中精度下降最小。

Conclusion: SnapStream成功实现了在具有静态图和连续批处理的生产推理系统中部署稀疏KV注意力技术，为大规模部署提供了可行的解决方案。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [24] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 该论文对miniF2F基准测试中的形式化和非形式化陈述进行了深入分析，发现由于形式化与非形式化陈述之间的差异，AI系统在数学奥林匹克竞赛问题上的准确率仅为36%。作者修正了所有错误并发布了miniF2F-v2，将准确率提升至70%。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在数学奥林匹克竞赛中的表现，分析形式化与非形式化陈述之间的差异对性能的影响，并改进基准测试质量。

Method: 对miniF2F基准测试中的问题进行分析，识别形式化与非形式化陈述之间的差异，修正所有错误和简化，创建miniF2F-v2数据集，并在新数据集上评估定理证明流程。

Result: 在原始miniF2F上最佳准确率为36%，在修正后的miniF2F-v2上提升至70%，表明形式化与非形式化陈述之间的对齐问题显著影响性能。

Conclusion: 高质量的基准测试对于评估形式推理领域的进展至关重要，能够更好地诊断自动形式化和定理证明模型的失败和成功模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [25] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了一种验证形式化可解释人工智能(XAI)解释器的新方法，并在PyXAI公开解释器上进行了验证，发现其在大多数数据集上产生错误解释。


<details>
  <summary>Details</summary>
Motivation: 形式化XAI相比非形式化方法具有理论严谨性保证，但实际实现的形式化解释器缺乏验证方法，需要开发验证方法来确保其正确性。

Method: 开发了一种新颖的形式化解释器验证方法，并对公开可用的PyXAI形式化解释器进行了评估。

Result: 实验发现PyXAI在大多数分析的数据集上计算出的解释存在错误，证实了所提验证方法的重要性。

Conclusion: 形式化解释器的实际实现需要严格的验证方法，本文提出的验证方法对于确保形式化解释器的正确性至关重要。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [26] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 本文提出了一种名为AAA的自动化Web可访问性审计框架，通过人机协作模式实现WCAG-EM标准的规模化执行，包含GRASP图采样方法和MaC多模态LLM助手两大创新组件。


<details>
  <summary>Details</summary>
Motivation: 当前网站用户界面大多不符合可访问性标准，现有审计方法资源密集且难以规模化，WCAG-EM标准虽然提供了结构化评估方法但需要大量人工投入。

Method: AAA框架包含两个核心创新：GRASP基于图的多模态采样方法，通过视觉、文本和关系线索的嵌入学习确保代表性页面覆盖；MaC多模态大语言模型助手，支持跨模态推理和高效任务协助。

Result: 实验证明该方法有效，提供了四个新数据集用于基准测试，发现小规模语言模型经过微调后可以作为专家系统使用。

Conclusion: 该框架实现了可扩展的端到端Web可访问性审计，通过AI增强的辅助为现实世界影响赋能。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [27] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly是首个在简化版Liar's Poker中达到精英人类水平的AI智能体，使用无模型、演员-评论家深度强化学习算法通过自我对弈训练，在单挑和多玩家游戏中均表现优异，超越了大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在德州扑克等游戏中已取得突破，但这些游戏的多玩家动态较为有限。研究者希望开发能在具有广泛多玩家参与的Liar's Poker中达到精英水平的AI。

Method: 使用无模型、演员-评论家的深度强化学习算法，通过自我对弈方式进行训练。

Result: Solly在胜率（超过50%）和收益方面达到精英人类水平，超越了具有推理能力的大型语言模型，开发了新颖的竞价策略并能有效随机化游戏。

Conclusion: Solly证明了深度强化学习能够在复杂的多玩家不完全信息游戏中达到精英人类水平，为AI在多玩家动态环境中的发展提供了重要进展。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [28] [Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData](https://arxiv.org/abs/2511.02849)
*Beyza Cinar,Maria Maleshkova*

Main category: eess.SP

TL;DR: 本研究改进了DiaData数据集的质量，通过IQR方法识别和处理异常值，使用线性插值和Stineman插值填补缺失值，分析了血糖与心率的相关性，并建立了低血糖分类基准模型。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病个体化治疗需要高质量数据支持，但现有数据存在异常值、噪声和小样本量问题，限制了可靠分析。

Method: 1) 使用四分位距方法识别异常值并替换为缺失值；2) 小间隔(≤25分钟)使用线性插值，大间隔(≥30且<120分钟)使用Stineman插值；3) 分析血糖与心率相关性；4) 使用ResNet模型建立低血糖分类基准。

Result: Stineman插值比线性插值能提供更真实的血糖估计；血糖与心率在低血糖前15-60分钟存在中等相关性；使用更多数据训练模型性能提升7%，使用质量优化数据相比原始数据提升2-3%。

Conclusion: 数据质量改进对1型糖尿病分析至关重要，高质量数据和适当的数据处理方法能显著提升模型性能，为个体化治疗提供更可靠的分析基础。

Abstract: Individualized therapy is driven forward by medical data analysis, which
provides insight into the patient's context. In particular, for Type 1 Diabetes
(T1D), which is an autoimmune disease, relationships between demographics,
sensor data, and context can be analyzed. However, outliers, noisy data, and
small data volumes cannot provide a reliable analysis. Hence, the research
domain requires large volumes of high-quality data. Moreover, missing values
can lead to information loss. To address this limitation, this study improves
the data quality of DiaData, an integration of 15 separate datasets containing
glucose values from 2510 subjects with T1D. Notably, we make the following
contributions: 1) Outliers are identified with the interquartile range (IQR)
approach and treated by replacing them with missing values. 2) Small gaps
($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30
and $<$ 120 min) with Stineman interpolation. Based on a visual comparison,
Stineman interpolation provides more realistic glucose estimates than linear
interpolation for larger gaps. 3) After data cleaning, the correlation between
glucose and heart rate is analyzed, yielding a moderate relation between 15 and
60 minutes before hypoglycemia ($\le$ 70 mg/dL). 4) Finally, a benchmark for
hypoglycemia classification is provided with a state-of-the-art ResNet model.
The model is trained with the Maindatabase and Subdatabase II of DiaData to
classify hypoglycemia onset up to 2 hours in advance. Training with more data
improves performance by 7% while using quality-refined data yields a 2-3% gain
compared to raw data.

</details>


### [29] [Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation](https://arxiv.org/abs/2511.02851)
*Rushuang Zhou,Yuan-Ting Zhang,M. Jamal Deen,Yining Dong*

Main category: eess.SP

TL;DR: LiteHeart是一个半监督知识蒸馏框架，通过区域感知蒸馏和跨层互信息模块，显著缩小了低成本与高成本心脏AI系统之间的诊断性能差距，在5个数据集上比现有方法提升了4.27%-7.10%的宏观F1分数。


<details>
  <summary>Details</summary>
Motivation: 先进的AI心脏监测系统依赖大量医疗数据和高计算资源，难以日常部署。低成本心脏智能系统使用可穿戴设备数据，但诊断性能与高成本系统存在显著差距。

Method: 提出LiteHeart半监督知识蒸馏框架，包含区域感知蒸馏模块（模拟医生关注诊断相关ECG区域）和跨层互信息模块（对齐LCCI和HCCI决策过程），采用半监督训练策略提升模型鲁棒性。

Result: 在覆盖38种心血管疾病的5个数据集上评估，LiteHeart显著缩小了LCCI和HCCI之间的性能差距，宏观F1分数比现有方法高出4.27%-7.10%。

Conclusion: LiteHeart显著提升了低成本心脏智能系统的诊断能力，为使用可穿戴技术实现可扩展、经济实惠且准确的日常心脏医疗保健铺平了道路。

Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac
monitoring is hindered by its reliance on extensive medical data and high
computational resources. Low-cost cardiac intelligence (LCCI) offers a
promising alternative by using wearable device data, such as 1-lead
electrocardiogram (ECG), but it suffers from a significant diagnostic
performance gap compared to high-cost cardiac intelligence (HCCI). To bridge
this gap, we propose LiteHeart, a semi-supervised knowledge distillation
framework. LiteHeart introduces a region-aware distillation module to mimic how
cardiologists focus on diagnostically relevant ECG regions and a cross-layer
mutual information module to align the decision processes of LCCI and HCCI
systems. Using a semi-supervised training strategy, LiteHeart further improves
model robustness under limited supervision. Evaluated on five datasets covering
over 38 cardiovascular diseases, LiteHeart substantially reduces the
performance gap between LCCI and HCCI, outperforming existing methods by 4.27%
to 7.10% in macro F1 score. These results demonstrate that LiteHeart
significantly enhances the diagnostic capabilities of low-cost cardiac
intelligence systems, paving the way for scalable, affordable, and accurate
daily cardiac healthcare using wearable technologies.

</details>


### [30] [Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring](https://arxiv.org/abs/2511.02853)
*Young-Seok Kweon,Gi-Hwan Shin,Ji-Yong Kim,Bokyeong Ryu,Seong-Whan Lee*

Main category: eess.SP

TL;DR: 提出了一种基于心电图的意识状态估计系统，使用解耦查询注意力变换器来捕捉心率变异性特征，在睡眠分期和麻醉监测任务上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于脑电图的方法易受噪声干扰且需要受控环境，需要开发非侵入性、可靠的意识状态监测方法。

Method: 使用带有解耦查询注意力的变换器模型，利用心电图信号捕捉区分意识和无意识状态的心率变异性特征。

Result: 在睡眠分期和麻醉水平监测任务上分别达到0.877和0.880的准确率，以及0.786和0.895的AUC值，优于基线模型。

Conclusion: 基于心电图的意识监测系统为脑电图方法提供了实用且稳健的替代方案，特别适合动态临床环境，有望提高患者安全。

Abstract: Conscious state estimation is important in various medical settings,
including sleep staging and anesthesia management, to ensure patient safety and
optimize health outcomes. Traditional methods predominantly utilize
electroencephalography (EEG), which faces challenges such as high sensitivity
to noise and the requirement for controlled environments. In this study, we
propose the consciousness-ECG transformer that leverages electrocardiography
(ECG) signals for non-invasive and reliable conscious state estimation. Our
approach employs a transformer with decoupled query attention to effectively
capture heart rate variability features that distinguish between conscious and
unconscious states. We implemented the conscious state estimation system with
real-time monitoring and validated our system on datasets involving sleep
staging and anesthesia level monitoring during surgeries. Experimental results
demonstrate that our model outperforms baseline models, achieving accuracies of
0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our
model achieves the highest area under curve values of 0.786 and 0.895 on sleep
staging and anesthesia level monitoring, respectively. The proposed system
offers a practical and robust alternative to EEG-based methods, particularly
suited for dynamic clinical environments. Our results highlight the potential
of ECG-based consciousness monitoring to enhance patient safety and advance our
understanding of conscious states.

</details>


### [31] [Analysis and Algorithm for Multi IRS Collaborative Localization via Hybrid Time Angle Estimation](https://arxiv.org/abs/2511.03133)
*Ziheng Zhang,Wen Chen,Qingqing Wu,Haoran Qin,Zhendong Li,Qiong Wu*

Main category: eess.SP

TL;DR: 本文提出了一种新型多智能反射面协作混合定位系统，通过联合时延和角度估计实现目标定位。系统部署多个IRS，通过处理所有反射单元的反射信号来估计时延和角度参数，并设计了高效的角度和位置估计算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决目标定位问题，特别是在低信噪比条件下的精确定位需求，需要开发能够充分利用多IRS协作优势的混合定位系统。

Method: 推导了半被动模型下的Fisher信息矩阵和Cramer-Rao界；设计了基于原子范数最小化的角度估计算法，将联合角度估计转化为凸优化问题；提出了三阶段定位算法，结合加权最小二乘、总体最小二乘和二次校正来处理系数矩阵和观测向量的误差。

Result: 数值仿真验证了系统的优越性，表明系统协作、混合定位和分布式部署提供了显著优势，特别是在低信噪比条件下，所提出的估计算法具有高精度。

Conclusion: 多IRS协作混合定位系统通过联合时延和角度估计，结合高效算法设计，在低信噪比条件下实现了高精度的目标定位，为智能反射面在定位应用中的潜力提供了有力证明。

Abstract: This paper proposes a novel multiple intelligent reflecting surfaces (IRSs)
collaborative hybrid localization system, which involves deploying multiple
IRSs near the target area and achieving target localization through joint time
delay and angle estimation. Specifically, echo signals from all reflective
elements are received by each sensor and jointly processed to estimate the time
delay and angle parameters. Based on the above model, we derive the Fisher
Information Matrix (FIM) for cascaded delay, Angle of Arrival (AOA), and Angle
of Departure (AOD) estimation in semi passive passive models, along with the
corresponding Cramer Rao Bound (CRB). To achieve precise estimation close to
the CRB, we design efficient algorithms for angle and location estimation. For
angle estimation, reflective signals are categorized into three cases based on
their rank, with different signal preprocessing. By constructing an atomic norm
set and minimizing the atomic norm, the joint angle estimation problem is
transformed into a convex optimization problem, and low-complexity estimation
of multiple AOA and AOD pairs is achieved using the Alternating Direction
Method of Multipliers (ADMM). For location estimation, we propose a three-stage
localization algorithm that combines weighted least squares, total least
squares, and quadratic correction to handle errors in the coefficient matrix
and observation vector, thus improving accuracy. Numerical simulations validate
the superiority of the proposed system, demonstrating that the system's
collaboration, hybrid localization, and distributed deployment provide
substantial benefits, as well as the accuracy of the proposed estimation
algorithms, particularly in low signal to noise ratio (SNR) condition.

</details>


### [32] [Decentralized Federated Learning with Distributed Aggregation Weight Optimization](https://arxiv.org/abs/2511.03284)
*Zhiyuan Zhai,Xiaojun Yuan,Xin Wang,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 本文提出了一种分布式聚合权重优化算法，用于去中心化联邦学习（DFL），通过本地设备间通信优化聚合权重，提高学习效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统DFL依赖中央实体收集信息和优化权重，这与去中心化本质不符。需要开发真正分布式的权重优化方法。

Method: 分析聚合权重对收敛的影响，将优化问题转化为特征值优化问题，提出基于次梯度的分布式算法，仅使用本地信息通过D2D通信获得最优权重。

Result: 数值结果表明所提算法在实际DFL部署中具有优越性，实现了真正分布式的学习系统。

Conclusion: 成功开发了与DFL去中心化特性一致的分布式聚合权重优化方法，使优化、通信和学习过程都能以分布式方式进行。

Abstract: Decentralized federated learning (DFL) is an emerging paradigm to enable edge
devices collaboratively training a learning model using a device-to-device
(D2D) communication manner without the coordination of a parameter server (PS).
Aggregation weights, also known as mixing weights, are crucial in DFL process,
and impact the learning efficiency and accuracy. Conventional design relies on
a so-called central entity to collect all local information and conduct system
optimization to obtain appropriate weights. In this paper, we develop a
distributed aggregation weight optimization algorithm to align with the
decentralized nature of DFL. We analyze convergence by quantitatively capturing
the impact of the aggregation weights over decentralized communication
networks. Based on the analysis, we then formulate a learning performance
optimization problem by designing the aggregation weights to minimize the
derived convergence bound. The optimization problem is further transformed as
an eigenvalue optimization problem and solved by our proposed subgradient-based
algorithm in a distributed fashion. In our algorithm, edge devices only need
local information to obtain the optimal aggregation weights through local (D2D)
communications, just like the learning itself. Therefore, the optimization,
communication, and learning process can be all conducted in a distributed
fashion, which leads to a genuinely distributed DFL system. Numerical results
demonstrate the superiority of the proposed algorithm in practical DFL
deployment.

</details>


### [33] [Performance Analysis of Wireless-Powered Pinching Antenna Systems](https://arxiv.org/abs/2511.03401)
*Kunrui Cao,Jingyu Chen,Panagiotis D. Diamantoulakis,Lei Zhou,Xingwang Li,Yuanwei Liu,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 本文提出了无线供电的夹持天线系统（PAS），通过灵活调整夹持天线位置建立强视距链路来减少自由空间路径损耗。研究了无线供电PAS的可靠性，推导了损耗和无损耗波导情况下的中断概率和遍历速率的闭式表达式，分析了波导和用户的最优部署。


<details>
  <summary>Details</summary>
Motivation: 探索夹持天线在提高无线供电通信系统性能方面的优势，研究无线供电PAS的可靠性，为波导和用户的部署提供有价值的指导。

Method: 引入无线供电PAS概念，推导损耗和无损耗波导情况下的中断概率和遍历速率的闭式表达式，分析波导和用户的最优部署策略。

Result: 吸收系数和用户区域尺寸的增加会导致更高的波导内和自由空间传播损耗，从而增加中断概率并降低遍历速率。在高吸收系数和长波导条件下，无线供电PAS的中断概率甚至比传统WPC系统更差，但遍历速率更好。系统存在最优时间分配因子和PS与AP间的最优距离。

Conclusion: 无线供电PAS在PS和AP分离部署于最优距离时性能优于集成到混合接入点的方案，系统性能受吸收系数和波导长度严重影响，需要优化部署策略。

Abstract: Pinching antenna system (PAS) serves as a groundbreaking paradigm that
enhances wireless communications by flexibly adjusting the position of pinching
antenna (PA) and establishing a strong line-of-sight (LoS) link, thereby
reducing the free-space path loss. This paper introduces the concept of
wireless-powered PAS, and investigates the reliability of wireless-powered PAS
to explore the advantages of PA in improving the performance of
wireless-powered communication (WPC) system. In addition, we derive the
closed-form expressions of outage probability and ergodic rate for the
practical lossy waveguide case and ideal lossless waveguide case, respectively,
and analyze the optimal deployment of waveguides and user to provide valuable
insights for guiding their deployments. The results show that an increase in
the absorption coefficient and in the dimensions of the user area leads to
higher in-waveguide and free-space propagation losses, respectively, which in
turn increase the outage probability and reduce the ergodic rate of the
wireless-powered PAS. However, the performance of wireless-powered PAS is
severely affected by the absorption coefficient and the waveguide length, e.g.,
under conditions of high absorption coefficient and long waveguide, the outage
probability of wireless-powered PAS is even worse than that of traditional WPC
system. While the ergodic rate of wireless-powered PAS is better than that of
traditional WPC system under conditions of high absorption coefficient and long
waveguide. Interestingly, the wireless-powered PAS has the optimal time
allocation factor and optimal distance between power station (PS) and access
point (AP) to minimize the outage probability or maximize the ergodic rate.
Moreover, the system performance of PS and AP separated at the optimal distance
between PS and AP is superior to that of PS and AP integrated into a hybrid
access point.

</details>


### [34] [A Modified Pulse and Design Framework to Halve the Complexity of OFDM Spectral Shaping Techniques](https://arxiv.org/abs/2511.03465)
*Javier Giménez,José A. Cortés,Francisco Javier Cañete,Eduardo Martos-Naya,Luis Díez*

Main category: eess.SP

TL;DR: 本文提出了一种改进的OFDM波形，旨在降低现有频谱整形技术的实现成本，可将优化系数和实现乘积数量减少高达50%。


<details>
  <summary>Details</summary>
Motivation: OFDM调制存在高带外发射问题，现有频谱整形技术虽然有效但涉及复杂的优化过程和实时实现成本。

Method: 通过对传统OFDM波形进行修改，为频谱整形技术建立降低成本的框架。

Result: 该方法可将优化中涉及的系数数量和实现中的乘积数量减少高达50%。

Conclusion: 该改进方法显著降低了频谱整形技术的实现成本，为未来研究提供了成本降低的框架。

Abstract: Orthogonal frequency division multiplexing (OFDM) is a widespread modulation
but suffers from high out-of-band emissions (OOBE). Spectral shaping strategies
such as precoding, active interference cancellation (AIC) and time-domain
methods are effective at reducing the OOBE but entail optimization procedures
and real-time implementation costs which might be considerable. This letter
proposes a modification of the conventional OFDM waveform aimed at reducing the
cost associated to many of the state-of-theart spectral shaping techniques and
sets a framework for future works that want to benefit from the same reduction.
This approach may reduce both the number of coefficients involved in the
optimization and the number of products of its implementation by up to 50%.

</details>

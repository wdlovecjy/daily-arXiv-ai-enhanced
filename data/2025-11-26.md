<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 5]
- [stat.ML](#stat.ML) [Total: 2]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 29]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk](https://arxiv.org/abs/2511.19574)
*Ruizhe Zhang,Jooyoung Kong,Dylan S. Small,William Bekerman*

Main category: stat.AP

TL;DR: 本研究开发了一种数据轮换框架，识别可复制的童年不良经历(ACEs)组合模式，用于更准确地筛查成人抑郁高风险人群，相比传统的ACE总分截断法，在相同特异性水平下灵敏度提高26%。


<details>
  <summary>Details</summary>
Motivation: 传统的单一累加ACE评分(如≥4分截断)在个体水平上的区分能力较差，需要开发更精确的方法来识别真正的高风险ACE组合模式，以优化临床筛查资源分配。

Method: 采用数据轮换框架，结合等渗子群选择(ISS)方法，在单调性假设下估计高风险子群；使用频率编码保留ACE强度信息，通过预定义风险阈值(τ)控制家族错误率。

Result: 在BRFSS 2022验证集上，相比ACE总分截断法，在相同特异性(0.95)水平下，使用可复制子群作为筛查规则可使灵敏度提高26%，提供更具体的触发标准。

Conclusion: 该方法识别出可复制的、基于模式的高风险子群，为临床筛查提供了更准确、易于实施的工具，能更有效地将稀缺的筛查资源定向到真正的高风险人群。

Abstract: Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $τ$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles.

</details>


### [2] [A Win-Expectancy Framework for Contextualizing Runs Batted In: Introducing ARBI and CRBI](https://arxiv.org/abs/2511.19642)
*Wuhuan Deng*

Main category: stat.AP

TL;DR: 本文提出了两个新的情境感知指标ARBI和CRBI，通过整合胜率期望来改进传统RBI统计的局限性，更准确地衡量击球员的进攻贡献。


<details>
  <summary>Details</summary>
Motivation: 传统RBI统计将所有打点视为同等价值，忽略了比赛情境因素如杠杆率、比分状态和得分对获胜概率的实际影响，无法准确反映击球员的真实贡献。

Method: ARBI根据得分事件前后的胜率期望变化重新调整每个RBI的价值，CRBI进一步区分具有相同WE变化的RBI，考虑事件结束时的最终WE值。

Result: ARBI和CRBI提供了经过校准的情境敏感指标，能够更准确地反映打点跑垒的真实价值，区分高杠杆和低杠杆情境下的得分贡献。

Conclusion: ARBI和CRBI现代化了RBI的解释，在球员评估、预测、合同评估和棒球分析决策中具有广泛应用前景。

Abstract: Runs Batted IN (RBI) records the number of runs a hitter directly drives in during their plate appearances and reflects a batter's ability to convert opportunities into scoring. Because producing runs determines game outcomes, RBI has long served as a central statistic in evaluating offensive performance. However, traditional RBI treats all batted-in runs equally and ignores th game context in which they occur, such as leverage, score state, and the actual impact of a run on a team's chance of winning. In this paper, we introduce two new context-aware metrics-Adjusted RBI (ARBI) and Contextual RBI (CRBI)-that address the fundamental limitations of RBI by incorporating Win Expectancy (WE). ARBI rescales each RBI according to the change in WE before and after the scoring event, assigning more value to runs that meaningfully shift the likelihood of winning and less to runs scored in low-leverage situations. We then extend this framework to CRBI, which further differentiates RBIs with the same WE change by accounting for the terminal WE at the end of the event. This refinement captures the idea that an RBI increasing WE from, for example, 0.45 to 0.65 has a larger competitive impact than one increasing WE from 0.05 to 0.25, even though both represent a 20% increase. Together, ARBI and CRBI provide calibrated, context-sensitive measures of offensive contribution that more accurately reflect the true value of run production. These metrics modernize the interpretation of RBI and have broad applications in player evaluation, forecasting, contract evaluation, and decision-making in baseball analytics.

</details>


### [3] [Non-stationarities in extreme hourly precipitation over the Piave Basin, northern Italy](https://arxiv.org/abs/2511.20069)
*Dáire Healy,Ilaria Prosdocimi,Isadora Antoniano-Villalobos*

Main category: stat.AP

TL;DR: 该研究使用意大利皮亚韦河流域的逐小时降水观测数据，分析极端降水的时空特征，发现边际分布和依赖结构都存在季节性模式，且空间依赖随事件极端程度增加而减弱。研究比较了多种协变量依赖模型，并采用灵活的max-id模型来捕捉极端降水过程的时空变异性。


<details>
  <summary>Details</summary>
Motivation: 准确描述极端降水过程对于更好估计相关风险至关重要，需要捕捉边际分布和依赖结构的季节性变化特征。

Method: 使用丰富的逐小时降水观测数据库，识别不同时空尺度的气候协变量，比较近期提出的协变量依赖模型在边际和依赖结构上的性能，采用灵活的max-id模型分析极端水平的时空变异性。

Result: 研究发现仅对边际分布建模非平稳性不能完全捕捉极端降水的变异性，还需要捕捉极端依赖的季节性变化。空间依赖随事件极端程度增加而减弱。

Conclusion: 建模极端降水时，不仅需要考虑边际分布的非平稳性，还必须同时考虑极端依赖结构的季节性变化，才能提供更现实的极端降水过程描述。

Abstract: We study the spatio-temporal features of extremal sub-daily precipitation data over the Piave river basin in northeast Italy using a rich database of observed hourly rainfall. Empirical evidence suggests that both the marginal and dependence structures for extreme precipitation in the area exhibit seasonal patterns, and spatial dependence appears to weaken as events become more extreme. We investigate factors affecting the marginal distributions, the spatial dependence and the interplay between them. Capturing these features is essential to provide a realistic description of extreme precipitation processes in order to better estimate their associated risks. With this aim, we identify various climatic covariates at different spatio-temporal scales and explore their usefulness. We go beyond existing literature by investigating and comparing the performance of recently proposed covariate-dependent models for both the marginal and dependence structures of extremes. Furthermore, a flexible max-id model, which encompasses both asymptotic dependence and independence, is used to learn about the spatio-temporal variability of rainfall processes at extreme levels. We find that modelling non-stationarity only at the marginal level does not fully capture the variability of precipitation extremes, and that it is important to also capture the seasonal variation of extremal dependence.

</details>


### [4] [Investigating access to support centers for Violence Against Women in Apulia: A Spatial analysis over multiple years](https://arxiv.org/abs/2511.20481)
*Leonardo Cefalo,Crescenza Calculli,Alessio Pollice*

Main category: stat.AP

TL;DR: 本研究提出贝叶斯时空泊松回归模型分析意大利南部地区女性暴力的空间变异性，发现支持服务可达性、教育水平和经济发展对暴力报告和发生率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 解决女性暴力在不同城市间空间变异性的建模挑战，理解社会经济特征和当地脆弱性对性别暴力发生率和报告的影响。

Method: 使用贝叶斯时空泊松回归模型，在集成嵌套拉普拉斯近似框架内比较四种空间模型，考虑空间依赖性。

Result: 支持服务可达性随居住地距离增加而降低，教育水平低导致弱势地区报告不足，经济发展可能与报告暴力发生率降低相关。

Conclusion: 空间建模在捕捉报告动态和为政策干预提供信息方面具有关键作用，强调了支持中心位置战略重要性。

Abstract: In this study, we address the challenge of modelling the spatial variability in violence against women across municipalities in a Southern Italian region by proposing a Bayesian spatio-temporal Poisson regression model. Using data on access to Local Anti-Violence Centers in the Apulia region from 2021 to 2024, we investigate the impact of municipality-level socioeconomic characteristics and local vulnerabilities on both the incidence and reporting of gender-based violence. To explicitly account for spatial dependence, we compare four spatial models within the Integrated Nested Laplace Approximation framework for Bayesian model estimation. We assess the relative fit of the competing models, discussing their prior assumptions, spatial confounding effects, and inferential implications. Our findings indicate that access to support services decreases with distance from the residential municipality, highlighting spatial constraints in reporting and the strategic importance of support center location. Furthermore, lower education levels appear to contribute to under-reporting in disadvantaged areas, while higher economic development may be associated with a lower incidence of reported violence. This study emphasises the critical role of spatial modelling in capturing reporting dynamics and informing policy interventions.

</details>


### [5] [Discovering Spatial Patterns of Readmission Risk Using a Bayesian Competing Risks Model with Spatially Varying Coefficients](https://arxiv.org/abs/2511.20616)
*Yueming Shen,Christian Pean,David Dunson,Samuel Berchuck*

Main category: stat.AP

TL;DR: 本文提出了一种贝叶斯方法，将点参考空间效应引入竞争风险比例风险模型，通过高斯过程先验和希尔伯特空间低秩近似提高计算效率，并应用于老年上肢骨折患者再入院风险分析。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）研究中需要考虑患者地理位置的影响，特别是在关注社会健康决定因素的背景下，需要能够处理空间效应的统计方法。

Method: 使用贝叶斯方法在竞争风险比例风险模型中引入空间效应，采用高斯过程先验处理空间变化的截距和斜率，使用希尔伯特空间低秩近似提高计算效率，基线风险曲线建模为分段常数，并引入新型乘法伽马过程先验进行收缩和平滑处理。

Result: 该方法在模拟和真实世界EHR数据分析中显示出改进的推断效率，并为下游政策决策提供了有价值的见解。

Conclusion: 所提出的方法能够有效处理大规模空间位置数据，在老年上肢骨折患者再入院风险分析中表现出良好的性能，为医疗政策制定提供了重要参考。

Abstract: Time-to-event models are commonly used to study associations between risk factors and disease outcomes in the setting of electronic health records (EHR). In recent years, focus has intensified on social determinants of health, highlighting the need for methods that account for patients' locations. We propose a Bayesian approach for introducing point-referenced spatial effects into a competing risks proportional hazards model. Our method leverages Gaussian process (GP) priors for spatially varying intercept and slope. To improve computational efficiency under a large number of spatial locations, we implemented a Hilbert space low-rank approximation of the GP. We modeled the baseline hazard curves as piecewise constant, and introduced a novel multiplicative gamma process prior to induce shrinkage and smoothing. A loss-based clustering method was then used on the spatial random effects to identify high-risk regions. We demonstrate the utility of this method through simulation and a real-world analysis of EHR data from Duke Hospital to study readmission risk of elderly patients with upper extremity fractures. Our results showed that the proposed method improved inference efficiency and provided valuable insights for downstream policy decisions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [6] [FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection](https://arxiv.org/abs/2511.19476)
*Jin Cui,Boran Zhao,Jiajun Xu,Jiaqi Guo,Shuo Guan,Pengju Ren*

Main category: stat.ML

TL;DR: FAST是一个无需DNN的分布匹配核心集选择框架，通过谱图理论和特征函数距离在频域捕获完整分布信息，显著优于现有方法，实现9.12%平均准确率提升、96.57%功耗降低和2.2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法存在局限性：DNN-based方法受模型参数限制且引入架构偏差；DNN-free方法依赖启发式缺乏理论保证。两者都未能明确约束分布等价性，且常用度量无法准确捕捉高阶矩差异。

Method: 将核心集选择建模为图约束优化问题，使用特征函数距离在频域捕获分布信息。提出衰减相位解耦CFD解决"消失相位梯度"问题，并设计渐进差异感知采样策略从低频到高频调度频率选择。

Result: 在多个基准测试中显著优于最先进方法，平均准确率提升9.12%，功耗降低96.57%，实现2.2倍平均加速。

Conclusion: FAST框架通过频域分布匹配和渐进采样策略，实现了高效、节能的核心集选择，为大规模深度学习训练提供了实用的数据压缩解决方案。

Abstract: Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a "vanishing phase gradient" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.

</details>


### [7] [Optimization and Regularization Under Arbitrary Objectives](https://arxiv.org/abs/2511.19628)
*Jared N. Lakhani,Etienne Pienaar*

Main category: stat.ML

TL;DR: 该研究探讨了MCMC方法在任意目标函数应用中的局限性，重点关注两阶段MCMC框架的性能受似然函数锐度参数的影响，并通过强化学习任务进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解MCMC方法在任意目标函数应用中的局限性，特别是两阶段MCMC框架的性能如何受似然函数锐度的影响，以及如何通过数据驱动正则化实现更好的性能。

Method: 采用两阶段MCMC框架，交替使用Metropolis-Hastings和Gibbs采样，引入锐度参数探索不同似然函数形式，并在强化学习任务（导航问题和井字棋游戏）中进行实证应用。

Result: 研究表明似然函数曲率同时控制着样本内性能和训练数据推断的正则化程度，在极端似然锐度情况下，后验质量会坍缩到单一主导模式。

Conclusion: 研究结论是过度的似然锐度会导致后验质量坍缩到单一模式，而混合方法（将MCMC第一块替换为迭代优化步骤）能达到与原MCMC框架几乎相同的性能。

Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [8] [Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter](https://arxiv.org/abs/2511.19805)
*Y. A. Rouzoumka,E. Terreaux,C. Morisseau,J. -P. Ovarlez,C. Ren*

Main category: eess.SP

TL;DR: 该论文研究了复数变分自编码器在雷达异常检测中的应用，提出了多种检测指标并与传统方法进行对比。


<details>
  <summary>Details</summary>
Motivation: 在复杂雷达环境中进行异常检测，需要更有效的检测方法来识别超出训练数据分布的样本。

Method: 使用复数变分自编码器，提出重构误差、基于隐空间的马氏距离和KL散度等检测指标，并与ANMF-Tyler检测器进行对比。

Result: 在合成和实验雷达数据上分析了所有检测器的性能，展示了各自的优势和弱点。

Conclusion: 复数变分自编码器在雷达异常检测中具有应用潜力，不同检测指标在不同场景下表现各异。

Abstract: We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.

</details>


### [9] [Parallel Delay-Doppler Estimation via Order-Reversed Two-Stage Prony Method](https://arxiv.org/abs/2511.19866)
*Yutaka Jitsumatsu,Liangchen Sun*

Main category: eess.SP

TL;DR: 提出了一种基于Prony的并行两阶段方法，用于OTFS系统中的延迟-多普勒估计，通过并行执行延迟优先和多普勒优先估计并融合结果，解决了路径特性相似引起的模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决OTFS系统中由于路径特性相似导致的延迟-多普勒估计模糊问题，为V2V和ISAC等未来应用提供可靠解决方案。

Method: 采用基于Prony的并行两阶段方法，分别进行延迟优先和多普勒优先估计，然后将结果融合以消除模糊性。

Result: 仿真结果表明，该方法在各种条件下都具有优越的估计精度和鲁棒性。

Conclusion: 该方法为车辆间通信和集成感知通信等未来应用提供了有前景的解决方案。

Abstract: This paper proposes a Prony-based parallel two-stage method for delay-Doppler estimation in OTFS systems. By performing delay-first and Doppler-first estimations in parallel and fusing the results, the method resolves ambiguities caused by similar path characteristics. The simulation results demonstrate the superior accuracy and robustness of the proposed method under various conditions. This method provides a promising solution for future applications such as Vehicle-to-Vehicle (V2V) and Integrated Sensing and Communication (ISAC).

</details>


### [10] [AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload](https://arxiv.org/abs/2511.19943)
*Akash Doshi,Pinar Sen,Kirill Ivanov,Wei Yang,June Namgoong,Runxin Wang,Rachel Wang,Taesang Yoo,Jing Jiang,Tingfang Ji*

Main category: eess.SP

TL;DR: 本文提出了一种基于Transformer的编码器和解码器设计，用于5G上行链路中非均匀分布的HARQ-ACK比特传输，通过联合源信道编码和功率整形技术，显著降低了传输功率需求。


<details>
  <summary>Details</summary>
Motivation: 传统信道编码假设物理层输入比特均匀分布，但HARQ-ACK比特本质上是非均匀分布的。对于此类信源，采用联合源信道编码可以获得显著的性能增益。

Method: 使用基于Transformer的编码器，采用"免费午餐"训练算法和每码字功率整形技术；开发了Neyman-Pearson测试的扩展版本，在解码器中实现NACK比特相对于ACK比特的不等错误保护。

Result: 与5G NR基线相比，在达到目标错误率时平均传输功率降低了3-6 dB，最大传输功率降低了2-3 dB，提供了显著的覆盖增益和功率节省。

Conclusion: 所提出的编码器和解码器设计在5G NR兼容的上行链路设置中有效利用了HARQ-ACK比特的非均匀分布特性，实现了显著的功率节省和覆盖增益。

Abstract: Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel "free-lunch" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.

</details>


### [11] [Joint Bit-Partitioning and Modulation Design for Digital AirComp](https://arxiv.org/abs/2511.20113)
*Xiaojing Yan,Carlo Fischione*

Main category: eess.SP

TL;DR: 本文提出两种比特分割方法（均匀分割和重要性自适应分割）来改进ChannelComp框架，通过将输入比特序列分组映射到调制符号，在多时隙传输中降低设计复杂度并提高计算可靠性。


<details>
  <summary>Details</summary>
Motivation: 为降低数字空中计算的调制设计复杂度并提高计算可靠性，需要将比特分割过程集成到ChannelComp框架中。

Method: 开发了两种比特分割方法：均匀比特分割通过最大最小优化设计调制，使用CCCP求解二阶锥规划子问题；重要性自适应比特分割联合优化调制和分割，外层使用模拟退火更新分割，内层使用CCCP求解调制设计。

Result: 数值结果显示两种方法在噪声信道中都能提供鲁棒计算，重要性自适应比特分割相比Sequential Modulation for AirComp在乘积计算中可降低高达5 dB的计算误差。

Conclusion: 比特分割方法能有效提高数字空中计算的性能，特别是重要性自适应比特分割在降低计算误差方面表现优异。

Abstract: For digital over-the-air computation, the ChannelComp framework has recently been proposed to design digital modulations to compute any arbitrary function over a multiple access channel. To reduce modulation design complexity while increasing computation reliability, this paper integrates a bit-partitioning procedure into ChannelComp. The key process is to partition the input bit sequence into several groups, map each group to a single modulation symbol and transmit the encoded symbol sequence across multiple time slots. With the objective to maximize a worst-case constellation distance, we develop two bit-partitioning methods. In uniform bit-partitioning, bits are evenly distributed across groups and modulation is designed via a max-min optimization, which is handled by a CCCP that solves a sequence of second-order cone programming subproblems. In importance-adaptive bit-partitioning (IABP), the bit allocation is adapted to the significance of individual bit positions, and the modulation and partitioning are jointly optimized. To keep the overall complexity manageable, simulated annealing is employed in the outer loop to update the partitioning, while a CCCP-based solver is used in the inner loop for modulation design. Numerical results show that both methods provide robust computation in noisy channels, and IABP achieves up to a 5 dB reduction in computation error compared to Sequential Modulation for AirComp, especially for product computation.

</details>


### [12] [Optimal Waveform Design for Continuous Aperture Array (CAPA)-aided ISAC Systems](https://arxiv.org/abs/2511.20203)
*Junjie Ye,Zhaolin Wang,Yuanwei Liu,Peichang Zhang,Lei Huang,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出了一种基于连续孔径阵列的集成感知与通信框架，设计了最优连续ISAC波形，在形成多目标感知定向波束的同时抑制多用户干扰。


<details>
  <summary>Details</summary>
Motivation: 传统离散阵列在集成感知与通信中存在性能限制，连续孔径阵列能够提供更好的波束形成能力和性能增益。

Method: 基于格林函数推导CAPA的定向波束模式，通过波数域优化获得参考感知波形，使用拉格朗日变换和变分法推导最优波形结构，通过二分搜索确定拉格朗日乘子。

Result: 数值结果表明，CAPA相比传统离散阵列在感知精度和通信可靠性方面均获得显著性能提升。

Conclusion: 连续孔径阵列为集成感知与通信系统提供了有效的解决方案，能够同时优化感知波束和通信性能。

Abstract: A novel continuous-aperture-array (CAPA)-aided integrated sensing and communication (ISAC) framework is proposed. Specifically, an optimal continuous ISAC waveform is designed to form a directive beampattern for multi-target sensing while suppressing the multi-user interference (MUI). To achieve the goal of optimal waveform design, the directional beampattern of CAPA is first derived based on Green's function, whereafter a reference sensing waveform is obtained through wavenumber-domain optimization. Based on the reference sensing waveform, a weighted functional programming on the tradeoff between sensing beampattern mismatch and MUI is formulated. To solve the resulting problem, an optimal CAPA-ISAC waveform structure is analytically derived using a Lagrangian-transformation and calculus-of-variations method, where the Lagrangian multiplier associated with the optimal waveform structure is determined via Bisection search. The obtained optimal waveform reveals that it is concurrently affected by the reference sensing waveform, the channel correlations and the channel-symbol correlations. Finally, numerical results validate the effectiveness of the proposed system and waveform design, demonstrating that CAPA can achieve significant performance gains against the ISAC designs based on conventional spatially discrete array in both sensing accuracy and communication reliability.

</details>


### [13] [Rectified Flow for Vision-Aided mmWave V2I Beam Prediction](https://arxiv.org/abs/2511.20265)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Chongwen Huang,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出基于整流流的流匹配框架，用于车对基础设施链路的视觉辅助波束预测，通过连续潜在流建模实现平滑波束轨迹和快速采样。


<details>
  <summary>Details</summary>
Motivation: 传统方法建模离散波束索引序列存在局限性，需要更高效的波束预测方法来支持车辆通信的实时需求。

Method: 使用基于常微分方程的向量场学习连续潜在流，引入终端流约束确保有限步积分下的全局一致性，稳定长期预测。

Result: 相比RNN和LSTM基线显著提升top-K准确率，接近大型语言模型方法性能，在GPU和CPU上分别实现10倍和10^4倍的推理加速。

Conclusion: 流匹配框架为视觉辅助波束预测提供了高效解决方案，在准确性和计算效率方面均表现出色。

Abstract: This paper proposes a flow matching (FM) framework based on rectified flow for vision-aided beam prediction in vehicle-to-infrastructure (V2I) links. Instead of modeling discrete beam index sequences, the method learns a continuous latent flow governed by an ordinary differential equation (ODE)-based vector field, enabling smooth beam trajectories and fast sampling. A terminal flow constraint enforces global consistency under finite-step integration, stabilizing long-term prediction. The resulting FM-based model significantly improves top-K accuracy over RNN and LSTM baselines, approaches the performance of large language model-based approaches, and achieves inference speedups on the order of 10 x and 10^4 x on identical GPU and CPU deployments, respectively.

</details>


### [14] [Digital Twin-Assisted High-Precision Massive MIMO Localization in Urban Canyons](https://arxiv.org/abs/2511.20453)
*Ziqin Zhou,Hui Chen,Gerhard Steinböck,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出一种结合数字孪生模型和RANSAC算法的三阶段鲁棒定位方法，用于解决城市峡谷环境中的无线定位问题，通过将NLOS路径转化为几何信息实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 解决城市峡谷环境中无线定位面临的测量噪声和严重非视距传播挑战，降低对直接视距路径的依赖并减少系统部署成本。

Method: 采用三阶段算法：1) 使用数字孪生模型进行几何路径关联；2) 利用RANSAC算法识别可靠的视距和单次反射非视距路径，排除多次反射异常值；3) 在筛选出的内点集上进行最终优化，估计用户位置和时钟偏差。

Result: 仿真验证表明，该方法通过数字孪生将非视距路径转化为有价值的几何信息，能够实现精确的定位，降低对直接视距的依赖，并显著降低系统部署成本。

Conclusion: 该方法适合实际部署，为城市峡谷环境中的无线定位提供了一种有效的解决方案。

Abstract: High-precision wireless localization in urban canyons is challenged by noisy measurements and severe non-line-of-sight (NLOS) propagation. This paper proposes a robust three-stage algorithm synergizing a digital twin (DT) model with the random sample consensus (RANSAC) algorithm to overcome these limitations. The method leverages the DT for geometric path association and employs RANSAC to identify reliable line-of-sight (LOS) and single-bounce NLOS paths while rejecting multi-bounce outliers. A final optimization on the resulting inlier set estimates the user's position and clock bias. Simulations validate that by effectively turning NLOS paths into valuable geometric information via the DT, the approach enables accurate localization, reduces reliance on direct LOS, and significantly lowers system deployment costs, making it suitable for practical deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探讨了使用可穿戴设备和AI方法预测慢性疼痛和鸦片使用障碍患者的疼痛峰值，发现机器学习模型预测准确率较高，但大语言模型在此领域表现有限。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对接受鸦片使用障碍药物治疗患者的慢性疼痛和鸦片使用障碍的循证综合治疗方案，需要开发新的监测和干预方法。

Method: 使用可穿戴设备监测患者数据，结合多种AI方法（包括机器学习和大语言模型）分析疼痛峰值的临床相关性。

Result: 机器学习模型在预测疼痛峰值方面达到相对较高的准确率（>0.7），而大语言模型在提供疼痛峰值洞察方面表现有限。

Conclusion: 可穿戴设备实时监测结合先进AI模型可促进疼痛峰值的早期检测，支持个性化干预，但需要开发能够在此领域提供可操作见解的大语言模型。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [16] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLMs）在教育评估项目与内容标准对齐方面的应用潜力，发现GPT-4o-mini等模型能在83-94%的情况下正确识别对齐状态，结合候选技能预筛选策略后，正确技能出现在前五建议中的概率超过95%。


<details>
  <summary>Details</summary>
Motivation: 传统的人工对齐审查虽然准确但耗时耗力，特别是在大型项目库中。研究旨在验证LLMs是否能加速这一过程而不牺牲准确性。

Method: 使用超过12,000个K-5年级的项目-技能对，测试了三种LLMs（GPT-3.5 Turbo、GPT-4o-mini和GPT-4o）在三个任务上的表现：识别未对齐项目、从完整标准集中选择正确技能、在分类前缩小候选列表。

Result: GPT-4o-mini在约83-94%的情况下正确识别对齐状态；数学领域表现强劲，阅读领域因标准语义重叠较多而表现较低；预筛选候选技能显著改善结果，正确技能出现在前五建议中的概率超过95%。

Conclusion: LLMs，特别是结合候选筛选策略时，能显著减少项目审查的人工负担同时保持对齐准确性。建议开发结合LLM筛选和人工审查的混合流程，为持续项目验证和教学对齐提供可扩展解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [17] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于评估指导的提示优化方法，通过建立系统化的提示评估框架和训练无执行评估器来预测多维度质量分数，从而实现可解释的、查询相关的提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要优化静态模板，在复杂动态用户场景中效果有限。现有查询相关方法依赖不稳定的文本反馈或黑盒奖励模型，提供弱且不可解释的优化信号。提示质量本身缺乏统一系统定义，导致评估信号碎片化不可靠。

Method: 首先建立面向性能的系统化提示评估框架，开发并微调无执行评估器直接从文本预测多维度质量分数。评估器指导指标感知优化器诊断失败模式并以可解释、查询相关的方式重写提示。

Result: 评估器在预测提示性能方面达到最强准确度，评估指导的优化在八个数据集和三个骨干模型上持续超越静态模板和查询相关基线方法。

Conclusion: 提出了统一的、基于指标的提示质量视角，证明评估指导的优化管道能在多样化任务中提供稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [18] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个包含15种隐藏故障模式的系统级分类法，用于分析现实世界LLM应用中的失败模式，并探讨了评估与监控实践之间的差距，以及部署LLM的生产挑战和可靠性设计原则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正被快速集成到决策支持工具和自动化工作流中，但其在生产环境中的行为仍未被充分理解，且其故障模式与传统机器学习模型有根本性差异。

Method: 提出系统级分类法识别15种隐藏故障模式，分析评估与监控实践的差距，并研究部署LLM的生产挑战。

Result: 建立了涵盖多步推理漂移、潜在不一致性、上下文边界退化、工具调用错误等故障模式的分类体系，揭示了现有基准测试在稳定性、可重复性和工作流集成方面的不足。

Conclusion: 通过将LLM可靠性重新定义为系统工程问题而非纯模型中心问题，为未来评估方法、AI系统鲁棒性和可靠LLM部署研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [19] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 本文提出了一个简单有效的框架，在CPDC 2025挑战赛中统一改进了GPU和API两个赛道。核心方法包括上下文工程和GRPO训练，在最终评估中获得了多个赛道的前三名成绩。


<details>
  <summary>Details</summary>
Motivation: 解决常识人物对话挑战中的工具调用稳定性、执行可靠性和角色扮演指导问题，同时通过强化学习缓解小样本过拟合，提升任务导向对话性能。

Method: 1. 上下文工程：动态工具修剪和人物特征裁剪进行输入压缩，结合参数归一化和函数合并等后处理技术；2. GPU赛道采用GRPO训练，用强化学习替代监督微调，直接优化奖励信号。

Result: 团队在最终评估中排名：Task 2 API第1名，Task 1 API第2名，Task 3 API和GPU赛道均第3名，证明了方法的有效性。

Conclusion: 提出的统一框架在CPDC 2025挑战赛中表现出色，上下文工程和GRPO训练的结合显著提升了对话系统的性能和稳定性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [20] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航研究指标与商业可行性进行定量评估的微导航经济测试平台，通过完整的成本-收益分析揭示任务成功率优化与经济部署优化的根本差异。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准主要关注任务成功率指标，但忽视了商业部署自主配送机器人所需的经济可行性，这对商业化应用至关重要。

Method: CostNav建模完整的经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业参数，从缩减规模模拟扩展到实际配送场景。

Result: 基线模型达到43.0%的服务水平协议合规率，但商业上不可行：每次运行损失30.009美元，无盈亏平衡点，因为运营成本主要由碰撞引起的维护成本主导，占每次运行成本的99.7%。

Conclusion: CostNav弥合了导航研究与商业部署之间的差距，为评估基于规则的导航、模仿学习和成本感知强化学习训练奠定了基础，使能够基于数据做出经济权衡决策。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [21] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW框架通过构建和精炼经验学习知识库来优化LLM智能体，在保持计算效率的同时提升任务精度10-20%，减少API调用10-15%，实现更快的执行时间。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的智能体训练方法计算开销大，收敛困难，且生成的策略难以解释、适应或增量改进。需要一种更实用、可解释的智能体优化方法。

Method: 引入BREW框架，通过知识库构建和精炼来优化智能体。采用任务评分器和行为准则学习洞察，利用状态空间搜索确保鲁棒性，并引入有效的记忆分区方法以提高检索和精炼效率。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等真实世界基准测试中，BREW实现了任务精度10-20%的提升，API/工具调用减少10-15%，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: BREW将知识库确立为模块化、可控的智能体优化基础，作为塑造行为的显式杠杆，实现了透明、可解释和可扩展的行为塑造方式。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [22] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文旨在澄清主动推理的概念，将其与自由能原理分离，提出在离散状态空间中实现主动推理的优化问题可表述为约束散度最小化问题，可通过标准平均场方法求解，无需依赖期望自由能概念。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理与自由能原理的关系，提供不依赖期望自由能的主动推理实现方法。

Method: 将主动推理在离散状态空间中的优化问题表述为约束散度最小化问题，使用标准平均场方法求解。

Result: 提出的感知/行动散度准则在建模感知时与变分自由能一致，在建模行动时与期望自由能泛函相差一个熵正则化项。

Conclusion: 主动推理可以通过约束散度最小化框架实现，无需依赖期望自由能概念，为主动推理提供了更清晰的理论基础。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 提出了基于部分信息分解(PID)的框架，量化多模态模型中各模态的贡献，将预测信息分解为独特、冗余和协同成分，使用迭代比例拟合程序(IPFP)实现可扩展的推理分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于准确性的方法，将移除模态后的性能下降解释为其影响力，但这类结果驱动指标无法区分模态是本身具有信息价值，还是仅通过与其他模态交互才产生价值，特别是在跨注意力架构中。

Method: 基于部分信息分解(PID)框架，将内部嵌入中的预测信息分解为独特、冗余和协同成分，开发基于迭代比例拟合程序(IPFP)的算法，无需重新训练即可计算层和数据集级别的贡献。

Result: 提供了原则性的、表示级别的多模态行为视图，相比基于结果的指标提供更清晰和可解释的洞察。

Conclusion: 该框架能够量化多模态模型中各模态的贡献，区分模态的固有信息价值与通过交互产生的价值，为多模态行为分析提供更精确的工具。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [24] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 本文系统研究了MoE-LLMs在任务特定使用下的可剪枝性，揭示了知识损失-恢复的权衡关系，并提出了防御策略以防止未经授权的模型压缩和微调。


<details>
  <summary>Details</summary>
Motivation: MoE架构的模块化结构引入了独特的安全漏洞：攻击者可以通过剪枝专家并廉价微调剩余部分来压缩或重新利用模型，从而绕过许可和安全约束。

Method: 开发了专家归因框架识别对特定任务最关键的专家子集，评估剪枝和重新对齐这些专家的性能权衡，使用主动学习驱动的微调方法。

Result: 发现关键的知识损失-恢复权衡：虽然可以隔离某些专家来保持任务准确性，但如果没有针对性的重新对齐，会出现显著性能下降。

Conclusion: 提出了防御策略，包括纠缠专家训练和选择性微调协议，使MoE模型更难被未经授权压缩和微调，为MoE-LLMs的安全专业化提供了首个系统评估框架。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [25] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 该论文提出了一种开放世界机器学习模型，通过发现未知类别和增量学习新类别来实现持续学习，在开放世界学习和持续学习方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型遵循封闭世界假设，难以保留先前学到的知识用于未来任务，而自动化智能系统需要学习新类别和已知任务。

Method: 模型包含两个连接任务：首先发现数据中的未知类别并创建新类别，然后对每个新类别进行增量学习，实现持续学习能力。

Result: 模型在开放世界学习中优于现有方法，在持续学习方面表现优异，四个迭代中最高平均准确率达到82.54%，最低准确率为65.87%。

Conclusion: 该模型能够在开放和持续学习环境中有效扩展对数据的理解并随时间改进，为开放世界机器学习提供了有效解决方案。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [26] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文针对双层优化问题，在光滑非凸-强凸设置下，建立了新的困难实例，得出了在确定性和随机一阶oracle模型下的非平凡下界。确定性情况下需要至少Ω(κ³/²ε⁻²)次oracle调用，随机情况下需要至少Ω(κ⁵/²ε⁻⁴)次随机oracle调用，这些下界优于单层非凸优化和非凸-强凸min-max问题的最优下界。


<details>
  <summary>Details</summary>
Motivation: 尽管双层优化的上界保证已被广泛研究，但由于双层结构的复杂性，下界方面的进展有限。本文旨在填补这一空白，为双层优化建立更严格的下界。

Method: 开发新的困难实例，在光滑非凸-强凸设置下，分别针对确定性和随机一阶oracle模型进行分析，推导最优下界。

Result: 在确定性情况下，任何一阶零尊重算法需要至少Ω(κ³/²ε⁻²)次oracle调用来找到ε-精确的稳定点；在随机情况下，需要至少Ω(κ⁵/²ε⁻⁴)次随机oracle调用。这些下界优于相关设置中的已知最佳边界。

Conclusion: 研究结果揭示了当前双层优化上下界之间的显著差距，表明即使是简化机制（如具有二次下层目标的机制）也值得进一步研究，以理解标准一阶oracle下双层优化的最优复杂度。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [27] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 本文系统研究了知识蒸馏、结构化剪枝和低比特量化三种LLM压缩技术的独立效果和组合顺序，发现在Qwen2.5 3B模型上，P-KD-Q（剪枝-知识蒸馏-量化）序列能实现3.68倍压缩比并保持良好性能，而早期应用量化会导致严重性能下降。


<details>
  <summary>Details</summary>
Motivation: LLM需要大量计算资源，在受限环境中部署需要模型压缩。现有压缩技术的独立效果已有研究，但它们的交互作用和最优顺序尚不明确。

Method: 在Qwen2.5 3B模型上系统评估多种压缩流水线，包括单技术和三技术序列，使用困惑度、G-Eval、清晰度、提示对齐和压缩比作为评估指标。

Result: 量化提供最大的独立压缩，剪枝引入中等质量下降。技术顺序显著影响最终模型质量：P-KD-Q序列表现最佳，而早期应用量化的流水线因不可逆信息损失而性能严重下降。

Conclusion: 研究为在资源受限环境中部署LLM提供了实用的、顺序感知的压缩流水线设计指导，强调技术应用顺序对最终性能的关键影响。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [28] [Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM](https://arxiv.org/abs/2511.19496)
*Yang Liu,Xiaolong Zhong,Ling Jiang*

Main category: cs.LG

TL;DR: Xmodel-2.5是一个13亿参数的小型语言模型，专为边缘计算和成本敏感部署设计，采用μP训练方法、1.4T token的课程学习，并在衰减阶段从AdamW切换到Muon优化器，在13个推理任务上平均提升4.58%性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的推理和工具使用能力，但计算需求大，不适合边缘或成本敏感部署场景，因此需要开发高效的小型语言模型。

Method: 使用最大更新参数化（μP）训练方法，采用1.4T token的Warmup-Stable-Decay课程学习策略，在衰减阶段从AdamW切换到Muon优化器，并使用FP8混合精度训练。

Result: 在衰减阶段切换优化器策略使13个推理任务平均性能提升4.58%，同时保持其他超参数不变，验证了早期AdamW稳定性与后期Muon锐化相结合的优势。

Conclusion: Xmodel-2.5作为drop-in agent core，通过创新的训练策略实现了高效的小型语言模型，所有检查点、配方和评估代码均已开源发布。

Abstract: Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\,\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.

</details>


### [29] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: 本文提出了一种分层双策略框架，用于选择性知识遗忘，在医疗领域精确移除专业知识同时保留基础医学能力，仅需修改0.1%的参数即可实现82.7%的遗忘率和88.5%的知识保留。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中存在隐私风险，特别是训练数据记忆化问题，需要解决包含不完善或隐私敏感患者信息的专业知识移除需求。

Method: 采用分层双策略框架，结合几何约束梯度更新选择性调节目标参数，以及概念感知的令牌级干预，通过统一的四级医学概念层次区分保护关键令牌和遗忘目标令牌。

Result: 在MedMCQA（外科）和MHQA（焦虑、抑郁、创伤）数据集上的评估显示，实现了82.7%的遗忘率和88.5%的知识保留率，同时仅需修改0.1%的参数。

Conclusion: 该框架在保持强大隐私保证的同时，满足了临床研究中监管合规性、可审计性和伦理标准的关键需求。

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [30] [When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics](https://arxiv.org/abs/2511.19548)
*Yiven,Zhu*

Main category: cs.LG

TL;DR: 本文提出了一个基于模型的框架，探讨神经数据在何种条件下可以合法地为政策制定提供福利判断依据，而不仅仅是描述行为。


<details>
  <summary>Details</summary>
Motivation: 神经经济学承诺将福利分析建立在神经和计算证据基础上，但需要明确神经数据何时能真正为福利判断提供依据，而非仅描述行为。

Method: 开发了一个非经验性的、基于模型的框架，将神经信号、计算决策模型和规范性福利标准三个层次联系起来，在行动者-批评者强化学习模型中形式化了从神经活动到潜在价值和预测误差，再到福利主张的推理路径。

Result: 只有当神经-计算映射得到充分验证、决策模型能识别"真实"利益与情境依赖错误、福利标准被明确指定和辩护时，神经证据才能约束福利判断。

Conclusion: 神经数据和人工智能系统的内部奖励信号都是计算量，不能在没有明确规范性模型的情况下被视为福利衡量标准。作者为监管者和神经AI系统设计者制定了神经经济学福利推理清单。

Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.

</details>


### [31] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的在线差分进化稀疏特征选择方法（ODESFS），用于处理高维流数据中的缺失值问题，通过潜在因子分析和差分进化实现特征选择优化。


<details>
  <summary>Details</summary>
Motivation: 现有在线稀疏流特征选择方法在特征评估方面存在显著局限性，导致性能下降，需要改进特征选择和缺失数据处理能力。

Method: ODESFS结合两个关键创新：1）使用潜在因子分析模型进行缺失值填补；2）通过差分进化进行特征重要性评估。

Result: 在六个真实世界数据集上的实验表明，ODESFS始终优于最先进的OSFS和OS2FS方法，能够选择最优特征子集并获得更高的准确率。

Conclusion: ODESFS通过改进的缺失值处理和特征评估机制，显著提升了在线流特征选择的性能。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [32] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF是一个基于最优传输理论的模型融合框架，通过发现应用于任务向量的共同掩码来对齐任务特定模型的语义几何，解决参数插值引起的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法主要依赖权重空间的参数插值，但这会导致特征空间的显著分布偏移并削弱任务特定知识。

Method: OTMF使用最优传输计划发现应用于任务向量的共同掩码，选择性地提取可转移和任务无关的组件，同时保留每个任务的独特结构身份。支持持续融合范式，增量集成新任务向量而无需重新访问先前任务。

Result: 在多个视觉和语言基准测试上的综合实验表明，OTMF在准确性和效率方面都达到了最先进的性能。

Conclusion: 该方法在模型融合方面具有实际和理论价值，能够有效解决分布偏移问题并实现高效的多任务融合。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [33] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出Inv^2A方法，通过利用LLM的潜在空间不变性来改进语言模型反演攻击，在9个数据集上平均BLEU得分提升4.77%，同时减少对大型反演语料库的依赖。


<details>
  <summary>Details</summary>
Motivation: 语言模型反演(LMI)对用户隐私和系统安全构成威胁，现有方法效果有限且依赖大量数据。作者观察到LLM潜在空间具有不变性特征，提出利用这一特性改进反演攻击效果。

Method: 提出不变潜在空间假设(ILSH)，包括源不变性和循环不变性。设计Inv^2A方法，将LLM视为不变解码器，仅学习轻量级反演编码器。采用两阶段训练：对比对齐(源不变性)和监督强化(循环不变性)。

Result: 在9个数据集上的实验表明，Inv^2A比基线方法平均BLEU得分提升4.77%，同时减少了对大型反演语料库的依赖。分析还显示现有防御措施提供的保护有限。

Conclusion: Inv^2A通过利用LLM潜在空间不变性有效提升了语言模型反演攻击性能，揭示了现有防御措施的不足，强调了开发更强防御策略的必要性。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [34] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: DISCO是一个开源的分布式协作学习平台，允许非技术用户在不共享原始数据的情况下协作构建机器学习模型，支持联邦学习和去中心化范式，提供多种隐私保护和权重聚合策略。


<details>
  <summary>Details</summary>
Motivation: 数据共享存在隐私、知识产权和法律限制等问题，这不仅分散了预测模型的统计能力，还造成了可访问性偏见，使得模型准确性不公平地分配给那些有资源克服这些问题的机构。

Method: DISCO通过Web应用程序在浏览器中本地训练模型，采用模块化设计，支持联邦学习和去中心化范式，提供不同级别的隐私保证和多种权重聚合策略，实现模型个性化和偏见弹性。

Result: 开发了一个跨平台的协作学习工具，可在包括智能手机在内的各种设备上运行，无需编程知识即可使用。

Conclusion: DISCO平台为非技术用户提供了安全、可访问的协作机器学习解决方案，解决了数据共享障碍，促进了公平的模型开发。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [35] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于认证式块级提取Transformer机制并支持认证局部编辑的框架，通过提取结构化替代实现并提供机器可检查的证书来约束近似误差。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏形式化保证，无法明确提取或编辑后的模型与原始模型在相关输入上的偏差范围。

Method: 提出BlockCert框架，基于预训练Transformer和提示分布提取残差块的结构化替代实现，提供约束近似误差、记录覆盖指标和哈希底层工件的证书，并在Lean 4中形式化Lipschitz组合定理。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上应用，获得高块级覆盖率和小的残差误差，在TinyLlama设置中完全拼接模型在压力提示上与基线困惑度匹配在约6e-5范围内。

Conclusion: 块级提取与显式证书对于真实Transformer语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [36] [Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization](https://arxiv.org/abs/2511.19851)
*Kun Guo,Xuefei Li,Xijun Wang,Howard H. Yang,Wei Feng,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出了一种混合分割与联邦学习（HSFL）方法，通过联合优化学习模式选择、批次大小以及通信和计算资源来加速分布式学习，显著减少了达到目标准确率所需的延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）支持低延迟并行训练但可能收敛到较低准确率，而分割学习（SL）能实现更高准确率但延迟较大。为了结合两者的优势，需要开发一种混合方法并优化相关参数以加速学习过程。

Method: 首先分析收敛性，揭示学习模式与批次大小之间的相互作用；然后制定延迟最小化问题，提出两阶段解决方案：使用块坐标下降法求解松弛问题获得局部最优解，再通过舍入算法恢复整数批次大小以实现接近最优性能。

Result: 实验结果表明，与现有方法相比，所提出的方法显著加速了达到目标准确率的收敛过程。

Conclusion: HSFL通过联合优化学习模式、批次大小和资源分配，有效平衡了准确率和延迟，实现了比单独使用FL或SL更好的性能。

Abstract: Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.

</details>


### [37] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文针对强化学习微调大语言模型时出现的多样性崩溃问题，提出了理论分析和解决方案。通过证明RL微调存在选择和强化偏差导致多样性崩溃，作者开发了差分平滑方法，在正确轨迹上应用奖励修改，同时提升正确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有解决RL微调多样性崩溃的方法都是启发式的，存在三个问题：需要在正确性和多样性之间权衡、效果因任务而异、不同方法甚至相互矛盾。需要建立理论基础并提供统一解决方案。

Method: 提出差分平滑方法，基于理论分析在正确轨迹上应用奖励修改。该方法通过理论证明能够同时改善正确性和多样性，优于传统RL和基于熵的启发式方法。

Result: 在1B到7B参数的模型上进行了广泛实验，涵盖CountDown和真实世界数学推理等领域。差分平滑在Pass@1和Pass@k指标上均取得提升，在AIME24数据集上最高提升6.7%。

Conclusion: 差分平滑方法在理论上优于现有启发式方法，实验证明其在不同规模模型和任务上都能一致地同时提升正确性和多样性，解决了RL微调中的多样性崩溃问题。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [38] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出了一种按需多任务稀疏框架，通过最大化参数重用和块粒度分解来最小化任务切换时的I/O开销，在自动驾驶平台上实现了6.6倍的平均切换加速。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏化方法在优化单个任务时忽略了频繁任务切换带来的显著I/O开销，需要专门设计来最小化切换成本。

Method: 将权重分解为可重用的块粒度单元，跨任务对齐稀疏结构以最大化重叠，动态加载下一个任务所需的小型差异块集合。

Result: 在真实自动驾驶平台上的实验表明，该框架实现了优越的切换效率，相比现有稀疏方法平均加速任务切换超过6.6倍。

Conclusion: 该按需多任务稀疏框架通过参数重用和动态块加载，有效缓解了传统整体方法固有的冷启动延迟，显著提升了任务切换效率。

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [39] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种针对多模态属性图（MMAGs）的双图滤波（DGF）方案，通过特征级去噪和三交叉对比学习策略，有效解决了现有多视图聚类方法在处理预训练模型输出的多模态属性时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法过度依赖视图间的高相关性，忽视了预训练语言和视觉模型输出的多模态属性中存在的低模态相关性和强特征噪声问题，导致聚类性能不佳。

Method: 提出双图滤波（DGF）方案，包含特征级去噪组件和三交叉对比训练策略，通过跨模态、邻域和社区的实例级对比学习来学习鲁棒且具有区分度的节点表示。

Result: 在八个基准MMAG数据集上的综合实验表明，DGF在聚类质量方面能够一致且显著地优于多种最先进的基线方法。

Conclusion: DGF方案通过创新的图滤波设计和对比学习策略，有效克服了传统多视图图聚类方法的局限性，在多模态属性图聚类任务中取得了优越性能。

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [40] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: 提出SOMBRL方法，基于不确定性乐观原则，通过结合外部奖励和认知不确定性进行探索，在多种强化学习设置中实现亚线性遗憾，并在仿真和硬件实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习中未知系统动态下的高效探索问题，传统方法在非线性动态系统中缺乏理论保证。

Method: 学习不确定性感知的动态模型，贪婪地最大化外部奖励和智能体认知不确定性的加权和，兼容任何策略优化器或规划器。

Result: 在有限时域、折扣无限时域和非情景设置中证明了对非线性动态的亚线性遗憾，在状态和视觉控制环境中表现优异，在动态RC汽车硬件上超越现有最优方法。

Conclusion: SOMBRL为基于模型的强化学习提供了灵活、可扩展且有理论保证的探索解决方案，展示了原则性探索在MBRL中的优势。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [41] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 本研究应用LightGBM模型对比特币实现波动率进行确定性和概率性预测，使用69个市场、行为和宏观经济指标作为预测因子，并比较了与计量经济学和机器学习基准模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发能够有效捕捉加密货币市场非线性和高方差特征的波动率预测模型，同时提供对波动动态的可解释性见解。

Method: 采用LightGBM模型进行确定性和概率性预测，对于概率性预测探索了两种分位数方法：使用pinball损失函数的直接分位数回归，以及将点预测转换为预测分布的残差模拟方法。使用增益和置换特征重要性技术识别主要波动驱动因素。

Result: 结果表明LGBM模型有效捕捉了加密货币市场的非线性特征，识别出交易量、滞后波动率指标、投资者关注度和市值为主要波动驱动因素，在预测性能上优于基准模型。

Conclusion: LGBM模型能够有效预测比特币波动率，同时提供对波动动态的可解释性见解，为加密货币市场风险管理提供有价值的工具。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [42] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导、可解释的时空学习框架，用于空气污染预测，在性能和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 准确的空气污染预测对公共健康至关重要，但现有模型在性能和可解释性之间存在权衡。

Method: 将污染物浓度的时空行为分解为两个透明加性模块：物理引导的传输核和可解释的注意力机制。

Result: 在斯德哥尔摩地区数据集上的评估表明，该模型在多个预测时间范围内始终优于最先进的基线方法。

Conclusion: 该模型将高预测性能与时空可解释性相结合，为实际空气质量管理提供了更可靠的基础。

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [43] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文提出了一种更细粒度的视角，将Transformer中的注意力头和MLP层分解为正交奇异方向，揭示了单个组件内叠加的独立计算。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性方法通常将注意力头和MLP层视为不可分割单元，忽视了它们内部可能学习到的功能子结构。

Method: 通过将Transformer组件分解为正交奇异方向，分析IOI、GP和GT等标准任务中的计算模式。

Result: 发现先前识别的典型功能头（如名称移动器）编码了多个重叠子功能，这些子功能与不同的奇异方向对齐。计算图中的节点在特定低秩方向上表现出强激活。

Conclusion: Transformer计算比先前假设的更加分布式、结构化和组合性，这为细粒度机制可解释性开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [44] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: SAPO是一种软自适应策略优化方法，通过温度控制的门机制替代硬裁剪，在保持序列级一致性的同时实现更稳定高效的强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GSPO和GRPO）使用硬裁剪处理重要性比率的高方差问题，难以同时保持稳定性和有效学习。

Method: 提出软自适应策略优化（SAPO），使用平滑的温度控制门机制自适应地衰减离策略更新，同时保留有用的学习信号。

Result: 在数学推理基准测试中，SAPO表现出更好的训练稳定性和更高的Pass@1性能；在Qwen3-VL模型系列上的应用显示其在不同任务和模型规模上都能带来一致的性能提升。

Conclusion: SAPO为LLMs的强化学习训练提供了更可靠、可扩展和有效的优化策略。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [45] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: 本文提出了一种用于卫星星座的联邦学习算法，通过本地训练、压缩和误差反馈机制减少通信开销，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座的普及，需要解决在这些星座上进行学习任务的问题，特别是设计通信效率高且准确的联邦学习方法。

Method: 采用联邦学习框架，卫星收集并本地处理数据，地面站聚合本地模型。使用本地训练减少通信次数，压缩减少通信量，并引入误差反馈机制提高准确性。

Result: 通过理论分析证明了算法的收敛性，并在真实空间场景的模拟中展示了优于现有技术的性能。

Conclusion: 提出的算法在卫星星座联邦学习中实现了通信效率和模型准确性的良好平衡，误差反馈机制具有更广泛的适用性。

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [46] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: ROOT是一种鲁棒正交化优化器，通过双鲁棒机制解决大语言模型优化中的正交化精度维数脆弱性和异常值诱导噪声问题，在噪声和非凸场景下实现更快收敛和更优性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模扩大，大语言模型对算法不精确性和训练不稳定性的敏感性加剧。现有基于动量正交化的优化器存在正交化精度维数脆弱性和异常值诱导噪声脆弱性两个关键鲁棒性限制。

Method: 提出ROOT优化器：1）使用自适应牛顿迭代和细粒度系数的维数鲁棒正交化方案，确保不同架构配置下的一致性精度；2）通过近端优化的优化鲁棒框架，抑制异常值噪声同时保留有意义的梯度方向。

Result: 大量实验表明，ROOT相比Muon和Adam类优化器实现了显著改进的鲁棒性，具有更快的收敛速度和更优的最终性能，特别是在噪声和非凸场景下。

Conclusion: ROOT为开发能够处理现代大规模模型训练复杂性的鲁棒精确优化器建立了新范式。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [47] [Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning](https://arxiv.org/abs/2511.20349)
*M. E. A. Kherchouche,F. Galpin,T. Dumas,F. Schnitzler,D. Menard,L. Zhang*

Main category: cs.LG

TL;DR: 本文研究了VVC帧内分区的复杂度问题，提出了两种机器学习方法来加速RDO过程中的穷举搜索。第一种是基于回归的方法预测CU的归一化RD成本，第二种是基于强化学习的方法将分区决策建模为MDP问题。两种方法都利用相邻块的RD成本作为输入特征，并通过预设阈值选择合适的分区方式。


<details>
  <summary>Details</summary>
Motivation: 解决VVC帧内分区在RDO过程中穷举搜索带来的高计算复杂度问题，加速编码过程。

Method: 1. 回归方法：预测CU的归一化RD成本；2. 强化学习方法：将分区决策建模为MDP问题，使用DQN算法从两个深度的CU决策轨迹中学习。两种方法都利用相邻块的RD成本作为输入特征，并通过预设阈值进行分区选择。

Result: 提出的两种方法都是尺寸无关的，并成功整合了相邻块的RD成本作为输入特征，能够有效加速VVC帧内分区的RDO过程。

Conclusion: 通过机器学习和强化学习方法可以有效解决VVC帧内分区的复杂度问题，提出的尺寸无关方法具有较好的实用价值，能够显著提升编码效率。

Abstract: In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.

</details>


### [48] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习模型来预测荷兰泽兰河口双壳类软体动物中的河豚毒素污染，识别出日照时间、全球辐射、水温和氯化物浓度是主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 自2012年以来，欧洲温带水域的双壳类软体动物中发现河豚毒素，导致食品安全风险和经济损失，需要早期预测河豚毒素污染。

Method: 使用气象和水文特征作为输入，开发了基于深度学习的可解释模型来预测河豚毒素污染的存在或缺失。

Result: 模型识别出日出时间、日落时间、全球辐射、水温和氯化物浓度对河豚毒素污染贡献最大，表明有效日照时间是重要驱动因素。

Conclusion: 该可解释深度学习模型识别出的环境因素可用于减轻海洋毒素风险，为食品行业和主管部门提供有价值的工具。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [49] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文对Adam优化器中的偏差校正组件进行了系统性研究，发现在最优超参数配置下，偏差校正不会改善最终测试性能，有时甚至有害。


<details>
  <summary>Details</summary>
Motivation: Adam优化器是现代深度学习的基石，但其各个组件的经验必要性常常被理所当然地接受。偏差校正这一组件的贡献仍然缺乏深入理解。

Method: 通过在视觉和语言建模任务上进行一系列系统性消融实验，研究偏差校正的作用。

Result: 在最优超参数配置下，包含偏差校正不会改善最终测试性能；除非实施适当的学习率调度，否则偏差校正有时会对性能产生负面影响。

Conclusion: 偏差校正可被重新解释为一种隐式学习率调度，其行为强烈依赖于平滑超参数β1、β2的选择。研究结果挑战了普遍包含该组件的做法。

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [50] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: UFNO-FiLM通过引入FiLM层解耦标量输入与空间特征，并使用空间加权损失函数，在保持UFNO高频和低频分量优势的同时，显著提升了预测精度，特别是在地下多相流应用中实现了21%的MAE降低。


<details>
  <summary>Details</summary>
Motivation: UFNO虽然通过并行UNet路径改进了FNO的预测精度，但存在两个主要问题：1）将标量输入（如温度、注入速率）作为空间分布场处理，导致在频域中处理冗余的恒定信号；2）标准损失函数未考虑误差敏感性的空间变化，限制了在重要物理区域的性能。

Method: 1. 使用特征线性调制（FiLM）层解耦标量输入与空间特征，避免将恒定信号引入傅里叶变换；2. 采用空间加权损失函数，优先学习关键区域。

Result: 在地下多相流实验中，相比UFNO实现了21%的气体饱和度平均绝对误差（MAE）降低，证明了该方法在提高预测精度方面的有效性。

Conclusion: UFNO-FiLM通过FiLM层和空间加权损失的创新组合，有效解决了UFNO在处理标量输入和空间误差敏感性方面的局限性，为物理场预测提供了更高效的架构。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [51] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文揭示了自适应优化器与归一化最速下降(NSD)之间的紧密联系，提出了自适应平滑性概念，并在凸和非凸设置下建立了自适应优化器的收敛理论，同时证明了自适应平滑性能够实现Nesterov动量加速，并扩展了随机优化中的自适应梯度方差概念。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器在仅适应当前梯度时会退化为归一化最速下降，但两者的分析依赖于不同的几何结构。本文旨在建立自适应优化器的统一理论框架，特别是将自适应平滑性概念扩展到非凸设置，并探索其在加速和随机优化中的应用。

Method: 将自适应平滑性理论扩展到非凸设置，建立自适应优化器的收敛特性；在凸设置下证明自适应平滑性能够实现Nesterov动量加速；引入自适应梯度方差概念，为随机优化提供维度无关的收敛保证。

Result: 证明了自适应平滑性精确刻画了自适应优化器的收敛行为；在凸设置下实现了Nesterov动量加速，这在标准平滑性下对于某些非欧几何是无法实现的；自适应梯度方差为随机优化提供了维度无关的收敛保证。

Conclusion: 自适应平滑性为自适应优化器提供了统一的理论框架，揭示了其与归一化最速下降的深层联系，并在加速和随机优化中展现出优于传统方法的理论保证，特别是在非欧几何设置下。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>

{"id": "2510.16094", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16094", "abs": "https://arxiv.org/abs/2510.16094", "authors": ["Carsten Andrich", "Isabella Varga", "Tobias F. Nowack", "Alexander Ihlow", "Sebastian Giehl", "Michael Schubert", "Reiner S. Thom\u00e4", "Matthias A. Hein"], "title": "Wideband Antenna Deconvolution for Bistatic Millimeter Wave Radar Reflectivity Measurements", "comment": "5 pages, 5 figures, submitted to EuCAP'26", "summary": "Bistatic radar measurements offer unique spatial diversity and enhanced\ntarget characterization capabilities, rendering them increasingly vital for\ncontemporary sensing application research. The reliability of such measurements\nis contingent upon precise system and antenna calibration. The prevailing\ntechnique is the substitution method, which involves the use of known reference\nobjects. We propose an over-the-air calibration algorithm for spherical\nbistatic measurement systems. Our method is both significantly simpler and\ntwice as fast as existing algorithms. The application of our technique to\nreflectivity measurements of a metal sphere from 76 to 81 GHz demonstrates a\ndynamic range enhancement of up to 40 dB when compared with uncalibrated data.\nA comparison with simulation data demonstrates a high degree of agreement\nbetween measurement and simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7403\u9762\u53cc\u57fa\u5730\u6d4b\u91cf\u7cfb\u7edf\u7684\u7a7a\u4e2d\u6821\u51c6\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u7b80\u5355\u4e14\u901f\u5ea6\u5feb\u4e24\u500d\u3002\u572876-81 GHz\u9891\u6bb5\u5bf9\u91d1\u5c5e\u7403\u53cd\u5c04\u7387\u6d4b\u91cf\u7684\u5e94\u7528\u663e\u793a\uff0c\u4e0e\u672a\u6821\u51c6\u6570\u636e\u76f8\u6bd4\uff0c\u52a8\u6001\u8303\u56f4\u63d0\u9ad8\u4e8640 dB\u3002", "motivation": "\u53cc\u57fa\u5730\u96f7\u8fbe\u6d4b\u91cf\u5177\u6709\u72ec\u7279\u7684\u7a7a\u95f4\u591a\u6837\u6027\u548c\u589e\u5f3a\u7684\u76ee\u6807\u8868\u5f81\u80fd\u529b\uff0c\u5728\u73b0\u4ee3\u4f20\u611f\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u6b64\u7c7b\u6d4b\u91cf\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7cbe\u786e\u7684\u7cfb\u7edf\u4e0e\u5929\u7ebf\u6821\u51c6\uff0c\u800c\u5f53\u524d\u4e3b\u6d41\u6280\u672f\u662f\u4f7f\u7528\u5df2\u77e5\u53c2\u8003\u7269\u4f53\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7403\u9762\u53cc\u57fa\u5730\u6d4b\u91cf\u7cfb\u7edf\u7684\u7a7a\u4e2d\u6821\u51c6\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u663e\u8457\u7b80\u5316\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "result": "\u572876-81 GHz\u9891\u6bb5\u5bf9\u91d1\u5c5e\u7403\u8fdb\u884c\u53cd\u5c04\u7387\u6d4b\u91cf\u7684\u5e94\u7528\u8868\u660e\uff0c\u4e0e\u672a\u6821\u51c6\u6570\u636e\u76f8\u6bd4\uff0c\u52a8\u6001\u8303\u56f4\u63d0\u9ad8\u4e8640 dB\u3002\u4e0e\u4eff\u771f\u6570\u636e\u7684\u6bd4\u8f83\u663e\u793a\u6d4b\u91cf\u4e0e\u4eff\u771f\u4e4b\u95f4\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a7a\u4e2d\u6821\u51c6\u7b97\u6cd5\u4e3a\u7403\u9762\u53cc\u57fa\u5730\u6d4b\u91cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u5feb\u901f\u7684\u6821\u51c6\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u91cf\u52a8\u6001\u8303\u56f4\u548c\u4e0e\u4eff\u771f\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.16172", "categories": ["eess.SP", "cs.MS", "51-08", "D.2.2; D.2.8; D.2.13"], "pdf": "https://arxiv.org/pdf/2510.16172", "abs": "https://arxiv.org/abs/2510.16172", "authors": ["J\u00e9rome Eertmans", "Sophie Lequeu", "Beno\u00eet Legat", "Laurent Jacques", "Claude Oestges"], "title": "Fast, Differentiable, GPU-Accelerated Ray Tracing for Multiple Diffraction and Reflection Paths", "comment": "5 pages, 3 figures, submitted to EuCAP 2026", "summary": "We present a fast, differentiable, GPU-accelerated optimization method for\nray path tracing in environments containing planar reflectors and straight\ndiffraction edges. Based on Fermat's principle, our approach reformulates the\npath-finding problem as the minimization of total path length, enabling\nefficient parallel execution on modern GPU architectures. Unlike existing\nmethods that require separate algorithms for reflections and diffractions, our\nunified formulation maintains consistent problem dimensions across all\ninteraction sequences, making it particularly suitable for vectorized\ncomputation. Through implicit differentiation, we achieve efficient gradient\ncomputation without differentiating through solver iterations, significantly\noutperforming traditional automatic differentiation approaches. Numerical\nsimulations demonstrate convergence rates comparable to specialized Newton\nmethods while providing superior scalability for large-scale applications. The\nmethod integrates seamlessly with differentiable programming libraries such as\nJAX and DrJIT, enabling new possibilities in inverse design and optimization\nfor wireless propagation modeling. The source code is openly available at\nhttps://github.com/jeertmans/fpt-jax.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d39\u9a6c\u539f\u7406\u7684\u5feb\u901f\u3001\u53ef\u5fae\u5206GPU\u52a0\u901f\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5305\u542b\u5e73\u9762\u53cd\u5c04\u5668\u548c\u76f4\u7ebf\u884d\u5c04\u8fb9\u7f18\u7684\u73af\u5883\u4e2d\u8ba1\u7b97\u5c04\u7ebf\u8def\u5f84\u8ffd\u8e2a\u3002\u8be5\u65b9\u6cd5\u5c06\u8def\u5f84\u5bfb\u627e\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u603b\u8def\u5f84\u957f\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u652f\u6301\u9ad8\u6548\u7684\u5e76\u884cGPU\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u53cd\u5c04\u548c\u884d\u5c04\u5206\u522b\u4f7f\u7528\u4e0d\u540c\u7684\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5904\u7406\u6846\u67b6\u3002\u4e3a\u4e86\u5728\u65e0\u7ebf\u4f20\u64ad\u5efa\u6a21\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u9006\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5904\u7406\u5404\u79cd\u4ea4\u4e92\u5e8f\u5217\u4e14\u9002\u5408\u5411\u91cf\u5316\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8d39\u9a6c\u539f\u7406\uff0c\u5c06\u8def\u5f84\u5bfb\u627e\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u603b\u8def\u5f84\u957f\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u3002\u901a\u8fc7\u9690\u5f0f\u5fae\u5206\u5b9e\u73b0\u9ad8\u6548\u68af\u5ea6\u8ba1\u7b97\uff0c\u907f\u514d\u901a\u8fc7\u6c42\u89e3\u5668\u8fed\u4ee3\u8fdb\u884c\u5fae\u5206\u3002\u8be5\u65b9\u6cd5\u4e0eJAX\u548cDrJIT\u7b49\u53ef\u5fae\u5206\u7f16\u7a0b\u5e93\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u6536\u655b\u901f\u5ea6\u4e0e\u4e13\u95e8\u7684\u725b\u987f\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u63d0\u4f9b\u4f18\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002\u76f8\u6bd4\u4f20\u7edf\u7684\u81ea\u52a8\u5fae\u5206\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u7ebf\u4f20\u64ad\u5efa\u6a21\u4e2d\u7684\u9006\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5b9e\u73b0\u4e86\u53cd\u5c04\u548c\u884d\u5c04\u7684\u7edf\u4e00\u5904\u7406\uff0c\u5e76\u652f\u6301\u9ad8\u6548\u7684GPU\u5e76\u884c\u8ba1\u7b97\u3002"}}
{"id": "2510.15940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15940", "abs": "https://arxiv.org/abs/2510.15940", "authors": ["Jialin Lu", "Kye Emond", "Kaiyu Yang", "Swarat Chaudhuri", "Weiran Sun", "Wuyang Chen"], "title": "Lean Finder: Semantic Search for Mathlib That Understands User Intents", "comment": null, "summary": "We present Lean Finder, a semantic search engine for Lean and mathlib that\nunderstands and aligns with the intents of mathematicians. Progress in formal\ntheorem proving is often hindered by the difficulty of locating relevant\ntheorems and the steep learning curve of the Lean 4 language, making\nadvancement slow and labor-intensive. Existing Lean search engines, though\nhelpful, rely primarily on informalizations (natural language translation of\nthe formal statements), while largely overlooking the mismatch with real-world\nuser queries. In contrast, we propose a user-centered semantic search tailored\nto the needs of mathematicians. Our approach begins by analyzing and clustering\nthe semantics of public Lean discussions, then fine-tuning text embeddings on\nsynthesized queries that emulate user intents. We further align Lean Finder\nwith mathematicians' preferences using diverse feedback signals, encoding it\nwith a rich awareness of their goals from multiple perspectives. Evaluations on\nreal-world queries, informalized statements, and proof states demonstrate that\nour Lean Finder achieves over $30\\%$ relative improvement compared to previous\nsearch engines and GPT-4o. In addition, Lean Finder is compatible with\nLLM-based theorem provers, bridging retrieval with formal reasoning. Lean\nFinder is available at: https://leanfinder.github.io", "AI": {"tldr": "Lean Finder\u662f\u4e00\u4e2a\u9488\u5bf9Lean\u548cmathlib\u7684\u8bed\u4e49\u641c\u7d22\u5f15\u64ce\uff0c\u901a\u8fc7\u7406\u89e3\u6570\u5b66\u5bb6\u610f\u56fe\u3001\u5206\u6790\u516c\u5f00\u8ba8\u8bba\u8bed\u4e49\u3001\u5fae\u8c03\u6587\u672c\u5d4c\u5165\u548c\u591a\u6837\u5316\u53cd\u9988\u4fe1\u53f7\uff0c\u76f8\u6bd4\u73b0\u6709\u641c\u7d22\u5f15\u64ce\u548cGPT-4o\u5b9e\u73b0\u4e8630%\u4ee5\u4e0a\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709Lean\u641c\u7d22\u5f15\u64ce\u4e3b\u8981\u4f9d\u8d56\u975e\u5f62\u5f0f\u5316\u7ffb\u8bd1\uff0c\u5ffd\u89c6\u4e86\u4e0e\u73b0\u5b9e\u7528\u6237\u67e5\u8be2\u7684\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u5b9a\u7406\u8bc1\u660e\u8fdb\u5c55\u7f13\u6162\u4e14\u8d39\u529b\u3002", "method": "\u5206\u6790\u5e76\u805a\u7c7b\u516c\u5f00Lean\u8ba8\u8bba\u7684\u8bed\u4e49\uff0c\u5728\u6a21\u62df\u7528\u6237\u610f\u56fe\u7684\u5408\u6210\u67e5\u8be2\u4e0a\u5fae\u8c03\u6587\u672c\u5d4c\u5165\uff0c\u4f7f\u7528\u591a\u6837\u5316\u53cd\u9988\u4fe1\u53f7\u4e0e\u6570\u5b66\u5bb6\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u3001\u975e\u5f62\u5f0f\u5316\u9648\u8ff0\u548c\u8bc1\u660e\u72b6\u6001\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4\u4e4b\u524d\u641c\u7d22\u5f15\u64ce\u548cGPT-4o\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "Lean Finder\u662f\u4e00\u4e2a\u7528\u6237\u4e2d\u5fc3\u7684\u8bed\u4e49\u641c\u7d22\u5de5\u5177\uff0c\u80fd\u591f\u4e0e\u57fa\u4e8eLLM\u7684\u5b9a\u7406\u8bc1\u660e\u5668\u517c\u5bb9\uff0c\u6865\u63a5\u68c0\u7d22\u4e0e\u5f62\u5f0f\u63a8\u7406\u3002"}}
{"id": "2510.15944", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15944", "abs": "https://arxiv.org/abs/2510.15944", "authors": ["Tianyu Bell Pan", "Mengdi Zhu", "Alexa Jordyn Cole", "Ronald Wilson", "Damon L. Woodard"], "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift", "comment": null, "summary": "Multimodal learning systems often struggle in non-stationary environments due\nto concept drift, where changing data distributions can degrade performance.\nModality-specific drifts and the lack of mechanisms for continuous, stable\nadaptation compound this challenge. This paper introduces LS-OGD, a novel\nadaptive control framework for robust multimodal learning in the presence of\nconcept drift. LS-OGD uses an online controller that dynamically adjusts the\nmodel's learning rate and the fusion weights between different data modalities\nin response to detected drift and evolving prediction errors. We prove that\nunder bounded drift conditions, the LS-OGD system's prediction error is\nuniformly ultimately bounded and converges to zero if the drift ceases.\nAdditionally, we demonstrate that the adaptive fusion strategy effectively\nisolates and mitigates the impact of severe modality-specific drift, thereby\nensuring system resilience and fault tolerance. These theoretical guarantees\nestablish a principled foundation for developing reliable and continuously\nadapting multimodal learning systems.", "AI": {"tldr": "LS-OGD\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u548c\u6a21\u6001\u878d\u5408\u6743\u91cd\u6765\u5e94\u5bf9\u6982\u5ff5\u6f02\u79fb\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9762\u4e34\u6982\u5ff5\u6f02\u79fb\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6a21\u6001\u7279\u5b9a\u7684\u6f02\u79fb\u548c\u7f3a\u4e4f\u6301\u7eed\u7a33\u5b9a\u9002\u5e94\u673a\u5236\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5b66\u4e60\u7387\u548c\u4e0d\u540c\u6570\u636e\u6a21\u6001\u4e4b\u95f4\u7684\u878d\u5408\u6743\u91cd\uff0c\u54cd\u5e94\u68c0\u6d4b\u5230\u7684\u6f02\u79fb\u548c\u9884\u6d4b\u8bef\u5dee\u53d8\u5316\u3002", "result": "\u5728\u6709\u9650\u6f02\u79fb\u6761\u4ef6\u4e0b\uff0cLS-OGD\u7cfb\u7edf\u7684\u9884\u6d4b\u8bef\u5dee\u88ab\u8bc1\u660e\u662f\u5747\u5300\u6700\u7ec8\u6709\u754c\u7684\uff0c\u5982\u679c\u6f02\u79fb\u505c\u6b62\u5219\u6536\u655b\u5230\u96f6\uff1b\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u80fd\u6709\u6548\u9694\u79bb\u548c\u51cf\u8f7b\u6a21\u6001\u7279\u5b9a\u6f02\u79fb\u7684\u5f71\u54cd\u3002", "conclusion": "LS-OGD\u4e3a\u5f00\u53d1\u53ef\u9760\u4e14\u6301\u7eed\u81ea\u9002\u5e94\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u9762\u5bf9\u6982\u5ff5\u6f02\u79fb\u65f6\u7684\u97e7\u6027\u548c\u5bb9\u9519\u80fd\u529b\u3002"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u4f5c\u4e3a\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u54f2\u5b66\u89c1\u89e3\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u5f3a\u8c03\u667a\u80fd\u662f\u6267\u884c\u8fc7\u7a0b\u800c\u975e\u5c5e\u6027\uff0c\u901a\u8fc7\u529f\u80fd\u5206\u79bb\u5b9e\u73b0\u66f4\u8fde\u8d2f\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u8ba4\u77e5\u7406\u89e3\u7684\u95ee\u9898\uff0c\u586b\u8865\u8ba4\u77e5\u67b6\u6784\u7684\u7a7a\u767d\uff0c\u4ece\u672c\u4f53\u8bba\u8f6c\u5411\u8ba4\u8bc6\u8bba\u89c6\u89d2\u63a2\u7d22\u8ba4\u77e5\u6d8c\u73b0\u7684\u6761\u4ef6\u3002", "method": "\u57fa\u4e8e\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u6784\u5efa\u5305\u542b\u5224\u65ad\u3001\u8bb0\u5fc6\u3001\u63a7\u5236\u3001\u884c\u52a8\u548c\u8c03\u8282\u7684\u8fde\u7eed\u5faa\u73af\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u54f2\u5b66\u7406\u8bba\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\u3002", "result": "SCL\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u8bba\uff0c\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u6bd4\u5355\u4e00\u63d0\u793a\u7cfb\u7edf\u4ea7\u751f\u66f4\u8fde\u8d2f\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "conclusion": "\u771f\u6b63\u7684\u8fdb\u6b65\u9700\u8981\u5b9e\u73b0\u8ba4\u77e5\u539f\u5219\u7684\u7ed3\u6784\u5316\u67b6\u6784\u800c\u975e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u5bf9\u5fc3\u667a\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548cAI\u4ea7\u751f\u91cd\u8981\u5f71\u54cd\uff0c\u5c06\u77e5\u8bc6\u89c6\u4e3a\u5728\u73b0\u8c61\u5b66\u8fde\u8d2f\u5faa\u73af\u4e2d\u7684\u6301\u7eed\u91cd\u5efa\u3002"}}
{"id": "2510.16397", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16397", "abs": "https://arxiv.org/abs/2510.16397", "authors": ["Yiming Xu", "Dongfang Xu", "Shenghui Song", "Dusit Niyato"], "title": "Adaptive Sensing Performance Design for Enhancing Secure Communication in Networked ISAC Systems", "comment": "16 pages", "summary": "The channel state information (CSI) of an eavesdropper is crucial for\nphysical layer security (PLS) design, but it is difficult to obtain due to the\npassive and non-cooperative nature of the eavesdropper. To this end, integrated\nsensing and communication (ISAC) offers a novel solution by estimating the CSI\nof the eavesdropper based on sensing information. However, existing studies\nnormally impose explicit and fixed sensing performance requirement without\nconsidering the varying communication conditions, which hinders the system from\nfully exploiting the synergy between sensing and communication. To address this\nissue, this paper proposes sensing-enhanced secure communication with adaptive\nsensing performance. Specifically, we formulate the sensing performance\nimplicitly in the information leakage rate and adaptively optimize it for the\nminimization of the power consumption, offering enhanced flexibility and\nadaptability in sensing performance. We consider both centralized and\ndecentralized designs to thoroughly investigate the impact of network structure\non system performance and complexity. Specifically, we devise a block\ncoordinate descent (BCD)-based method for centralized design. For decentralized\ndesign, we develop an optimization framework based on consensus alternating\ndirection method of multipliers (ADMM) to reduce complexity and information\nexchange overhead. Experimental results demonstrate the advantage of the\nproposed implicit sensing performance requirement design due to its capability\nto adaptively adjust the sensing performance to enhance the system performance\nfor varying system configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u611f\u77e5\u6027\u80fd\u7684\u611f\u77e5\u589e\u5f3a\u5b89\u5168\u901a\u4fe1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u611f\u77e5\u6027\u80fd\u9690\u542b\u5728\u4fe1\u606f\u6cc4\u6f0f\u7387\u4e2d\u5e76\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u4ee5\u6700\u5c0f\u5316\u529f\u8017\uff0c\u4e3a\u7269\u7406\u5c42\u5b89\u5168\u63d0\u4f9b\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7531\u4e8e\u7a83\u542c\u8005\u7684\u88ab\u52a8\u548c\u975e\u5408\u4f5c\u6027\u8d28\uff0c\u83b7\u53d6\u5176\u4fe1\u9053\u72b6\u6001\u4fe1\u606f(CSI)\u5bf9\u4e8e\u7269\u7406\u5c42\u5b89\u5168\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u4f46\u5f88\u56f0\u96be\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u65bd\u52a0\u660e\u786e\u56fa\u5b9a\u7684\u611f\u77e5\u6027\u80fd\u8981\u6c42\uff0c\u800c\u4e0d\u8003\u8651\u53d8\u5316\u7684\u901a\u4fe1\u6761\u4ef6\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u5145\u5206\u5229\u7528\u611f\u77e5\u4e0e\u901a\u4fe1\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u589e\u5f3a\u5b89\u5168\u901a\u4fe1\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u6027\u80fd\u9690\u542b\u5728\u4fe1\u606f\u6cc4\u6f0f\u7387\u4e2d\u5e76\u81ea\u9002\u5e94\u4f18\u5316\u3002\u8bbe\u8ba1\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u4e24\u79cd\u65b9\u6848\uff1a\u96c6\u4e2d\u5f0f\u91c7\u7528\u57fa\u4e8e\u5757\u5750\u6807\u4e0b\u964d(BCD)\u7684\u65b9\u6cd5\uff1b\u5206\u5e03\u5f0f\u91c7\u7528\u57fa\u4e8e\u5171\u8bc6\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5(ADMM)\u7684\u4f18\u5316\u6846\u67b6\u4ee5\u964d\u4f4e\u590d\u6742\u6027\u548c\u4fe1\u606f\u4ea4\u6362\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u9690\u5f0f\u611f\u77e5\u6027\u80fd\u8981\u6c42\u8bbe\u8ba1\u5177\u6709\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u611f\u77e5\u6027\u80fd\uff0c\u4ee5\u589e\u5f3a\u4e0d\u540c\u7cfb\u7edf\u914d\u7f6e\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u6027\u80fd\u8bbe\u8ba1\uff0c\u4e3a\u7269\u7406\u5c42\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a83\u542c\u8005CSI\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u5145\u5206\u5229\u7528\u4e86\u611f\u77e5\u4e0e\u901a\u4fe1\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002"}}
{"id": "2510.16509", "categories": ["stat.ML", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.16509", "abs": "https://arxiv.org/abs/2510.16509", "authors": ["Ziad Ghanem", "Chang Hyunwoong", "Preskella Mrad"], "title": "A Bayesian Framework for Symmetry Inference in Chaotic Attractors", "comment": null, "summary": "Detecting symmetry from data is a fundamental problem in signal analysis,\nproviding insight into underlying structure and constraints. When data emerge\nas trajectories of dynamical systems, symmetries encode structural properties\nof the dynamics that enable model reduction, principled comparison across\nconditions, and detection of regime changes. While recent optimal transport\nmethods provide practical tools for data-driven symmetry detection in this\nsetting, they rely on deterministic thresholds and lack uncertainty\nquantification, limiting robustness to noise and ability to resolve\nhierarchical symmetry structures. We present a Bayesian framework that\nformulates symmetry detection as probabilistic model selection over a lattice\nof candidate subgroups, using a Gibbs posterior constructed from Wasserstein\ndistances between observed data and group-transformed copies. We establish\nthree theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal\nsymmetry consistent with data, $(ii)$ conjugation equivariance ensuring\nframe-independence, and $(iii)$ stability bounds under perturbations for\nrobustness to noise. Posterior inference is performed via Metropolis-Hastings\nsampling and numerical experiments on equivariant dynamical systems and\nsynthetic point clouds demonstrate accurate symmetry recovery under high noise\nand small sample sizes. An application to human gait dynamics reveals symmetry\nchanges induced by mechanical constraints, demonstrating the framework's\nutility for statistical inference in biomechanical and dynamical systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u6846\u67b6\u7528\u4e8e\u4ece\u52a8\u6001\u7cfb\u7edf\u6570\u636e\u4e2d\u68c0\u6d4b\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u5c06\u5bf9\u79f0\u6027\u68c0\u6d4b\u5efa\u6a21\u4e3a\u5728\u5019\u9009\u5b50\u7fa4\u683c\u4e0a\u7684\u6982\u7387\u6a21\u578b\u9009\u62e9\uff0c\u4f7f\u7528\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684Gibbs\u540e\u9a8c\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u5bf9\u566a\u58f0\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u7cfb\u7edf\u6570\u636e\u4e2d\u7684\u5bf9\u79f0\u6027\u68c0\u6d4b\u662f\u4fe1\u53f7\u5206\u6790\u7684\u57fa\u672c\u95ee\u9898\uff0c\u80fd\u591f\u63ed\u793a\u5e95\u5c42\u7ed3\u6784\u548c\u7ea6\u675f\u3002\u73b0\u6709\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u9608\u503c\u4e14\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u548c\u5206\u5c42\u5bf9\u79f0\u7ed3\u6784\u7684\u89e3\u6790\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5c06\u5bf9\u79f0\u6027\u68c0\u6d4b\u8868\u8ff0\u4e3a\u5728\u5019\u9009\u5b50\u7fa4\u683c\u4e0a\u7684\u6982\u7387\u6a21\u578b\u9009\u62e9\uff0c\u4f7f\u7528\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684Gibbs\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7Metropolis-Hastings\u91c7\u6837\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u566a\u58f0\u548c\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u80fd\u591f\u51c6\u786e\u6062\u590d\u5bf9\u79f0\u6027\u3002\u5728\u4eba\u6b65\u6001\u52a8\u529b\u5b66\u5e94\u7528\u4e2d\u63ed\u793a\u4e86\u673a\u68b0\u7ea6\u675f\u5f15\u8d77\u7684\u5bf9\u79f0\u6027\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u7269\u529b\u5b66\u548c\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u7edf\u8ba1\u63a8\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16244", "categories": ["stat.AP", "62R10, 91D20"], "pdf": "https://arxiv.org/pdf/2510.16244", "abs": "https://arxiv.org/abs/2510.16244", "authors": ["Zhe Michelle Dong", "Han Lin Shang", "Francis Hui", "Aaron Bruhn"], "title": "A Compositional Approach to Modelling Cause-specific Mortality with Zero Counts", "comment": "42 pages, 14 figures, 5 tables", "summary": "Understanding and forecasting mortality by cause is an essential branch of\nactuarial science, with wide-ranging implications for decision-makers in public\npolicy and industry. To accurately capture trends in cause-specific mortality,\nit is critical to consider dependencies between causes of death and produce\nforecasts by age and cause coherent with aggregate mortality forecasts. One way\nto achieve these aims is to model cause-specific deaths using compositional\ndata analysis (CODA), treating the density of deaths by age and cause as a set\nof dependent, non-negative values that sum to one. A major drawback of standard\nCODA methods is the challenge of zero values, which frequently occur in\ncause-of-death mortality modelling. Thus, we propose using a compositional\npower transformation, the $\\alpha$-transformation, to model cause-specific\nlife-table death counts. The $\\alpha$-transformation offers a statistically\nrigorous approach to handling zero value subgroups in CODA compared to\n\\emph{ad-hoc} techniques: adding an arbitrarily small amount. We illustrate the\n$\\alpha$-transformation on England and Wales, and US death counts by cause from\nthe Human Cause-of-Death database, for cardiovascular-related causes of death.\nResults demonstrate the $\\alpha$-transformation improves forecast accuracy of\ncause-specific life-table death counts compared with log-ratio-based CODA\ntransformations. The forecasts suggest declines in proportions of deaths from\nmajor cardiovascular causes (myocardial infarction and other ischemic heart\ndiseases (IHD)).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u03b1-\u53d8\u6362\u7684\u7ec4\u6210\u6570\u636e\u5206\u6790\u65b9\u6cd5\u6765\u6539\u8fdb\u7279\u5b9a\u6b7b\u56e0\u6b7b\u4ea1\u7387\u9884\u6d4b\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5904\u7406\u96f6\u503c\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u5bf9\u6570\u6bd4\u53d8\u6362\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u7279\u5b9a\u6b7b\u56e0\u6b7b\u4ea1\u7387\u5bf9\u7cbe\u7b97\u79d1\u5b66\u548c\u516c\u5171\u653f\u7b56\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8003\u8651\u6b7b\u56e0\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4e0e\u603b\u4f53\u6b7b\u4ea1\u7387\u9884\u6d4b\u4fdd\u6301\u4e00\u81f4\u3002\u6807\u51c6\u7ec4\u6210\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u96f6\u503c\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7ec4\u6210\u6570\u636e\u5206\u6790\u548c\u03b1-\u53d8\u6362\u6765\u5efa\u6a21\u7279\u5b9a\u6b7b\u56e0\u751f\u547d\u8868\u6b7b\u4ea1\u8ba1\u6570\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u7ec4\u6210\u6570\u636e\u4e2d\u7684\u96f6\u503c\u5b50\u7ec4\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e25\u8c28\u7684\u65b9\u6cd5\u3002", "result": "\u03b1-\u53d8\u6362\u76f8\u6bd4\u57fa\u4e8e\u5bf9\u6570\u6bd4\u7684\u7ec4\u6210\u6570\u636e\u53d8\u6362\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7279\u5b9a\u6b7b\u56e0\u751f\u547d\u8868\u6b7b\u4ea1\u8ba1\u6570\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002\u9884\u6d4b\u663e\u793a\u4e3b\u8981\u5fc3\u8840\u7ba1\u75be\u75c5\u6b7b\u4ea1\u6bd4\u4f8b\u5448\u4e0b\u964d\u8d8b\u52bf\u3002", "conclusion": "\u03b1-\u53d8\u6362\u4e3a\u5904\u7406\u7279\u5b9a\u6b7b\u56e0\u6b7b\u4ea1\u7387\u5efa\u6a21\u4e2d\u7684\u96f6\u503c\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u6b7b\u4ea1\u7387\u8d8b\u52bf\u3002"}}
{"id": "2510.16495", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16495", "abs": "https://arxiv.org/abs/2510.16495", "authors": ["Muhammad Khalil", "Ke Wang", "Jinho Choi"], "title": "Performance Evaluation of High Power Microwave Systems Against UAVs A Probabilistic Antenna Propagation Framework with Sensitivity Analysis", "comment": "10", "summary": "We develop a probabilistic, antenna- and propagation-centric framework to\nquantify the effectiveness of high-power microwave (HPM) engagements against\nunmanned aerial vehicles (UAVs). The model couples stochastic UAV kinematics, a\nbeam-steering jitter-to-gain mapping, and atmospheric propagation (free-space\nspreading with gaseous and rain loss) to obtain closed-form statistics of the\nreceived pulse energy. From these, we derive analytically evaluable per-pulse\nand cumulative neutralization probabilities using log-normal closures and\nGaussian--Hermite quadrature, and we provide a dwell-time expression under a\nstandard pulse-independence assumption. Analytical predictions closely match\nlarge-scale Monte-Carlo simulations across broad parameter ranges. For a\nrepresentative commercial threshold $E_{\\mathrm{th}} = 10^{-2}\\,\\mathrm{J}$,\nthe model predicts $\\bar{P}_{\\mathrm{kill}} \\gtrsim 0.4$ per pulse and\n$P_{\\mathrm{kill,tot}} > 99\\%$ within about $0.1\\,\\mathrm{s}$ at kHz PRF; for\nhardened platforms with $E_{\\mathrm{th}} = 10^{-1}\\,\\mathrm{J}$,\n$\\bar{P}_{\\mathrm{kill}} < 1\\%$ and $P_{\\mathrm{kill,tot}} < 20\\%$ after\n$1\\,\\mathrm{s}$. A closed-form sensitivity (elasticity) analysis shows\nperformance is dominated by slant range ($S_{\\bar{R}} \\approx -2$), with strong\nsecondary dependence on aperture diameter and transmit power; pointing jitter\nand atmospheric variability are comparatively less influential in the evaluated\nregimes. The framework yields fast, accurate, and physics-faithful performance\npredictions and exposes clear antenna/propagation design levers for HPM system\nsizing and risk-aware mission planning.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6982\u7387\u6846\u67b6\u6765\u91cf\u5316\u9ad8\u529f\u7387\u5fae\u6ce2\u5bf9\u6297\u65e0\u4eba\u673a\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u8026\u5408\u968f\u673a\u65e0\u4eba\u673a\u8fd0\u52a8\u5b66\u3001\u6ce2\u675f\u6307\u5411\u6296\u52a8-\u589e\u76ca\u6620\u5c04\u548c\u5927\u6c14\u4f20\u64ad\uff0c\u63a8\u5bfc\u51fa\u63a5\u6536\u8109\u51b2\u80fd\u91cf\u7684\u95ed\u5f0f\u7edf\u8ba1\u91cf\uff0c\u5e76\u8ba1\u7b97\u8109\u51b2\u4e2d\u548c\u6982\u7387\u3002", "motivation": "\u9700\u8981\u91cf\u5316\u9ad8\u529f\u7387\u5fae\u6ce2\u5bf9\u6297\u65e0\u4eba\u673a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u548c\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u5feb\u901f\u51c6\u786e\u7684\u6027\u80fd\u9884\u6d4b\u3002", "method": "\u91c7\u7528\u6982\u7387\u6027\u3001\u5929\u7ebf\u548c\u4f20\u64ad\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\u65e0\u4eba\u673a\u8fd0\u52a8\u5b66\u3001\u6ce2\u675f\u6307\u5411\u6296\u52a8-\u589e\u76ca\u6620\u5c04\u548c\u5927\u6c14\u4f20\u64ad\uff0c\u4f7f\u7528\u5bf9\u6570\u6b63\u6001\u95ed\u5305\u548c\u9ad8\u65af-\u57c3\u5c14\u7c73\u7279\u6c42\u79ef\u63a8\u5bfc\u53ef\u89e3\u6790\u8ba1\u7b97\u7684\u8109\u51b2\u4e2d\u548c\u6982\u7387\u3002", "result": "\u5bf9\u4e8e\u5546\u4e1a\u65e0\u4eba\u673a\uff08\u9608\u503c0.01J\uff09\uff0c\u6a21\u578b\u9884\u6d4b\u6bcf\u8109\u51b2\u6740\u4f24\u6982\u7387\u22650.4\uff0c\u5728kHz\u8109\u51b2\u91cd\u590d\u9891\u7387\u4e0b0.1\u79d2\u5185\u603b\u6740\u4f24\u6982\u7387>99%\uff1b\u5bf9\u4e8e\u52a0\u56fa\u5e73\u53f0\uff08\u9608\u503c0.1J\uff09\uff0c\u6bcf\u8109\u51b2\u6740\u4f24\u6982\u7387<1%\uff0c1\u79d2\u540e\u603b\u6740\u4f24\u6982\u7387<20%\u3002\u6027\u80fd\u4e3b\u8981\u53d7\u659c\u8ddd\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u51c6\u786e\u4e14\u7b26\u5408\u7269\u7406\u7684\u6027\u80fd\u9884\u6d4b\uff0c\u63ed\u793a\u4e86HPM\u7cfb\u7edf\u8bbe\u8ba1\u548c\u98ce\u9669\u611f\u77e5\u4efb\u52a1\u89c4\u5212\u7684\u6e05\u6670\u5929\u7ebf/\u4f20\u64ad\u8bbe\u8ba1\u6760\u6746\u3002"}}
{"id": "2510.16551", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16551", "abs": "https://arxiv.org/abs/2510.16551", "authors": ["Khaled Boughanmi", "Kamel Jedidi", "Nour Jedidi"], "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction", "comment": null, "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u4ea7\u54c1\u548c\u670d\u52a1\u5c5e\u6027\u3001\u7279\u5f81\u53ca\u76f8\u5173\u60c5\u611f\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8425\u9500\u7406\u8bba\uff0c\u533a\u5206\u611f\u77e5\u5c5e\u6027\u548c\u53ef\u64cd\u4f5c\u7279\u5f81\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u4e14\u5177\u6709\u7ba1\u7406\u6307\u5bfc\u610f\u4e49\u7684\u89c1\u89e3\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u7f16\u7801\u5ba2\u6237\u8bc4\u8bba\u8017\u65f6\u8017\u529b\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5904\u7406\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u63d0\u53d6\u5ba2\u6237\u8bc4\u8bba\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7ba1\u7406\u6d1e\u5bdf\u3002", "method": "\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e868\u79cd\u63d0\u793a\u53d8\u4f53\uff0c\u5e94\u7528\u4e8e20,000\u6761Yelp\u661f\u5df4\u514b\u8bc4\u8bba\u3002\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u4e00\u81f4\u6027\u548c\u5ba2\u6237\u8bc4\u5206\u9884\u6d4b\u6709\u6548\u6027\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793aLLM\u4e0e\u4eba\u5de5\u7f16\u7801\u8005\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u4e14\u9884\u6d4b\u6709\u6548\u6027\u5f88\u5f3a\u3002LLM\u5904\u7406\u6bcf\u6761\u8bc4\u8bba\u4ec5\u97002\u79d2\uff0c\u800c\u4eba\u5de5\u7f16\u7801\u4e2d\u4f4d\u65f6\u95f4\u4e3a6\u5206\u949f\uff0c\u5b9e\u73b0\u4e86\u4eba\u5de5\u65e0\u6cd5\u8fbe\u5230\u7684\u5927\u89c4\u6a21\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u5f71\u54cd\u5ba2\u6237\u6ee1\u610f\u5ea6\u7684\u5173\u952e\u5c5e\u6027\u548c\u7279\u5f81\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5b9a\u4f4d\"\u6109\u60a6\u70b9\"\u3001\u89e3\u51b3\"\u75db\u70b9\"\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u3002\u6a21\u62df\u8868\u660e\u4f18\u5316\u5173\u952e\u670d\u52a1\u7279\u5f81\u60c5\u611f\u53ef\u83b7\u5f971-2%\u7684\u5355\u5e97\u5e73\u5747\u6536\u5165\u589e\u957f\u3002"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\uff0c\u5373\u4f7f\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\u8ba9\u6a21\u578b\u80fd\u591f\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\uff0c\u4e5f\u65e0\u6cd5\u907f\u514d\u8fd9\u79cd\u5d29\u6e83\u3002\u6a21\u578b\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u5176\u6a21\u5f0f\u662f\u5426\u5339\u914d\u6b63\u786e\u89e3\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u6027\u80fd\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\uff0c\u68c0\u9a8c\u662f\u5426\u7531\u4e8e\u6a21\u578b\u9700\u8981\u81ea\u884c\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\u800c\u5bfc\u81f4\u8bc4\u4f30\u6df7\u6dc6\u3002", "method": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u5176\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u79fb\u52a8\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\u5e76\u91cd\u65b0\u63d0\u793a\u4e0b\u4e00\u6b65\u3002\u8fdb\u884cLLM\u53c2\u6570\u5316\u7b56\u7565\u5206\u6790\u3002", "result": "\u73af\u5883\u63a5\u53e3\u7684\u8bbf\u95ee\u5e76\u4e0d\u80fd\u5ef6\u8fdf\u6216\u6d88\u9664\u6027\u80fd\u5d29\u6e83\u3002\u6a21\u578b\u7b56\u7565\u4e0e\u6700\u4f18\u7b56\u7565\u548c\u5747\u5300\u968f\u673a\u7b56\u7565\u7684\u5dee\u5f02\u9010\u6e10\u589e\u5927\uff0c\u8868\u660e\u5728\u6bcf\u79cd\u590d\u6742\u5ea6\u7ea7\u522b\u90fd\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u6a21\u5f0f\u662f\u5426\u53cd\u6620\u95ee\u9898\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u786e\u5b9e\u5b58\u5728\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\uff0c\u8fd9\u79cd\u5d29\u6e83\u4e0e\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\u65e0\u5173\uff0c\u800c\u662f\u6e90\u4e8e\u6a21\u578b\u672c\u8eab\u7684\u6a21\u5f0f\u5d29\u6e83\u884c\u4e3a\u3002\u7c7b\u4f3c\u73b0\u8c61\u53ef\u80fd\u4e5f\u5b58\u5728\u4e8e\u5176\u4ed6\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u3002"}}
{"id": "2510.15950", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15950", "abs": "https://arxiv.org/abs/2510.15950", "authors": ["Arianna Francesconi", "Donato Cappetta", "Fabio Rebecchi", "Paolo Soda", "Valerio Guarrasi", "Rosa Sicilia"], "title": "Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics", "comment": "Proceedings of the Workshop on Artificial Intelligence for Biomedical\n  Data (AIBio 2025), 28th European Conference on Artificial Intelligence 2025,\n  Springer CCIS", "summary": "Parkinson's disease (PD) presents a growing global challenge, affecting over\n10 million individuals, with prevalence expected to double by 2040. Early\ndiagnosis remains difficult due to the late emergence of motor symptoms and\nlimitations of traditional clinical assessments. In this study, we propose a\nnovel pipeline that leverages keystroke dynamics as a non-invasive and scalable\nbiomarker for remote PD screening and telemonitoring. Our methodology involves\nthree main stages: (i) preprocessing of data from four distinct datasets,\nextracting four temporal signals and addressing class imbalance through the\ncomparison of three methods; (ii) pre-training eight state-of-the-art\ndeep-learning architectures on the two largest datasets, optimizing temporal\nwindowing, stride, and other hyperparameters; (iii) fine-tuning on an\nintermediate-sized dataset and performing external validation on a fourth,\nindependent cohort. Our results demonstrate that hybrid convolutional-recurrent\nand transformer-based models achieve strong external validation performance,\nwith AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal\nconvolutional model attains an AUC-ROC of 91.14% in external validation,\noutperforming existing methods that rely solely on internal validation. These\nfindings underscore the potential of keystroke dynamics as a reliable digital\nbiomarker for PD, offering a promising avenue for early detection and\ncontinuous monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fb\u952e\u52a8\u529b\u5b66\u7684\u65b0\u578b\u7ba1\u9053\uff0c\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\u7684\u8fdc\u7a0b\u7b5b\u67e5\u548c\u8fdc\u7a0b\u76d1\u6d4b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u53d6\u5f97\u4e86\u8d85\u8fc790%\u7684AUC-ROC\u548c70%\u4ee5\u4e0a\u7684F1-Score\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u5f71\u54cd\u5168\u7403\u8d85\u8fc71000\u4e07\u4eba\uff0c\u9884\u8ba1\u52302040\u5e74\u60a3\u75c5\u7387\u5c06\u7ffb\u500d\u3002\u7531\u4e8e\u8fd0\u52a8\u75c7\u72b6\u51fa\u73b0\u8f83\u665a\u4e14\u4f20\u7edf\u4e34\u5e8a\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e9\u671f\u8bca\u65ad\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a(i) \u9884\u5904\u7406\u56db\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6570\u636e\uff0c\u63d0\u53d6\u56db\u4e2a\u65f6\u95f4\u4fe1\u53f7\u5e76\u6bd4\u8f83\u4e09\u79cd\u65b9\u6cd5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b(ii) \u5728\u4e24\u4e2a\u6700\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u516b\u79cd\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4f18\u5316\u65f6\u95f4\u7a97\u53e3\u3001\u6b65\u957f\u7b49\u8d85\u53c2\u6570\uff1b(iii) \u5728\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u7b2c\u56db\u4e2a\u72ec\u7acb\u961f\u5217\u4e2d\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002", "result": "\u6df7\u5408\u5377\u79ef-\u5faa\u73af\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\uff0cAUC-ROC\u5f97\u5206\u8d85\u8fc790%\uff0cF1-Score\u8d85\u8fc770%\u3002\u7279\u522b\u662f\u65f6\u95f4\u5377\u79ef\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8fbe\u523091.14%\u7684AUC-ROC\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u5185\u90e8\u9a8c\u8bc1\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u51fb\u952e\u52a8\u529b\u5b66\u4f5c\u4e3a\u5e15\u91d1\u68ee\u75c5\u53ef\u9760\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\u7684\u6f5c\u529b\uff0c\u4e3a\u65e9\u671f\u68c0\u6d4b\u548c\u6301\u7eed\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2510.16963", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16963", "abs": "https://arxiv.org/abs/2510.16963", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ismail Guvenc"], "title": "Stochastic Geometry Analysis of Asymmetric Uplink Interference for Urban UAV-RC Networks", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) have emerged as a flexible platform for\nproviding coverage over challenging environments, particularly for public\nsafety and surveillance missions in urban areas. However, deploying the UAVs in\ndense urban areas introduces unique challenges, most notably asymmetric uplink\n(UL, remote controller to UAV) interference due to a higher chance of\nline-of-sight (LoS) interference at the UAV. In this letter, we propose a\nstochastic geometry framework to tractably analyze the large-scale asymmetric\ninterference in urban areas. We incorporate a log-Gaussian Cox process (LGCP)\nmodel to capture the spatial correlation of the interference field in both UL\nand downlink (DL) as a function of the UAV altitude and the two-dimensional\n(2-D) distance between the remote controller and UAV. To quantify the UL and\nthe DL interference asymmetry, we also define the interference asymmetry ratio\ncharacterizing the interference disparity between the UL and the DL. Our\nnumerical results demonstrate that the interference asymmetry ratio increases\nas the UAV altitude and 2-D distance increase, highlighting that the UL\ninterference worsens.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u51e0\u4f55\u6846\u67b6\u6765\u5206\u6790\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u901a\u4fe1\u7684\u4e0d\u5bf9\u79f0\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7LGCP\u6a21\u578b\u6355\u6349\u5e72\u6270\u573a\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u5b9a\u4e49\u4e86\u5e72\u6270\u4e0d\u5bf9\u79f0\u6bd4\u6765\u91cf\u5316\u4e0a\u4e0b\u884c\u5e72\u6270\u5dee\u5f02\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u72ec\u7279\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u7531\u4e8e\u65e0\u4eba\u673a\u66f4\u5bb9\u6613\u53d7\u5230\u89c6\u8ddd\u5e72\u6270\u800c\u5bfc\u81f4\u7684\u4e0a\u884c\u94fe\u8def\u4e0d\u5bf9\u79f0\u5e72\u6270\u95ee\u9898\u3002", "method": "\u91c7\u7528\u968f\u673a\u51e0\u4f55\u6846\u67b6\u548clog-Gaussian Cox\u8fc7\u7a0b(LGCP)\u6a21\u578b\u6765\u6355\u6349\u5e72\u6270\u573a\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5206\u6790\u5e72\u6270\u968f\u65e0\u4eba\u673a\u9ad8\u5ea6\u548c\u4e8c\u7ef4\u8ddd\u79bb\u7684\u53d8\u5316\u89c4\u5f8b\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5e72\u6270\u4e0d\u5bf9\u79f0\u6bd4\u968f\u7740\u65e0\u4eba\u673a\u9ad8\u5ea6\u548c\u4e8c\u7ef4\u8ddd\u79bb\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u8868\u660e\u4e0a\u884c\u94fe\u8def\u5e72\u6270\u60c5\u51b5\u6076\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5206\u6790\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u901a\u4fe1\u7684\u4e0d\u5bf9\u79f0\u5e72\u6270\u7279\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u90e8\u7f72\u548c\u5e72\u6270\u7ba1\u7406\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow.", "AI": {"tldr": "ProofFlow\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u8bc1\u660e\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u6784\u5efa\u903b\u8f91\u4f9d\u8d56\u56fe\u548c\u4f7f\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u8bc1\u660e\u8bed\u4e49\u548c\u903b\u8f91\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u9a8c\u8bc1\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4f46\u7ecf\u5e38\u65e0\u6cd5\u4fdd\u6301\u539f\u59cb\u4eba\u5de5\u7f16\u5199\u8bc1\u660e\u7684\u8bed\u4e49\u542b\u4e49\u548c\u903b\u8f91\u7ed3\u6784\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5c06\u7ed3\u6784\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\u3002", "method": "ProofFlow\u9996\u5148\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\u6765\u6620\u5c04\u8bc1\u660e\u6b65\u9aa4\u4e4b\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u7cfb\u7edf\u5730\u5f62\u5f0f\u5316\u6bcf\u4e2a\u6b65\u9aa4\u4e3a\u4e2d\u95f4\u5f15\u7406\uff0c\u4ece\u800c\u4fdd\u6301\u539f\u59cb\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cProofFlow\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0cProofScore\u5f97\u5206\u4e3a0.545\uff0c\u663e\u8457\u8d85\u8fc7\u5168\u8bc1\u660e\u5f62\u5f0f\u5316\uff080.123\uff09\u548c\u6b65\u9aa4\u8bc1\u660e\u5f62\u5f0f\u5316\uff080.072\uff09\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ProofFlow\u901a\u8fc7\u5173\u6ce8\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u8bc1\u660e\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5176\u6d41\u6c34\u7ebf\u3001\u57fa\u51c6\u548c\u8bc4\u5206\u6307\u6807\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.16360", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16360", "abs": "https://arxiv.org/abs/2510.16360", "authors": ["Yuchen Xiao", "Corwin Zigler", "Peter H. Hennings", "Alexandros Savvaidis"], "title": "Time-Varying Confounding Bias in Observational Geoscience with Application to Induced Seismicity", "comment": null, "summary": "Evidence derived primarily from physical models has identified saltwater\ndisposal as the dominant causal factor that contributes to induced seismicity.\nTo complement physical models, statistical/machine learning (ML) models are\ndesigned to measure associations from observational data, either with\nparametric regression models or more flexible ML models. However, it is often\ndifficult to interpret the statistical significance of a parameter or the\npredicative power of a model as evidence of causation. We adapt a causal\ninference framework with the potential outcomes perspective to explicitly\ndefine what we meant by causal effect and declare necessary identification\nconditions to recover unbiased causal effect estimates. In particular, we\nillustrate the threat of time-varying confounding in observational longitudinal\ngeoscience data through simulations and adapt established statistical methods\nfor longitudinal analysis from the causal interference literature to estimate\nthe effect of wastewater disposal on earthquakes in the Fort-Worth Basin of\nNorth Central Texas from 2013 to 2016.", "AI": {"tldr": "\u672c\u6587\u91c7\u7528\u56e0\u679c\u63a8\u65ad\u6846\u67b6\u5206\u6790\u5e9f\u6c34\u6ce8\u5165\u4e0e\u8bf1\u53d1\u5730\u9707\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u6a21\u62df\u5c55\u793a\u65f6\u95f4\u53d8\u5316\u6df7\u6742\u56e0\u7d20\u7684\u5a01\u80c1\uff0c\u5e76\u5e94\u7528\u7eb5\u5411\u5206\u6790\u65b9\u6cd5\u4f30\u8ba1\u5e9f\u6c34\u5904\u7f6e\u5bf9\u5730\u9707\u7684\u5f71\u54cd\u3002", "motivation": "\u8865\u5145\u7269\u7406\u6a21\u578b\uff0c\u4f7f\u7528\u7edf\u8ba1/\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u6d4b\u91cf\u5173\u8054\u6027\uff0c\u4f46\u96be\u4ee5\u5c06\u53c2\u6570\u7edf\u8ba1\u663e\u8457\u6027\u6216\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u89e3\u91ca\u4e3a\u56e0\u679c\u8bc1\u636e\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u7ed3\u679c\u89c6\u89d2\u7684\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u660e\u786e\u5b9a\u4e49\u56e0\u679c\u6548\u5e94\uff0c\u58f0\u660e\u5fc5\u8981\u7684\u8bc6\u522b\u6761\u4ef6\u4ee5\u6062\u590d\u65e0\u504f\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\uff0c\u7279\u522b\u5173\u6ce8\u65f6\u95f4\u53d8\u5316\u6df7\u6742\u56e0\u7d20\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5c55\u793a\u4e86\u65f6\u95f4\u53d8\u5316\u6df7\u6742\u56e0\u7d20\u5728\u89c2\u6d4b\u6027\u7eb5\u5411\u5730\u5b66\u6570\u636e\u4e2d\u7684\u5a01\u80c1\uff0c\u5e76\u5e94\u7528\u56e0\u679c\u63a8\u65ad\u6587\u732e\u4e2d\u5efa\u7acb\u7684\u7edf\u8ba1\u65b9\u6cd5\u4f30\u8ba1\u5e9f\u6c34\u5904\u7f6e\u5bf9\u5730\u9707\u7684\u5f71\u54cd\u3002", "conclusion": "\u56e0\u679c\u63a8\u65ad\u6846\u67b6\u4e3a\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\u5e9f\u6c34\u5904\u7f6e\u4e0e\u8bf1\u53d1\u5730\u9707\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u53d8\u5316\u6df7\u6742\u56e0\u7d20\u3002"}}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "J\u00f6rg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MO|RE\u6570\u636e\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u613f\u666f\uff0c\u65e8\u5728\u5c06\u8fd0\u52a8\u79d1\u5b66\u4e2d\u7684\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u5316\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u4eba\u7fa4\u7684\u751f\u7406\u548c\u8ba4\u77e5\u80fd\u529b\uff0c\u9700\u8981\u6d4b\u8bd5\u4e0e\u4eba\u7c7b\u8868\u73b0\u76f8\u5173\u7684\u5404\u79cd\u56e0\u7d20\u3002\u8fd0\u52a8\u8868\u73b0\u6d4b\u8bd5\u4f5c\u4e3a\u8fd0\u52a8\u79d1\u5b66\u7814\u7a76\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u80fd\u591f\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u8eab\u4f53\u5065\u5eb7\u72b6\u51b5\u5e76\u4f7f\u5176\u5177\u6709\u53ef\u6bd4\u6027\u3002", "method": "\u57fa\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u8bba\u6784\u5efa\u672c\u4f53\uff0c\u91cd\u70b9\u5f62\u5f0f\u5316\u8868\u793a\u8ba1\u5212\u89c4\u8303\u3001\u7279\u5b9a\u8fc7\u7a0b\u548c\u76f8\u5173\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u3002\u5728MO|RE\u6570\u636e\u57fa\u7840\u4e0a\u521b\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u5efa\u6a21\u548c\u5171\u4eab\u6807\u51c6\u5316\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u8de8\u7814\u7a76\u4f7f\u7528\u5e76\u88ab\u673a\u5668\u7406\u89e3\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\uff0cMO|RE\u6570\u636e\u80fd\u591f\u4ee5\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u7684\u65b9\u5f0f\u88ab\u5efa\u6a21\u548c\u5171\u4eab\uff0c\u8fd9\u5c06\u6539\u53d8\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u4f7f\u7528\u65b9\u5f0f\u3002"}}
{"id": "2510.16740", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16740", "abs": "https://arxiv.org/abs/2510.16740", "authors": ["Biswabrata Pradhan", "Rathin Das"], "title": "Bayesian reliability acceptance sampling plans for competing risks data under interval censoring", "comment": null, "summary": "We obtain a reliability acceptance sampling plan for independent competing\nrisk data under interval censoring schemes using the Bayesian approach. At\nfirst, the Bayesian reliability acceptance sampling plan is obtained where the\ndecision criteria of accepting a lot is pre-fixed. For large samples, computing\nBayes risk is computationally intensive. Therefore, an approximate Bayes risk\nis obtained using the asymptotic properties of the maximum likelihood\nestimators. Lastly, the Bayesian reliability acceptance sampling plan is\nobtained, where the decision function is arbitrary. The manufacturer can derive\nan optimal decision function by minimizing the Bayes risk among all decision\nfunctions. This optimal decision function is known as Bayes decision function.\nThe optimal sampling plan is obtained by minimizing the Bayes risk. The\nalgorithms are provided for the computation of optimum Bayesian reliability\nacceptance sampling plan. Numerical results are provided and comparisons\nbetween the Bayesian reliability acceptance sampling plans are carried out.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u533a\u95f4\u5220\u5931\u72ec\u7acb\u7ade\u4e89\u98ce\u9669\u6570\u636e\u4e0b\u4f7f\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u9a8c\u6536\u62bd\u6837\u8ba1\u5212\uff0c\u5305\u62ec\u56fa\u5b9a\u51b3\u7b56\u51c6\u5219\u548c\u6700\u4f18\u51b3\u7b56\u51fd\u6570\u7684\u62bd\u6837\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u7b97\u6cd5\u548c\u6570\u503c\u6bd4\u8f83\u3002", "motivation": "\u9488\u5bf9\u5927\u6837\u672c\u4e0b\u8d1d\u53f6\u65af\u98ce\u9669\u8ba1\u7b97\u590d\u6742\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6e10\u8fd1\u6027\u8d28\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4e3a\u5236\u9020\u5546\u63d0\u4f9b\u6700\u4f18\u51b3\u7b56\u51fd\u6570\u4ee5\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u6784\u5efa\u53ef\u9760\u6027\u9a8c\u6536\u62bd\u6837\u8ba1\u5212\uff0c\u5305\u62ec\u56fa\u5b9a\u51b3\u7b56\u51c6\u5219\u548c\u4efb\u610f\u51b3\u7b56\u51fd\u6570\u4e24\u79cd\u65b9\u6848\uff0c\u5229\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u6e10\u8fd1\u6027\u8d28\u8ba1\u7b97\u8fd1\u4f3c\u8d1d\u53f6\u65af\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u98ce\u9669\u83b7\u5f97\u6700\u4f18\u51b3\u7b56\u51fd\u6570\u548c\u62bd\u6837\u8ba1\u5212\u3002", "result": "\u5f00\u53d1\u4e86\u8ba1\u7b97\u6700\u4f18\u8d1d\u53f6\u65af\u53ef\u9760\u6027\u9a8c\u6536\u62bd\u6837\u8ba1\u5212\u7684\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u6570\u503c\u7ed3\u679c\uff0c\u5e76\u5bf9\u4e0d\u540c\u8d1d\u53f6\u65af\u53ef\u9760\u6027\u9a8c\u6536\u62bd\u6837\u8ba1\u5212\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u533a\u95f4\u5220\u5931\u7ade\u4e89\u98ce\u9669\u6570\u636e\u4e0b\u7684\u53ef\u9760\u6027\u9a8c\u6536\u62bd\u6837\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u98ce\u9669\u53ef\u4ee5\u83b7\u5f97\u6700\u4f18\u51b3\u7b56\u51fd\u6570\u548c\u62bd\u6837\u8ba1\u5212\uff0c\u4e3a\u5236\u9020\u5546\u63d0\u4f9b\u5b9e\u7528\u7684\u8d28\u91cf\u9a8c\u6536\u5de5\u5177\u3002"}}
{"id": "2510.15960", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.15960", "abs": "https://arxiv.org/abs/2510.15960", "authors": ["Sana Kordoghli", "Abdelhakim Settar", "Oumayma Belaati", "Mohammad Alkhatib"], "title": "Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling", "comment": "41 pages, 21 figures", "summary": "This work contributes to advancing sustainable energy and waste management\nstrategies by investigating the thermochemical conversion of food-based biomass\nthrough pyrolysis, highlighting the role of artificial intelligence (AI) in\nenhancing process modelling accuracy and optimization efficiency. The main\nobjective is to explore the potential of underutilized biomass resources, such\nas spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen\nproduction. Specifically, it aims to optimize the pyrolysis process while\nevaluating the performance of these resources both individually and as blends.\nProximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC\nanalyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS\n- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential\nbut had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1\nexhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic\nmodelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS\nas the most accurate. These approaches provide a detailed understanding of the\npyrolysis process, with particular emphasis on the integration of artificial\nintelligence. An LSTM model trained with lignocellulosic data predicted TGA\ncurves with exceptional accuracy (R^2: 0.9996-0.9998).", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u70ed\u89e3\u6280\u672f\u8f6c\u5316\u98df\u7269\u57fa\u751f\u7269\u8d28\uff08\u5496\u5561\u6e23\u548c\u67a3\u6838\uff09\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u4f18\u5316\u8fc7\u7a0b\u5efa\u6a21\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u6301\u7eed\u6c22\u80fd\u751f\u4ea7\u548c\u5e9f\u7269\u7ba1\u7406\u3002", "motivation": "\u63a2\u7d22\u672a\u5145\u5206\u5229\u7528\u7684\u751f\u7269\u8d28\u8d44\u6e90\uff08\u5982\u5496\u5561\u6e23\u548c\u67a3\u6838\uff09\u5728\u53ef\u6301\u7eed\u6c22\u80fd\u751f\u4ea7\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u70ed\u89e3\u6280\u672f\u5b9e\u73b0\u80fd\u6e90\u56de\u6536\u548c\u5e9f\u7269\u7ba1\u7406\u3002", "method": "\u5bf9\u7eaf\u67a3\u6838\u3001\u5496\u5561\u6e23\u53ca\u5176\u6df7\u5408\u6837\u54c1\u8fdb\u884c\u591a\u9879\u5206\u6790\uff08\u5de5\u4e1a\u5206\u6790\u3001\u5143\u7d20\u5206\u6790\u3001\u7ea4\u7ef4\u5206\u6790\u3001\u70ed\u91cd\u5206\u6790\u3001\u52a8\u529b\u5b66\u5206\u6790\u7b49\uff09\uff0c\u4f7f\u7528\u7b49\u8f6c\u5316\u7387\u65b9\u6cd5\uff08KAS\u3001FWO\u3001Friedman\uff09\u8fdb\u884c\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u5e76\u5f00\u53d1LSTM\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u9884\u6d4b\u70ed\u91cd\u66f2\u7ebf\u3002", "result": "\u6df7\u5408\u6837\u54c13\uff0825%\u67a3\u6838-75%\u5496\u5561\u6e23\uff09\u5177\u6709\u6700\u4f73\u6c22\u4ea7\u7387\u6f5c\u529b\u4f46\u6d3b\u5316\u80fd\u6700\u9ad8\uff08313.24 kJ/mol\uff09\uff0c\u6df7\u5408\u6837\u54c11\uff0875%\u67a3\u6838-25%\u5496\u5561\u6e23\uff09\u6d3b\u5316\u80fd\u6700\u4f73\uff08161.75 kJ/mol\uff09\u3002KAS\u65b9\u6cd5\u88ab\u786e\u5b9a\u4e3a\u6700\u51c6\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0cLSTM\u6a21\u578b\u9884\u6d4b\u70ed\u91cd\u66f2\u7ebf\u7cbe\u5ea6\u6781\u9ad8\uff08R\u00b2: 0.9996-0.9998\uff09\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u5728\u70ed\u89e3\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5496\u5561\u6e23\u548c\u67a3\u6838\u6df7\u5408\u7269\u5177\u6709\u53ef\u6301\u7eed\u6c22\u80fd\u751f\u4ea7\u7684\u6f5c\u529b\uff0c\u4e3a\u751f\u7269\u8d28\u80fd\u6e90\u8f6c\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u7f6e\u6362\u96c6\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ece\u968f\u673a\u6709\u9650\u96c6\u548cDempster-Shafer\u7406\u8bba\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u51b2\u7a81\uff0c\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0\u5ea6\u91cf\u5b9a\u4e49\u7f6e\u6362\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u6269\u5c55\u4e3aRPST\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u968f\u673a\u7f6e\u6362\u96c6\u4f5c\u4e3a\u5904\u7406\u5305\u542b\u987a\u5e8f\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u9700\u8981\u6709\u6548\u5ea6\u91cf\u4e24\u4e2a\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4ee5\u652f\u6301\u987a\u5e8f\u7ed3\u6784\u4e0d\u786e\u5b9a\u4fe1\u606f\u878d\u5408\u3002", "method": "\u4ece\u7f6e\u6362\u89c2\u5bdf\u51fa\u53d1\uff0c\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0\u5ea6\u91cf\u5b9a\u4e49\u7f6e\u6362\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u975e\u91cd\u53e0\u57fa\u7840\u7684RPS\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5c06RPST\u89c6\u4e3aDST\u7684\u6269\u5c55\uff0c\u5229\u7528\u65b0\u589e\u7684\u987a\u5e8f\u4fe1\u606f\u8868\u5f81\u5b9a\u6027\u503e\u5411\u6027\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6240\u63d0\u51b2\u7a81\u5ea6\u91cf\u7684\u884c\u4e3a\u548c\u6027\u8d28\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u81ea\u7136\u7684\u9876\u90e8\u52a0\u6743\u7279\u6027\uff0c\u80fd\u4eceDST\u89c6\u89d2\u6709\u6548\u5ea6\u91cfRPS\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u7684\u7075\u6d3b\u9009\u62e9\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\u4e0d\u4ec5\u5177\u5907\u7406\u8bba\u4f18\u52bf\uff0c\u8fd8\u80fd\u4e3a\u987a\u5e8f\u7ed3\u6784\u4e0d\u786e\u5b9a\u4fe1\u606f\u878d\u5408\u63d0\u4f9b\u5b9e\u7528\u7684\u51b2\u7a81\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2510.16728", "categories": ["stat.ML", "cs.LG", "math.PR", "stat.ME", "60L10, 60L20, 62G05, 62G08"], "pdf": "https://arxiv.org/pdf/2510.16728", "abs": "https://arxiv.org/abs/2510.16728", "authors": ["Christian Bayer", "Davit Gogolashvili", "Luca Pelizzari"], "title": "Local regression on path spaces with signature metrics", "comment": null, "summary": "We study nonparametric regression and classification for path-valued data. We\nintroduce a functional Nadaraya-Watson estimator that combines the signature\ntransform from rough path theory with local kernel regression. The signature\ntransform provides a principled way to encode sequential data through iterated\nintegrals, enabling direct comparison of paths in a natural metric space. Our\napproach leverages signature-induced distances within the classical kernel\nregression framework, achieving computational efficiency while avoiding the\nscalability bottlenecks of large-scale kernel matrix operations. We establish\nfinite-sample convergence bounds demonstrating favorable statistical properties\nof signature-based distances compared to traditional metrics in\ninfinite-dimensional settings. We propose robust signature variants that\nprovide stability against outliers, enhancing practical performance.\nApplications to both synthetic and real-world data - including stochastic\ndifferential equation learning and time series classification - demonstrate\ncompetitive accuracy while offering significant computational advantages over\nexisting methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b7e\u540d\u53d8\u6362\u548c\u5c40\u90e8\u6838\u56de\u5f52\u7684\u51fd\u6570\u578bNadaraya-Watson\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u8def\u5f84\u503c\u6570\u636e\u7684\u975e\u53c2\u6570\u56de\u5f52\u548c\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u8def\u5f84\u503c\u6570\u636e\u65f6\u9762\u4e34\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8ba1\u7b97\u548c\u7edf\u8ba1\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u7f16\u7801\u5e8f\u5217\u6570\u636e\u5e76\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u7c97\u7cd9\u8def\u5f84\u7406\u8bba\u4e2d\u7684\u7b7e\u540d\u53d8\u6362\u4e0e\u5c40\u90e8\u6838\u56de\u5f52\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u7b7e\u540d\u8bf1\u5bfc\u7684\u8ddd\u79bb\u5728\u7ecf\u5178\u6838\u56de\u5f52\u6846\u67b6\u4e2d\u8fdb\u884c\u8ba1\u7b97\uff0c\u907f\u514d\u5927\u89c4\u6a21\u6838\u77e9\u9635\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002", "result": "\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u6536\u655b\u754c\uff0c\u8bc1\u660e\u57fa\u4e8e\u7b7e\u540d\u7684\u8ddd\u79bb\u5728\u65e0\u9650\u7ef4\u8bbe\u7f6e\u4e2d\u76f8\u6bd4\u4f20\u7edf\u5ea6\u91cf\u5177\u6709\u66f4\u4f18\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u6027\u7cbe\u5ea6\u548c\u663e\u8457\u8ba1\u7b97\u4f18\u52bf\u3002", "conclusion": "\u7b7e\u540d\u53d8\u6362\u4e3a\u8def\u5f84\u503c\u6570\u636e\u7684\u975e\u53c2\u6570\u56de\u5f52\u548c\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7edf\u8ba1\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\u3002"}}
{"id": "2510.16745", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G10, 62G20, 62P05, 46E22"], "pdf": "https://arxiv.org/pdf/2510.16745", "abs": "https://arxiv.org/abs/2510.16745", "authors": ["Rohan Sen"], "title": "Kernel-Based Nonparametric Tests For Shape Constraints", "comment": "31 pages, 1 figure", "summary": "We develop a reproducing kernel Hilbert space (RKHS) framework for\nnonparametric mean-variance optimization and inference on shape constraints of\nthe optimal rule. We derive statistical properties of the sample estimator and\nprovide rigorous theoretical guarantees, such as asymptotic consistency, a\nfunctional central limit theorem, and a finite-sample deviation bound that\nmatches the Monte Carlo rate up to regularization. Building on these findings,\nwe introduce a joint Wald-type statistic to test for shape constraints over\nfinite grids. The approach comes with an efficient computational procedure\nbased on a pivoted Cholesky factorization, facilitating scalability to large\ndatasets. Empirical tests suggest favorably of the proposed methodology.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u53c2\u6570\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u548c\u6700\u4f18\u89c4\u5219\u5f62\u72b6\u7ea6\u675f\u7684\u63a8\u65ad\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u975e\u53c2\u6570\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u95ee\u9898\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u5bf9\u6700\u4f18\u89c4\u5219\u5f62\u72b6\u7ea6\u675f\u7684\u7edf\u8ba1\u63a8\u65ad\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6846\u67b6\uff0c\u63a8\u5bfc\u6837\u672c\u4f30\u8ba1\u91cf\u7684\u7edf\u8ba1\u6027\u8d28\uff0c\u5305\u62ec\u6e10\u8fd1\u4e00\u81f4\u6027\u3001\u6cdb\u51fd\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u6709\u9650\u6837\u672c\u504f\u5dee\u754c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u679c\uff0c\u5f15\u5165\u8054\u5408Wald\u578b\u7edf\u8ba1\u91cf\u6765\u6d4b\u8bd5\u6709\u9650\u7f51\u683c\u4e0a\u7684\u5f62\u72b6\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u4e3b\u5143Cholesky\u5206\u89e3\u7684\u9ad8\u6548\u8ba1\u7b97\u7a0b\u5e8f\u3002", "result": "\u83b7\u5f97\u4e86\u4e25\u683c\u7684\u7406\u8ad6\u4fdd\u8bc1\uff0c\u5305\u62ec\u6e10\u8fd1\u4e00\u81f4\u6027\u3001\u6cdb\u51fd\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u4e0e\u8499\u7279\u5361\u7f57\u7387\u5339\u914d\u7684\u6709\u9650\u6837\u672c\u504f\u5dee\u754c\u3002\u7ecf\u9a8c\u6d4b\u8bd5\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684RKHS\u6846\u67b6\u4e3a\u5f62\u72b6\u7ea6\u675f\u4e0b\u7684\u975e\u53c2\u6570\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u8ba1\u63a8\u65ad\u5de5\u5177\uff0c\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u6027\u8d28\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.15964", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15964", "abs": "https://arxiv.org/abs/2510.15964", "authors": ["Tuowei Wang", "Kun Li", "Zixu Hao", "Donglin Bai", "Ju Ren", "Yaoxue Zhang", "Ting Cao", "Mao Yang"], "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity", "comment": null, "summary": "The adaptation of pre-trained large language models (LLMs) to diverse\ndownstream tasks via fine-tuning is critical for numerous applications.\nHowever, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques\npresents significant challenges in terms of time investments and operational\ncosts. In this paper, we first introduce a nuanced form of sparsity, termed\nShadowy Sparsity, which is distinctive in fine-tuning and has not been\nadequately addressed for acceleration. Under Shadowy Sparsity, we propose Long\nExposure, an efficient system to accelerate PEFT for LLMs. Long Exposure\ncomprises three key components: Shadowy-sparsity Exposer employs a prolonged\nsensing range to capture more sparsity details under shadowy sparsity;\nSequence-oriented Predictor provides efficient yet accurate predictions to\nhandle large sequence inputs and constantly-evolving parameters; and\nDynamic-aware Operator facilitates more structured computational patterns and\ncoalesced memory accesses, addressing dynamic sparse operations. Extensive\nevaluations show that Long Exposure outperforms state-of-the-arts with up to a\n$2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements\nin accelerating PEFT for LLMs.", "AI": {"tldr": "Long Exposure\u662f\u4e00\u4e2a\u9ad8\u6548\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u51b3\u5fae\u8c03\u4e2d\u7279\u6709\u7684Shadowy Sparsity\u95ee\u9898\u6765\u52a0\u901f\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aShadowy-sparsity Exposer\u3001Sequence-oriented Predictor\u548cDynamic-aware Operator\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad82.49\u500d\u7684\u7aef\u5230\u7aef\u5fae\u8c03\u52a0\u901f\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5e26\u6765\u65f6\u95f4\u6295\u5165\u548c\u8fd0\u8425\u6210\u672c\u7684\u663e\u8457\u6311\u6218\u3002\u8bba\u6587\u53d1\u73b0\u4e86\u4e00\u79cd\u5fae\u8c03\u7279\u6709\u7684Shadowy Sparsity\u5f62\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u5176\u52a0\u901f\u95ee\u9898\u3002", "method": "\u63d0\u51faLong Exposure\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Shadowy-sparsity Exposer\u4f7f\u7528\u5ef6\u957f\u611f\u77e5\u8303\u56f4\u6355\u83b7\u66f4\u591a\u7a00\u758f\u6027\u7ec6\u8282\uff1b2) Sequence-oriented Predictor\u63d0\u4f9b\u9ad8\u6548\u51c6\u786e\u9884\u6d4b\u4ee5\u5904\u7406\u5927\u5e8f\u5217\u8f93\u5165\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u53c2\u6570\uff1b3) Dynamic-aware Operator\u4fc3\u8fdb\u66f4\u7ed3\u6784\u5316\u7684\u8ba1\u7b97\u6a21\u5f0f\u548c\u5408\u5e76\u5185\u5b58\u8bbf\u95ee\uff0c\u89e3\u51b3\u52a8\u6001\u7a00\u758f\u64cd\u4f5c\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cLong Exposure\u5728\u7aef\u5230\u7aef\u5fae\u8c03\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad82.49\u500d\u7684\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Long Exposure\u4e3a\u52a0\u901fLLMs\u7684PEFT\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8fdb\u5c55\uff0c\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9Shadowy Sparsity\u8bbe\u8ba1\u7684\u7cfb\u7edf\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8c03\u6548\u7387\u3002"}}
{"id": "2510.17502", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17502", "abs": "https://arxiv.org/abs/2510.17502", "authors": ["Li-Hsiang Shen"], "title": "6D Movable Metasurface (6DMM) in Downlink NOMA Transmissions", "comment": null, "summary": "This letter proposes a novel six-dimensional movable metasurface\n(6DMM)-assisted downlink non-orthogonal multiple access (NOMA) system, in which\na conventional base station (BS) equipped with fixed antennas serves multiple\nusers with the assistance of a reconfigurable intelligent surface (RIS) with\nsix-dimensional spatial configurability. In contrast to traditional RIS with\nstatic surface, the proposed 6DMM architecture allows each element to\ndynamically adjust its position and orient the whole metasurface in\nyaw-pitch-roll axes, enabling both in spatial and electromagnetic controls. We\nformulate a sum-rate maximization problem that jointly optimizes the BS\nNOMA-based beamforming, phase-shifts, element positions, and rotation angles of\nmetasurface under constraints of NOMA power levels, unit-modulus of\nphase-shifts, power budget, inter-element separation and boundaries of element\nposition/orientation. Due to non-convexity and high-dimensionality, we employ a\nprobabilistic cross-entropy optimization (CEO) scheme to iteratively refine the\nsolution distribution based on maximizing likelihood and elite solution\nsampling. Simulation results show that the proposed CEO-based 6DMM-NOMA\narchitecture achieves substantial rate performance gains compared to 6DMM\nsub-structures, conventional static RIS, and other multiple access mechanisms.\nIt also highlights the effectiveness of CEO providing probabilistic\noptimization for solving high-dimensional scalable metasurface.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516d\u7ef4\u53ef\u79fb\u52a8\u8d85\u8868\u9762\u8f85\u52a9\u7684\u4e0b\u884c\u975e\u6b63\u4ea4\u591a\u5740\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8d85\u8868\u9762\u5143\u7d20\u4f4d\u7f6e\u548c\u65b9\u5411\uff0c\u5b9e\u73b0\u7a7a\u95f4\u548c\u7535\u78c1\u63a7\u5236\uff0c\u5e76\u4f7f\u7528\u6982\u7387\u4ea4\u53c9\u71b5\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u5b58\u5728\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u66f4\u9ad8\u7a7a\u95f4\u914d\u7f6e\u81ea\u7531\u5ea6\u7684\u8d85\u8868\u9762\u67b6\u6784\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u516d\u7ef4\u53ef\u79fb\u52a8\u8d85\u8868\u9762\u67b6\u6784\uff0c\u6bcf\u4e2a\u5143\u7d20\u53ef\u52a8\u6001\u8c03\u6574\u4f4d\u7f6e\u548c\u504f\u822a-\u4fef\u4ef0-\u6eda\u8f6c\u89d2\u5ea6\uff0c\u5e76\u5229\u7528\u6982\u7387\u4ea4\u53c9\u71b5\u4f18\u5316\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u3001\u76f8\u79fb\u3001\u5143\u7d20\u4f4d\u7f6e\u548c\u65cb\u8f6c\u89d2\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684CEO-based 6DMM-NOMA\u67b6\u6784\u76f8\u6bd46DMM\u5b50\u7ed3\u6784\u3001\u4f20\u7edf\u9759\u6001RIS\u548c\u5176\u4ed6\u591a\u5740\u673a\u5236\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u7387\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u516d\u7ef4\u53ef\u79fb\u52a8\u8d85\u8868\u9762\u7ed3\u5408\u6982\u7387\u4ea4\u53c9\u71b5\u4f18\u5316\u4e3a\u9ad8\u7ef4\u53ef\u6269\u5c55\u8d85\u8868\u9762\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.16937", "categories": ["stat.ML", "cs.LG", "stat.ME", "G.3"], "pdf": "https://arxiv.org/pdf/2510.16937", "abs": "https://arxiv.org/abs/2510.16937", "authors": ["Vikram Kher", "Argyris Oikonomou", "Manolis Zampetakis"], "title": "Prediction-Augmented Trees for Reliable Statistical Inference", "comment": "45 pages, 9 Figures", "summary": "The remarkable success of machine learning (ML) in predictive tasks has led\nscientists to incorporate ML predictions as a core component of the scientific\ndiscovery pipeline. This was exemplified by the landmark achievement of\nAlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions\ncan be safely used in statistical analysis of data towards scientific\ndiscovery. In particular, we follow the framework introduced by Angelopoulos et\nal. (2023). In this framework, we assume access to a small set of $n$\ngold-standard labeled samples, a much larger set of $N$ unlabeled samples, and\na ML model that can be used to impute the labels of the unlabeled data points.\nWe introduce two new learning-augmented estimators: (1) Prediction-Augmented\nResidual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both\nestimators have significant advantages over existing estimators like PPI and\nPPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),\nrespectively. PART is a decision-tree based estimator built using a greedy\ncriterion. We first characterize PART's asymptotic distribution and demonstrate\nhow to construct valid confidence intervals. Then we show that PART outperforms\nexisting methods in real-world datasets from ecology, astronomy, and census\nreports, among other domains. This leads to estimators with higher confidence,\nwhich is the result of using both the gold-standard samples and the machine\nlearning predictions. Finally, we provide a formal proof of the advantage of\nPART by exploring PAQ, an estimation that arises when considering the limit of\nPART when the depth its tree grows to infinity. Under appropriate assumptions\nin the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}\n+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u4f30\u8ba1\u5668PART\u548cPAQ\uff0c\u7528\u4e8e\u5728\u7edf\u8ba1\u5206\u6790\u4e2d\u5b89\u5168\u4f7f\u7528ML\u9884\u6d4b\u3002\u8fd9\u4e9b\u4f30\u8ba1\u5668\u7ed3\u5408\u4e86\u5c11\u91cf\u9ec4\u91d1\u6807\u51c6\u6807\u8bb0\u6837\u672c\u548c\u5927\u91cf\u672a\u6807\u8bb0\u6837\u672c\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u80fd\u6784\u5efa\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6210\u529f\uff0c\u79d1\u5b66\u5bb6\u4eec\u5f00\u59cb\u5c06ML\u9884\u6d4b\u4f5c\u4e3a\u79d1\u5b66\u53d1\u73b0\u6d41\u7a0b\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u5728\u7edf\u8ba1\u6570\u636e\u5206\u6790\u4e2d\u5b89\u5168\u5730\u4f7f\u7528ML\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u53ea\u6709\u5c11\u91cf\u9ec4\u91d1\u6807\u51c6\u6807\u8bb0\u6837\u672c\u548c\u5927\u91cf\u672a\u6807\u8bb0\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u5b66\u4e60\u589e\u5f3a\u4f30\u8ba1\u5668\uff1a1\uff09\u57fa\u4e8e\u51b3\u7b56\u6811\u7684PART\u4f30\u8ba1\u5668\uff0c\u4f7f\u7528\u8d2a\u5fc3\u51c6\u5219\u6784\u5efa\uff1b2\uff09PAQ\u4f30\u8ba1\u5668\uff0c\u5f53PART\u6811\u7684\u6df1\u5ea6\u8d8b\u4e8e\u65e0\u7a77\u65f6\u7684\u6781\u9650\u60c5\u51b5\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u7ed3\u5408\u4e86\u9ec4\u91d1\u6807\u51c6\u6837\u672c\u548cML\u9884\u6d4b\u3002", "result": "PART\u5728\u751f\u6001\u5b66\u3001\u5929\u6587\u5b66\u548c\u4eba\u53e3\u666e\u67e5\u7b49\u591a\u4e2a\u9886\u57df\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684PPI\u548cPPI++\u65b9\u6cd5\u3002PAQ\u7684\u65b9\u5dee\u6536\u7f29\u7387\u4e3aO(N^{-1} + n^{-4})\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684O(N^{-1}+n^{-1})\u901f\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684PART\u548cPAQ\u4f30\u8ba1\u5668\u80fd\u591f\u6709\u6548\u5229\u7528ML\u9884\u6d4b\u548c\u9ec4\u91d1\u6807\u51c6\u6837\u672c\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u4f30\u8ba1\u7ed3\u679c\uff0c\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5b89\u5168\u4f7f\u7528ML\u9884\u6d4b\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.16986", "categories": ["stat.ML", "cs.LG", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.16986", "abs": "https://arxiv.org/abs/2510.16986", "authors": ["Hamza Cherkaoui", "H\u00e9l\u00e8ne Halconruy", "Yohan Petetin"], "title": "Adaptive Sample Sharing for Linear Regression", "comment": null, "summary": "In many business settings, task-specific labeled data are scarce or costly to\nobtain, which limits supervised learning on a specific task. To address this\nchallenge, we study sample sharing in the case of ridge regression: leveraging\nan auxiliary data set while explicitly protecting against negative transfer. We\nintroduce a principled, data-driven rule that decides how many samples from an\nauxiliary dataset to add to the target training set. The rule is based on an\nestimate of the transfer gain i.e. the marginal reduction in the predictive\nerror. Building on this estimator, we derive finite-sample guaranties: under\nstandard conditions, the procedure borrows when it improves parameter\nestimation and abstains otherwise. In the Gaussian feature setting, we analyze\nwhich data set properties ensure that borrowing samples reduces the predictive\nerror. We validate the approach in synthetic and real datasets, observing\nconsistent gains over strong baselines and single-task training while avoiding\nnegative transfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5cad\u56de\u5f52\u7684\u6837\u672c\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u89c4\u5219\u51b3\u5b9a\u4ece\u8f85\u52a9\u6570\u636e\u96c6\u4e2d\u501f\u7528\u591a\u5c11\u6837\u672c\uff0c\u4ee5\u9632\u6b62\u8d1f\u8fc1\u79fb\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u5546\u4e1a\u73af\u5883\u4e2d\uff0c\u7279\u5b9a\u4efb\u52a1\u7684\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u8fd9\u9650\u5236\u4e86\u76d1\u7763\u5b66\u4e60\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u7814\u7a76\u5982\u4f55\u5728\u5cad\u56de\u5f52\u4e2d\u5b89\u5168\u5730\u5229\u7528\u8f85\u52a9\u6570\u636e\u96c6\uff0c\u540c\u65f6\u660e\u786e\u9632\u6b62\u8d1f\u8fc1\u79fb\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8f6c\u79fb\u589e\u76ca\u4f30\u8ba1\u7684\u539f\u5219\u6027\u6570\u636e\u9a71\u52a8\u89c4\u5219\uff0c\u8be5\u89c4\u5219\u4f30\u8ba1\u9884\u6d4b\u8bef\u5dee\u7684\u8fb9\u9645\u51cf\u5c11\u91cf\uff0c\u5e76\u636e\u6b64\u51b3\u5b9a\u4ece\u8f85\u52a9\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u591a\u5c11\u6837\u672c\u5230\u76ee\u6807\u8bad\u7ec3\u96c6\u3002\u5728\u9ad8\u65af\u7279\u5f81\u8bbe\u7f6e\u4e0b\u5206\u6790\u786e\u4fdd\u501f\u7528\u6837\u672c\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u7684\u6570\u636e\u96c6\u5c5e\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u548c\u5355\u4efb\u52a1\u8bad\u7ec3\u83b7\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u6210\u529f\u907f\u514d\u4e86\u8d1f\u8fc1\u79fb\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u6539\u5584\u53c2\u6570\u4f30\u8ba1\u65f6\u501f\u7528\u6837\u672c\uff0c\u5426\u5219\u4fdd\u6301\u514b\u5236\uff0c\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6837\u672c\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2510.15967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15967", "abs": "https://arxiv.org/abs/2510.15967", "authors": ["Zhengyi Zhong", "Wenzheng Jiang", "Weidong Bao", "Ji Wang", "Cheems Wang", "Guanbo Wang", "Yongheng Deng", "Ju Ren"], "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set", "comment": "Accepted by NeurIPS2025", "summary": "Conventional federated learning (FL) assumes a closed world with a fixed\ntotal number of clients. In contrast, new clients continuously join the FL\nprocess in real-world scenarios, introducing new knowledge. This raises two\ncritical demands: detecting new knowledge, i.e., knowledge discovery, and\nintegrating it into the global model, i.e., knowledge adaptation. Existing\nresearch focuses on coarse-grained knowledge discovery, and often sacrifices\nsource domain performance and adaptation efficiency. To this end, we propose a\nfine-grained federated domain adaptation approach in open set (Gains). Gains\nsplits the model into an encoder and a classifier, empirically revealing\nfeatures extracted by the encoder are sensitive to domain shifts while\nclassifier parameters are sensitive to class increments. Based on this, we\ndevelop fine-grained knowledge discovery and contribution-driven aggregation\ntechniques to identify and incorporate new knowledge. Additionally, an\nanti-forgetting mechanism is designed to preserve source domain performance,\nensuring balanced adaptation. Experimental results on multi-domain datasets\nacross three typical data-shift scenarios demonstrate that Gains significantly\noutperforms other baselines in performance for both source-domain and\ntarget-domain clients. Code is available at:\nhttps://github.com/Zhong-Zhengyi/Gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u8054\u90a6\u57df\u81ea\u9002\u5e94\u65b9\u6cd5Gains\uff0c\u7528\u4e8e\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u4e2d\u65b0\u5ba2\u6237\u7aef\u4e0d\u65ad\u52a0\u5165\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u53d1\u73b0\u548c\u77e5\u8bc6\u9002\u5e94\u6765\u68c0\u6d4b\u548c\u6574\u5408\u65b0\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u57df\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u65b0\u5ba2\u6237\u7aef\u4f1a\u4e0d\u65ad\u52a0\u5165\u5e76\u5e26\u6765\u65b0\u77e5\u8bc6\uff0c\u800c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5047\u8bbe\u5c01\u95ed\u4e16\u754c\u4e14\u5ba2\u6237\u7aef\u6570\u91cf\u56fa\u5b9a\u3002\u73b0\u6709\u7814\u7a76\u5728\u77e5\u8bc6\u53d1\u73b0\u65b9\u9762\u7c92\u5ea6\u8f83\u7c97\uff0c\u4e14\u5f80\u5f80\u727a\u7272\u6e90\u57df\u6027\u80fd\u548c\u9002\u5e94\u6548\u7387\u3002", "method": "\u5c06\u6a21\u578b\u5206\u4e3a\u7f16\u7801\u5668\u548c\u5206\u7c7b\u5668\uff0c\u53d1\u73b0\u7f16\u7801\u5668\u63d0\u53d6\u7684\u7279\u5f81\u5bf9\u57df\u504f\u79fb\u654f\u611f\uff0c\u800c\u5206\u7c7b\u5668\u53c2\u6570\u5bf9\u7c7b\u522b\u589e\u91cf\u654f\u611f\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u53d1\u73b0\u548c\u8d21\u732e\u9a71\u52a8\u7684\u805a\u5408\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6297\u9057\u5fd8\u673a\u5236\u6765\u4fdd\u6301\u6e90\u57df\u6027\u80fd\u3002", "result": "\u5728\u4e09\u79cd\u5178\u578b\u6570\u636e\u504f\u79fb\u573a\u666f\u4e0b\u7684\u591a\u57df\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cGains\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u5ba2\u6237\u7aef\u7684\u6027\u80fd\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Gains\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5f00\u653e\u4e16\u754c\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u65b0\u77e5\u8bc6\u53d1\u73b0\u548c\u9002\u5e94\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6e90\u57df\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u76ee\u6807\u57df\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5e73\u8861\u7684\u81ea\u9002\u5e94\u3002"}}
{"id": "2510.17063", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17063", "abs": "https://arxiv.org/abs/2510.17063", "authors": ["Shunan Sheng", "Bohan Wu", "Alberto Gonz\u00e1lez-Sanz"], "title": "Mode Collapse of Mean-Field Variational Inference", "comment": null, "summary": "Mean-field variational inference (MFVI) is a widely used method for\napproximating high-dimensional probability distributions by product measures.\nIt has been empirically observed that MFVI optimizers often suffer from mode\ncollapse. Specifically, when the target measure $\\pi$ is a mixture $\\pi = w P_0\n+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a\nsingle component of the mixture. This work provides the first theoretical\nexplanation of mode collapse in MFVI. We introduce the notion to capture the\nseparatedness of the two mixture components -- called\n$\\varepsilon$-separateness -- and derive explicit bounds on the fraction of\nmass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are\n$\\varepsilon$-separated for sufficiently small $\\varepsilon$. Our results\nsuggest that the occurrence of mode collapse crucially depends on the relative\nposition of the components. To address this issue, we propose the rotational\nvariational inference (RoVI), which augments MFVI with a rotation matrix. The\nnumerical studies support our theoretical findings and demonstrate the benefits\nof RoVI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86\u5e73\u5747\u573a\u53d8\u5206\u63a8\u65ad(MFVI)\u4e2d\u7684\u6a21\u5f0f\u574d\u584c\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u65cb\u8f6c\u53d8\u5206\u63a8\u65ad(RoVI)\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "MFVI\u5728\u903c\u8fd1\u9ad8\u7ef4\u6982\u7387\u5206\u5e03\u65f6\u7ecf\u5e38\u51fa\u73b0\u6a21\u5f0f\u574d\u584c\u95ee\u9898\uff0c\u5373\u5f53\u76ee\u6807\u5206\u5e03\u662f\u6df7\u5408\u5206\u5e03\u65f6\uff0cMFVI\u4f18\u5316\u5668\u503e\u5411\u4e8e\u5c06\u5927\u90e8\u5206\u8d28\u91cf\u96c6\u4e2d\u5728\u5355\u4e2a\u6df7\u5408\u5206\u91cf\u4e0a\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e00\u73b0\u8c61\u7684\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u5f15\u5165\u03b5-\u5206\u79bb\u5ea6\u7684\u6982\u5ff5\u6765\u91cf\u5316\u4e24\u4e2a\u6df7\u5408\u5206\u91cf\u7684\u5206\u79bb\u7a0b\u5ea6\uff0c\u63a8\u5bfc\u51fa\u5f53P\u2080\u548cP\u2081\u5145\u5206\u03b5-\u5206\u79bb\u65f6\uff0c\u4efb\u4f55MFVI\u4f18\u5316\u5668\u5206\u914d\u7ed9\u6bcf\u4e2a\u5206\u91cf\u7684\u8d28\u91cf\u6bd4\u4f8b\u7684\u663e\u5f0f\u754c\u9650\u3002\u4e3a\u89e3\u51b3\u6a21\u5f0f\u574d\u584c\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65cb\u8f6c\u53d8\u5206\u63a8\u65ad(RoVI)\uff0c\u5728MFVI\u57fa\u7840\u4e0a\u589e\u52a0\u65cb\u8f6c\u77e9\u9635\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6a21\u5f0f\u574d\u584c\u7684\u53d1\u751f\u5173\u952e\u53d6\u51b3\u4e8e\u6df7\u5408\u5206\u91cf\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002\u6570\u503c\u7814\u7a76\u652f\u6301\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u5e76\u8bc1\u660e\u4e86RoVI\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3aMFVI\u4e2d\u7684\u6a21\u5f0f\u574d\u584c\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848RoVI\uff0c\u901a\u8fc7\u65cb\u8f6c\u64cd\u4f5c\u6539\u5584\u4e86\u53d8\u5206\u63a8\u65ad\u5728\u6df7\u5408\u5206\u5e03\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2510.15968", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15968", "abs": "https://arxiv.org/abs/2510.15968", "authors": ["Zhen Huang", "Hong Wang", "Wenkai Yang", "Muxi Tang", "Depeng Xie", "Ting-Jung Lin", "Yu Zhang", "Wei W. Xing", "Lei He"], "title": "Self-Attention to Operator Learning-based 3D-IC Thermal Simulation", "comment": null, "summary": "Thermal management in 3D ICs is increasingly challenging due to higher power\ndensities. Traditional PDE-solving-based methods, while accurate, are too slow\nfor iterative design. Machine learning approaches like FNO provide faster\nalternatives but suffer from high-frequency information loss and high-fidelity\ndata dependency. We introduce Self-Attention U-Net Fourier Neural Operator\n(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to\ncapture long-range dependencies and model local high-frequency features\neffectively. Transfer learning is employed to fine-tune low-fidelity data,\nminimizing the need for extensive high-fidelity datasets and speeding up\ntraining. Experiments demonstrate that SAU-FNO achieves state-of-the-art\nthermal prediction accuracy and provides an 842x speedup over traditional FEM\nmethods, making it an efficient tool for advanced 3D IC thermal simulations.", "AI": {"tldr": "SAU-FNO\u662f\u4e00\u79cd\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548cU-Net\u7684\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u7528\u4e8e3D IC\u70ed\u7ba1\u7406\uff0c\u76f8\u6bd4\u4f20\u7edfFEM\u65b9\u6cd5\u63d0\u901f842\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u70ed\u9884\u6d4b\u3002", "motivation": "3D IC\u4e2d\u70ed\u7ba1\u7406\u56e0\u529f\u7387\u5bc6\u5ea6\u589e\u52a0\u800c\u65e5\u76ca\u56f0\u96be\uff0c\u4f20\u7edfPDE\u65b9\u6cd5\u867d\u51c6\u786e\u4f46\u901f\u5ea6\u6162\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5982FNO\u867d\u5feb\u4f46\u5b58\u5728\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u548c\u9ad8\u4fdd\u771f\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u63d0\u51faSAU-FNO\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548cU-Net\u4e0eFNO\uff0c\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u5e76\u6709\u6548\u5efa\u6a21\u5c40\u90e8\u9ad8\u9891\u7279\u5f81\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u5fae\u8c03\u4f4e\u4fdd\u771f\u6570\u636e\u4ee5\u51cf\u5c11\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSAU-FNO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u70ed\u9884\u6d4b\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edfFEM\u65b9\u6cd5\u63d0\u4f9b842\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SAU-FNO\u662f\u7528\u4e8e\u9ad8\u7ea73D IC\u70ed\u6a21\u62df\u7684\u9ad8\u6548\u5de5\u5177\uff0c\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "TEAM-PHI\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u548c\u9009\u62e9\u533b\u7597\u4fe1\u606f\u53bb\u6807\u8bc6\u5316\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u5bb6\u6807\u6ce8\u3002", "motivation": "\u533b\u7597\u4fe1\u606f\u53bb\u6807\u8bc6\u5316\u5bf9\u4e8e\u5b89\u5168\u91cd\u7528\u4e34\u5e8a\u7b14\u8bb0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u6210\u672c\u9ad8\u6602\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6bd4\u8f83\u548c\u9009\u62e9\u3002", "method": "\u90e8\u7f72\u591a\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u72ec\u7acb\u5224\u65adPHI\u63d0\u53d6\u7684\u6b63\u786e\u6027\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u7ed3\u679c\uff0c\u751f\u6210\u7a33\u5b9a\u53ef\u91cd\u73b0\u7684\u6392\u540d\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u7b14\u8bb0\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI\u80fd\u4ea7\u751f\u4e00\u81f4\u51c6\u786e\u7684\u6392\u540d\uff0c\u5c3d\u7ba1\u4e2a\u4f53\u8bc4\u4f30\u8005\u5b58\u5728\u5dee\u5f02\uff0c\u4f46LLM\u6295\u7968\u80fd\u53ef\u9760\u5730\u6536\u655b\u5230\u76f8\u540c\u7684\u6700\u4f73\u7cfb\u7edf\u3002", "conclusion": "TEAM-PHI\u901a\u8fc7\u7ed3\u5408\u72ec\u7acb\u8bc4\u4f30\u667a\u80fd\u4f53\u548cLLM\u591a\u6570\u6295\u7968\uff0c\u4e3aPHI\u53bb\u6807\u8bc6\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u8bc4\u4f30\u548c\u6700\u4f73\u6a21\u578b\u9009\u62e9\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17741", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17741", "abs": "https://arxiv.org/abs/2510.17741", "authors": ["Navid Reyhanian", "Reza Ghaderi Zefreh", "Parisa Ramezani", "Emil Bjornson"], "title": "Precoding for Uplink RIS-Assisted Cell-Free MIMO-OFDM Systems with Hardware Impairments", "comment": null, "summary": "This paper studies a reconfigurable intelligent surface (RIS)-assisted\ncell-free massive multiple-input multiple-output (CF-mMIMO) system with\nmultiple RISs. Joint design of transmit precoding, RIS coefficients, and\nreceive combining is investigated for uplink sum-rate maximization under\nin-phase and quadrature phase imbalance (IQI) at user equipments (UEs) and\naccess points (APs). A weighted minimum mean squared error (WMMSE) based block\ncoordinate descent (BCD) approach is proposed, where novel iterative methods\nare developed to efficiently solve the BCD subproblems. The efficiency of\nproposed approaches is demonstrated relative to heuristic methods via extensive\nsimulations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RIS\u8f85\u52a9\u7684CF-mMIMO\u7cfb\u7edf\uff0c\u5728UE\u548cAP\u5b58\u5728IQI\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7WMMSE-BCD\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u4f20\u8f93\u9884\u7f16\u7801\u3001RIS\u7cfb\u6570\u548c\u63a5\u6536\u7ec4\u5408\uff0c\u4ee5\u6700\u5927\u5316\u4e0a\u884c\u94fe\u8def\u548c\u901f\u7387\u3002", "motivation": "\u5728RIS\u8f85\u52a9\u7684CF-mMIMO\u7cfb\u7edf\u4e2d\uff0cUE\u548cAP\u7684IQI\u4f1a\u4e25\u91cd\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eWMMSE\u7684BCD\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u8fed\u4ee3\u7b97\u6cd5\u6765\u9ad8\u6548\u89e3\u51b3BCD\u5b50\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684WMMSE-BCD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3RIS\u8f85\u52a9CF-mMIMO\u7cfb\u7edf\u4e2d\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5b58\u5728IQI\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.17348", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17348", "abs": "https://arxiv.org/abs/2510.17348", "authors": ["Marc Jourdan", "Achraf Azize"], "title": "Optimal Best Arm Identification under Differential Privacy", "comment": "92 pages, 2 figures, 2 tables. To be published in the Thirty-Ninth\n  Annual Conference on Neural Information Processing Systems", "summary": "Best Arm Identification (BAI) algorithms are deployed in data-sensitive\napplications, such as adaptive clinical trials or user studies. Driven by the\nprivacy concerns of these applications, we study the problem of\nfixed-confidence BAI under global Differential Privacy (DP) for Bernoulli\ndistributions. While numerous asymptotically optimal BAI algorithms exist in\nthe non-private setting, a significant gap remains between the best lower and\nupper bounds in the global DP setting. This work reduces this gap to a small\nmultiplicative constant, for any privacy budget $\\epsilon$. First, we provide a\ntighter lower bound on the expected sample complexity of any $\\delta$-correct\nand $\\epsilon$-global DP strategy. Our lower bound replaces the\nKullback-Leibler (KL) divergence in the transportation cost used by the\nnon-private characteristic time with a new information-theoretic quantity that\noptimally trades off between the KL divergence and the Total Variation distance\nscaled by $\\epsilon$. Second, we introduce a stopping rule based on these\ntransportation costs and a private estimator of the means computed using an\narm-dependent geometric batching. En route to proving the correctness of our\nstopping rule, we derive concentration results of independent interest for the\nLaplace distribution and for the sum of Bernoulli and Laplace distributions.\nThird, we propose a Top Two sampling rule based on these transportation costs.\nFor any budget $\\epsilon$, we show an asymptotic upper bound on its expected\nsample complexity that matches our lower bound to a multiplicative constant\nsmaller than $8$. Our algorithm outperforms existing $\\delta$-correct and\n$\\epsilon$-global DP BAI algorithms for different values of $\\epsilon$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5168\u5c40\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u7ea6\u675f\u4e0b\u7684\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u6700\u4f73\u81c2\u8bc6\u522b\uff08BAI\uff09\u95ee\u9898\uff0c\u9488\u5bf9\u4f2f\u52aa\u5229\u5206\u5e03\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u4fe1\u606f\u8bba\u91cf\u6765\u4f18\u5316KL\u6563\u5ea6\u548c\u603b\u53d8\u5dee\u8ddd\u79bb\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u5339\u914d\u4e0b\u754c\u5230\u5e38\u6570\u500d\u5185\u7684\u4e0a\u754c\u7b97\u6cd5\u3002", "motivation": "BAI\u7b97\u6cd5\u5e94\u7528\u4e8e\u6570\u636e\u654f\u611f\u573a\u666f\uff08\u5982\u81ea\u9002\u5e94\u4e34\u5e8a\u8bd5\u9a8c\uff09\uff0c\u5b58\u5728\u9690\u79c1\u62c5\u5fe7\u3002\u73b0\u6709\u5168\u5c40DP\u8bbe\u7f6e\u4e0b\u7684BAI\u7b97\u6cd5\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u4e0b\u754c\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u8981\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "1) \u63d0\u4f9b\u66f4\u7d27\u7684\u4e0b\u754c\uff0c\u7528\u65b0\u7684\u4fe1\u606f\u8bba\u91cf\u66ff\u6362KL\u6563\u5ea6\uff1b2) \u57fa\u4e8e\u8fd0\u8f93\u6210\u672c\u7684\u505c\u6b62\u89c4\u5219\u548c\u81c2\u4f9d\u8d56\u51e0\u4f55\u6279\u5904\u7406\u7684\u79c1\u6709\u5747\u503c\u4f30\u8ba1\u5668\uff1b3) \u57fa\u4e8e\u8fd0\u8f93\u6210\u672c\u7684Top Two\u91c7\u6837\u89c4\u5219\u3002", "result": "\u5bf9\u4e8e\u4efb\u610f\u9690\u79c1\u9884\u7b97\u03b5\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u671f\u671b\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u4e0b\u754c\u5339\u914d\u7684\u6e10\u8fd1\u4e0a\u754c\uff0c\u4e58\u6027\u5e38\u6570\u5c0f\u4e8e8\uff0c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u663e\u8457\u7f29\u5c0f\u4e86\u5168\u5c40DP\u8bbe\u7f6e\u4e0bBAI\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u4e0b\u754c\u5dee\u8ddd\uff0c\u4e3a\u6570\u636e\u654f\u611f\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4BAI\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15970", "abs": "https://arxiv.org/abs/2510.15970", "authors": ["Yang Ba", "Mohammad Sadeq Abolhasani", "Rong Pan"], "title": "Predict Training Data Quality via Its Geometry in Metric Space", "comment": "Accepted to the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "High-quality training data is the foundation of machine learning and\nartificial intelligence, shaping how models learn and perform. Although much is\nknown about what types of data are effective for training, the impact of the\ndata's geometric structure on model performance remains largely underexplored.\nWe propose that both the richness of representation and the elimination of\nredundancy within training data critically influence learning outcomes. To\ninvestigate this, we employ persistent homology to extract topological features\nfrom data within a metric space, thereby offering a principled way to quantify\ndiversity beyond entropy-based measures. Our findings highlight persistent\nhomology as a powerful tool for analyzing and enhancing the training data that\ndrives AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6301\u4e45\u540c\u8c03\u5206\u6790\u8bad\u7ec3\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u6570\u636e\u8868\u793a\u7684\u4e30\u5bcc\u6027\u548c\u5197\u4f59\u6d88\u9664\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u662f\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u7684\u57fa\u7840\uff0c\u4f46\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u8ba4\u4e3a\u6570\u636e\u7684\u8868\u793a\u4e30\u5bcc\u6027\u548c\u5197\u4f59\u6d88\u9664\u5bf9\u5b66\u4e60\u7ed3\u679c\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6301\u4e45\u540c\u8c03\u4ece\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u6570\u636e\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d85\u8d8a\u57fa\u4e8e\u71b5\u7684\u5ea6\u91cf\u6765\u91cf\u5316\u591a\u6837\u6027\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6301\u4e45\u540c\u8c03\u662f\u5206\u6790\u548c\u589e\u5f3a\u9a71\u52a8AI\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u7684\u5f3a\u5927\u5de5\u5177\u3002", "conclusion": "\u6301\u4e45\u540c\u8c03\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347AI\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17472", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17472", "abs": "https://arxiv.org/abs/2510.17472", "authors": ["Paula Cordero-Encinar", "Andrew B. Duncan"], "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs", "comment": null, "summary": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u8bc1\u660eLLMs\u4e2d\u81ea\u4e00\u81f4\u6027\u548c\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u63d0\u4f9b\u7edf\u8ba1\u8bc1\u4e66\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u505c\u6b62\u89c4\u5219MMC\u6765\u51cf\u5c11\u8ba4\u8bc1\u6240\u9700\u6837\u672c\u6570\u3002", "motivation": "\u5f53\u524d\u81ea\u4e00\u81f4\u6027\u548c\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u9ad8LLM\u53ef\u9760\u6027\uff0c\u4f46\u5176\u5185\u5728\u673a\u5236\u548c\u7edf\u8ba1\u4fdd\u8bc1\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u7edf\u8ba1\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u6570\u6295\u7968\u63d0\u4f9b\u81ea\u4e00\u81f4\u6027\u7684\u7edf\u8ba1\u8bc1\u4e66\uff0c\u63a8\u5bfc\u6709\u9650\u6837\u672c\u548c\u4efb\u610f\u65f6\u95f4\u6709\u6548\u7684\u96c6\u4e2d\u754c\u9650\uff0c\u5f15\u5165MMC\u5e8f\u5217\u505c\u6b62\u89c4\u5219\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u540e\u8bad\u7ec3\u76ee\u6807\u6765\u4f18\u5316\u9510\u5ea6\u4e0e\u504f\u5dee\u7684\u6743\u8861\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u6570\u6295\u7968\u805a\u5408\u7b54\u6848\u4ee5\u9ad8\u6982\u7387\u4e0e\u6a21\u578b\u7ec8\u7aef\u5206\u5e03\u7684\u4f17\u6570\u4e00\u81f4\uff0cTTRL\u901a\u8fc7\u6307\u6570\u503e\u659c\u4f7f\u7b54\u6848\u5206\u5e03\u5411\u4f17\u6570\u96c6\u4e2d\uff0c\u4ece\u800c\u51cf\u5c11\u8ba4\u8bc1\u6240\u9700\u6837\u672c\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u5355\u4e00\u7edf\u8ba1\u6846\u67b6\u5185\u89e3\u91ca\u4e86\u81ea\u4e00\u81f4\u6027\u548cTTRL\u8fd9\u4e24\u79cd\u6838\u5fc3\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff0c\u4e3a\u65e0\u6807\u7b7e\u3001\u53ef\u8bc1\u660e\u53ef\u9760\u7684\u63a8\u7406LLMs\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.15978", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.15978", "abs": "https://arxiv.org/abs/2510.15978", "authors": ["Junchao Gong", "Jingyi Xu", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Kun Chen", "Wanghan Xu", "Weidong Yang", "Xiaokang Yang", "Lei Bai"], "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space", "comment": null, "summary": "Weather prediction is a critical task for human society, where impressive\nprogress has been made by training artificial intelligence weather prediction\n(AIWP) methods with reanalysis data. However, reliance on reanalysis data\nlimits the AIWPs with shortcomings, including data assimilation biases and\ntemporal discrepancies. To liberate AIWPs from the reanalysis data, observation\nforecasting emerges as a transformative paradigm for weather prediction. One of\nthe key challenges in observation forecasting is learning spatiotemporal\ndynamics across disparate measurement systems with irregular high-resolution\nobservation data, which constrains the design and prediction of AIWPs. To this\nend, we propose our DAWP as an innovative framework to enable AIWPs to operate\nin a complete observation space by initialization with an artificial\nintelligence data assimilation (AIDA) module. Specifically, our AIDA module\napplies a mask multi-modality autoencoder(MMAE)for assimilating irregular\nsatellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a\nspatiotemporal decoupling transformer with cross-regional boundary conditioning\n(CBC), learning the dynamics in observation space, to enable sub-image-based\nglobal observation forecasting. Comprehensive experiments demonstrate that AIDA\ninitialization significantly improves the roll out and efficiency of AIWP.\nAdditionally, we show that DAWP holds promising potential to be applied in\nglobal precipitation forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86DAWP\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6570\u636e\u540c\u5316\u6a21\u5757(AIDA)\u5c06AI\u5929\u6c14\u9884\u6d4b\u4ece\u518d\u5206\u6790\u6570\u636e\u4f9d\u8d56\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u5b9e\u73b0\u57fa\u4e8e\u89c2\u6d4b\u6570\u636e\u7684\u76f4\u63a5\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfAI\u5929\u6c14\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u518d\u5206\u6790\u6570\u636e\uff0c\u5b58\u5728\u6570\u636e\u540c\u5316\u504f\u5dee\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\u89c2\u6d4b\u9884\u6d4b\u4f5c\u4e3a\u4e00\u79cd\u53d8\u9769\u6027\u8303\u5f0f\uff0c\u53ef\u4ee5\u89e3\u653eAI\u5929\u6c14\u9884\u6d4b\u5bf9\u518d\u5206\u6790\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faDAWP\u6846\u67b6\uff0c\u5305\u542bAIDA\u6a21\u5757\u548cAIWP\u6a21\u5757\u3002AIDA\u4f7f\u7528\u63a9\u7801\u591a\u6a21\u6001\u81ea\u7f16\u7801\u5668(MMAE)\u540c\u5316\u4e0d\u89c4\u5219\u536b\u661f\u89c2\u6d4b\u6570\u636e\uff1bAIWP\u91c7\u7528\u65f6\u7a7a\u89e3\u8026transformer\u548c\u8de8\u533a\u57df\u8fb9\u754c\u6761\u4ef6(CBC)\u5b66\u4e60\u89c2\u6d4b\u7a7a\u95f4\u4e2d\u7684\u52a8\u6001\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAIDA\u521d\u59cb\u5316\u663e\u8457\u63d0\u9ad8\u4e86AIWP\u7684\u6eda\u52a8\u9884\u6d4b\u80fd\u529b\u548c\u6548\u7387\u3002DAWP\u5728\u5168\u7403\u964d\u6c34\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u826f\u597d\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "DAWP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u89c2\u6d4b\u6570\u636e\u76f4\u63a5\u8fdb\u884c\u5929\u6c14\u9884\u6d4b\uff0c\u6446\u8131\u4e86\u5bf9\u518d\u5206\u6790\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3aAI\u5929\u6c14\u9884\u6d4b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17406", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17406", "abs": "https://arxiv.org/abs/2510.17406", "authors": ["Tiezhi Wang", "Wilhelm Haverkamp", "Nils Strodthoff"], "title": "S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction", "comment": null, "summary": "The electrocardiogram (ECG) exemplifies biosignal-based time series with\ncontinuous, temporally ordered structure reflecting cardiac physiological and\npathophysiological dynamics. Detailed analysis of these dynamics has proven\nchallenging, as conventional methods capture either global trends or local\nwaveform features but rarely their simultaneous interplay at high temporal\nresolution. To bridge global and local signal analysis, we introduce S4ECG, a\nnovel deep learning architecture leveraging structured state space models for\nmulti-epoch arrhythmia classification. Our joint multi-epoch predictions\nsignificantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,\nwith atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,\ndemonstrating superior performance in-distribution and enhanced\nout-of-distribution robustness. Systematic investigation reveals optimal\ntemporal dependency windows spanning 10-20 minutes for peak performance. This\nwork contributes to a paradigm shift toward temporally-aware arrhythmia\ndetection algorithms, opening new possibilities for ECG interpretation, in\nparticular for complex arrhythmias like atrial fibrillation and atrial flutter.", "AI": {"tldr": "S4ECG\u662f\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u591a\u65f6\u6bb5\u5fc3\u5f8b\u5931\u5e38\u5206\u7c7b\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u65f6\u6bb5\u65b9\u6cd5\uff0c\u5728\u623f\u98a4\u7279\u5f02\u6027\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7535\u56fe\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5168\u5c40\u8d8b\u52bf\u548c\u5c40\u90e8\u6ce2\u5f62\u7279\u5f81\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4ea4\u4e92\u4f5c\u7528\uff0c\u9700\u8981\u6865\u63a5\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u53f7\u5206\u6790\u3002", "method": "\u5f15\u5165S4ECG\u67b6\u6784\uff0c\u5229\u7528\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u591a\u65f6\u6bb5\u5fc3\u5f8b\u5931\u5e38\u5206\u7c7b\uff0c\u7cfb\u7edf\u7814\u7a76\u6700\u4f73\u65f6\u95f4\u4f9d\u8d56\u7a97\u53e3\u3002", "result": "\u591a\u65f6\u6bb5\u8054\u5408\u9884\u6d4b\u5728\u5b8f\u89c2AUROC\u4e0a\u6bd4\u5355\u65f6\u6bb5\u65b9\u6cd5\u63d0\u9ad81.0-11.6%\uff0c\u623f\u98a4\u7279\u5f02\u6027\u4ece0.718-0.979\u63d0\u5347\u81f30.967-0.998\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u52a8\u4e86\u5fc3\u5f8b\u5931\u5e38\u68c0\u6d4b\u7b97\u6cd5\u5411\u65f6\u95f4\u611f\u77e5\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u4e3a\u5fc3\u7535\u56fe\u89e3\u91ca\u7279\u522b\u662f\u590d\u6742\u5fc3\u5f8b\u5931\u5e38\u5982\u623f\u98a4\u548c\u623f\u6251\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.17543", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17543", "abs": "https://arxiv.org/abs/2510.17543", "authors": ["Jiayi Huang", "Sangwoo Park", "Nicola Paoletti", "Osvaldo Simeone"], "title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment", "comment": "Under Review", "summary": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u5bf9\u9f50\u7684\u8fb9\u7f18-\u4e91\u7ea7\u8054\u673a\u5236(CAb)\uff0c\u901a\u8fc7\u591a\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\u786e\u4fdd\u8fb9\u7f18\u6a21\u578b\u9884\u6d4b\u96c6\u6ee1\u8db3\u4e91\u7ea7\u6761\u4ef6\u8986\u76d6\u8981\u6c42\uff0c\u5728\u4fdd\u6301\u76ee\u6807\u6761\u4ef6\u8986\u76d6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5411\u4e91\u7aef\u7684\u5378\u8f7d\u3002", "motivation": "\u8fb9\u7f18\u667a\u80fd\u867d\u7136\u80fd\u901a\u8fc7\u7d27\u51d1\u7684\u7aef\u4fa7\u6a21\u578b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u4f46\u4fdd\u8bc1\u53ef\u9760\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u786e\u4fdd\u8fb9\u7f18\u6a21\u578b\u8fd4\u56de\u7684\u9884\u6d4b\u96c6\u80fd\u591f\u4ee5\u7528\u6237\u6307\u5b9a\u7684\u6982\u7387\u5305\u542b\u771f\u5b9e\u6807\u7b7e\uff0c\u5c31\u50cf\u4e91\u6a21\u578b\u4ea7\u751f\u7684\u4e00\u6837\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fdd\u5f62\u5bf9\u9f50(CAb)\u7684\u7ea7\u8054\u673a\u5236\uff0c\u5c06\u8fb9\u7f18\u5230\u4e91\u7aef\u7684\u5347\u7ea7\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u591a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u4f7f\u7528\u4fdd\u5f62\u5bf9\u9f50\u6765\u9009\u62e9\u54ea\u4e9b\u8f93\u5165\u53ef\u4ee5\u5728\u8fb9\u7f18\u5b89\u5168\u5904\u7406\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u8fb9\u7f18\u9884\u6d4b\u96c6\uff0c\u5305\u62ec\u4fdd\u5f62\u9884\u6d4b\u7684\u53d8\u4f53\u3002", "result": "\u5728CIFAR-100\u56fe\u50cf\u5206\u7c7b\u548cTeleQnA\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAb\u7ea7\u8054\u65b9\u6cd5\u5728\u4fdd\u6301\u76ee\u6807\u6761\u4ef6\u8986\u76d6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5411\u4e91\u7aef\u7684\u5378\u8f7d\uff0c\u9884\u6d4b\u96c6\u5927\u5c0f\u4ec5\u6709\u9002\u5ea6\u589e\u52a0\u3002", "conclusion": "CAb\u7ea7\u8054\u65b9\u6cd5\u4e3a\u8fb9\u7f18\u9884\u6d4b\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u786e\u4fdd\u6ee1\u8db3\u4e91\u7ea7\u6761\u4ef6\u8986\u76d6\u8981\u6c42\uff0c\u540c\u65f6\u66b4\u9732\u4e86\u8986\u76d6\u5ea6\u3001\u5ef6\u8fdf\u7387\u548c\u96c6\u5408\u5927\u5c0f\u4e4b\u95f4\u7684\u53ef\u8c03\u6743\u8861\u3002"}}
{"id": "2510.15985", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15985", "abs": "https://arxiv.org/abs/2510.15985", "authors": ["Zexi Tan", "Tao Xie", "Binbin Sun", "Xiang Zhang", "Yiqun Zhang", "Yiu-Ming Cheung"], "title": "MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction", "comment": "Accepted to PRICAI 2025", "summary": "Sepsis is a life-threatening infectious syndrome associated with high\nmortality in intensive care units (ICUs). Early and accurate sepsis prediction\n(SP) is critical for timely intervention, yet remains challenging due to subtle\nearly manifestations and rapidly escalating mortality. While AI has improved SP\nefficiency, existing methods struggle to capture weak early temporal signals.\nThis paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)\nmechanism to construct enriched feature views, coupled with a Cascaded\nDual-convolution Time-series Attention (CDTA) module for multi-scale temporal\nrepresentation learning. The proposed MEET-Sepsis framework achieves\ncompetitive prediction accuracy using only 20% of the ICU monitoring time\nrequired by SOTA methods, significantly advancing early SP. Extensive\nvalidation confirms its efficacy. Code is available at:\nhttps://github.com/yueliangy/MEET-Sepsis.", "AI": {"tldr": "\u63d0\u51faMEET-Sepsis\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5185\u6e90\u6027\u89c6\u56fe\u8868\u793a\u589e\u5f3a\u673a\u5236\u548c\u7ea7\u8054\u53cc\u5377\u79ef\u65f6\u95f4\u5e8f\u5217\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ec5\u970020%ICU\u76d1\u6d4b\u65f6\u95f4\u5373\u53ef\u5b9e\u73b0\u7ade\u4e89\u6027\u7684\u8113\u6bd2\u75c7\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u8113\u6bd2\u75c7\u662fICU\u4e2d\u6b7b\u4ea1\u7387\u9ad8\u7684\u5a01\u80c1\u751f\u547d\u7684\u611f\u67d3\u7efc\u5408\u5f81\uff0c\u65e9\u671f\u51c6\u786e\u9884\u6d4b\u5bf9\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5fae\u5f31\u7684\u65e9\u671f\u65f6\u95f4\u4fe1\u53f7\u3002", "method": "\u91c7\u7528\u591a\u5185\u6e90\u6027\u89c6\u56fe\u8868\u793a\u589e\u5f3a\u673a\u5236\u6784\u5efa\u4e30\u5bcc\u7279\u5f81\u89c6\u56fe\uff0c\u7ed3\u5408\u7ea7\u8054\u53cc\u5377\u79ef\u65f6\u95f4\u5e8f\u5217\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u591a\u5c3a\u5ea6\u65f6\u95f4\u8868\u793a\u5b66\u4e60\u3002", "result": "\u4ec5\u970020%ICU\u76d1\u6d4b\u65f6\u95f4\u5373\u53ef\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u6027\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u65e9\u671f\u8113\u6bd2\u75c7\u9884\u6d4b\u3002", "conclusion": "MEET-Sepsis\u6846\u67b6\u5728\u65e9\u671f\u8113\u6bd2\u75c7\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u5e7f\u6cdb\u9a8c\u8bc1\u786e\u8ba4\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSELECT\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u9009\u62e9\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u64e6\u9664\u7684\u951a\u70b9\u56fa\u5b9a\u95ee\u9898\uff0c\u907f\u514d\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\uff0c\u5bfc\u81f4\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faSELECT\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u64e6\u9664\u951a\u70b9\u5e76\u8bc6\u522b\u8fb9\u754c\u951a\u70b9\u4ee5\u4fdd\u62a4\u76f8\u5173\u6982\u5ff5\u3002", "result": "SELECT\u4f5c\u4e3a\u901a\u7528\u951a\u70b9\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u9ad8\u6548\u9002\u914d\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5355\u4e2a\u6982\u5ff5\u951a\u70b9\u6316\u6398\u5e73\u5747\u4ec5\u97004\u79d2\u3002", "conclusion": "\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6846\u67b6SELECT\u6709\u6548\u89e3\u51b3\u4e86\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6982\u5ff5\u64e6\u9664\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.15986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15986", "abs": "https://arxiv.org/abs/2510.15986", "authors": ["Sifeddine Sellami", "Juba Agoun", "Lamia Yessad", "Louenas Bounia"], "title": "User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis", "comment": "in French language, Plate-Forme Intelligence Artificielle, Jun 2025,\n  Dijon (FRANCE), France", "summary": "Sleep disorders have a major impact on patients' health and quality of life,\nbut their diagnosis remains complex due to the diversity of symptoms. Today,\ntechnological advances, combined with medical data analysis, are opening new\nperspectives for a better understanding of these disorders. In particular,\nexplainable artificial intelligence (XAI) aims to make AI model decisions\nunderstandable and interpretable for users. In this study, we propose a\nclustering-based method to group patients according to different sleep disorder\nprofiles. By integrating an explainable approach, we identify the key factors\ninfluencing these pathologies. An experiment on anonymized real data\nillustrates the effectiveness and relevance of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u7528\u4e8e\u6839\u636e\u7761\u7720\u969c\u788d\u7279\u5f81\u5bf9\u60a3\u8005\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u5f71\u54cd\u8fd9\u4e9b\u75c5\u7406\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u7761\u7720\u969c\u788d\u5bf9\u60a3\u8005\u5065\u5eb7\u548c\u751f\u6d3b\u8d28\u91cf\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u4f46\u7531\u4e8e\u75c7\u72b6\u591a\u6837\u6027\uff0c\u8bca\u65ad\u4ecd\u7136\u590d\u6742\u3002\u6280\u672f\u8fdb\u5c55\u548c\u533b\u7597\u6570\u636e\u5206\u6790\u4e3a\u66f4\u597d\u7406\u89e3\u8fd9\u4e9b\u969c\u788d\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\uff0c\u5c06\u60a3\u8005\u6309\u4e0d\u540c\u7761\u7720\u969c\u788d\u7279\u5f81\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u6574\u5408\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u5728\u533f\u540d\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u805a\u7c7b\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u7761\u7720\u969c\u788d\u60a3\u8005\u7684\u4e0d\u540c\u7279\u5f81\u6a21\u5f0f\uff0c\u5e76\u4e3a\u7406\u89e3\u8fd9\u4e9b\u969c\u788d\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.16368", "categories": ["cs.AI", "cs.HC", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.16368", "abs": "https://arxiv.org/abs/2510.16368", "authors": ["Ali Shirali"], "title": "The Burden of Interactive Alignment with Inconsistent Preferences", "comment": "Published as a conference paper at NeurIPS 2025", "summary": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5177\u6709\u4e0d\u4e00\u81f4\u504f\u597d\u7684\u7528\u6237\u5982\u4f55\u901a\u8fc7\u6218\u7565\u6027\u5730\u4e0e\u7b97\u6cd5\u4e92\u52a8\u6765\u5f15\u5bfc\u7b97\u6cd5\u66f4\u597d\u5730\u7b26\u5408\u5176\u771f\u5b9e\u5174\u8da3\u3002\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u65f6\u95f4\u8de8\u5ea6\u9608\u503c\uff1a\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u9f50\uff0c\u800c\u77ed\u89c6\u7684\u7528\u6237\u5219\u4f1a\u88ab\u7b97\u6cd5\u76ee\u6807\u6240\u540c\u5316\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u7528\u6237\u4e0e\u7b97\u6cd5\u4e92\u52a8\u4e2d\u7684\u4e0d\u4e00\u81f4\u504f\u597d\u95ee\u9898\u3002\u7528\u6237\u53ef\u80fd\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u5728\u4f4e\u4ef7\u503c\u5185\u5bb9\u4e0a\uff0c\u65e0\u610f\u4e2d\u5411\u7b97\u6cd5\u4f20\u9012\u9519\u8bef\u4fe1\u53f7\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u8fd9\u7c7b\u7528\u6237\u9700\u8981\u4ec0\u4e48\u6761\u4ef6\u624d\u80fd\u8ba9\u7b97\u6cd5\u4e0e\u5176\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\uff1f", "method": "\u5c06\u7528\u6237\u7684\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7406\u6027\u7cfb\u7edf2\uff08\u51b3\u5b9a\u662f\u5426\u53c2\u4e0e\uff09\u548c\u51b2\u52a8\u7cfb\u7edf1\uff08\u51b3\u5b9a\u53c2\u4e0e\u65f6\u957f\uff09\u7684\u5206\u79bb\u3002\u91c7\u7528\u591a\u9886\u5bfc\u8005-\u5355\u8ddf\u968f\u8005\u7684\u6269\u5c55Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u7528\u6237\uff08\u7cfb\u7edf2\uff09\u901a\u8fc7\u627f\u8bfa\u53c2\u4e0e\u7b56\u7565\u6765\u9886\u5bfc\uff0c\u7b97\u6cd5\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u4e92\u52a8\u505a\u51fa\u6700\u4f73\u54cd\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u5bf9\u9f50\u8d1f\u62c5\uff08alignment burden\uff09\u2014\u2014\u7528\u6237\u5fc5\u987b\u4f18\u5316\u7684\u6700\u5c0f\u65f6\u95f4\u8de8\u5ea6\u3002\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u9f50\uff0c\u800c\u77ed\u89c6\u7528\u6237\u5219\u4f1a\u88ab\u7b97\u6cd5\u76ee\u6807\u540c\u5316\u3002\u5373\u4f7f\u662f\u4e00\u4e2a\u5c0f\u7684\u3001\u6709\u6210\u672c\u7684\u4fe1\u53f7\uff08\u5982\u989d\u5916\u70b9\u51fb\uff09\u4e5f\u80fd\u663e\u8457\u964d\u4f4e\u8fd9\u4e2a\u5173\u952e\u65f6\u95f4\u8de8\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u91ca\u4e86\u5177\u6709\u4e0d\u4e00\u81f4\u504f\u597d\u7684\u7528\u6237\u5982\u4f55\u5728Stackelberg\u5747\u8861\u4e2d\u4f7f\u53c2\u4e0e\u9a71\u52a8\u7684\u7b97\u6cd5\u4e0e\u5176\u5174\u8da3\u5bf9\u9f50\uff0c\u65e2\u7a81\u51fa\u4e86\u5b9e\u73b0\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4e5f\u6307\u51fa\u4e86\u6f5c\u5728\u7684\u8865\u6551\u63aa\u65bd\u3002"}}
{"id": "2510.16161", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16161", "abs": "https://arxiv.org/abs/2510.16161", "authors": ["Ankitkumar Joshi", "Milos Hauskrecht"], "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction", "comment": null, "summary": "Modeling irregularly sampled multivariate time series is a persistent\nchallenge in domains like healthcare and sensor networks. While recent works\nhave explored a variety of complex learning architectures to solve the\nprediction problems for irregularly sampled time series, it remains unclear\nwhat are the true benefits of some of these architectures, and whether clever\nmodifications of simpler and more efficient RNN-based algorithms are still\ncompetitive, i.e. they are on par with or even superior to these methods. In\nthis work, we propose and study GRUwE: Gated Recurrent Unit with Exponential\nbasis functions, that builds upon RNN-based architectures for observations made\nat irregular times. GRUwE supports both regression-based and event-based\npredictions in continuous time. GRUwE works by maintaining a Markov state\nrepresentation of the time series that updates with the arrival of irregular\nobservations. The Markov state update relies on two reset mechanisms: (i)\nobservation-triggered reset, and (ii) time-triggered reset of the GRU state\nusing learnable exponential decays, to support the predictions in continuous\ntime. Our empirical evaluations across several real-world benchmarks on\nnext-observation and next-event prediction tasks demonstrate that GRUwE can\nindeed achieve competitive to superior performance compared to the recent\nstate-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers\ncompelling advantages: it is easy to implement, requires minimal\nhyper-parameter tuning efforts, and significantly reduces the computational\noverhead in the online deployment.", "AI": {"tldr": "GRUwE\u662f\u4e00\u79cd\u57fa\u4e8eRNN\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6307\u6570\u8870\u51cf\u673a\u5236\u5b9e\u73b0\u8fde\u7eed\u65f6\u95f4\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u5b66\u4e60\u67b6\u6784\u7684\u771f\u5b9e\u6548\u76ca\u4e0d\u660e\u786e\uff0c\u9700\u8981\u9a8c\u8bc1\u7b80\u5355\u9ad8\u6548\u7684RNN\u6539\u8fdb\u65b9\u6cd5\u662f\u5426\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002", "method": "\u63d0\u51faGRUwE\uff08\u5e26\u6307\u6570\u57fa\u51fd\u6570\u7684\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff09\uff0c\u901a\u8fc7\u4e24\u79cd\u91cd\u7f6e\u673a\u5236\u66f4\u65b0\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\uff1a\u89c2\u6d4b\u89e6\u53d1\u91cd\u7f6e\u548c\u65f6\u95f4\u89e6\u53d1\u91cd\u7f6e\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6307\u6570\u8870\u51cf\u652f\u6301\u8fde\u7eed\u65f6\u95f4\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRUwE\u5728\u4e0b\u4e00\u89c2\u6d4b\u548c\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u751a\u81f3\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "GRUwE\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u800c\u4e14\u5b9e\u73b0\u7b80\u5355\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u9700\u6c42\u5c11\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5728\u7ebf\u90e8\u7f72\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2510.16185", "categories": ["cs.LG", "cs.AI", "cs.FL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16185", "abs": "https://arxiv.org/abs/2510.16185", "authors": ["Daniel Donnelly", "Angelo Ferrando", "Francesco Belardinelli"], "title": "Expressive Reward Synthesis with the Runtime Monitoring Language", "comment": null, "summary": "A key challenge in reinforcement learning (RL) is reward (mis)specification,\nwhereby imprecisely defined reward functions can result in unintended, possibly\nharmful, behaviours. Indeed, reward functions in RL are typically treated as\nblack-box mappings from state-action pairs to scalar values. While effective in\nmany settings, this approach provides no information about why rewards are\ngiven, which can hinder learning and interpretability. Reward Machines address\nthis issue by representing reward functions as finite state automata, enabling\nthe specification of structured, non-Markovian reward functions. However, their\nexpressivity is typically bounded by regular languages, leaving them unable to\ncapture more complex behaviours such as counting or parametrised conditions. In\nthis work, we build on the Runtime Monitoring Language (RML) to develop a novel\nclass of language-based Reward Machines. By leveraging the built-in memory of\nRML, our approach can specify reward functions for non-regular, non-Markovian\ntasks. We demonstrate the expressiveness of our approach through experiments,\nhighlighting additional advantages in flexible event-handling and task\nspecification over existing Reward Machine-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00\uff08RML\uff09\u7684\u65b0\u578b\u8bed\u8a00\u5956\u52b1\u673a\uff0c\u80fd\u591f\u8868\u8fbe\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u673a\u8868\u8fbe\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u51fd\u6570\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\u6620\u5c04\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\uff1b\u4f20\u7edf\u5956\u52b1\u673a\u53ea\u80fd\u8868\u8fbe\u6b63\u5219\u8bed\u8a00\uff0c\u65e0\u6cd5\u5904\u7406\u8ba1\u6570\u6216\u53c2\u6570\u5316\u6761\u4ef6\u7b49\u590d\u6742\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00\uff08RML\uff09\u6784\u5efa\u65b0\u578b\u8bed\u8a00\u5956\u52b1\u673a\uff0c\u5229\u7528RML\u7684\u5185\u7f6e\u5185\u5b58\u673a\u5236\u6765\u6307\u5b9a\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u5956\u52b1\u673a\u7684\u65b9\u6cd5\uff0c\u5728\u4e8b\u4ef6\u5904\u7406\u548c\u4efb\u52a1\u89c4\u8303\u65b9\u9762\u66f4\u52a0\u7075\u6d3b\u3002", "conclusion": "\u57fa\u4e8eRML\u7684\u8bed\u8a00\u5956\u52b1\u673a\u6269\u5c55\u4e86\u5956\u52b1\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.15992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15992", "abs": "https://arxiv.org/abs/2510.15992", "authors": ["Ziming Dai", "Tuo Zhang", "Fei Gao", "Xingyi Cai", "Xiaofei Wang", "Cheng Zhang", "Wenyu Wang", "Chengjie Zang"], "title": "Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments", "comment": null, "summary": "The growing industrial demand for customized and cost-efficient large\nlanguage models (LLMs) is fueled by the rise of vertical, domain-specific tasks\nand the need to optimize performance under constraints such as latency and\nbudget. Knowledge distillation, as an efficient model compression and transfer\ntechnique, offers a feasible solution. However, existing distillation\nframeworks often require manual intervention and struggle to meet such complex\nuser-defined distillation requirements. To bridge this gap, we propose Stratos,\nan end-to-end LLM distillation pipeline that automates server and model\nselection, knowledge distillation, and deployment in distributed cloud\nenvironments. Given user-defined constraints on model performance and system\nbudget, Stratos automatically selects Pareto-optimal servers, dynamically\nmatches teacher-student pairs, and adapts distillation strategies based on task\ncomplexity to optimize cloud hosting. Experiments show that Stratos produces a\nstudent model that achieves four times the accuracy of its GPT-4o teacher\nbaseline on a rare, domain-specific Mahjong reasoning task with reverse\nsynthetic data and knowledge injection. Moreover, it achieves reduced latency\nand cost without compromising accuracy. These results highlight its promise for\nvertical-domain LLM deployment.", "AI": {"tldr": "Stratos\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684LLM\u84b8\u998f\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u670d\u52a1\u5668\u9009\u62e9\u3001\u5e08\u751f\u6a21\u578b\u914d\u5bf9\u548c\u84b8\u998f\u7b56\u7565\uff0c\u5728\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7ea6\u675f\u4e0b\u4f18\u5316\u4e91\u90e8\u7f72\uff0c\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5b9e\u73b04\u500d\u7cbe\u5ea6\u63d0\u5347\u5e76\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u5de5\u4e1a\u5bf9\u5b9a\u5236\u5316\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u6c42\u589e\u957f\uff0c\u73b0\u6709\u84b8\u998f\u6846\u67b6\u9700\u8981\u4eba\u5de5\u5e72\u9884\u4e14\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u7528\u6237\u9700\u6c42\u3002", "method": "\u63d0\u51faStratos\u7aef\u5230\u7aefLLM\u84b8\u998f\u7ba1\u9053\uff0c\u81ea\u52a8\u5316\u670d\u52a1\u5668\u548c\u6a21\u578b\u9009\u62e9\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u5206\u5e03\u5f0f\u4e91\u73af\u5883\u90e8\u7f72\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728\u7279\u5b9a\u9ebb\u5c06\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5b66\u751f\u6a21\u578b\u8fbe\u5230GPT-4o\u6559\u5e08\u57fa\u51c64\u500d\u7cbe\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "Stratos\u5c55\u793a\u4e86\u5728\u5782\u76f4\u9886\u57dfLLM\u90e8\u7f72\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u81ea\u52a8\u6ee1\u8db3\u590d\u6742\u7ea6\u675f\u6761\u4ef6\u5e76\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2510.15996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15996", "abs": "https://arxiv.org/abs/2510.15996", "authors": ["Ozan K. Tonguz", "Federico Taschin"], "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning", "comment": null, "summary": "One of the major problems in Machine Learning (ML) and Artificial\nIntelligence (AI) is the fact that the probability distribution of the test\ndata in the real world could deviate substantially from the probability\ndistribution of the training data set. When this happens, the predictions of an\nML system or an AI agent could involve large errors which is very troublesome\nand undesirable. While this is a well-known hard problem plaguing the AI and ML\nsystems' accuracy and reliability, in certain applications such errors could be\ncritical for safety and reliability of AI and ML systems. One approach to deal\nwith this problem is to monitor and measure the deviation in the probability\ndistribution of the test data in real time and to compensate for this\ndeviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov\n(KS) Test for measuring the distribution shift and we show how the KS distance\ncan be used to quantify the distribution shift and its impact on an AI agent's\nperformance. Our results suggest that KS distance could be used as a valuable\nstatistical tool for monitoring and measuring the distribution shift. More\nspecifically, it is shown that even a distance of KS=0.02 could lead to about\n50\\% increase in the travel time at a single intersection using a Reinforcement\nLearning agent which is quite significant. It is hoped that the use of KS Test\nand KS distance in AI-based smart transportation could be an important step\nforward for gauging the performance degradation of an AI agent in real time and\nthis, in turn, could help the AI agent to cope with the distribution shift in a\nmore informed manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u6765\u76d1\u6d4b\u548c\u91cf\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u4e2d\uff0cKS\u8ddd\u79bb=0.02\u53ef\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u5355\u4e2a\u4ea4\u53c9\u53e3\u7684\u65c5\u884c\u65f6\u95f4\u589e\u52a0\u7ea650%\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u6d4b\u8bd5\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u6982\u7387\u5206\u5e03\u504f\u79bb\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u5206\u5e03\u504f\u79fb\u4f1a\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5c24\u5176\u5371\u9669\u3002", "method": "\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u6765\u6d4b\u91cf\u5206\u5e03\u504f\u79fb\uff0c\u5229\u7528KS\u8ddd\u79bb\u91cf\u5316\u5206\u5e03\u504f\u79fb\u53ca\u5176\u5bf9AI\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660eKS\u8ddd\u79bb\u53ef\u4f5c\u4e3a\u76d1\u6d4b\u5206\u5e03\u504f\u79fb\u7684\u6709\u4ef7\u503c\u7edf\u8ba1\u5de5\u5177\uff0c\u5373\u4f7fKS=0.02\u7684\u5c0f\u504f\u79fb\u4e5f\u4f1a\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u5355\u4e2a\u4ea4\u53c9\u53e3\u7684\u65c5\u884c\u65f6\u95f4\u663e\u8457\u589e\u52a0\u7ea650%\u3002", "conclusion": "\u5728\u57fa\u4e8eAI\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u4f7f\u7528KS\u68c0\u9a8c\u548cKS\u8ddd\u79bb\uff0c\u53ef\u4ee5\u5b9e\u65f6\u8bc4\u4f30AI\u667a\u80fd\u4f53\u7684\u6027\u80fd\u9000\u5316\uff0c\u5e2e\u52a9\u667a\u80fd\u4f53\u4ee5\u66f4\u660e\u667a\u7684\u65b9\u5f0f\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u3002"}}
{"id": "2510.16466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16466", "abs": "https://arxiv.org/abs/2510.16466", "authors": ["Siddhartha Krothapalli", "Tridib Kumar Das", "Praveen Kumar", "Naveen Suravarpu", "Pratik Narang"], "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights", "comment": "11 pages, 1 figure, 4 tables", "summary": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback.", "AI": {"tldr": "ReviewSense\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u53ef\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u6709\u9488\u5bf9\u6027\u7684\u5546\u4e1a\u5efa\u8bae\uff0c\u8d85\u8d8a\u4f20\u7edf\u504f\u597d\u9884\u6d4b\u7cfb\u7edf\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "motivation": "\u968f\u7740\u5ba2\u6237\u53cd\u9988\u5bf9\u6218\u7565\u589e\u957f\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u4ece\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u4e2d\u63d0\u53d6\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002\u4f20\u7edfAI\u7cfb\u7edf\u64c5\u957f\u9884\u6d4b\u7528\u6237\u504f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u9762\u5411\u4e1a\u52a1\u7684\u89c4\u8303\u6027\u5efa\u8bae\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faReviewSense\u6846\u67b6\uff0c\u6574\u5408\u805a\u7c7b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u548c\u4e13\u5bb6\u9a71\u52a8\u8bc4\u4f30\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u4e1a\u52a1\u5bfc\u5411\u6d41\u7a0b\uff0c\u8bc6\u522b\u5173\u952e\u8d8b\u52bf\u3001\u91cd\u590d\u95ee\u9898\u548c\u5177\u4f53\u5173\u6ce8\u70b9\u3002", "result": "\u521d\u6b65\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u5efa\u8bae\u4e0e\u5546\u4e1a\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u7a81\u663e\u5176\u5728\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u9a71\u52a8\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316\u5546\u4e1a\u7b56\u7565\u548c\u6700\u5927\u5316\u5ba2\u6237\u53cd\u9988\u5f71\u54cd\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.16211", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16211", "abs": "https://arxiv.org/abs/2510.16211", "authors": ["Henrique Pickler", "Jorge K. S. Kamassury", "Danilo Silva"], "title": "Benchmarking noisy label detection methods", "comment": null, "summary": "Label noise is a common problem in real-world datasets, affecting both model\ntraining and validation. Clean data are essential for achieving strong\nperformance and ensuring reliable evaluation. While various techniques have\nbeen proposed to detect noisy labels, there is no clear consensus on optimal\napproaches. We perform a comprehensive benchmark of detection methods by\ndecomposing them into three fundamental components: label agreement function,\naggregation method, and information gathering approach (in-sample vs\nout-of-sample). This decomposition can be applied to many existing detection\nmethods, and enables systematic comparison across diverse approaches. To fairly\ncompare methods, we propose a unified benchmark task, detecting a fraction of\ntraining samples equal to the dataset's noise rate. We also introduce a novel\nmetric: the false negative rate at this fixed operating point. Our evaluation\nspans vision and tabular datasets under both synthetic and real-world noise\nconditions. We identify that in-sample information gathering using average\nprobability aggregation combined with the logit margin as the label agreement\nfunction achieves the best results across most scenarios. Our findings provide\npractical guidance for designing new detection methods and selecting techniques\nfor specific applications.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6807\u7b7e\u566a\u58f0\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5c06\u65b9\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u3001\u805a\u5408\u65b9\u6cd5\u548c\u4fe1\u606f\u6536\u96c6\u65b9\u5f0f\uff09\u6765\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\uff0c\u4f7f\u7528\u5bf9\u6570\u8fb9\u9645\u4f5c\u4e3a\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u3001\u5e73\u5747\u6982\u7387\u805a\u5408\u548c\u6837\u672c\u5185\u4fe1\u606f\u6536\u96c6\u7684\u7ec4\u5408\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4f1a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6548\u679c\u3002\u867d\u7136\u5df2\u6709\u591a\u79cd\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u6280\u672f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6700\u4f18\u65b9\u6cd5\u7684\u660e\u786e\u5171\u8bc6\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u548c\u8bc4\u4f30\u3002", "method": "\u5c06\u566a\u58f0\u68c0\u6d4b\u65b9\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u57fa\u672c\u7ec4\u4ef6\uff1a\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u3001\u805a\u5408\u65b9\u6cd5\u548c\u4fe1\u606f\u6536\u96c6\u65b9\u5f0f\uff08\u6837\u672c\u5185vs\u6837\u672c\u5916\uff09\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u57fa\u51c6\u4efb\u52a1\u548c\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff08\u56fa\u5b9a\u64cd\u4f5c\u70b9\u7684\u5047\u9634\u6027\u7387\uff09\uff0c\u5728\u89c6\u89c9\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u5bf9\u5408\u6210\u548c\u771f\u5b9e\u566a\u58f0\u6761\u4ef6\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u5bf9\u6570\u8fb9\u9645\u4f5c\u4e3a\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u3001\u5e73\u5747\u6982\u7387\u805a\u5408\u548c\u6837\u672c\u5185\u4fe1\u606f\u6536\u96c6\u7684\u7ec4\u5408\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5206\u89e3\u548c\u6bd4\u8f83\uff0c\u672c\u6587\u4e3a\u8bbe\u8ba1\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5\u548c\u4e3a\u7279\u5b9a\u5e94\u7528\u9009\u62e9\u5408\u9002\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u8bc6\u522b\u51fa\u4e86\u5728\u591a\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f18\u7684\u68c0\u6d4b\u65b9\u6cd5\u7ec4\u5408\u3002"}}
{"id": "2510.16511", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16511", "abs": "https://arxiv.org/abs/2510.16511", "authors": ["Dongchan Cho", "Jiho Han", "Keumyeong Kang", "Minsang Kim", "Honggyu Ryu", "Namsoon Jung"], "title": "Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Real-world multivariate time series anomalies are rare and often unlabeled.\nAdditionally, prevailing methods rely on increasingly complex architectures\ntuned to benchmarks, detecting only fragments of anomalous segments and\noverstating performance. In this paper, we introduce OracleAD, a simple and\ninterpretable unsupervised framework for multivariate time series anomaly\ndetection. OracleAD encodes each variable's past sequence into a single causal\nembedding to jointly predict the present time point and reconstruct the input\nwindow, effectively modeling temporal dynamics. These embeddings then undergo a\nself-attention mechanism to project them into a shared latent space and capture\nspatial relationships. These relationships are not static, since they are\nmodeled by a property that emerges from each variable's temporal dynamics. The\nprojected embeddings are aligned to a Stable Latent Structure (SLS)\nrepresenting normal-state relationships. Anomalies are identified using a dual\nscoring mechanism based on prediction error and deviation from the SLS,\nenabling fine-grained anomaly diagnosis at each time point and across\nindividual variables. Since any noticeable SLS deviation originates from\nembeddings that violate the learned temporal causality of normal data, OracleAD\ndirectly pinpoints the root-cause variables at the embedding level. OracleAD\nachieves state-of-the-art results across multiple real-world datasets and\nevaluation protocols, while remaining interpretable through SLS.", "AI": {"tldr": "OracleAD\u662f\u4e00\u4e2a\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u65e0\u76d1\u7763\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5d4c\u5165\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u5c06\u6295\u5f71\u5d4c\u5165\u5bf9\u9f50\u5230\u8868\u793a\u6b63\u5e38\u72b6\u6001\u5173\u7cfb\u7684\u7a33\u5b9a\u6f5c\u5728\u7ed3\u6784(SLS)\uff0c\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u548cSLS\u504f\u5dee\u7684\u53cc\u91cd\u8bc4\u5206\u673a\u5236\u8bc6\u522b\u5f02\u5e38\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u7a00\u5c11\u4e14\u901a\u5e38\u65e0\u6807\u7b7e\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u67b6\u6784\uff0c\u53ea\u80fd\u68c0\u6d4b\u5f02\u5e38\u7247\u6bb5\u7247\u6bb5\u4e14\u5938\u5927\u6027\u80fd\u3002", "method": "\u5c06\u6bcf\u4e2a\u53d8\u91cf\u7684\u8fc7\u53bb\u5e8f\u5217\u7f16\u7801\u4e3a\u56e0\u679c\u5d4c\u5165\u6765\u8054\u5408\u9884\u6d4b\u5f53\u524d\u65f6\u95f4\u70b9\u548c\u91cd\u5efa\u8f93\u5165\u7a97\u53e3\uff0c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5d4c\u5165\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u5c06\u6295\u5f71\u5d4c\u5165\u5bf9\u9f50\u5230\u7a33\u5b9a\u6f5c\u5728\u7ed3\u6784(SLS)\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u901a\u8fc7SLS\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "OracleAD\u901a\u8fc7\u76f4\u63a5\u5b9a\u4f4d\u8fdd\u53cd\u6b63\u5e38\u6570\u636e\u65f6\u95f4\u56e0\u679c\u6027\u7684\u5d4c\u5165\uff0c\u5728\u5d4c\u5165\u7ea7\u522b\u76f4\u63a5\u8bc6\u522b\u6839\u672c\u539f\u56e0\u53d8\u91cf\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u8bca\u65ad\u3002"}}
{"id": "2510.16513", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16513", "abs": "https://arxiv.org/abs/2510.16513", "authors": ["Dhruv Gupta", "Aditya Nagarsekar", "Vraj Shah", "Sujith Thomas"], "title": "eDCF: Estimating Intrinsic Dimension using Local Connectivity", "comment": "58 pages (35 (main) + 23 (appendix)), 54 figures (27 (main) + 27\n  (appendix))", "summary": "Modern datasets often contain high-dimensional features exhibiting complex\ndependencies. To effectively analyze such data, dimensionality reduction\nmethods rely on estimating the dataset's intrinsic dimension (id) as a measure\nof its underlying complexity. However, estimating id is challenging due to its\ndependence on scale: at very fine scales, noise inflates id estimates, while at\ncoarser scales, estimates stabilize to lower, scale-invariant values. This\npaper introduces a novel, scalable, and parallelizable method called eDCF,\nwhich is based on Connectivity Factor (CF), a local connectivity-based metric,\nto robustly estimate intrinsic dimension across varying scales. Our method\nconsistently matches leading estimators, achieving comparable values of mean\nabsolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our\napproach also attains higher exact intrinsic dimension match rates, reaching up\nto 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling\nunder medium to high noise levels and large datasets. Further, we showcase our\nmethod's ability to accurately detect fractal geometries in decision\nboundaries, confirming its utility for analyzing realistic, structured data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u901a\u6027\u56e0\u5b50(CF)\u7684\u65b0\u578b\u53ef\u6269\u5c55\u5e76\u884c\u65b9\u6cd5eDCF\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7a33\u5065\u4f30\u8ba1\u9ad8\u7ef4\u6570\u636e\u7684\u672c\u5f81\u7ef4\u5ea6\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u5177\u6709\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u7684\u9ad8\u7ef4\u7279\u5f81\uff0c\u800c\u672c\u5f81\u7ef4\u5ea6\u4f30\u8ba1\u9762\u4e34\u5c3a\u5ea6\u4f9d\u8d56\u7684\u6311\u6218\uff1a\u5728\u7cbe\u7ec6\u5c3a\u5ea6\u4e0b\u566a\u58f0\u4f1a\u81a8\u80c0\u4f30\u8ba1\u503c\uff0c\u5728\u7c97\u7c92\u5ea6\u5c3a\u5ea6\u4e0b\u4f30\u8ba1\u503c\u7a33\u5b9a\u5230\u8f83\u4f4e\u7684\u5c3a\u5ea6\u4e0d\u53d8\u503c\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8fde\u901a\u6027\u56e0\u5b50(CF)\u7684eDCF\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5c40\u90e8\u8fde\u901a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5e76\u884c\u5316\u80fd\u529b\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0e\u9886\u5148\u4f30\u8ba1\u5668\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u4e2d\u7b49\u81f3\u9ad8\u566a\u58f0\u6c34\u5e73\u548c\u5927\u6570\u636e\u96c6\u4e0b\uff0c\u7cbe\u786e\u672c\u5f81\u7ef4\u5ea6\u5339\u914d\u7387\u9ad8\u8fbe25.0%\uff0c\u4f18\u4e8eMLE(16.7%)\u548cTWO-NN(12.5%)\u3002", "conclusion": "eDCF\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u51b3\u7b56\u8fb9\u754c\u4e2d\u7684\u5206\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5206\u6790\u73b0\u5b9e\u7ed3\u6784\u5316\u6570\u636e\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.16530", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16530", "abs": "https://arxiv.org/abs/2510.16530", "authors": ["Ashutosh Srivastava", "Lokesh Nagalapatti", "Gautam Jajoo", "Aniket Vashishtha", "Parameswari Krishnamurthy", "Amit Sharma"], "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks", "comment": null, "summary": "Recent claims of strong performance by Large Language Models (LLMs) on causal\ndiscovery are undermined by a key flaw: many evaluations rely on benchmarks\nlikely included in pretraining corpora. Thus, apparent success suggests that\nLLM-only methods, which ignore observational data, outperform classical\nstatistical approaches. We challenge this narrative by asking: Do LLMs truly\nreason about causal structure, and how can we measure it without memorization\nconcerns? Can they be trusted for real-world scientific discovery? We argue\nthat realizing LLMs' potential for causal analysis requires two shifts: (P.1)\ndeveloping robust evaluation protocols based on recent scientific studies to\nguard against dataset leakage, and (P.2) designing hybrid methods that combine\nLLM-derived knowledge with data-driven statistics. To address P.1, we encourage\nevaluating discovery methods on novel, real-world scientific studies. We\noutline a practical recipe for extracting causal graphs from recent\npublications released after an LLM's training cutoff, ensuring relevance and\npreventing memorization while capturing both established and novel relations.\nCompared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,\nthey perform far worse on our curated graphs, underscoring the need for\nstatistical grounding. Supporting P.2, we show that using LLM predictions as\npriors for the classical PC algorithm significantly improves accuracy over both\nLLM-only and purely statistical methods. We call on the community to adopt\nscience-grounded, leakage-resistant benchmarks and invest in hybrid causal\ndiscovery methods suited to real-world inquiry.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591LLM\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u6307\u51fa\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u65b0\u79d1\u5b66\u7814\u7a76\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u7ed3\u5408LLM\u77e5\u8bc6\u4e0e\u7edf\u8ba1\u65b9\u6cd5\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u6311\u6218LLM\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u8bf4\u6cd5\uff0c\u56e0\u4e3a\u73b0\u6709\u8bc4\u4f30\u53ef\u80fd\u53d7\u5230\u9884\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u7684\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u5b9e\u7528\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u8f6c\u53d8\uff1a\u5f00\u53d1\u57fa\u4e8e\u65b0\u79d1\u5b66\u7814\u7a76\u7684\u8bc4\u4f30\u534f\u8bae\u4ee5\u9632\u6b62\u6570\u636e\u6cc4\u9732\uff0c\u8bbe\u8ba1\u7ed3\u5408LLM\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u7edf\u8ba1\u7684\u6df7\u5408\u65b9\u6cd5\u3002\u901a\u8fc7\u4eceLLM\u8bad\u7ec3\u622a\u6b62\u540e\u53d1\u5e03\u7684\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u56e0\u679c\u56fe\u6765\u521b\u5efa\u65b0\u57fa\u51c6\u3002", "result": "\u76f8\u6bd4BNLearn\u57fa\u51c6\u4e2dLLM\u63a5\u8fd1\u5b8c\u7f8e\u7684\u8868\u73b0\uff0c\u5728\u4f5c\u8005\u7b56\u5212\u7684\u56fe\u4e0aLLM\u8868\u73b0\u5dee\u5f97\u591a\u3002\u4f7f\u7528LLM\u9884\u6d4b\u4f5c\u4e3a\u7ecf\u5178PC\u7b97\u6cd5\u7684\u5148\u9a8c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u7eafLLM\u548c\u7eaf\u7edf\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u91c7\u7528\u57fa\u4e8e\u79d1\u5b66\u3001\u6297\u6cc4\u9732\u7684\u57fa\u51c6\uff0c\u5e76\u6295\u8d44\u4e8e\u9002\u5408\u771f\u5b9e\u4e16\u754c\u7814\u7a76\u7684\u6df7\u5408\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.", "AI": {"tldr": "MERCI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\u6765\u589e\u5f3aLLM\u63a8\u7406\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u63a8\u7406\u591a\u6837\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u548c\u6709\u9650\u63a2\u7d22\uff0c\u5bfc\u81f4LLM\u9677\u5165\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u597d\u7684\u63a2\u7d22\u673a\u5236\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u8ba1\u6570\u63a2\u7d22\u601d\u60f3\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7Coin Flipping Network\u4f30\u8ba1\u63a8\u7406\u8f68\u8ff9\u7684\u4f2a\u8ba1\u6570\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5e76\u96c6\u6210\u5230GRPO\u7b49\u5148\u8fdbRL\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMERCI\u9f13\u52b1\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u601d\u7ef4\u94fe\uff0c\u663e\u8457\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u6027\u80fd\uff0c\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u5c40\u90e8\u5e38\u89c4\u53d1\u73b0\u66f4\u597d\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u5185\u5728\u52a8\u673a\u53ef\u4ee5\u4f7f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u63a2\u7d22\u66f4\u52a0\u53ef\u9760\u6709\u6548\u3002"}}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u89c4\u6a21AI\u6a21\u578b\u5bf9\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u6db5\u76d6\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u3001\u4e34\u5e8a\u8f85\u52a9\u548c\u75be\u75c5\u5e94\u7528\u7b49\u9886\u57df\uff0c\u5f3a\u8c03AI\u5728\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u7b49\u6311\u6218\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u5e26\u6765\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u4fc3\u8fdb\u4e86\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u548c\u795e\u7ecf\u6570\u636e\u7684\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u795e\u7ecf\u79d1\u5b66\u5404\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u5206\u6790\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\uff1a\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u4e0e\u795e\u7ecf\u89e3\u7801\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u4e0e\u57fa\u56e0\u7ec4\u5efa\u6a21\u3001\u4e34\u5e8a\u8f85\u52a9\u4e0e\u8f6c\u5316\u6846\u67b6\u3001\u795e\u7ecf\u7cfb\u7edf\u548c\u7cbe\u795e\u75be\u75c5\u7684\u7279\u5b9a\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u7b49\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u6311\u6218\uff0c\u5e76\u4fc3\u8fdb\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7684\u53cc\u5411\u4e92\u52a8\uff0c\u5c06\u751f\u7269\u5b66\u7ea6\u675f\u7eb3\u5165\u6a21\u578b\u5f00\u53d1\u3002", "conclusion": "\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u795e\u7ecf\u79d1\u5b66\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5efa\u7acb\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3001\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u4ee5\u53ca\u5168\u9762\u7684\u4e34\u5e8a\u4f7f\u7528\u4f26\u7406\u6307\u5357\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5173\u952e\u795e\u7ecf\u79d1\u5b66\u6570\u636e\u96c6\u7684\u7cfb\u7edf\u6e05\u5355\u3002"}}
{"id": "2510.17085", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17085", "abs": "https://arxiv.org/abs/2510.17085", "authors": ["Yiling Chen", "Shi Feng", "Paul Kattuman", "Fang-Yi Yu"], "title": "Data Reliability Scoring", "comment": "39 pages, 5 figures", "summary": "How can we assess the reliability of a dataset without access to ground\ntruth? We introduce the problem of reliability scoring for datasets collected\nfrom potentially strategic sources. The true data are unobserved, but we see\noutcomes of an unknown statistical experiment that depends on them. To\nbenchmark reliability, we define ground-truth-based orderings that capture how\nmuch reported data deviate from the truth. We then propose the Gram determinant\nscore, which measures the volume spanned by vectors describing the empirical\ndistribution of the observed data and experiment outcomes. We show that this\nscore preserves several ground-truth based reliability orderings and, uniquely\nup to scaling, yields the same reliability ranking of datasets regardless of\nthe experiment -- a property we term experiment agnosticism. Experiments on\nsynthetic noise models, CIFAR-10 embeddings, and real employment data\ndemonstrate that the Gram determinant score effectively captures data quality\nacross diverse observation processes.", "AI": {"tldr": "\u63d0\u51faGram\u884c\u5217\u5f0f\u8bc4\u5206\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u65e0\u6cd5\u83b7\u53d6\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u9a8c\u65e0\u5173\u6027\uff0c\u80fd\u6709\u6548\u8861\u91cf\u62a5\u544a\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u504f\u5dee\u7a0b\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u6cd5\u83b7\u53d6\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8bc4\u4f30\u6765\u81ea\u6f5c\u5728\u7b56\u7565\u6027\u6765\u6e90\u7684\u6570\u636e\u96c6\u53ef\u9760\u6027\u95ee\u9898\u3002\u771f\u5b9e\u6570\u636e\u4e0d\u53ef\u89c2\u6d4b\uff0c\u53ea\u80fd\u770b\u5230\u4f9d\u8d56\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u672a\u77e5\u7edf\u8ba1\u5b9e\u9a8c\u7ed3\u679c\u3002", "method": "\u63d0\u51faGram\u884c\u5217\u5f0f\u8bc4\u5206\uff0c\u901a\u8fc7\u8ba1\u7b97\u63cf\u8ff0\u89c2\u6d4b\u6570\u636e\u548c\u5b9e\u9a8c\u7ed3\u679c\u7ecf\u9a8c\u5206\u5e03\u7684\u5411\u91cf\u6240\u5f20\u6210\u7684\u4f53\u79ef\u6765\u5ea6\u91cf\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u9a8c\u65e0\u5173\u6027\uff0c\u5373\u65e0\u8bba\u5b9e\u9a8c\u5982\u4f55\uff0c\u90fd\u80fd\u4ea7\u751f\u76f8\u540c\u7684\u6570\u636e\u96c6\u53ef\u9760\u6027\u6392\u5e8f\u3002", "result": "\u5728\u5408\u6210\u566a\u58f0\u6a21\u578b\u3001CIFAR-10\u5d4c\u5165\u548c\u771f\u5b9e\u5c31\u4e1a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGram\u884c\u5217\u5f0f\u8bc4\u5206\u80fd\u591f\u6709\u6548\u6355\u6349\u4e0d\u540c\u89c2\u6d4b\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "Gram\u884c\u5217\u5f0f\u8bc4\u5206\u662f\u4e00\u79cd\u6709\u6548\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u53ef\u9760\u6027\u6392\u5e8f\uff0c\u5e76\u5177\u6709\u5b9e\u9a8c\u65e0\u5173\u6027\u7684\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2510.17103", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17103", "abs": "https://arxiv.org/abs/2510.17103", "authors": ["Shinji Ito", "Kevin Jamieson", "Haipeng Luo", "Arnab Maiti", "Taira Tsuchiya"], "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback", "comment": "49 pages", "summary": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u805a\u5408bandit\u53cd\u9988\u4e0b\u7684\u8868\u683cMDP\u6700\u4f73\u4e24\u5168(BOBW)\u7b97\u6cd5\uff0c\u5728\u5df2\u77e5\u8f6c\u79fb\u6982\u7387\u65f6\u5b9e\u73b0O(log T)\u968f\u673a\u9057\u61be\u548cO(\u221aT)\u5bf9\u6297\u9057\u61be\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u7684\u4e0b\u754c\u8bc1\u660e\u6700\u4f18\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u805a\u5408bandit\u53cd\u9988\u6a21\u578b\u4e0b\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u4e0b\u5b66\u4e60\u8005\u53ea\u80fd\u89c2\u5bdf\u5230\u6bcf\u4e2aepisode\u7684\u7d2f\u79ef\u635f\u5931\u800c\u975e\u5355\u4e2a\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u635f\u5931\u3002\u73b0\u6709\u5de5\u4f5c\u4ec5\u5173\u6ce8\u6700\u574f\u60c5\u51b5\u5206\u6790\uff0c\u672c\u6587\u9996\u6b21\u7814\u7a76\u80fd\u5728\u968f\u673a\u548c\u5bf9\u6297\u73af\u5883\u4e2d\u90fd\u5b9e\u73b0\u4f4e\u9057\u61be\u7684BOBW\u7b97\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8e\u5360\u7528\u5ea6\u91cf\u7684FTRL\u3001\u81ea\u8fb9\u754c\u6280\u672f\u4ee5\u53ca\u53d7\u5728\u7ebf\u6700\u77ed\u8def\u5f84\u95ee\u9898\u542f\u53d1\u7684\u65b0\u635f\u5931\u4f30\u8ba1\u5668\u3002\u5bf9\u4e8e\u672a\u77e5\u8f6c\u79fb\u6982\u7387\u7684\u60c5\u51b5\uff0c\u8fd8\u878d\u5165\u4e86\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6280\u672f\u3002", "result": "\u5728\u5df2\u77e5\u8f6c\u79fb\u6982\u7387\u65f6\uff0c\u7b97\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e2d\u8fbe\u5230O(log T)\u9057\u61be\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u8fbe\u5230O(\u221aT)\u9057\u61be\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u7684\u4e0b\u754c\u3002\u540c\u65f6\u4e3a\u6700\u77ed\u8def\u5f84\u95ee\u9898\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e2a\u4f53\u95f4\u9699\u76f8\u5173\u7684\u4e0b\u754c\u548c\u8fd1\u6700\u4f18BOBW\u7b97\u6cd5\u3002", "conclusion": "\u6210\u529f\u8bbe\u8ba1\u4e86\u9996\u4e2a\u5728\u805a\u5408bandit\u53cd\u9988\u4e0b\u8868\u683cMDP\u7684BOBW\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\uff0c\u5e76\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u672a\u77e5\u8f6c\u79fb\u6982\u7387\u7684\u60c5\u51b5\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u805a\u5408\u53cd\u9988\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2510.16045", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16045", "abs": "https://arxiv.org/abs/2510.16045", "authors": ["Mengtao Lv", "Ruiqi Zhu", "Xinyu Wang", "Yun Li"], "title": "AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization", "comment": "12 pages, 6 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious kinds of tasks, while the billion or even trillion parameters bring\nstorage and efficiency bottlenecks for inference. Quantization, particularly\nfloating-point quantization, is known to be capable of speeding up LLM\ninference by reducing memory footprint and data movement during the inference\nprocess. For the first time, we advance the floating-point quantization\nexploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,\nto further approach the quantization sweet spot. AMS-Quant incorporates two\nnovel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,\nwhich groups k quantized weights and lets them share the least significant\nmantissa bit, allowing us to further approach the minimum quantization\nbit-width without accuracy loss. (2) It introduces Adaptive Searching, which\nemploys an offline optimization strategy to minimize the accuracy degradation\nintroduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA\nLinear kernels, which translates memory savings into wall-clock latency\nreduction by reducing memory access. Extensive experiments on large-scale\ndatasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3\nand FP4.25-e2m2, and significantly speed up the LLM decoding over FP16\ninference (2.8x and 3.2x), with negligible accuracy loss.", "AI": {"tldr": "AMS-Quant\u662f\u4e00\u79cd\u65b0\u7684\u6d6e\u70b9\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u6574\u6570\u4f4d\u5bbd\u91cf\u5316\u548c\u4e24\u79cd\u5173\u952e\u6280\u672f\uff08\u5c3e\u6570\u4f4d\u5171\u4eab\u548c\u81ea\u9002\u5e94\u641c\u7d22\uff09\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5e9e\u5927\u5e26\u6765\u5b58\u50a8\u548c\u63a8\u7406\u6548\u7387\u74f6\u9888\uff0c\u6d6e\u70b9\u91cf\u5316\u867d\u80fd\u52a0\u901f\u63a8\u7406\u4f46\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u6574\u6570\u4f4d\u5bbd\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u7cbe\u7ec6\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u63a5\u8fd1\u91cf\u5316\u6700\u4f73\u70b9\u3002", "method": "\u63d0\u51faAMS-Quant\u65b9\u6cd5\uff1a1\uff09\u5c3e\u6570\u4f4d\u5171\u4eab\u6280\u672f\uff0c\u5c06k\u4e2a\u91cf\u5316\u6743\u91cd\u5206\u7ec4\u5171\u4eab\u6700\u4f4e\u6709\u6548\u5c3e\u6570\u4f4d\uff1b2\uff09\u81ea\u9002\u5e94\u641c\u7d22\u6280\u672f\uff0c\u91c7\u7528\u79bb\u7ebf\u4f18\u5316\u7b56\u7565\u6700\u5c0f\u5316\u5171\u4eab\u5e26\u6765\u7684\u7cbe\u5ea6\u635f\u5931\uff1b3\uff09\u5b9e\u73b0\u9ad8\u6548CUDA\u7ebf\u6027\u6838\uff0c\u5c06\u5185\u5b58\u8282\u7701\u8f6c\u5316\u4e3a\u5b9e\u9645\u5ef6\u8fdf\u964d\u4f4e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAMS-Quant\u53ef\u5c06\u6a21\u578b\u91cf\u5316\u4e3aFP-5.33-e2m3\u548cFP4.25-e2m2\uff0c\u76f8\u6bd4FP16\u63a8\u7406\u5206\u522b\u5b9e\u73b02.8\u500d\u548c3.2\u500d\u7684\u89e3\u7801\u52a0\u901f\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "AMS-Quant\u9996\u6b21\u5c06\u6d6e\u70b9\u91cf\u5316\u4ece\u6574\u6570\u4f4d\u5bbd\u6269\u5c55\u5230\u975e\u6574\u6570\u4f4d\u5bbd\uff0c\u901a\u8fc7\u521b\u65b0\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u5b58\u50a8\u548c\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.16051", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16051", "abs": "https://arxiv.org/abs/2510.16051", "authors": ["Sofiya Garkot", "Maksym Shamrai", "Ivan Synytsia", "Mariya Hirna"], "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration", "comment": "22 pages", "summary": "Autonomous agents capable of operating complex graphical user interfaces\n(GUIs) have the potential to transform desktop automation. While recent\nadvances in large language models (LLMs) have significantly improved UI\nunderstanding, navigating full-window, multi-application desktop environments\nremains a major challenge. Data availability is limited by costly manual\nannotation, closed-source datasets and surface-level synthetic pipelines. We\nintroduce GUIrilla, an automated scalable framework that systematically\nexplores applications via native accessibility APIs to address the critical\ndata collection challenge in GUI automation. Our framework focuses on macOS -\nan ecosystem with limited representation in current UI datasets - though many\nof its components are designed for broader cross-platform applicability.\nGUIrilla organizes discovered interface elements and crawler actions into\nhierarchical GUI graphs and employs specialized interaction handlers to achieve\ncomprehensive application coverage. Using the application graphs from GUIrilla\ncrawler, we construct and release GUIrilla-Task, a large-scale dataset of\n27,171 functionally grounded tasks across 1,108 macOS applications, each\nannotated with full-desktop and window-level screenshots, accessibility\nmetadata, and semantic action traces. Empirical results show that tuning\nLLM-based agents on GUIrilla-Task significantly improves performance on\ndownstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro\nbenchmark while using 97% less data. We also release macapptree, an open-source\nlibrary for reproducible collection of structured accessibility metadata, along\nwith the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold\nbenchmark, and the framework code to support open research in desktop autonomy.", "AI": {"tldr": "GUIrilla\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u751f\u53ef\u8bbf\u95ee\u6027API\u7cfb\u7edf\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u51b3GUI\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\u3002\u8be5\u6846\u67b6\u6784\u5efa\u4e86GUIrilla-Task\u6570\u636e\u96c6\uff0c\u5305\u542b27,171\u4e2a\u529f\u80fd\u57fa\u7840\u4efb\u52a1\uff0c\u8986\u76d61,108\u4e2amacOS\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728UI\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u56fe\u5f62\u7528\u6237\u754c\u9762\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u5f53\u524d\u6570\u636e\u53ef\u7528\u6027\u53d7\u9650\u4e8e\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u3001\u95ed\u6e90\u6570\u636e\u96c6\u548c\u8868\u9762\u7ea7\u5408\u6210\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u5728macOS\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u539f\u751f\u53ef\u8bbf\u95ee\u6027API\u7cfb\u7edf\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\uff0c\u5c06\u53d1\u73b0\u7684\u754c\u9762\u5143\u7d20\u548c\u722c\u866b\u52a8\u4f5c\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316GUI\u56fe\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u4ea4\u4e92\u5904\u7406\u5668\u5b9e\u73b0\u5168\u9762\u5e94\u7528\u8986\u76d6\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u529f\u80fd\u57fa\u7840\u4efb\u52a1\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86GUIrilla-Task\u6570\u636e\u96c6\uff0827,171\u4e2a\u4efb\u52a1\uff0c1,108\u4e2a\u5e94\u7528\uff09\uff0c\u5728ScreenSpot Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5408\u6210\u57fa\u7ebf\uff0c\u4f7f\u7528\u6570\u636e\u91cf\u51cf\u5c1197%\u3002\u53d1\u5e03\u4e86macapptree\u5f00\u6e90\u5e93\u3001GUIrilla-Gold\u57fa\u51c6\u6d4b\u8bd5\u548c\u6846\u67b6\u4ee3\u7801\u3002", "conclusion": "GUIrilla\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u684c\u9762\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u6536\u96c6\u74f6\u9888\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u5e94\u7528\u63a2\u7d22\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742GUI\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u684c\u9762\u81ea\u4e3b\u6027\u7684\u5f00\u653e\u7814\u7a76\u3002"}}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faELMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u5668\u548c\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u7684\u8bed\u4e49\u566a\u58f0\u3001\u6a21\u6001\u51b2\u7a81\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5b58\u5728\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u65f6\u9762\u4e34\u8bed\u4e49\u566a\u58f0\u3001\u6a21\u6001\u51b2\u7a81\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8865\u5168\u65b9\u6cd5\u3002", "method": "\u63d0\u51faELMM\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u7684\u591a\u89c6\u56fe\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u5668\u6765\u538b\u7f29\u56fe\u50cf\u6807\u8bb0\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\u6765\u51cf\u5c11\u5197\u4f59\u5c42\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u6295\u5f71\u8865\u507f\u526a\u679d\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728FB15k-237-IMG\u548cWN18-IMG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELMM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "ELMM\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.17268", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17268", "abs": "https://arxiv.org/abs/2510.17268", "authors": ["Anthony Frion", "David S Greenberg"], "title": "Uncertainty-aware data assimilation through variational inference", "comment": null, "summary": "Data assimilation, consisting in the combination of a dynamical model with a\nset of noisy and incomplete observations in order to infer the state of a\nsystem over time, involves uncertainty in most settings. Building upon an\nexisting deterministic machine learning approach, we propose a variational\ninference-based extension in which the predicted state follows a multivariate\nGaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing\nground, we show that our new model enables to obtain nearly perfectly\ncalibrated predictions, and can be integrated in a wider variational data\nassimilation pipeline in order to achieve greater benefit from increasing\nlengths of data assimilation windows. Our code is available at\nhttps://github.com/anthony-frion/Stochastic_CODA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u5c06\u786e\u5b9a\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u5230\u72b6\u6001\u9884\u6d4b\u9075\u5faa\u591a\u5143\u9ad8\u65af\u5206\u5e03\u7684\u60c5\u51b5\uff0c\u7528\u4e8e\u6570\u636e\u540c\u5316\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "motivation": "\u6570\u636e\u540c\u5316\u6d89\u53ca\u5c06\u52a8\u6001\u6a21\u578b\u4e0e\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u89c2\u6d4b\u76f8\u7ed3\u5408\u6765\u63a8\u65ad\u7cfb\u7edf\u72b6\u6001\uff0c\u5728\u5927\u591a\u6570\u8bbe\u7f6e\u4e2d\u90fd\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u7684\u786e\u5b9a\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6269\u5c55\u5230\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u72b6\u6001\u9884\u6d4b\u9075\u5faa\u591a\u5143\u9ad8\u65af\u5206\u5e03\u7684\u6a21\u578b\uff0c\u4f7f\u7528\u6df7\u6c8cLorenz-96\u52a8\u529b\u5b66\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u53ef\u4ee5\u96c6\u6210\u5230\u66f4\u5e7f\u6cdb\u7684\u53d8\u5206\u6570\u636e\u540c\u5316\u6d41\u7a0b\u4e2d\u3002", "result": "\u65b0\u6a21\u578b\u80fd\u591f\u83b7\u5f97\u8fd1\u4e4e\u5b8c\u7f8e\u6821\u51c6\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u968f\u7740\u6570\u636e\u540c\u5316\u7a97\u53e3\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u53ef\u4ee5\u5728\u66f4\u5e7f\u6cdb\u7684\u53d8\u5206\u6570\u636e\u540c\u5316\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u66f4\u5927\u7684\u6548\u76ca\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u7684\u968f\u673a\u65b9\u6cd5\u5728\u6570\u636e\u540c\u5316\u4e2d\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u6821\u51c6\u826f\u597d\u7684\u9884\u6d4b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6269\u5c55\u6570\u636e\u540c\u5316\u7a97\u53e3\u65f6\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.16802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16802", "abs": "https://arxiv.org/abs/2510.16802", "authors": ["Chao Li", "Yuru Wang"], "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation", "comment": "14 pages", "summary": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u5efa\u6a21\u6846\u67b6\u2014\u2014\u9886\u57df\u60c5\u5883\u5316\u6982\u5ff5\u56fe\uff08CDC\uff09\uff0c\u901a\u8fc7\u5c06\u9886\u57df\u63d0\u5347\u4e3a\u6982\u5ff5\u8868\u793a\u7684\u4e00\u7b49\u5143\u7d20\uff0c\u514b\u670d\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u56fa\u5b9a\u672c\u4f53\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u8de8\u9886\u57df\u7c7b\u6bd4\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u53d7\u9650\u4e8e\u56fa\u5b9a\u672c\u4f53\uff0c\u5c06\u9886\u57df\u4f5c\u4e3a\u9690\u5f0f\u4e0a\u4e0b\u6587\u800c\u975e\u663e\u5f0f\u63a8\u7406\u7ec4\u4ef6\uff0c\u5bfc\u81f4\u6982\u5ff5\u7ec4\u7ec7\u50f5\u5316\u3002", "method": "\u91c7\u7528C-D-C\u4e09\u5143\u7ec4\u7ed3\u6784<\u6982\u5ff5, \u5173\u7cfb@\u9886\u57df, \u6982\u5ff5'>\uff0c\u5c06\u9886\u57df\u89c4\u8303\u4f5c\u4e3a\u6309\u9700\u5b9a\u4e49\u7684\u52a8\u6001\u5206\u7c7b\u7ef4\u5ea6\uff0c\u57fa\u4e8e\u8ba4\u77e5-\u8bed\u8a00\u540c\u6784\u6620\u5c04\u539f\u7406\uff0c\u5b9e\u73b0\u4eba\u7c7b\u901a\u8fc7\u4e0a\u4e0b\u6587\u6846\u67b6\u7406\u89e3\u6982\u5ff5\u7684\u65b9\u5f0f\u3002", "result": "\u5728Prolog\u4e2d\u5b9e\u73b0CDC\u5e76\u5177\u5907\u5b8c\u6574\u63a8\u7406\u80fd\u529b\uff0c\u6848\u4f8b\u7814\u7a76\u8868\u660eCDC\u80fd\u591f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u8de8\u9886\u57df\u7c7b\u6bd4\u548c\u4e2a\u6027\u5316\u77e5\u8bc6\u5efa\u6a21\u3002", "conclusion": "CDC\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u57fa\u4e8e\u672c\u4f53\u7684\u77e5\u8bc6\u8868\u793a\u9650\u5236\uff0c\u4e3a\u77e5\u8bc6\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u52a8\u6001\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.17503", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17503", "abs": "https://arxiv.org/abs/2510.17503", "authors": ["El Mahdi Chayti", "Martin Jaggi"], "title": "Stochastic Difference-of-Convex Optimization with Momentum", "comment": null, "summary": "Stochastic difference-of-convex (DC) optimization is prevalent in numerous\nmachine learning applications, yet its convergence properties under small batch\nsizes remain poorly understood. Existing methods typically require large\nbatches or strong noise assumptions, which limit their practical use. In this\nwork, we show that momentum enables convergence under standard smoothness and\nbounded variance assumptions (of the concave part) for any batch size. We prove\nthat without momentum, convergence may fail regardless of stepsize,\nhighlighting its necessity. Our momentum-based algorithm achieves provable\nconvergence and demonstrates strong empirical performance.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u52a8\u91cf\u65b9\u6cd5\u5bf9\u4e8e\u968f\u673aDC\u4f18\u5316\u7684\u91cd\u8981\u6027\uff0c\u80fd\u591f\u5728\u4efb\u4f55\u6279\u91cf\u5927\u5c0f\u4e0b\u5b9e\u73b0\u6536\u655b\uff0c\u800c\u65e0\u9700\u52a8\u91cf\u5219\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u3002", "motivation": "\u968f\u673aDC\u4f18\u5316\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u5c0f\u6279\u91cf\u5927\u5c0f\u4e0b\u7684\u6536\u655b\u7279\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u6279\u91cf\u6216\u5f3a\u566a\u58f0\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u91cf\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u6807\u51c6\u5e73\u6ed1\u6027\u548c\u6709\u754c\u65b9\u5dee\u5047\u8bbe\uff08\u9488\u5bf9\u51f9\u90e8\u5206\uff09\uff0c\u8bc1\u660e\u52a8\u91cf\u662f\u5b9e\u73b0\u6536\u655b\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u52a8\u91cf\u65b9\u6cd5\u80fd\u591f\u5728\u4efb\u4f55\u6279\u91cf\u5927\u5c0f\u4e0b\u5b9e\u73b0\u6536\u655b\uff0c\u800c\u65e0\u9700\u52a8\u91cf\u5219\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\uff0c\u65e0\u8bba\u6b65\u957f\u5982\u4f55\u9009\u62e9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u7b97\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u5b9e\u8bc1\u6027\u80fd\u3002", "conclusion": "\u52a8\u91cf\u662f\u968f\u673aDC\u4f18\u5316\u4e2d\u5b9e\u73b0\u6536\u655b\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u63d0\u51fa\u7684\u52a8\u91cf\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u5c0f\u6279\u91cf\u573a\u666f\u4e0b\u7684\u6536\u655b\u95ee\u9898\u3002"}}
{"id": "2510.16071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16071", "abs": "https://arxiv.org/abs/2510.16071", "authors": ["Qinxuan Wang", "Chuang Wang", "Mingyu Zhang", "Jingwei Sun", "Peipei Yang", "Shuo Tang", "Shiming Xiang"], "title": "MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data", "comment": null, "summary": "Neural operators have emerged as a powerful data-driven paradigm for solving\nPartial Differential Equations (PDEs), offering orders-of-magnitude\nacceleration over traditional solvers. However, existing approaches still\nsuffer from limited accuracy and scalability, particularly on irregular domains\nwhere fluid flows exhibit rich multiscale structures. In this work, we\nintroduce the Multiscale Neural Operator (MNO), a new architecture for\nComputational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point\nclouds. MNO explicitly decomposes information across three scales: a global\ndimension-shrinkage attention module for long-range dependencies, a local graph\nattention module for neighborhood-level interactions, and a micro point-wise\nattention module for fine-grained details. This design preserves multiscale\ninductive biases while remaining computationally efficient. We evaluate MNO on\nfour diverse benchmarks, covering both steady-state and unsteady flow scenarios\nwith up to 300K points. Across all tasks, MNO consistently outperforms\nstate-of-the-art baselines, reducing prediction errors by 5% to 40% and\ndemonstrating improved robustness in challenging 3D CFD problems. Our results\nhighlight the importance of explicit multiscale design for neural operators and\nestablish MNO as a scalable framework for learning complex fluid dynamics on\nirregular domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e09\u7ef4\u975e\u7ed3\u6784\u5316\u70b9\u4e91\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u591a\u5c3a\u5ea6\u795e\u7ecf\u7b97\u5b50(MNO)\uff0c\u901a\u8fc7\u663e\u5f0f\u5206\u89e3\u4e09\u4e2a\u5c3a\u5ea6\u7684\u4fe1\u606f\u6765\u63d0\u5347\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u4ecd\u5b58\u5728\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u4e0d\u89c4\u5219\u57df\u4e0a\u6d41\u4f53\u6d41\u52a8\u8868\u73b0\u51fa\u4e30\u5bcc\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u65f6\u3002", "method": "MNO\u67b6\u6784\u663e\u5f0f\u5206\u89e3\u4e09\u4e2a\u5c3a\u5ea6\uff1a\u5168\u5c40\u7ef4\u5ea6\u6536\u7f29\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\uff0c\u5c40\u90e8\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u90bb\u57df\u7ea7\u4ea4\u4e92\uff0c\u5fae\u89c2\u70b9\u7ea7\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u7cbe\u7ec6\u7ec6\u8282\u3002", "result": "\u5728\u56db\u4e2a\u6db5\u76d6\u7a33\u6001\u548c\u975e\u7a33\u6001\u6d41\u52a8\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMNO\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e5%\u81f340%\uff0c\u5728\u5177\u670930\u4e07\u4e2a\u70b9\u7684\u6311\u6218\u60273D CFD\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u663e\u5f0f\u591a\u5c3a\u5ea6\u8bbe\u8ba1\u5bf9\u795e\u7ecf\u7b97\u5b50\u81f3\u5173\u91cd\u8981\uff0cMNO\u4e3a\u5728\u4e0d\u89c4\u5219\u57df\u4e0a\u5b66\u4e60\u590d\u6742\u6d41\u4f53\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2510.16074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16074", "abs": "https://arxiv.org/abs/2510.16074", "authors": ["Jing He", "Hua Jiang", "Cheng Li", "Siqian Xin", "Shuzhen Yang"], "title": "Early-stopping for Transformer model training", "comment": null, "summary": "This work introduces a novel theoretical framework grounded in Random Matrix\nTheory (RMT) for analyzing Transformer training dynamics. We focus on the\nunderlying mechanisms that drive performance improvements and derive principled\nearly-stopping criteria. Empirically, we observe that the spectral density of\nthe shallow self-attention matrix V consistently evolves into a heavy-tailed\ndistribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we\ndemarcate training into three stages: structural exploration, heavy-tailed\nstructure stabilization, and convergence saturation. This staging provides\nguidance for preliminary stopping decisions. Crucially, we propose two\nconsistent and validation-free criteria: a quantitative metric for heavy-tailed\ndynamics and a novel spectral signature indicative of convergence. The strong\nalignment between these criteria highlights the utility of RMT for monitoring\nand diagnosing the progression of Transformer model training.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u968f\u673a\u77e9\u9635\u7406\u8bba\u63d0\u51fa\u5206\u6790Transformer\u8bad\u7ec3\u52a8\u6001\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u51fa\u539f\u5219\u6027\u7684\u65e9\u505c\u51c6\u5219\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u77e9\u9635\u7684\u8c31\u5bc6\u5ea6\u6f14\u5316\u8bc6\u522b\u8bad\u7ec3\u4e09\u9636\u6bb5\uff0c\u5e76\u63d0\u51fa\u4e24\u4e2a\u65e0\u9700\u9a8c\u8bc1\u7684\u6536\u655b\u5224\u65ad\u6807\u51c6\u3002", "motivation": "\u7406\u89e3Transformer\u8bad\u7ec3\u52a8\u6001\u7684\u5e95\u5c42\u673a\u5236\uff0c\u4e3a\u6027\u80fd\u6539\u8fdb\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5efa\u7acb\u65e0\u9700\u9a8c\u8bc1\u6570\u636e\u7684\u65e9\u505c\u51c6\u5219\u3002", "method": "\u57fa\u4e8e\u968f\u673a\u77e9\u9635\u7406\u8bba\u5206\u6790\u81ea\u6ce8\u610f\u529b\u77e9\u9635\u7684\u8c31\u5bc6\u5ea6\u6f14\u5316\uff0c\u4f7f\u7528\u5e42\u5f8b\u62df\u5408\u4f5c\u4e3a\u63a2\u9488\u8bc6\u522b\u8bad\u7ec3\u9636\u6bb5\uff0c\u63d0\u51fa\u5b9a\u91cf\u5ea6\u91cf\u91cd\u5c3e\u52a8\u6001\u548c\u8c31\u7279\u5f81\u6536\u655b\u6807\u5fd7\u3002", "result": "\u53d1\u73b0\u6d45\u5c42\u81ea\u6ce8\u610f\u529b\u77e9\u9635V\u7684\u8c31\u5bc6\u5ea6\u59cb\u7ec8\u6f14\u5316\u4e3a\u91cd\u5c3e\u5206\u5e03\uff0c\u8bad\u7ec3\u53ef\u5206\u4e3a\u7ed3\u6784\u63a2\u7d22\u3001\u91cd\u5c3e\u7ed3\u6784\u7a33\u5b9a\u548c\u6536\u655b\u9971\u548c\u4e09\u9636\u6bb5\uff0c\u63d0\u51fa\u7684\u4e24\u4e2a\u51c6\u5219\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u968f\u673a\u77e9\u9635\u7406\u8bba\u4e3a\u76d1\u63a7\u548c\u8bca\u65adTransformer\u6a21\u578b\u8bad\u7ec3\u8fdb\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63d0\u51fa\u7684\u65e9\u505c\u51c6\u5219\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "ToolCritic\u662f\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u68c0\u6d4b8\u79cd\u7279\u5b9a\u5de5\u5177\u8c03\u7528\u9519\u8bef\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe13%\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u4ecd\u7136\u963b\u788d\u5176\u53ef\u9760\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u6539\u8fdb\u673a\u5236\u3002", "method": "\u63d0\u51faToolCritic\u6846\u67b6\uff0c\u5b9a\u4e498\u79cd\u7279\u5b9a\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7c7b\u578b\uff0c\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3ToolCritic\uff0c\u8ba9\u5177\u5907\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u4e3bLLM\u6839\u636eToolCritic\u7684\u53cd\u9988\u4fee\u6b63\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToolCritic\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6821\u6b63\u6280\u672f\uff09\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe13%\u3002", "conclusion": "ToolCritic\u4ee3\u8868\u4e86\u5728\u73b0\u5b9e\u5bf9\u8bdd\u5e94\u7528\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5065\u7684LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u6709\u524d\u666f\u7684\u4e00\u6b65\u3002"}}
{"id": "2510.16076", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16076", "abs": "https://arxiv.org/abs/2510.16076", "authors": ["SeongKu Kang", "Jianxun Lian", "Dongha Lee", "Wonbin Kweon", "Sanghwan Jang", "Jaehyun Lee", "Jindong Wang", "Xing Xie", "Hwanjo Yu"], "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recommender systems suffer from biases that cause the collected feedback to\nincompletely reveal user preference. While debiasing learning has been\nextensively studied, they mostly focused on the specialized (called\ncounterfactual) test environment simulated by random exposure of items,\nsignificantly degrading accuracy in the typical (called factual) test\nenvironment based on actual user-item interactions. In fact, each test\nenvironment highlights the benefit of a different aspect: the counterfactual\ntest emphasizes user satisfaction in the long-terms, while the factual test\nfocuses on predicting subsequent user behaviors on platforms. Therefore, it is\ndesirable to have a model that performs well on both tests rather than only\none. In this work, we introduce a new learning framework, called Bias-adaptive\nPreference distillation Learning (BPL), to gradually uncover user preferences\nwith dual distillation strategies. These distillation strategies are designed\nto drive high performance in both factual and counterfactual test environments.\nEmploying a specialized form of teacher-student distillation from a biased\nmodel, BPL retains accurate preference knowledge aligned with the collected\nfeedback, leading to high performance in the factual test. Furthermore, through\nself-distillation with reliability filtering, BPL iteratively refines its\nknowledge throughout the training process. This enables the model to produce\nmore accurate predictions across a broader range of user-item combinations,\nthereby improving performance in the counterfactual test. Comprehensive\nexperiments validate the effectiveness of BPL in both factual and\ncounterfactual tests. Our implementation is accessible via:\nhttps://github.com/SeongKu-Kang/BPL.", "AI": {"tldr": "BPL\u662f\u4e00\u4e2a\u65b0\u7684\u63a8\u8350\u7cfb\u7edf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u84b8\u998f\u7b56\u7565\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u90fd\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u6536\u96c6\u7684\u53cd\u9988\u4e0d\u80fd\u5b8c\u5168\u63ed\u793a\u7528\u6237\u504f\u597d\u3002\u73b0\u6709\u53bb\u504f\u5b66\u4e60\u4e3b\u8981\u4e13\u6ce8\u4e8e\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\uff0c\u4f46\u5728\u57fa\u4e8e\u5b9e\u9645\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7684\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u5f00\u53d1\u5728\u4e24\u79cd\u6d4b\u8bd5\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u504f\u5dee\u81ea\u9002\u5e94\u504f\u597d\u84b8\u998f\u5b66\u4e60\uff08BPL\uff09\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u84b8\u998f\u7b56\u7565\uff1a1\uff09\u4ece\u6709\u504f\u6a21\u578b\u8fdb\u884c\u5e08\u751f\u84b8\u998f\uff0c\u4fdd\u7559\u4e0e\u6536\u96c6\u53cd\u9988\u4e00\u81f4\u7684\u51c6\u786e\u504f\u597d\u77e5\u8bc6\uff1b2\uff09\u901a\u8fc7\u53ef\u9760\u6027\u8fc7\u6ee4\u7684\u81ea\u84b8\u998f\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u7cbe\u70bc\u77e5\u8bc6\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BPL\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "BPL\u6846\u67b6\u80fd\u591f\u9010\u6b65\u63ed\u793a\u7528\u6237\u504f\u597d\uff0c\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u90fd\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2510.17149", "categories": ["cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17149", "abs": "https://arxiv.org/abs/2510.17149", "authors": ["Hongyi Du", "Jiaqi Su", "Jisen Li", "Lijie Ding", "Yingxuan Yang", "Peixuan Han", "Xiangru Tang", "Kunlun Zhu", "Jiaxuan You"], "title": "Which LLM Multi-Agent Protocol to Choose?", "comment": "Under review at ICLR 2026.Code and benchmark artifacts:\n  https://github.com/ulab-uiuc/AgentProtocols", "summary": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale.", "AI": {"tldr": "ProtocolBench\u662f\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u4fe1\u534f\u8bae\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u534f\u8bae\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5ef6\u8fdf\u3001\u6d88\u606f\u5f00\u9500\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\u534f\u8bae\u9009\u62e9\u5bf9\u7cfb\u7edf\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86ProtocolRouter\u5b66\u4e60\u578b\u534f\u8bae\u8def\u7531\u5668\u6765\u4f18\u5316\u534f\u8bae\u9009\u62e9\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u901a\u4fe1\u534f\u8bae\u5c42\u6210\u4e3a\u5f71\u54cd\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u76ee\u524d\u534f\u8bae\u9009\u62e9\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u5bfc\uff0c\u4e3b\u8981\u4f9d\u8d56\u76f4\u89c9\u3002", "method": "\u5f15\u5165ProtocolBench\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u4ece\u56db\u4e2a\u53ef\u6d4b\u91cf\u7ef4\u5ea6\uff08\u4efb\u52a1\u6210\u529f\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u6d88\u606f/\u5b57\u8282\u5f00\u9500\u3001\u6545\u969c\u6062\u590d\u80fd\u529b\uff09\u6bd4\u8f83\u4e0d\u540c\u534f\u8bae\u3002\u540c\u65f6\u63d0\u51faProtocolRouter\u5b66\u4e60\u578b\u534f\u8bae\u8def\u7531\u5668\uff0c\u6839\u636e\u9700\u6c42\u548c\u8fd0\u884c\u65f6\u4fe1\u53f7\u9009\u62e9\u6700\u4f18\u534f\u8bae\u3002", "result": "\u534f\u8bae\u9009\u62e9\u663e\u8457\u5f71\u54cd\u7cfb\u7edf\u884c\u4e3a\uff1a\u5728Streaming Queue\u573a\u666f\u4e2d\uff0c\u603b\u4f53\u5b8c\u6210\u65f6\u95f4\u5dee\u5f02\u8fbe36.5%\uff0c\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u8fdf\u5dee\u5f023.48\u79d2\u3002ProtocolRouter\u76f8\u6bd4\u6700\u4f73\u5355\u534f\u8bae\u57fa\u7ebf\uff0c\u5c06Fail-Storm\u6062\u590d\u65f6\u95f4\u51cf\u5c1118.1%\uff0c\u5e76\u5728GAIA\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cProtocolBench\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0cProtocolRouter\u901a\u8fc7\u667a\u80fd\u534f\u8bae\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.16089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16089", "abs": "https://arxiv.org/abs/2510.16089", "authors": ["William Hoy", "Nurcin Celik"], "title": "STABLE: Gated Continual Learning for Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly require mechanisms for continual\nadaptation without full retraining. However, sequential updates can lead to\ncatastrophic forgetting, where new edits degrade previously acquired knowledge.\nThis work presents STABLE, a gated continual self editing framework that\nconstrains forgetting during sequential updates using parameter efficient fine\ntuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate\nedit is evaluated against a stability budget using one of three metrics: (i)\nExact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,\nreflecting reduced model confidence; and (iii) KL divergence, quantifying\ndistributional drift between the base and adapted models. If a threshold is\nexceeded, the LoRA update is rescaled through a clipping procedure or rejected.\nExperiments on the Qwen-2.5-7B model show that gating effectively mitigates\nforgetting while preserving adaptability. EM based gating achieved the highest\ncumulative performance in short continual learning sequences. Our results show\nthat different gating strategies can achieve comparable distribution shift\n(measured by KL divergence) while producing different accuracy outcomes,\nhighlighting the importance of gating design in continual adaptation. This\napproach offers a principled method for continual model editing, enabling LLMs\nto integrate new knowledge while maintaining reliability. Code:\nhttps://github.com/Bhoy1/STABLE", "AI": {"tldr": "STABLE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u673a\u5236\u7684\u6301\u7eed\u81ea\u7f16\u8f91\u6846\u67b6\uff0c\u4f7f\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u4e09\u79cd\u6307\u6807\u8bc4\u4f30\u7f16\u8f91\u7a33\u5b9a\u6027\uff0c\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u66f4\u65b0\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6301\u7eed\u9002\u5e94\u65b0\u77e5\u8bc6\u4f46\u907f\u514d\u5b8c\u5168\u91cd\u8bad\u7ec3\uff0c\u800c\u987a\u5e8f\u66f4\u65b0\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u65b0\u7f16\u8f91\u4f1a\u964d\u4f4e\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u4e09\u79cd\u6307\u6807\uff08\u7cbe\u786e\u5339\u914d\u4e0b\u964d\u3001\u6bd4\u7279\u589e\u52a0\u3001KL\u6563\u5ea6\uff09\u8bc4\u4f30\u5019\u9009\u7f16\u8f91\u7684\u7a33\u5b9a\u6027\uff0c\u8d85\u8fc7\u9608\u503c\u65f6\u5bf9LoRA\u66f4\u65b0\u8fdb\u884c\u88c1\u526a\u6216\u62d2\u7edd\u3002", "result": "\u5728Qwen-2.5-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u95e8\u63a7\u673a\u5236\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u540c\u65f6\u4fdd\u6301\u9002\u5e94\u6027\uff0c\u57fa\u4e8eEM\u7684\u95e8\u63a7\u5728\u77ed\u6301\u7eed\u5b66\u4e60\u5e8f\u5217\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u7d2f\u79ef\u6027\u80fd\u3002", "conclusion": "\u4e0d\u540c\u95e8\u63a7\u7b56\u7565\u53ef\u4ee5\u5b9e\u73b0\u76f8\u5f53\u7684\u5206\u5e03\u504f\u79fb\u4f46\u4ea7\u751f\u4e0d\u540c\u7684\u51c6\u786e\u5ea6\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u95e8\u63a7\u8bbe\u8ba1\u5728\u6301\u7eed\u9002\u5e94\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6301\u7eed\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2510.17172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17172", "abs": "https://arxiv.org/abs/2510.17172", "authors": ["Shun Huang", "Wenlu Xing", "Shijia Geng", "Hailong Wang", "Guangkun Nie", "Gongzheng Tang", "Chenyang He", "Shenda Hong"], "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients", "comment": null, "summary": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408ECG\u57fa\u7840\u6a21\u578b\u548c\u53ef\u89e3\u91caXGBoost\u5206\u7c7b\u5668\u7684\u6df7\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u98ce\u9669\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u662f\u9662\u5185\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u4f20\u7edf\u98ce\u9669\u8bc4\u5206\u6027\u80fd\u6709\u9650\uff0c\u800c\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u4efb\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528ECG\u57fa\u7840\u6a21\u578b\u63d0\u53d6150\u7ef4\u8bca\u65ad\u6982\u7387\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u540e\u8bad\u7ec3XGBoost\u5206\u7c7b\u5668\uff0c\u5e76\u91c7\u7528SHAP\u65b9\u6cd5\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u6df7\u5408\u6a21\u578bAUC\u8fbe\u52300.801\uff0c\u4f18\u4e8eKNN\u3001RNN\u548c1D-CNN\u6a21\u578b\uff0cSHAP\u5206\u6790\u663e\u793a\u6a21\u578b\u8bc6\u522b\u7279\u5f81\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3aVT/VF\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u6709\u6548\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\u5728\u6784\u5efa\u53ef\u4fe1\u8d56\u3001\u53ef\u89e3\u91caAI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.16123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16123", "abs": "https://arxiv.org/abs/2510.16123", "authors": ["Federico Malato", "Ville Hautam\u00e4ki"], "title": "Zero-shot World Models via Search in Memory", "comment": "10 pages, 8 figures in main text + appendices", "summary": "World Models have vastly permeated the field of Reinforcement Learning. Their\nability to model the transition dynamics of an environment have greatly\nimproved sample efficiency in online RL. Among them, the most notorious example\nis Dreamer, a model that learns to act in a diverse set of image-based\nenvironments. In this paper, we leverage similarity search and stochastic\nrepresentations to approximate a world model without a training procedure. We\nestablish a comparison with PlaNet, a well-established world model of the\nDreamer family. We evaluate the models on the quality of latent reconstruction\nand on the perceived similarity of the reconstructed image, on both next-step\nand long horizon dynamics prediction. The results of our study demonstrate that\na search-based world model is comparable to a training based one in both cases.\nNotably, our model show stronger performance in long-horizon prediction with\nrespect to the baseline on a range of visually different environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u7684\u4e16\u754c\u6a21\u578b\uff0c\u65e0\u9700\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u548c\u957f\u671f\u52a8\u6001\u9884\u6d4b\u65b9\u9762\u4e0e\u8bad\u7ec3\u578b\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u5728\u957f\u671f\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5229\u7528\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u6765\u8fd1\u4f3c\u4e16\u754c\u6a21\u578b\uff0c\u907f\u514d\u590d\u6742\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u6a21\u578b\u6784\u5efa\u6548\u7387\u3002", "method": "\u4f7f\u7528\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4e0eDreamer\u5bb6\u65cf\u7684PlaNet\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u641c\u7d22\u578b\u4e16\u754c\u6a21\u578b\u5728\u6f5c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u611f\u77e5\u76f8\u4f3c\u6027\u65b9\u9762\u4e0e\u8bad\u7ec3\u578b\u6a21\u578b\u76f8\u5f53\uff0c\u5728\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u53ef\u4ee5\u6784\u5efa\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.17463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17463", "abs": "https://arxiv.org/abs/2510.17463", "authors": ["Cor Steging", "Tadeusz Zbiegie\u0144"], "title": "Label Indeterminacy in AI & Law", "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 38th International Conference on Legal Knowledge and Information\n  Systems (JURIX) in Turin, December 9 to 11 of 2025", "summary": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff08label indeterminacy\uff09\u95ee\u9898\uff0c\u5373\u6cd5\u5f8b\u6848\u4ef6\u7ed3\u679c\u53ef\u80fd\u56e0\u4eba\u4e3a\u5e72\u9884\uff08\u5982\u548c\u89e3\u3001\u4e0a\u8bc9\u7b49\uff09\u800c\u6539\u53d8\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u4e0d\u5177\u786e\u5b9a\u6027\u3002\u4f5c\u8005\u901a\u8fc7\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u6807\u7b7e\u6784\u5efa\u65b9\u5f0f\u5982\u4f55\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u901a\u5e38\u5c06\u8fc7\u53bb\u6848\u4ef6\u7ed3\u679c\u89c6\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u6cd5\u5f8b\u7ed3\u679c\u5f80\u5f80\u53d7\u5230\u672a\u8bb0\u5f55\u7684\u4eba\u4e3a\u5e72\u9884\u5f71\u54cd\uff0c\u5bfc\u81f4\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u5728\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4f8b\u5206\u7c7b\u7684\u80cc\u666f\u4e0b\uff0c\u7814\u7a76\u4e0d\u540c\u6807\u7b7e\u6784\u5efa\u65b9\u6cd5\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u5904\u7406\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u7684\u73b0\u6709\u65b9\u6cd5\u53ca\u5176\u5047\u8bbe\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6807\u7b7e\u7684\u6784\u5efa\u65b9\u5f0f\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u5bf9\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u7684\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662fAI\u4e0e\u6cd5\u5f8b\u9886\u57df\u7684\u91cd\u8981\u5173\u6ce8\u70b9\uff0c\u9700\u8981\u5728\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u4e88\u4ee5\u8003\u8651\uff0c\u5c3d\u7ba1\u73b0\u6709\u5904\u7406\u65b9\u6cd5\u90fd\u57fa\u4e8e\u65e0\u6cd5\u9a8c\u8bc1\u7684\u5047\u8bbe\u3002"}}
{"id": "2510.16167", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16167", "abs": "https://arxiv.org/abs/2510.16167", "authors": ["Archie Chaudhury"], "title": "Alignment is Localized: A Causal Probe into Preference Layers", "comment": null, "summary": "Reinforcement Learning frameworks, particularly those utilizing human\nannotations, have become an increasingly popular method for preference\nfine-tuning, where the outputs of a language model are tuned to match a certain\nset of behavioral policies or guidelines. Reinforcement Learning through Human\nFeedback (RLHF) is perhaps the most popular implementation of such a framework,\nparticularly for aligning LMs toward safety and human intent. However, the\ninternal workings of how such alignment is achieved remain largely opaque. In\nthis work, we systematically analyze preference optimization for language model\nalignment by applying layer-wide causal patching between a base model and its\ntuned counterpart across human preference pairs. We implement our methodology\non \\textit{Llama-3.2-1B}, and find that alignment is spatially localized:\nmid-layer activations encode a distinct subspace that causally determines\nreward-consistent behavior, while early and late layers remain largely\nunaffected. Utilizing LASSO regression, we also find that only a small number\nof layers possess non-zero coefficients linking activation distances to reward\ngains. Overall, we show that, at least for some language models, alignment from\nhuman-based, preferential tuning is a directional, low rank process, rather\nthan diffuse and parameteric.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c42\u95f4\u56e0\u679c\u4fee\u8865\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u673a\u5236\uff0c\u53d1\u73b0\u5bf9\u9f50\u8fc7\u7a0b\u662f\u7a7a\u95f4\u5c40\u90e8\u5316\u7684\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u4e2d\u5c42\u6fc0\u6d3b\u7684\u7279\u5b9a\u5b50\u7a7a\u95f4\uff0c\u800c\u975e\u6269\u6563\u7684\u53c2\u6570\u5316\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u5df2\u6210\u4e3a\u8bed\u8a00\u6a21\u578b\u504f\u597d\u5fae\u8c03\u7684\u6d41\u884c\u65b9\u6cd5\uff0c\u4f46\u5176\u5185\u90e8\u5bf9\u9f50\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5bf9\u9f50\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u3002", "method": "\u5728Llama-3.2-1B\u6a21\u578b\u4e0a\u5e94\u7528\u5c42\u95f4\u56e0\u679c\u4fee\u8865\u6280\u672f\uff0c\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u4e0e\u5176\u8c03\u4f18\u7248\u672c\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u4f7f\u7528LASSO\u56de\u5f52\u5206\u6790\u6fc0\u6d3b\u8ddd\u79bb\u4e0e\u5956\u52b1\u589e\u76ca\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5bf9\u9f50\u662f\u7a7a\u95f4\u5c40\u90e8\u5316\u7684\uff1a\u4e2d\u5c42\u6fc0\u6d3b\u7f16\u7801\u4e86\u51b3\u5b9a\u5956\u52b1\u4e00\u81f4\u884c\u4e3a\u7684\u7279\u5b9a\u5b50\u7a7a\u95f4\uff0c\u800c\u65e9\u671f\u548c\u665a\u671f\u5c42\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\uff1b\u53ea\u6709\u5c11\u6570\u5c42\u5177\u6709\u8fde\u63a5\u6fc0\u6d3b\u8ddd\u79bb\u4e0e\u5956\u52b1\u589e\u76ca\u7684\u975e\u96f6\u7cfb\u6570\u3002", "conclusion": "\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u662f\u4e00\u4e2a\u65b9\u5411\u6027\u7684\u4f4e\u79e9\u8fc7\u7a0b\uff0c\u800c\u975e\u6269\u6563\u7684\u53c2\u6570\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe\uff08MAIDs\uff09\u4f5c\u4e3a\u56fe\u5f62\u6846\u67b6\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8eMAIDs\u7684\u5b9a\u5411\u5e72\u9884\u8303\u5f0f\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u6280\u672fPSI\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5e72\u9884\uff0c\u907f\u514d\u5168\u5c40\u6307\u5bfc\u7684\u590d\u6742\u6027\u3002", "motivation": "\u5728\u5927\u89c4\u6a21MARL\u4e2d\uff0c\u5bf9\u6574\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u884c\u5168\u5c40\u4eba\u7c7b\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u73b0\u6709\u534f\u8c03\u673a\u5236\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u7814\u7a76\uff0c\u7f3a\u4e4f\u6613\u7528\u7684\u7814\u7a76\u5de5\u5177\u3002", "method": "\u5f15\u5165MAIDs\u4f5c\u4e3a\u56fe\u5f62\u6846\u67b6\u5206\u6790\u73b0\u6709MARL\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u57fa\u4e8eMAIDs\u7684\u5b9a\u5411\u5e72\u9884\u8303\u5f0f\uff0c\u5e94\u7528\u56e0\u679c\u63a8\u65ad\u6280\u672fPSI\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5e72\u9884\uff0c\u901a\u8fc7\u6700\u5927\u5316\u56e0\u679c\u6548\u5e94\u8fbe\u6210\u590d\u5408\u671f\u671b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b9a\u5411\u5e72\u9884\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u76f8\u5173\u6027\u56fe\u5206\u6790\u7684\u7ed3\u679c\u3002", "conclusion": "MAIDs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u5206\u6790\u548c\u8bbe\u8ba1MARL\u4ea4\u4e92\u8303\u5f0f\uff0c\u5b9a\u5411\u5e72\u9884\u80fd\u591f\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u95ee\u9898\uff0cPSI\u6280\u672f\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u671f\u671b\u7ed3\u679c\u3002"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u51fa\u9519\u8bef\u7b54\u6848\u65f6\u4ecd\u80fd\u611f\u77e5\u5230\u6b63\u786e\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\"\u770b\u89c1\u4f46\u4e0d\u76f8\u4fe1\"\u3002\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u5e72\u9884\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u7cfb\u7edf\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5931\u8d25\u7684\u539f\u56e0\uff1a\u662f\u672a\u80fd\u611f\u77e5\u89c6\u89c9\u8bc1\u636e\u8fd8\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5c42\u95f4\u6ce8\u610f\u529b\u52a8\u6001\u5206\u6790\uff0c\u53d1\u73b0\u6d45\u5c42\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u6df1\u5c42\u7a00\u758f\u4f46\u53ef\u9760\u5730\u5173\u6ce8\u5c40\u90e8\u8bc1\u636e\u533a\u57df\u3002\u5f15\u5165\u57fa\u4e8e\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u7684\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u5e72\u9884\u65b9\u6cd5\u5728LLaVA\u3001Qwen\u3001Gemma\u548cInternVL\u7b49\u591a\u4e2aVLM\u5bb6\u65cf\u4e2d\u4e00\u81f4\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u6a21\u578b\u5185\u90e8\u7f16\u7801\u4e86\u53ef\u9760\u8bc1\u636e\u4f46\u672a\u5145\u5206\u5229\u7528\u3002", "conclusion": "VLMs\u5185\u90e8\u7f16\u7801\u53ef\u9760\u8bc1\u636e\u4f46\u5229\u7528\u4e0d\u8db3\uff0c\u4f7f\u8fd9\u4e9b\u4fe1\u53f7\u663e\u5f0f\u5316\u53ef\u4ee5\u5f25\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u8fdbVLM\u7684\u8bca\u65ad\u7406\u89e3\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.16233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16233", "abs": "https://arxiv.org/abs/2510.16233", "authors": ["Patricia West", "Michelle WL Wan", "Alexander Hepburn", "Edwin Simpson", "Raul Santos-Rodriguez", "Jeffrey N Clark"], "title": "Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal", "comment": null, "summary": "Climate change demands effective legislative action to mitigate its impacts.\nThis study explores the application of machine learning (ML) to understand the\nprogression of climate policy from announcement to adoption, focusing on\npolicies within the European Green Deal. We present a dataset of 165 policies,\nincorporating text and metadata. We aim to predict a policy's progression\nstatus, and compare text representation methods, including TF-IDF, BERT, and\nClimateBERT. Metadata features are included to evaluate the impact on\npredictive performance. On text features alone, ClimateBERT outperforms other\napproaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance\nwith the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods\nfrom explainable AI highlights the influence of factors such as policy wording\nand metadata including political party and country representation. These\nfindings underscore the potential of ML tools in supporting climate policy\nanalysis and decision-making.", "AI": {"tldr": "\u672c\u7814\u7a76\u5e94\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6b27\u76df\u7eff\u8272\u534f\u8bae\u6c14\u5019\u653f\u7b56\u7684\u8fdb\u5c55\u72b6\u6001\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6587\u672c\u8868\u793a\u65b9\u6cd5\uff0c\u53d1\u73b0ClimateBERT\u5728\u7eaf\u6587\u672c\u7279\u5f81\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u7ed3\u5408\u5143\u6570\u636e\u540eBERT\u6548\u679c\u66f4\u597d\uff0c\u63ed\u793a\u4e86\u653f\u7b56\u63aa\u8f9e\u548c\u653f\u6cbb\u56e0\u7d20\u5bf9\u653f\u7b56\u8fdb\u5c55\u7684\u5f71\u54cd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u9700\u8981\u6709\u6548\u7684\u7acb\u6cd5\u884c\u52a8\u6765\u7f13\u89e3\u5176\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u5982\u4f55\u5e2e\u52a9\u7406\u89e3\u6c14\u5019\u653f\u7b56\u4ece\u5ba3\u5e03\u5230\u91c7\u7eb3\u7684\u8fdb\u5c55\u8fc7\u7a0b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b165\u9879\u653f\u7b56\u7684\u6587\u672c\u548c\u5143\u6570\u636e\u6570\u636e\u96c6\uff0c\u4f7f\u7528TF-IDF\u3001BERT\u548cClimateBERT\u7b49\u6587\u672c\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5143\u6570\u636e\u7279\u5f81\u9884\u6d4b\u653f\u7b56\u8fdb\u5c55\u72b6\u6001\uff0c\u5e76\u5e94\u7528\u53ef\u89e3\u91caAI\u65b9\u6cd5\u5206\u6790\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u7eaf\u6587\u672c\u7279\u5f81\u4e0bClimateBERT\u8868\u73b0\u6700\u4f73\uff08RMSE=0.17\uff0cR\u00b2=0.29\uff09\uff0c\u7ed3\u5408\u5143\u6570\u636e\u540eBERT\u6548\u679c\u66f4\u597d\uff08RMSE=0.16\uff0cR\u00b2=0.38\uff09\uff0c\u53ef\u89e3\u91caAI\u5206\u6790\u663e\u793a\u653f\u7b56\u63aa\u8f9e\u3001\u653f\u515a\u80cc\u666f\u548c\u56fd\u5bb6\u4ee3\u8868\u6027\u7b49\u56e0\u7d20\u5bf9\u653f\u7b56\u8fdb\u5c55\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5728\u652f\u6301\u6c14\u5019\u653f\u7b56\u5206\u6790\u548c\u51b3\u7b56\u5236\u5b9a\u65b9\u9762\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u80fd\u591f\u8bc6\u522b\u5f71\u54cd\u653f\u7b56\u8fdb\u5c55\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.16292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16292", "abs": "https://arxiv.org/abs/2510.16292", "authors": ["Yutong Wang", "Haiyu Wang", "Sai Qian Zhang"], "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models", "comment": "Accepted as Spotlight paper by NeurIPS 2025", "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5947\u5f02\u503c\u5206\u89e3(SVD)\u548c\u91cf\u5316\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u5e94\u7528\u6027\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3(SVD)\u5bf9\u8054\u5408\u67e5\u8be2(Q)\u3001\u952e(K)\u3001\u503c(V)\u6743\u91cd\u77e9\u9635\u8fdb\u884c\u5904\u7406\u4ee5\u51cf\u5c11KV\u7f13\u5b58\u5927\u5c0f\uff0c\u5e76\u5f15\u5165\u52a8\u6001SVD\u79e9\u5206\u914d\u7b56\u7565\uff0c\u540c\u65f6\u5e94\u7528\u91cf\u5316\u5230\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u786c\u4ef6\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8d85\u8fc710%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u91cf\u5316\u6216SVD\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16440", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16440", "abs": "https://arxiv.org/abs/2510.16440", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ECML-PKDD 2025\u9ad8\u80fd\u7269\u7406\u53d1\u73b0\u9c81\u68d2\u5b66\u4e60\u6311\u6218\u8d5b\u4e2dTask 1\u7684\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u4efb\u52a1\u8981\u6c42\u8bbe\u8ba1\u5bf9\u6297\u653b\u51fb\u4ee5\u6700\u5927\u5316\u8bef\u5206\u7c7b\u540c\u65f6\u6700\u5c0f\u5316\u6270\u52a8\u3002", "motivation": "\u8be5\u4efb\u52a1\u7684\u52a8\u673a\u662f\u8bbe\u8ba1\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6270\u52a8\u6700\u5c0f\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316\u5206\u7c7b\u6a21\u578b\u7684\u8bef\u5206\u7c7b\u7387\uff0c\u4ee5\u6d4b\u8bd5\u6a21\u578b\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u591a\u8f6e\u68af\u5ea6\u57fa\u7b56\u7565\uff0c\u5229\u7528\u6a21\u578b\u7684\u53ef\u5fae\u5206\u7ed3\u6784\uff0c\u7ed3\u5408\u968f\u673a\u521d\u59cb\u5316\u548c\u6837\u672c\u6df7\u5408\u6280\u672f\u6765\u589e\u5f3a\u653b\u51fb\u6548\u679c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u5728\u6270\u52a8\u5927\u5c0f\u548c\u6b3a\u9a97\u6210\u529f\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u591a\u8f6e\u68af\u5ea6\u57fa\u653b\u51fb\u7b56\u7565\u7ed3\u5408\u968f\u673a\u521d\u59cb\u5316\u548c\u6837\u672c\u6df7\u5408\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u6297\u6837\u672c\uff0c\u5728\u4fdd\u6301\u5c0f\u6270\u52a8\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8bef\u5206\u7c7b\u7387\u3002"}}
{"id": "2510.16448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16448", "abs": "https://arxiv.org/abs/2510.16448", "authors": ["Yongxiang Hua", "Haoyu Cao", "Zhou Tao", "Bocheng Li", "Zihao Wu", "Chaohu Liu", "Linli Xu"], "title": "Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts", "comment": "ACM MM25", "summary": "Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling\nlarge vision-language models, offering substantial capacity while maintaining\ncomputational efficiency through dynamic, sparse activation of experts.\nHowever, existing routing mechanisms, typically based on similarity scoring,\nstruggle to effectively capture the underlying input structure. This limitation\nleads to a trade-off between expert specialization and balanced computation,\nhindering both scalability and performance. We propose Input Domain Aware MoE,\na novel routing framework that leverages a probabilistic mixture model to\nbetter partition the input space. By modeling routing probabilities as a\nmixture of distributions, our method enables experts to develop clear\nspecialization boundaries while achieving balanced utilization. Unlike\nconventional approaches, our routing mechanism is trained independently of\ntask-specific objectives, allowing for stable optimization and decisive expert\nassignments. Empirical results on vision-language tasks demonstrate that our\nmethod consistently outperforms existing sMoE approaches, achieving higher task\nperformance and improved expert utilization balance.", "AI": {"tldr": "\u63d0\u51faInput Domain Aware MoE\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6df7\u5408\u6a21\u578b\u66f4\u597d\u5730\u5212\u5206\u8f93\u5165\u7a7a\u95f4\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76f8\u4f3c\u6027\u5f97\u5206\u7684\u8def\u7531\u673a\u5236\u96be\u4ee5\u6709\u6548\u6355\u6349\u8f93\u5165\u7ed3\u6784\uff0c\u5bfc\u81f4\u4e13\u5bb6\u4e13\u4e1a\u5316\u4e0e\u8ba1\u7b97\u5e73\u8861\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd", "method": "\u4f7f\u7528\u6982\u7387\u6df7\u5408\u6a21\u578b\u5efa\u6a21\u8def\u7531\u6982\u7387\uff0c\u4f7f\u4e13\u5bb6\u5f62\u6210\u6e05\u6670\u7684\u4e13\u4e1a\u5316\u8fb9\u754c\uff0c\u8def\u7531\u673a\u5236\u72ec\u7acb\u4e8e\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u4efb\u52a1\u6027\u80fd\u548c\u6539\u5584\u7684\u4e13\u5bb6\u5229\u7528\u5e73\u8861", "conclusion": "\u63d0\u51fa\u7684\u8def\u7531\u6846\u67b6\u901a\u8fc7\u66f4\u597d\u7684\u8f93\u5165\u7a7a\u95f4\u5212\u5206\u5b9e\u73b0\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u8ba1\u7b97\u5e73\u8861\u7684\u6539\u8fdb"}}
{"id": "2510.16687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16687", "abs": "https://arxiv.org/abs/2510.16687", "authors": ["Shurong Lin", "Eric D. Kolaczyk", "Adam Smith", "Elliot Paquette"], "title": "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares", "comment": null, "summary": "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u6269\u6563\u65b9\u6cd5\u7cbe\u786e\u5206\u6790\u566a\u58f0SGD\uff0c\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u63d0\u4f9b\u7edf\u8ba1\u98ce\u9669\u548c\u9690\u79c1\u635f\u5931\u52a8\u6001\u7684\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\uff0c\u5e76\u7814\u7a76\u4e86\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u654f\u611f\u5ea6\u663e\u5f0f\u77e5\u8bc6\u7684\u566a\u58f0SGD\u53d8\u4f53\u3002", "motivation": "\u4f18\u5316\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u76f8\u4e92\u4f5c\u7528\u5df2\u6210\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u4e3b\u9898\u3002\u566a\u58f0SGD\u5df2\u6210\u4e3a\u5173\u952e\u7b97\u6cd5\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u63d0\u4f9b\u7edf\u8ba1\u98ce\u9669\u548c\u9690\u79c1\u635f\u5931\u7684\u5404\u79cd\u754c\u9650\uff0c\u800c\u8fc7\u7a0b\u7684\u7cbe\u786e\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u6269\u6563\u65b9\u6cd5\u5206\u6790\u566a\u58f0SGD\uff0c\u63d0\u4f9b\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\uff1b\u7814\u7a76\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u654f\u611f\u5ea6\u663e\u5f0f\u77e5\u8bc6\u7684\u566a\u58f0SGD\u53d8\u4f53\uff0c\u4e13\u6ce8\u4e8e\u5e26\u21132\u6b63\u5219\u5316\u7684\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u6355\u6349\u9ad8\u7ef4\u73af\u5883\u4e2d\u7edf\u8ba1\u98ce\u9669\u6f14\u5316\u548c\u9690\u79c1\u635f\u5931\u52a8\u6001\uff0c\u76f8\u6bd4\u73b0\u6709\u9700\u8981\u68af\u5ea6\u88c1\u526a\u6765\u5f3a\u5236\u654f\u611f\u5ea6\u7684\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u6269\u6563\u65b9\u6cd5\u4e3a\u5206\u6790\u566a\u58f0SGD\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u8fde\u7eed\u65f6\u95f4\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u65e0\u9700\u663e\u5f0f\u654f\u611f\u5ea6\u77e5\u8bc6\u7684\u53d8\u4f53\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.16695", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16695", "abs": "https://arxiv.org/abs/2510.16695", "authors": ["Iman Deznabi", "Peeyush Kumar", "Madalina Fiterau"], "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting", "comment": null, "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen\nconditions without direct historical data, posing a significant challenge for\ntraditional forecasting methods. We introduce a Resolution-Aware\nRetrieval-Augmented Forecasting model that enhances predictive accuracy by\nleveraging spatial correlations and temporal frequency characteristics. By\ndecomposing signals into different frequency components, our model employs\nresolution-aware retrieval, where lower-frequency components rely on broader\nspatial context, while higher-frequency components focus on local influences.\nThis allows the model to dynamically retrieve relevant data and adapt to new\nlocations with minimal historical context.\n  Applied to microclimate forecasting, our model significantly outperforms\ntraditional forecasting methods, numerical weather prediction models, and\nmodern foundation time series models, achieving 71% lower MSE than HRRR and 34%\nlower MSE than Chronos on the ERA5 dataset.\n  Our results highlight the effectiveness of retrieval-augmented and\nresolution-aware strategies, offering a scalable and data-efficient solution\nfor zero-shot forecasting in microclimate modeling and beyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u8fa8\u7387\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u65f6\u95f4\u9891\u7387\u7279\u5f81\u6765\u63d0\u9ad8\u96f6\u6837\u672c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6a21\u578b\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u4e0d\u540c\u9891\u7387\u5206\u91cf\uff0c\u91c7\u7528\u5206\u8fa8\u7387\u611f\u77e5\u68c0\u7d22\u7b56\u7565\uff0c\u4f4e\u9891\u5206\u91cf\u4f9d\u8d56\u66f4\u5e7f\u6cdb\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u9ad8\u9891\u5206\u91cf\u5173\u6ce8\u5c40\u90e8\u5f71\u54cd\u3002", "motivation": "\u96f6\u6837\u672c\u9884\u6d4b\u65e8\u5728\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5386\u53f2\u6570\u636e\u7684\u672a\u89c1\u6761\u4ef6\u4e0b\u7684\u7ed3\u679c\uff0c\u8fd9\u5bf9\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u65b0\u4f4d\u7f6e\u4e14\u5386\u53f2\u4e0a\u4e0b\u6587\u6700\u5c11\u7684\u65b9\u6cd5\u3002", "method": "\u5206\u8fa8\u7387\u611f\u77e5\u68c0\u7d22\u589e\u5f3a\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u4fe1\u53f7\u9891\u7387\u5206\u89e3\u548c\u5206\u8fa8\u7387\u611f\u77e5\u68c0\u7d22\u673a\u5236\uff0c\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u6570\u636e\u3002\u4f4e\u9891\u5206\u91cf\u4f7f\u7528\u66f4\u5e7f\u6cdb\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u9ad8\u9891\u5206\u91cf\u805a\u7126\u5c40\u90e8\u5f71\u54cd\u3002", "result": "\u5728\u5fae\u6c14\u5019\u9884\u6d4b\u5e94\u7528\u4e2d\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3001\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u548c\u73b0\u4ee3\u57fa\u7840\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u5728ERA5\u6570\u636e\u96c6\u4e0a\u6bd4HRRR\u7684MSE\u964d\u4f4e71%\uff0c\u6bd4Chronos\u7684MSE\u964d\u4f4e34%\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u548c\u5206\u8fa8\u7387\u611f\u77e5\u7b56\u7565\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4e3a\u5fae\u6c14\u5019\u5efa\u6a21\u53ca\u5176\u4ed6\u9886\u57df\u7684\u96f6\u6837\u672c\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16743", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16743", "abs": "https://arxiv.org/abs/2510.16743", "authors": ["Viktoria Schram", "Markus Hiller", "Daniel Beck", "Trevor Cohn"], "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "comment": "Accepted to NeurIPS 2025", "summary": "The prediction of learning curves for Natural Language Processing (NLP)\nmodels enables informed decision-making to meet specific performance\nobjectives, while reducing computational overhead and lowering the costs\nassociated with dataset acquisition and curation. In this work, we formulate\nthe prediction task as a multitask learning problem, where each task's data is\nmodelled as being organized within a two-layer hierarchy. To model the shared\ninformation and dependencies across tasks and hierarchical levels, we employ\nlatent variable multi-output Gaussian Processes, enabling to account for task\ncorrelations and supporting zero-shot prediction of learning curves (LCs). We\ndemonstrate that this approach facilitates the development of probabilistic\nscaling laws at lower costs. Applying an active learning strategy, LCs can be\nqueried to reduce predictive uncertainty and provide predictions close to\nground truth scaling laws. We validate our framework on three small-scale NLP\ndatasets with up to $30$ LCs. These are obtained from nanoGPT models, from\nbilingual translation using mBART and Transformer models, and from multilingual\ntranslation using M2M100 models of varying sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u6765\u9884\u6d4bNLP\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\uff0c\u652f\u6301\u96f6\u6837\u672c\u9884\u6d4b\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9884\u6d4bNLP\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\u53ef\u4ee5\u5728\u6ee1\u8db3\u7279\u5b9a\u6027\u80fd\u76ee\u6807\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u6570\u636e\u96c6\u83b7\u53d6\u6210\u672c\uff0c\u5b9e\u73b0\u66f4\u660e\u667a\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u5c06\u5b66\u4e60\u66f2\u7ebf\u9884\u6d4b\u4efb\u52a1\u5efa\u6a21\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u6765\u5efa\u6a21\u4efb\u52a1\u95f4\u548c\u5c42\u6b21\u95f4\u7684\u5171\u4eab\u4fe1\u606f\u548c\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u8f83\u4f4e\u6210\u672c\u5f00\u53d1\u6982\u7387\u6027\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u63a5\u8fd1\u771f\u5b9e\u7f29\u653e\u5b9a\u5f8b\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u5728\u4e09\u4e2a\u5c0f\u578bNLP\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4bNLP\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\uff0c\u652f\u6301\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5e76\u4e3a\u6a21\u578b\u7f29\u653e\u63d0\u4f9b\u6982\u7387\u6027\u6307\u5bfc\u3002"}}
{"id": "2510.16774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16774", "abs": "https://arxiv.org/abs/2510.16774", "authors": ["Yuguang Yue", "Irakli Salia", "Samuel Hunt", "Christopher Green", "Wenzhe Shi", "Jonathan J Hunt"], "title": "Learning to play: A Multimodal Agent for 3D Game-Play", "comment": "International Conference on Computer Vision Workshop on Multi-Modal\n  Reasoning for Agentic Intelligence", "summary": "We argue that 3-D first-person video games are a challenging environment for\nreal-time multi-modal reasoning. We first describe our dataset of human\ngame-play, collected across a large variety of 3-D first-person games, which is\nboth substantially larger and more diverse compared to prior publicly disclosed\ndatasets, and contains text instructions. We demonstrate that we can learn an\ninverse dynamics model from this dataset, which allows us to impute actions on\na much larger dataset of publicly available videos of human game play that lack\nrecorded actions. We then train a text-conditioned agent for game playing using\nbehavior cloning, with a custom architecture capable of realtime inference on a\nconsumer GPU. We show the resulting model is capable of playing a variety of\n3-D games and responding to text input. Finally, we outline some of the\nremaining challenges such as long-horizon tasks and quantitative evaluation\nacross a large set of games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6837\u5316\u76843D\u7b2c\u4e00\u4eba\u79f0\u6e38\u620f\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u80fd\u591f\u5b9e\u65f6\u63a8\u7406\u7684\u6587\u672c\u6761\u4ef6\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u591a\u6e38\u620f\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "3D\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6e38\u620f\u4e3a\u5b9e\u65f6\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\uff0c\u9700\u8981\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6536\u96c6\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u4eba\u7c7b\u6e38\u620f\u6570\u636e\u96c6\uff0c\u5b66\u4e60\u9006\u52a8\u529b\u5b66\u6a21\u578b\u6765\u63a8\u65ad\u7f3a\u5931\u52a8\u4f5c\uff0c\u4f7f\u7528\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u6587\u672c\u6761\u4ef6\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u7684\u81ea\u5b9a\u4e49\u67b6\u6784\u3002", "result": "\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u80fd\u591f\u5728\u591a\u79cd3D\u6e38\u620f\u4e2d\u54cd\u5e94\u6587\u672c\u6307\u4ee4\u8fdb\u884c\u6e38\u620f\uff0c\u5e76\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "\u867d\u7136\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u6e38\u620f\u5b9a\u91cf\u8bc4\u4f30\u7b49\u6311\u6218\u3002"}}
{"id": "2510.16780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16780", "abs": "https://arxiv.org/abs/2510.16780", "authors": ["Chang Wu", "Zhiyuan Liu", "Wen Shu", "Liang Wang", "Yanchen Luo", "Wenqiang Lei", "Yatao Bian", "Junfeng Fang", "Xiang Wang"], "title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding", "comment": null, "summary": "Masked graph modeling (MGM) is a promising approach for molecular\nrepresentation learning (MRL).However, extending the success of re-mask\ndecoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting\nchallenges: avoiding 2D structure leakage to the decoder, while still providing\nsufficient 2D context for reconstructing re-masked atoms.To address these\nchallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with\nSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in its\nSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant information\nfrom encoder representations while preserving the 2D graph structures.This SRD\nis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)\nencoder alongside a structure-independent decoder. We analyze that SRD,\ncombined with the structure-independent decoder, enhances the encoder's role in\nMRL. Extensive experiments show that 3D-GSRD achieves strong downstream\nperformance, setting a new state-of-the-art on 7 out of 8 targets in the widely\nused MD17 molecular property prediction benchmark. The code is released at\nhttps://github.com/WuChang0124/3D-GSRD.", "AI": {"tldr": "3D-GSRD\u662f\u4e00\u79cd\u7528\u4e8e\u5206\u5b50\u8868\u793a\u5b66\u4e60\u76843D\u5206\u5b50\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u91cd\u63a9\u7801\u89e3\u7801\u89e3\u51b32D\u52303D\u63a9\u7801\u56fe\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5728MD17\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5c06\u63a9\u7801\u56fe\u5efa\u6a21\u4ece2D\u6269\u5c55\u52303D\u9762\u4e34\u4e24\u4e2a\u51b2\u7a81\u6311\u6218\uff1a\u907f\u514d2D\u7ed3\u6784\u6cc4\u6f0f\u5230\u89e3\u7801\u5668\uff0c\u540c\u65f6\u4e3a\u91cd\u6784\u91cd\u63a9\u7801\u539f\u5b50\u63d0\u4f9b\u8db3\u591f\u76842D\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u91cd\u63a9\u7801\u89e3\u7801(SRD)\uff0c\u4ec5\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u91cd\u63a9\u78013D\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u75592D\u56fe\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u4e0e3D\u5173\u7cfb\u53d8\u6362\u5668\u7f16\u7801\u5668\u548c\u7ed3\u6784\u65e0\u5173\u89e3\u7801\u5668\u534f\u540c\u96c6\u6210\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684MD17\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c3D-GSRD\u57288\u4e2a\u76ee\u6807\u4e2d\u76847\u4e2a\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u9009\u62e9\u6027\u91cd\u63a9\u7801\u89e3\u7801\u4e0e\u7ed3\u6784\u65e0\u5173\u89e3\u7801\u5668\u76f8\u7ed3\u5408\u589e\u5f3a\u4e86\u7f16\u7801\u5668\u5728\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c3D-GSRD\u5728\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16806", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16806", "abs": "https://arxiv.org/abs/2510.16806", "authors": ["Weilin Wan", "Weizhong Zhang", "Cheng Jin"], "title": "Computational Budget Should Be Considered in Data Selection", "comment": null, "summary": "Data selection improves computational efficiency by choosing informative\nsubsets of training samples. However, existing methods ignore the compute\nbudget, treating data selection and importance evaluation independently of\ncompute budget constraints. Yet empirical studies show no algorithm can\nconsistently outperform others (or even random selection) across varying\nbudgets. We therefore argue that compute budget must be integral to\ndata-selection strategies, since different budgets impose distinct requirements\non data quantity, quality, and distribution for effective training. To this\nend, we propose a novel Computational budget-Aware Data Selection (CADS) method\nand naturally formulate it into a bilevel optimization framework, where the\ninner loop trains the model within the constraints of the computational budget\non some selected subset of training data, while the outer loop optimizes data\nselection based on model evaluation. Our technical contributions lie in\naddressing two main challenges in solving this bilevel optimization problem:\nthe expensive Hessian matrix estimation for outer-loop gradients and the\ncomputational burden of achieving inner-loop optimality during iterations. To\nsolve the first issue, we propose a probabilistic reparameterization strategy\nand compute the gradient using a Hessian-free policy gradient estimator. To\naddress the second challenge, we transform the inner optimization problem into\na penalty term in the outer objective, further discovering that we only need to\nestimate the minimum of a one-dimensional loss to calculate the gradient,\nsignificantly improving efficiency. Extensive experiments show that our method\nachieves performance gains of up to 14.42% over baselines in vision and\nlanguage benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9884\u7b97\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5CADS\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u5c06\u8ba1\u7b97\u9884\u7b97\u6574\u5408\u5230\u6570\u636e\u9009\u62e9\u7b56\u7565\u4e2d\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe14.42%\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5ffd\u7565\u4e86\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\uff0c\u800c\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u6ca1\u6709\u7b97\u6cd5\u80fd\u5728\u4e0d\u540c\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff08\u751a\u81f3\u968f\u673a\u9009\u62e9\uff09\uff0c\u56e0\u6b64\u8ba1\u7b97\u9884\u7b97\u5fc5\u987b\u6210\u4e3a\u6570\u636e\u9009\u62e9\u7b56\u7565\u7684\u6838\u5fc3\u8981\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u8ba1\u7b97\u9884\u7b97\u611f\u77e5\u6570\u636e\u9009\u62e9\u65b9\u6cd5CADS\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u5185\u5c42\u5728\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\u4e0b\u5728\u9009\u5b9a\u6570\u636e\u5b50\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5916\u5c42\u57fa\u4e8e\u6a21\u578b\u8bc4\u4f30\u4f18\u5316\u6570\u636e\u9009\u62e9\u3002\u4f7f\u7528\u6982\u7387\u91cd\u53c2\u6570\u5316\u7b56\u7565\u548cHessian-free\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\u89e3\u51b3Hessian\u77e9\u9635\u4f30\u8ba1\u95ee\u9898\uff0c\u5c06\u5185\u5c42\u4f18\u5316\u8f6c\u5316\u4e3a\u5916\u5c42\u76ee\u6807\u4e2d\u7684\u60e9\u7f5a\u9879\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe14.42%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8ba1\u7b97\u9884\u7b97\u5e94\u6210\u4e3a\u6570\u636e\u9009\u62e9\u7b56\u7565\u7684\u7ec4\u6210\u90e8\u5206\uff0cCADS\u65b9\u6cd5\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u9884\u7b97\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.16807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16807", "abs": "https://arxiv.org/abs/2510.16807", "authors": ["Zhoutong Wu", "Yuan Zhang", "Yiming Dong", "Chenheng Zhang", "Cong Fang", "Kun Yuan", "Zhouchen Lin"], "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads", "comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}", "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.", "AI": {"tldr": "SkipV1Former\u662f\u4e00\u79cdTransformer\u53d8\u4f53\uff0c\u901a\u8fc7\u4ece\u7b2c\u4e00\u5c42\u7684Value\u5934\u4f7f\u7528\u8df3\u8dc3\u8fde\u63a5\u6765\u589e\u5f3a\u6a21\u578b\u8868\u793a\u80fd\u529b\u5e76\u51cf\u5c11KV\u7f13\u5b58\uff0c\u5728\u51cf\u5c11\u7ea625% KV\u7f13\u5b58\u7684\u540c\u65f6\u6539\u5584\u56f0\u60d1\u5ea6\u3002", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u5728\u6269\u5c55\u65f6\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u81ea\u56de\u5f52\u89e3\u7801\u4e2d\u7684KV\u7f13\u5b58\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u51cf\u5c11KV\u6210\u672c\uff0c\u8981\u4e48\u4ee5\u727a\u7272\u8868\u793a\u80fd\u529b\u4e3a\u4ee3\u4ef7\u6765\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u4ece\u7b2c\u4e8c\u4e2a\u5757\u5f00\u59cb\uff0c\u6bcf\u4e2a\u5c42\u91cd\u7528\u7b2c\u4e00\u5c42\u7684\u4e00\u534aValue\u5934\uff0c\u540c\u65f6\u6b63\u5e38\u8ba1\u7b97\u53e6\u4e00\u534a\uff0c\u4ece\u800c\u5c06Value\u6295\u5f71\u548cV\u7f13\u5b58\u51cf\u5c11\u8fd150%\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u672a\u538b\u7f29\u7684\u7b2c\u4e00\u5c42Value\u4fe1\u606f\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\uff0cSkipV1Former\u76f8\u6bd4\u6807\u51c6MHA Transformer\u548c\u67d0\u4e9b\u5148\u8fdb\u53d8\u4f53\uff0c\u5728\u51cf\u5c11\u7ea625% KV\u7f13\u5b58\u7684\u540c\u65f6\u6539\u5584\u4e86\u56f0\u60d1\u5ea6\u3002\u4e0eYOCO\u7ed3\u5408\u65f6\uff0cKV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c11\u8fd150%\u4e14\u6027\u80fd\u4ecd\u6709\u6240\u63d0\u5347\u3002", "conclusion": "SkipV1Former\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3aTransformer\u8868\u793a\u80fd\u529b\u5e76\u663e\u8457\u51cf\u5c11KV\u7f13\u5b58\uff0c\u4e14\u53ef\u901a\u8fc7\u989d\u591610-15%\u8ba1\u7b97\u91cf\u5bf9\u73b0\u6709MHA Transformer\u68c0\u67e5\u70b9\u8fdb\u884c\u4e0a\u8bad\u7ec3\u3002"}}
{"id": "2510.16811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16811", "abs": "https://arxiv.org/abs/2510.16811", "authors": ["Mohammad Shahverdikondori", "Jalal Etesami", "Negar Kiyavash"], "title": "Graph Learning is Suboptimal in Causal Bandits", "comment": "31 pages, 5 figures", "summary": "We study regret minimization in causal bandits under causal sufficiency where\nthe underlying causal structure is not known to the agent. Previous work has\nfocused on identifying the reward's parents and then applying classic bandit\nmethods to them, or jointly learning the parents while minimizing regret. We\ninvestigate whether such strategies are optimal. Somewhat counterintuitively,\nour results show that learning the parent set is suboptimal. We do so by\nproving that there exist instances where regret minimization and parent\nidentification are fundamentally conflicting objectives. We further analyze\nboth the known and unknown parent set size regimes, establish novel regret\nlower bounds that capture the combinatorial structure of the action space.\nBuilding on these insights, we propose nearly optimal algorithms that bypass\ngraph and parent recovery, demonstrating that parent identification is indeed\nunnecessary for regret minimization. Experiments confirm that there exists a\nlarge performance gap between our method and existing baselines in various\nenvironments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56e0\u679c\u5145\u5206\u6027\u4e0b\u56e0\u679c\u5f3a\u76d7\u95ee\u9898\u4e2d\u7684\u9057\u61be\u6700\u5c0f\u5316\uff0c\u53d1\u73b0\u5b66\u4e60\u7236\u8282\u70b9\u96c6\u662f\u6b21\u4f18\u7684\uff0c\u8bc1\u660e\u4e86\u9057\u61be\u6700\u5c0f\u5316\u548c\u7236\u8282\u70b9\u8bc6\u522b\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u51b2\u7a81\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed5\u8fc7\u56fe\u6062\u590d\u7684\u8fd1\u4e4e\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u5956\u52b1\u7684\u7236\u8282\u70b9\u7136\u540e\u5e94\u7528\u7ecf\u5178\u5f3a\u76d7\u65b9\u6cd5\uff0c\u6216\u8054\u5408\u5b66\u4e60\u7236\u8282\u70b9\u540c\u65f6\u6700\u5c0f\u5316\u9057\u61be\u3002\u672c\u6587\u7814\u7a76\u8fd9\u4e9b\u7b56\u7565\u662f\u5426\u6700\u4f18\uff0c\u5e76\u63a2\u8ba8\u9057\u61be\u6700\u5c0f\u5316\u548c\u7236\u8282\u70b9\u8bc6\u522b\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u5b58\u5728\u5b9e\u4f8b\u8868\u660e\u9057\u61be\u6700\u5c0f\u5316\u548c\u7236\u8282\u70b9\u8bc6\u522b\u662f\u51b2\u7a81\u76ee\u6807\uff0c\u5efa\u7acb\u6355\u83b7\u52a8\u4f5c\u7a7a\u95f4\u7ec4\u5408\u7ed3\u6784\u7684\u65b0\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u7ed5\u8fc7\u56fe\u548c\u7236\u8282\u70b9\u6062\u590d\u7684\u8fd1\u4e4e\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5728\u5404\u79cd\u73af\u5883\u4e2d\uff0c\u672c\u6587\u65b9\u6cd5\u4e0e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u7236\u8282\u70b9\u8bc6\u522b\u5bf9\u4e8e\u9057\u61be\u6700\u5c0f\u5316\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u5b66\u4e60\u7236\u8282\u70b9\u96c6\u662f\u6b21\u4f18\u7b56\u7565\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5b9e\u73b0\u8fd1\u4e4e\u6700\u4f18\u7684\u9057\u61be\u6700\u5c0f\u5316\u3002"}}
{"id": "2510.16814", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16814", "abs": "https://arxiv.org/abs/2510.16814", "authors": ["Simon Jaxy", "Anton Theys", "Patrick Willett", "W. Chris Carleton", "Ralf Vandam", "Pieter Libin"], "title": "Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity", "comment": null, "summary": "Archaeological predictive modelling estimates where undiscovered sites are\nlikely to occur by combining known locations with environmental, cultural, and\ngeospatial variables. We address this challenge using a deep learning approach\nbut must contend with structural label scarcity inherent to archaeology:\npositives are rare, and most locations are unlabeled. To address this, we adopt\na semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a\nsemantic segmentation model and evaluated on two datasets covering a\nrepresentative range of archaeological periods. Our approach employs dynamic\npseudolabeling, refined with a Conditional Random Field (CRF) implemented via\nan RNN, increasing label confidence under severe class imbalance. On a\ngeospatial dataset derived from a digital elevation model (DEM), our model\nperforms on par with the state-of-the-art, LAMAP, while achieving higher Dice\nscores. On raw satellite imagery, assessed end-to-end with stratified k-fold\ncross-validation, it maintains performance and yields predictive surfaces with\nimproved interpretability. Overall, our results indicate that semi-supervised\nlearning offers a promising approach to identifying undiscovered sites across\nlarge, sparsely annotated landscapes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8003\u53e4\u9884\u6d4b\u6a21\u578b\uff0c\u91c7\u7528\u534a\u76d1\u7763\u6b63\u672a\u6807\u8bb0\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u4f2a\u6807\u7b7e\u548c\u6761\u4ef6\u968f\u673a\u573a\u5904\u7406\u8003\u53e4\u6570\u636e\u4e2d\u6807\u7b7e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u8003\u53e4\u9884\u6d4b\u5efa\u6a21\u9700\u8981\u4ece\u5df2\u77e5\u9057\u5740\u4f4d\u7f6e\u7ed3\u5408\u73af\u5883\u3001\u6587\u5316\u548c\u5730\u7406\u7a7a\u95f4\u53d8\u91cf\u6765\u9884\u6d4b\u672a\u77e5\u9057\u5740\u4f4d\u7f6e\uff0c\u4f46\u9762\u4e34\u6b63\u6837\u672c\u7a00\u5c11\u548c\u5927\u591a\u6570\u4f4d\u7f6e\u672a\u6807\u8bb0\u7684\u7ed3\u6784\u6027\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u6b63\u672a\u6807\u8bb0\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e3a\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u4f7f\u7528\u52a8\u6001\u4f2a\u6807\u7b7e\u548c\u901a\u8fc7RNN\u5b9e\u73b0\u7684\u6761\u4ef6\u968f\u673a\u573a\u6765\u63d0\u9ad8\u6807\u7b7e\u7f6e\u4fe1\u5ea6\uff0c\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u57fa\u4e8e\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684LAMAP\u76f8\u5f53\uff0c\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684Dice\u5206\u6570\uff1b\u5728\u539f\u59cb\u536b\u661f\u56fe\u50cf\u4e0a\uff0c\u901a\u8fc7\u5206\u5c42k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u4fdd\u6301\u6027\u80fd\u5e76\u4ea7\u751f\u5177\u6709\u66f4\u597d\u53ef\u89e3\u91ca\u6027\u7684\u9884\u6d4b\u8868\u9762\u3002", "conclusion": "\u534a\u76d1\u7763\u5b66\u4e60\u4e3a\u5728\u5927\u89c4\u6a21\u7a00\u758f\u6807\u6ce8\u666f\u89c2\u4e2d\u8bc6\u522b\u672a\u53d1\u73b0\u9057\u5740\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16820", "abs": "https://arxiv.org/abs/2510.16820", "authors": ["Thomas Dooms", "Ward Gauderis"], "title": "Finding Manifolds With Bilinear Autoencoders", "comment": null, "summary": "Sparse autoencoders are a standard tool for uncovering interpretable latent\nrepresentations in neural networks. Yet, their interpretation depends on the\ninputs, making their isolated study incomplete. Polynomials offer a solution;\nthey serve as algebraic primitives that can be analysed without reference to\ninput and can describe structures ranging from linear concepts to complicated\nmanifolds. This work uses bilinear autoencoders to efficiently decompose\nrepresentations into quadratic polynomials. We discuss improvements that induce\nimportance ordering, clustering, and activation sparsity. This is an initial\nstep toward nonlinear yet analysable latents through their algebraic\nproperties.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u53cc\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u5c06\u795e\u7ecf\u8868\u793a\u5206\u89e3\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\uff0c\u4f5c\u4e3a\u53ef\u5206\u6790\u7684\u975e\u7ebf\u6027\u6f5c\u5728\u8868\u793a\u5de5\u5177\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4f9d\u8d56\u8f93\u5165\u8fdb\u884c\u89e3\u91ca\uff0c\u5355\u72ec\u7814\u7a76\u4e0d\u5b8c\u6574\u3002\u591a\u9879\u5f0f\u4f5c\u4e3a\u4ee3\u6570\u57fa\u5143\uff0c\u65e0\u9700\u53c2\u8003\u8f93\u5165\u5373\u53ef\u5206\u6790\uff0c\u80fd\u591f\u63cf\u8ff0\u4ece\u7ebf\u6027\u6982\u5ff5\u5230\u590d\u6742\u6d41\u5f62\u7684\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u53cc\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u9ad8\u6548\u5730\u5c06\u8868\u793a\u5206\u89e3\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u65b9\u6cd5\u4ee5\u8bf1\u5bfc\u91cd\u8981\u6027\u6392\u5e8f\u3001\u805a\u7c7b\u548c\u6fc0\u6d3b\u7a00\u758f\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5c06\u795e\u7ecf\u8868\u793a\u5206\u89e3\u4e3a\u53ef\u5206\u6790\u7684\u4e8c\u6b21\u591a\u9879\u5f0f\u5f62\u5f0f\uff0c\u8fd9\u662f\u901a\u8fc7\u4ee3\u6570\u6027\u8d28\u5b9e\u73b0\u975e\u7ebf\u6027\u4f46\u53ef\u5206\u6790\u6f5c\u5728\u8868\u793a\u7684\u7b2c\u4e00\u6b65\u3002", "conclusion": "\u53cc\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u4e3a\u901a\u8fc7\u4ee3\u6570\u6027\u8d28\u7814\u7a76\u975e\u7ebf\u6027\u6f5c\u5728\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u662f\u8fc8\u5411\u53ef\u5206\u6790\u975e\u7ebf\u6027\u6f5c\u5728\u8868\u793a\u7684\u91cd\u8981\u521d\u6b65\u5de5\u4f5c\u3002"}}
{"id": "2510.16882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16882", "abs": "https://arxiv.org/abs/2510.16882", "authors": ["Heming Zou", "Yixiu Mao", "Yun Qu", "Qi Wang", "Xiangyang Ji"], "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) is a commonly used technique to adapt large\nlanguage models (LLMs) to downstream tasks. In practice, SFT on a full dataset\nis computationally expensive and sometimes suffers from overfitting or bias\namplification. This facilitates the rise of data curation in SFT, which\nprioritizes the most valuable data to optimze. This work studies the online\nbatch selection family that dynamically scores and filters samples during the\ntraining process. However, existing popular methods often (i) rely merely on\nthe utility of data to select a subset while neglecting other crucial factors\nlike diversity, (ii) rely on external resources such as reference models or\nvalidation sets, and (iii) incur extra training time over full-dataset\ntraining. To address these limitations, this work develops \\textbf{UDS\n(Utility-Diversity Sampling)}, a framework for efficient online batch selection\nin SFT. UDS leverages the nuclear norm of the logits matrix to capture both\ndata utility and intra-sample diversity, while estimating inter-sample\ndiversity through efficient low-dimensional embedding comparisons with a\nlightweight memory buffer of historical samples. Such a design eliminates the\nneed for external resources and unnecessary backpropagation, securing\ncomputational efficiency. Experiments on multiple benchmarks demonstrate that\nUDS consistently outperforms state-of-the-art online batch selection methods\nunder varying data budgets, and significantly reduces training time compared to\nfull-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UDS\uff08Utility-Diversity Sampling\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u8fdb\u884c\u9ad8\u6548\u7684\u5728\u7ebf\u6279\u6b21\u9009\u62e9\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u6570\u636e\u6548\u7528\u548c\u591a\u6837\u6027\u6765\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u7684\u5168\u6570\u636e\u96c6\u76d1\u7763\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u6216\u653e\u5927\u504f\u5dee\uff0c\u73b0\u6709\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(i)\u4ec5\u4f9d\u8d56\u6570\u636e\u6548\u7528\u800c\u5ffd\u7565\u591a\u6837\u6027\uff1b(ii)\u9700\u8981\u5916\u90e8\u8d44\u6e90\u5982\u53c2\u8003\u6a21\u578b\u6216\u9a8c\u8bc1\u96c6\uff1b(iii)\u8bad\u7ec3\u65f6\u95f4\u8d85\u8fc7\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "method": "UDS\u6846\u67b6\u5229\u7528\u5bf9\u6570\u77e9\u9635\u7684\u6838\u8303\u6570\u6355\u83b7\u6570\u636e\u6548\u7528\u548c\u6837\u672c\u5185\u591a\u6837\u6027\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4f4e\u7ef4\u5d4c\u5165\u6bd4\u8f83\u548c\u5386\u53f2\u6837\u672c\u7684\u8f7b\u91cf\u7ea7\u5185\u5b58\u7f13\u51b2\u533a\u4f30\u8ba1\u6837\u672c\u95f4\u591a\u6837\u6027\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u548c\u989d\u5916\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUDS\u5728\u4e0d\u540c\u6570\u636e\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u4e0e\u5168\u6570\u636e\u96c6\u5fae\u8c03\u76f8\u6bd4\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "UDS\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6570\u636e\u6548\u7528\u548c\u591a\u6837\u6027\uff0c\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2510.16980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16980", "abs": "https://arxiv.org/abs/2510.16980", "authors": ["Kanghui Ning", "Zijie Pan", "Yushan Jiang", "Anderson Schneider", "Yuriy Nevmyvaka", "Dongjin Song"], "title": "Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision", "comment": null, "summary": "Time series reasoning is emerging as the next frontier in temporal analysis,\naiming to move beyond pattern recognition towards explicit, interpretable, and\ntrustworthy inference. This paper presents a BlueSky vision built on two\ncomplementary directions. One builds robust foundations for time series\nreasoning, centered on comprehensive temporal understanding, structured\nmulti-step reasoning, and faithful evaluation frameworks. The other advances\nsystem-level reasoning, moving beyond language-only explanations by\nincorporating multi-agent collaboration, multi-modal context, and\nretrieval-augmented approaches. Together, these directions outline a flexible\nand extensible framework for advancing time series reasoning, aiming to deliver\ninterpretable and trustworthy temporal intelligence across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u613f\u666f\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\uff1a\u4e00\u662f\u5efa\u7acb\u7a33\u5065\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u57fa\u7840\uff0c\u4e8c\u662f\u63a8\u8fdb\u7cfb\u7edf\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6b63\u6210\u4e3a\u65f6\u5e8f\u5206\u6790\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\uff0c\u65e8\u5728\u8d85\u8d8a\u6a21\u5f0f\u8bc6\u522b\uff0c\u5b9e\u73b0\u663e\u5f0f\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u3002", "method": "\u6784\u5efa\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\uff1a1\uff09\u5efa\u7acb\u7a33\u5065\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u57fa\u7840\uff0c\u5305\u62ec\u5168\u9762\u7684\u65f6\u5e8f\u7406\u89e3\u3001\u7ed3\u6784\u5316\u591a\u6b65\u63a8\u7406\u548c\u5fe0\u5b9e\u8bc4\u4f30\u6846\u67b6\uff1b2\uff09\u63a8\u8fdb\u7cfb\u7edf\u7ea7\u63a8\u7406\uff0c\u8d85\u8d8a\u7eaf\u8bed\u8a00\u89e3\u91ca\uff0c\u6574\u5408\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u548c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u4e3a\u4e0d\u540c\u9886\u57df\u63d0\u4f9b\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u8d56\u7684\u65f6\u5e8f\u667a\u80fd\u3002"}}
{"id": "2510.16981", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16981", "abs": "https://arxiv.org/abs/2510.16981", "authors": ["Ahmed Khaled", "Kaan Ozkara", "Tao Yu", "Mingyi Hong", "Youngsuk Park"], "title": "MuonBP: Faster Muon via Block-Periodic Orthogonalization", "comment": null, "summary": "Gradient orthogonalization is a simple strategy that shows great utility in\nspeeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)\ncombines gradient orthogonalization with first-order momentum and achieves\nsignificant improvement in data efficiency over Adam/AdamW (Loshchilov and\nHutter, 2019) for language model training. However, when using model\nparallelism, gradient orthogonalization introduces additional overhead compared\nto coordinate-wise optimizers (such as AdamW) due to additional gather and\nscatter operations on gradient matrix shards from different devices. This\nadditional communication can amount to a throughput hit of 5%-10% compared to\nAdam/AdamW. To remedy this, we propose Muon with Block-Periodic\nOrthogonalization (MuonBP), which applies orthogonalization independently to\nmatrix shards on each device and periodically performs full orthogonalization\nto maintain training stability at scale. We show how to adjust the learning\nrate from the baseline to MuonBP and give convergence guarantees for this\nalgorithm. Crucially, our theory dictates that we use two stepsizes: one for\nthe blockwise orthogonalization steps, and one for the full orthogonalization\nsteps. Our method is simple, requires minimal hyperparameter adjustments, and\nachieves competitive iteration complexity compared with baseline Muon while\nproviding per-iteration throughput comparable to coordinate-wise methods such\nas AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO\noptimizer state sharding, MuonBP achieves 8% throughput increase compared to\nMuon with no degradation in performance.", "AI": {"tldr": "MuonBP\u4f18\u5316\u5668\u901a\u8fc7\u5206\u5757\u5468\u671f\u6027\u6b63\u4ea4\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u6301Muon\u4f18\u5316\u5668\u6570\u636e\u6548\u7387\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u4e0eAdamW\u76f8\u5f53\u7684\u541e\u5410\u91cf\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8eAdam/AdamW\u7684\u6570\u636e\u6548\u7387\uff0c\u4f46\u5728\u6a21\u578b\u5e76\u884c\u73af\u5883\u4e0b\uff0c\u68af\u5ea6\u6b63\u4ea4\u5316\u64cd\u4f5c\u5f15\u5165\u4e86\u989d\u5916\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5bfc\u81f45%-10%\u7684\u541e\u5410\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faMuonBP\u65b9\u6cd5\uff1a\u5728\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u72ec\u7acb\u5bf9\u77e9\u9635\u5206\u5757\u8fdb\u884c\u6b63\u4ea4\u5316\uff0c\u5e76\u5468\u671f\u6027\u6267\u884c\u5b8c\u6574\u6b63\u4ea4\u5316\u4ee5\u7ef4\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b\u4f7f\u7528\u4e24\u4e2a\u5b66\u4e60\u7387\u5206\u522b\u5904\u7406\u5206\u5757\u6b63\u4ea4\u5316\u6b65\u9aa4\u548c\u5b8c\u6574\u6b63\u4ea4\u5316\u6b65\u9aa4\u3002", "result": "\u57288B\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u4f7f\u7528\u516b\u8def\u5f20\u91cf\u5e76\u884c\u548cZeRO\u4f18\u5316\u5668\u72b6\u6001\u5206\u7247\uff0cMuonBP\u76f8\u6bd4Muon\u5b9e\u73b0\u4e868%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4e14\u6027\u80fd\u6ca1\u6709\u4e0b\u964d\u3002", "conclusion": "MuonBP\u5728\u4fdd\u6301Muon\u4f18\u5316\u5668\u6570\u636e\u6548\u7387\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5750\u6807\u4f18\u5316\u5668\u76f8\u5f53\u7684\u541e\u5410\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17040", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17040", "abs": "https://arxiv.org/abs/2510.17040", "authors": ["Hoang-Son Nguyen", "Xiao Fu"], "title": "Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability", "comment": "30 pages, 3 figures", "summary": "Latent component identification from unknown nonlinear mixtures is a\nfoundational challenge in machine learning, with applications in tasks such as\ndisentangled representation learning and causal inference. Prior work in\nnonlinear independent component analysis (nICA) has shown that auxiliary\nsignals -- such as weak supervision -- can support identifiability of\nconditionally independent latent components. More recent approaches explore\nstructural assumptions, e.g., sparsity in the Jacobian of the mixing function,\nto relax such requirements. In this work, we introduce Diverse Influence\nComponent Analysis (DICA), a framework that exploits the convex geometry of the\nmixing function's Jacobian. We propose a Jacobian Volume Maximization\n(J-VolMax) criterion, which enables latent component identification by\nencouraging diversity in their influence on the observed variables. Under\nreasonable conditions, this approach achieves identifiability without relying\non auxiliary information, latent component independence, or Jacobian sparsity\nassumptions. These results extend the scope of identifiability analysis and\noffer a complementary perspective to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Diverse Influence Component Analysis (DICA)\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6df7\u5408\u51fd\u6570\u96c5\u53ef\u6bd4\u77e9\u9635\u4f53\u79ef\u6765\u8bc6\u522b\u975e\u7ebf\u6027\u6df7\u5408\u4e2d\u7684\u6f5c\u5728\u6210\u5206\uff0c\u65e0\u9700\u8f85\u52a9\u4fe1\u53f7\u3001\u6f5c\u5728\u6210\u5206\u72ec\u7acb\u6027\u6216\u96c5\u53ef\u6bd4\u7a00\u758f\u6027\u5047\u8bbe\u3002", "motivation": "\u975e\u7ebf\u6027\u72ec\u7acb\u6210\u5206\u5206\u6790(nICA)\u4e2d\uff0c\u4ece\u672a\u77e5\u975e\u7ebf\u6027\u6df7\u5408\u4e2d\u8bc6\u522b\u6f5c\u5728\u6210\u5206\u662f\u4e00\u4e2a\u57fa\u7840\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8f85\u52a9\u4fe1\u53f7\u6216\u7ed3\u6784\u5047\u8bbe\u6765\u652f\u6301\u53ef\u8bc6\u522b\u6027\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e0d\u4f9d\u8d56\u8fd9\u4e9b\u5047\u8bbe\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDICA\u6846\u67b6\u548c\u96c5\u53ef\u6bd4\u4f53\u79ef\u6700\u5927\u5316(J-VolMax)\u51c6\u5219\uff0c\u5229\u7528\u6df7\u5408\u51fd\u6570\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u51f8\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc7\u4fc3\u8fdb\u6f5c\u5728\u6210\u5206\u5bf9\u89c2\u6d4b\u53d8\u91cf\u7684\u5f71\u54cd\u591a\u6837\u6027\u6765\u5b9e\u73b0\u8bc6\u522b\u3002", "result": "\u5728\u5408\u7406\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u9700\u8f85\u52a9\u4fe1\u606f\u3001\u6f5c\u5728\u6210\u5206\u72ec\u7acb\u6027\u6216\u96c5\u53ef\u6bd4\u7a00\u758f\u6027\u5047\u8bbe\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u6269\u5c55\u4e86\u53ef\u8bc6\u522b\u6027\u5206\u6790\u7684\u8303\u56f4\u3002", "conclusion": "DICA\u6846\u67b6\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8865\u5145\u89c6\u89d2\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u51e0\u4f55\u7279\u6027\u5b9e\u73b0\u4e86\u66f4\u5e7f\u6cdb\u6761\u4ef6\u4e0b\u7684\u6f5c\u5728\u6210\u5206\u8bc6\u522b\u3002"}}
{"id": "2510.17212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17212", "abs": "https://arxiv.org/abs/2510.17212", "authors": ["Jundong Zhang", "Yuhui Situ", "Fanji Zhang", "Rongji Deng", "Tianqi Wei"], "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks", "comment": null, "summary": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle\ncrossing, often exhibit multimodal action distributions and stochastic returns.\nMost reinforcement learning (RL) methods assume unimodal Gaussian policies and\nrely on scalar-valued critics, which limits their effectiveness in HRHR\nsettings. We formally define HRHR tasks and theoretically show that Gaussian\npolicies cannot guarantee convergence to the optimal solution. To address this,\nwe propose a reinforcement learning framework that (i) discretizes continuous\naction spaces to approximate multimodal distributions, (ii) employs\nentropy-regularized exploration to improve coverage of risky but rewarding\nactions, and (iii) introduces a dual-critic architecture for more accurate\ndiscrete value distribution estimation. The framework scales to\nhigh-dimensional action spaces, supporting complex control domains. Experiments\non locomotion and manipulation benchmarks with high risks of failure\ndemonstrate that our method outperforms baselines, underscoring the importance\nof explicitly modeling multimodality and risk in RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3001\u71b5\u6b63\u5219\u5316\u63a2\u7d22\u548c\u53cc\u8bc4\u8bba\u5bb6\u67b6\u6784\u6765\u89e3\u51b3\u4f20\u7edf\u9ad8\u65af\u7b56\u7565\u5728HRHR\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u5355\u5cf0\u9ad8\u65af\u7b56\u7565\u548c\u6807\u91cf\u503c\u8bc4\u8bba\u5bb6\uff0c\u5728\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u4efb\u52a1\u901a\u5e38\u5177\u6709\u591a\u5cf0\u52a8\u4f5c\u5206\u5e03\u548c\u968f\u673a\u56de\u62a5\u3002", "method": "1) \u79bb\u6563\u5316\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u8fd1\u4f3c\u591a\u5cf0\u5206\u5e03\uff1b2) \u4f7f\u7528\u71b5\u6b63\u5219\u5316\u63a2\u7d22\u63d0\u9ad8\u5bf9\u9ad8\u98ce\u9669\u4f46\u9ad8\u56de\u62a5\u52a8\u4f5c\u7684\u8986\u76d6\uff1b3) \u5f15\u5165\u53cc\u8bc4\u8bba\u5bb6\u67b6\u6784\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u79bb\u6563\u503c\u5206\u5e03\u4f30\u8ba1\u3002", "result": "\u5728\u5177\u6709\u9ad8\u5931\u8d25\u98ce\u9669\u7684\u79fb\u52a8\u548c\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u591a\u5cf0\u6027\u548c\u98ce\u9669\u5efa\u6a21\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6269\u5c55\u5230\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\uff0c\u652f\u6301\u590d\u6742\u63a7\u5236\u9886\u57df\uff0c\u4e3a\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17160", "abs": "https://arxiv.org/abs/2510.17160", "authors": ["Derda Kaymak", "Gyuhak Kim", "Tomoya Kaichi", "Tatsuya Konishi", "Bing Liu"], "title": "Learning After Model Deployment", "comment": "Published at ECAI-2025", "summary": "In classic supervised learning, once a model is deployed in an application,\nit is fixed. No updates will be made to it during the application. This is\ninappropriate for many dynamic and open environments, where unexpected samples\nfrom unseen classes may appear. In such an environment, the model should be\nable to detect these novel samples from unseen classes and learn them after\nthey are labeled. We call this paradigm Autonomous Learning after Model\nDeployment (ALMD). The learning here is continuous and involves no human\nengineers. Labeling in this scenario is performed by human co-workers or other\nknowledgeable agents, which is similar to what humans do when they encounter an\nunfamiliar object and ask another person for its name. In ALMD, the detection\nof novel samples is dynamic and differs from traditional out-of-distribution\n(OOD) detection in that the set of in-distribution (ID) classes expands as new\nclasses are learned during application, whereas ID classes is fixed in\ntraditional OOD detection. Learning is also different from classic supervised\nlearning because in ALMD, we learn the encountered new classes immediately and\nincrementally. It is difficult to retrain the model from scratch using all the\npast data from the ID classes and the novel samples from newly discovered\nclasses, as this would be resource- and time-consuming. Apart from these two\nchallenges, ALMD faces the data scarcity issue because instances of new classes\noften appear sporadically in real-life applications. To address these issues,\nwe propose a novel method, PLDA, which performs dynamic OOD detection and\nincremental learning of new classes on the fly. Empirical evaluations will\ndemonstrate the effectiveness of PLDA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u4e3b\u6a21\u578b\u90e8\u7f72\u540e\u5b66\u4e60\uff08ALMD\uff09\u8303\u5f0f\uff0c\u89e3\u51b3\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u5728\u52a8\u6001\u73af\u5883\u4e2d\u65e0\u6cd5\u5904\u7406\u672a\u77e5\u7c7b\u522b\u6837\u672c\u7684\u95ee\u9898\u3002ALMD\u80fd\u591f\u52a8\u6001\u68c0\u6d4b\u65b0\u7c7b\u522b\u6837\u672c\u5e76\u8fdb\u884c\u589e\u91cf\u5b66\u4e60\uff0c\u65e0\u9700\u4eba\u5de5\u5de5\u7a0b\u5e08\u53c2\u4e0e\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u540e\u56fa\u5b9a\u4e0d\u53d8\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5f00\u653e\u73af\u5883\u4e2d\u51fa\u73b0\u7684\u672a\u77e5\u7c7b\u522b\u6837\u672c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u5e94\u7528\u8fc7\u7a0b\u4e2d\u6301\u7eed\u68c0\u6d4b\u65b0\u7c7b\u522b\u5e76\u5b66\u4e60\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faPLDA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u52a8\u6001OOD\u68c0\u6d4b\u548c\u589e\u91cf\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u65b0\u7c7b\u522b\uff0c\u65e0\u9700\u4f7f\u7528\u6240\u6709\u5386\u53f2\u6570\u636e\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u5c06\u8bc1\u660ePLDA\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "ALMD\u8303\u5f0f\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0cPLDA\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u65b0\u7c7b\u522b\u68c0\u6d4b\u548c\u589e\u91cf\u5b66\u4e60\u7684\u6311\u6218\u3002"}}
{"id": "2510.17189", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17189", "abs": "https://arxiv.org/abs/2510.17189", "authors": ["Wenxun Wang", "Shuchang Zhou", "Wenyu Sun", "Peiqin Sun", "Yongpan Liu"], "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "comment": null, "summary": "Transformers have shown remarkable performance in both natural language\nprocessing (NLP) and computer vision (CV) tasks. However, their real-time\ninference speed and efficiency are limited due to the inefficiency in Softmax\nand Layer Normalization (LayerNorm). Previous works based on function\napproximation suffer from inefficient implementation as they place emphasis on\ncomputation while disregarding memory overhead concerns. Moreover, such methods\nrely on retraining to compensate for approximation error which can be costly\nand inconvenient.\n  In this paper, we present SOLE, a hardware-software co-design for Softmax and\nLayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes\nlog2 quantization of exponent function and log-based division to approximate\nSoftmax while AILayerNorm adopts low-precision statistic calculation. Compared\nwith state-of-the-art designs, we achieve both low-precision calculation and\nlow bit-width storage on Softmax and LayerNorm. Experiments show that SOLE\nmaintains inference accuracy without retraining while offering orders of\nmagnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x\nenergy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements\nover prior state-of-the-art custom hardware for Softmax and LayerNorm,\nrespectively.", "AI": {"tldr": "SOLE\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7E2Softmax\u548cAILayerNorm\u5206\u522b\u4f18\u5316Transformer\u4e2d\u7684Softmax\u548cLayerNorm\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "Transformer\u6a21\u578b\u5728NLP\u548cCV\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u548c\u6548\u7387\u53d7\u5230Softmax\u548cLayerNorm\u64cd\u4f5c\u6548\u7387\u4f4e\u4e0b\u7684\u9650\u5236\u3002\u73b0\u6709\u57fa\u4e8e\u51fd\u6570\u903c\u8fd1\u7684\u65b9\u6cd5\u5b58\u5728\u5b9e\u73b0\u6548\u7387\u4f4e\u3001\u5ffd\u7565\u5185\u5b58\u5f00\u9500\u3001\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faSOLE\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff1aE2Softmax\u91c7\u7528\u6307\u6570\u51fd\u6570\u7684log2\u91cf\u5316\u548c\u57fa\u4e8e\u5bf9\u6570\u7684\u9664\u6cd5\u6765\u903c\u8fd1Softmax\uff1bAILayerNorm\u91c7\u7528\u4f4e\u7cbe\u5ea6\u7edf\u8ba1\u8ba1\u7b97\u3002\u4e24\u8005\u90fd\u5b9e\u73b0\u4e86\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u548c\u4f4e\u6bd4\u7279\u4f4d\u5bbd\u5b58\u50a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSOLE\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\uff0c\u76f8\u6bd4GPU\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u548c\u80fd\u8017\u8282\u7701\u3002\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u5b9a\u5236\u786c\u4ef6\uff0cSoftmax\u548cLayerNorm\u5206\u522b\u5b9e\u73b0\u4e863.04\u500d\u548c3.86\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u4ee5\u53ca2.82\u500d\u548c3.32\u500d\u7684\u9762\u79ef\u6548\u7387\u63d0\u5347\u3002", "conclusion": "SOLE\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86Transformer\u4e2dSoftmax\u548cLayerNorm\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u80fd\u6548\u3002"}}
{"id": "2510.17671", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17671", "abs": "https://arxiv.org/abs/2510.17671", "authors": ["Katarzyna Kobalczyk", "Zhiyuan Jerry Lin", "Benjamin Letham", "Zhuokai Zhao", "Maximilian Balandat", "Eytan Bakshy"], "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "comment": null, "summary": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5728\u73af\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u8f6c\u6362\u4e3a\u6807\u91cf\u6548\u7528\uff0c\u4ee5\u5728\u6570\u503c\u641c\u7d22\u7a7a\u95f4\u4e0a\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "motivation": "\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53cd\u9988\u5bf9\u4e8e\u5c06\u590d\u6742\u3001\u7ec6\u5fae\u6216\u4e3b\u89c2\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u7684\u4f18\u5316\u76ee\u6807\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u63a5\u53d7\u53d7\u9650\u7684\u53cd\u9988\u683c\u5f0f\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u9886\u57df\u7279\u5b9a\u95ee\u9898\u5b9a\u5236\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u5404\u79cd\u7c7b\u578b\u7684\u6587\u672c\u53cd\u9988\u8f6c\u6362\u4e3a\u4e00\u81f4\u7684\u6548\u7528\u4fe1\u53f7\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u6838\u51fd\u6570\u5373\u53ef\u5305\u542b\u7075\u6d3b\u7684\u7528\u6237\u5148\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6837\u672c\u6548\u7387\u548c\u539f\u5219\u6027\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u63a5\u53e3\uff0c\u800c\u4e14\u5728\u53cd\u9988\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u57fa\u7ebf\u548c\u4ec5\u4f7f\u7528LLM\u7684\u4f18\u5316\u5668\u3002", "conclusion": "\u8bed\u8a00\u5728\u73af\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u4f18\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2510.17709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17709", "abs": "https://arxiv.org/abs/2510.17709", "authors": ["Akhil S Anand", "Shambhuraj Sawant", "Jasper Hoffmann", "Dirk Reinhardt", "Sebastien Gros"], "title": "Closing the Sim2Real Performance Gap in RL", "comment": null, "summary": "Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u8c03\u6574\u6a21\u62df\u5668\u53c2\u6570\u6765\u89e3\u51b3Sim2Real\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6a21\u62df\u5668\u7cbe\u5ea6\u548cSim2Real RL\u65b9\u6cd5\u4e0d\u65ad\u53d1\u5c55\uff0c\u4f46\u7eaf\u6a21\u62df\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u4ecd\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u4f18\u5316\u7684\u6a21\u62df\u5668\u7cbe\u5ea6\u548c\u53d8\u5f02\u6027\u6307\u6807\u4e0e\u7b56\u7565\u7684\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4e0d\u4e00\u5b9a\u76f8\u5173\u3002", "method": "\u91c7\u7528\u53cc\u5c42RL\u6846\u67b6\uff1a\u5185\u5c42RL\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5916\u5c42RL\u8c03\u6574\u6a21\u62df\u6a21\u578b\u548c\u6a21\u62df\u5185\u5956\u52b1\u53c2\u6570\uff0c\u4ee5\u6700\u5927\u5316\u6a21\u62df\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u63a8\u5bfc\u5e76\u9a8c\u8bc1\u4e86\u5f00\u53d1\u80fd\u591f\u7f29\u5c0fSim2Real\u6027\u80fd\u5dee\u8ddd\u7684\u53cc\u5c42RL\u7b97\u6cd5\u6240\u9700\u7684\u6570\u5b66\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4f18\u5316\u6a21\u62df\u5668\u53c2\u6570\uff0c\u4e3a\u89e3\u51b3Sim2Real\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.17313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17313", "abs": "https://arxiv.org/abs/2510.17313", "authors": ["Tal Barami", "Nimrod Berman", "Ilan Naiman", "Amos H. Hason", "Rotem Ezra", "Omri Azencot"], "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations", "comment": null, "summary": "Learning disentangled representations in sequential data is a key goal in\ndeep learning, with broad applications in vision, audio, and time series. While\nreal-world data involves multiple interacting semantic factors over time, prior\nwork has mostly focused on simpler two-factor static and dynamic settings,\nprimarily because such settings make data collection easier, thereby\noverlooking the inherently multi-factor nature of real-world data. We introduce\nthe first standardized benchmark for evaluating multi-factor sequential\ndisentanglement across six diverse datasets spanning video, audio, and time\nseries. Our benchmark includes modular tools for dataset integration, model\ndevelopment, and evaluation metrics tailored to multi-factor analysis. We\nadditionally propose a post-hoc Latent Exploration Stage to automatically align\nlatent dimensions with semantic factors, and introduce a Koopman-inspired model\nthat achieves state-of-the-art results. Moreover, we show that Vision-Language\nModels can automate dataset annotation and serve as zero-shot disentanglement\nevaluators, removing the need for manual labels and human intervention.\nTogether, these contributions provide a robust and scalable foundation for\nadvancing multi-factor sequential disentanglement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u56e0\u7d20\u5e8f\u5217\u89e3\u7f20\u7ed3\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5305\u542b\u516d\u4e2a\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u81ea\u52a8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u6807\u6ce8\u548c\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5305\u542b\u591a\u4e2a\u76f8\u4e92\u4f5c\u7528\u7684\u8bed\u4e49\u56e0\u7d20\uff0c\u4f46\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u7684\u53cc\u56e0\u7d20\u9759\u6001\u548c\u52a8\u6001\u8bbe\u7f6e\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u7684\u591a\u56e0\u7d20\u672c\u8d28\u3002", "method": "\u5f15\u5165\u6807\u51c6\u5316\u57fa\u51c6\u3001\u540e\u9a8c\u6f5c\u5728\u63a2\u7d22\u9636\u6bb5\u81ea\u52a8\u5bf9\u9f50\u6f5c\u5728\u7ef4\u5ea6\u4e0e\u8bed\u4e49\u56e0\u7d20\uff0c\u63d0\u51faKoopman\u542f\u53d1\u7684\u6a21\u578b\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\u548c\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684Koopman\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6807\u6ce8\u548c\u4f5c\u4e3a\u89e3\u7f20\u7ed3\u8bc4\u4f30\u5668\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u4e3a\u63a8\u8fdb\u591a\u56e0\u7d20\u5e8f\u5217\u89e3\u7f20\u7ed3\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.17802", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17802", "abs": "https://arxiv.org/abs/2510.17802", "authors": ["Rui Pan", "Yang Luo", "Yuxing Liu", "Yang You", "Tong Zhang"], "title": "Unbiased Gradient Low-Rank Projection", "comment": null, "summary": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGUM\u7684\u65e0\u504f\u4f4e\u79e9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u91c7\u6837\u6280\u672f\u6d88\u9664\u68af\u5ea6\u4f4e\u79e9\u6295\u5f71\u7684\u504f\u5dee\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u57fa\u7840\u4f18\u5316\u7b97\u6cd5\u76f8\u540c\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u5728LLM\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5168\u53c2\u6570\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u68af\u5ea6\u4f4e\u79e9\u6295\u5f71\u65b9\u6cd5\uff08\u5982GaLore\uff09\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\u548c\u5b58\u5728\u56fa\u6709\u504f\u5dee\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u504f\u5dee\u5bfc\u81f4\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8eGaLore\u673a\u5236\u548cMuon\u7b97\u6cd5\uff0c\u91c7\u7528\u5c42\u95f4\u91c7\u6837\u6280\u672f\u6765\u6d88\u9664\u4f4e\u79e9\u6295\u5f71\u7684\u504f\u5dee\uff0c\u63d0\u51faGaLore Unbiased with Muon (GUM)\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660eGUM\u65b9\u6cd5\u5339\u914d\u57fa\u7840Muon\u7b97\u6cd5\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u79e9\u6280\u672f\u7684\u5185\u5b58\u6548\u7387\u3002\u5b9e\u8bc1\u5b9e\u9a8c\u663e\u793a\u5728LLM\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u4e2d\u4f18\u4e8eGaLore\uff0c\u751a\u81f3\u8d85\u8fc7\u5168\u53c2\u6570\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u6280\u672f\u901a\u8fc7\u66f4\u5747\u5300\u7684\u5c42\u5185\u77e5\u8bc6\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u53c2\u6570\u7a7a\u95f4\u7684\u66f4\u9ad8\u6548\u5229\u7528\u548c\u66f4\u597d\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4e3a\u5185\u5b58\u9ad8\u6548\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17562", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17562", "abs": "https://arxiv.org/abs/2510.17562", "authors": ["Dennis Wagner", "Arjun Nair", "Billy Joe Franks", "Justus Arweiler", "Aparna Muraleedharan", "Indra Jungjohann", "Fabian Hartung", "Mayank C. Ahuja", "Andriy Balinskyy", "Saurabh Varshneya", "Nabeel Hussain Syed", "Mayank Nagda", "Phillip Liznerski", "Steffen Reithermann", "Maja Rudolph", "Sebastian Vollmer", "Ralf Schulz", "Torsten Katz", "Stephan Mandt", "Michael Bortz", "Heike Leitte", "Daniel Neider", "Jakob Burger", "Fabian Jirasek", "Hans Hasse", "Sophie Fellenz", "Marius Kloft"], "title": "Formally Exploring Time-Series Anomaly Detection Evaluation Metrics", "comment": "73 pages, 13 figures", "summary": "Undetected anomalies in time series can trigger catastrophic failures in\nsafety-critical systems, such as chemical plant explosions or power grid\noutages. Although many detection methods have been proposed, their performance\nremains unclear because current metrics capture only narrow aspects of the task\nand often yield misleading results. We address this issue by introducing\nverifiable properties that formalize essential requirements for evaluating\ntime-series anomaly detection. These properties enable a theoretical framework\nthat supports principled evaluations and reliable comparisons. Analyzing 37\nwidely used metrics, we show that most satisfy only a few properties, and none\nsatisfy all, explaining persistent inconsistencies in prior results. To close\nthis gap, we propose LARM, a flexible metric that provably satisfies all\nproperties, and extend it to ALARM, an advanced variant meeting stricter\nrequirements.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u53ef\u9a8c\u8bc1\u7684\u5c5e\u6027\u6765\u5f62\u5f0f\u5316\u8bc4\u4f30\u8981\u6c42\uff0c\u5206\u6790\u4e8637\u4e2a\u5e38\u7528\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6ee1\u8db3\u6240\u6709\u5c5e\u6027\u7684\u65b0\u6307\u6807LARM\u53ca\u5176\u6269\u5c55ALARM\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u672a\u68c0\u6d4b\u5f02\u5e38\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u707e\u96be\u6027\u6545\u969c\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u8bc4\u4f30\u5b58\u5728\u7f3a\u9677\uff0c\u56e0\u4e3a\u5f53\u524d\u6307\u6807\u4ec5\u6355\u6349\u4efb\u52a1\u7684\u72ed\u7a84\u65b9\u9762\u4e14\u7ecf\u5e38\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "method": "\u5f15\u5165\u53ef\u9a8c\u8bc1\u5c5e\u6027\u6765\u5f62\u5f0f\u5316\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u672c\u8981\u6c42\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u652f\u6301\u539f\u5219\u6027\u8bc4\u4f30\u548c\u53ef\u9760\u6bd4\u8f83\uff0c\u5206\u679037\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u6ee1\u8db3\u6240\u6709\u5c5e\u6027\u7684\u65b0\u6307\u6807LARM\u53ca\u5176\u6269\u5c55ALARM\u3002", "result": "\u5206\u6790\u663e\u793a\u5927\u591a\u6570\u73b0\u6709\u6307\u6807\u4ec5\u6ee1\u8db3\u5c11\u6570\u5c5e\u6027\uff0c\u6ca1\u6709\u4e00\u4e2a\u6307\u6807\u6ee1\u8db3\u6240\u6709\u5c5e\u6027\uff0c\u8fd9\u89e3\u91ca\u4e86\u5148\u524d\u7ed3\u679c\u4e2d\u6301\u7eed\u5b58\u5728\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684LARM\u548cALARM\u6307\u6807\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7684\u7a7a\u767d\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.17690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17690", "abs": "https://arxiv.org/abs/2510.17690", "authors": ["Xihong Su"], "title": "Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning", "comment": "Dissertation", "summary": "This dissertation makes three main contributions. First, We identify a new\nconnection between policy gradient and dynamic programming in MMDPs and propose\nthe Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov\npolicy that maximizes the discounted return averaged over the uncertain models.\nCADP adjusts model weights iteratively to guarantee monotone policy\nimprovements to a local maximum. Second, We establish sufficient and necessary\nconditions for the exponential ERM Bellman operator to be a contraction and\nprove the existence of stationary deterministic optimal policies for ERM-TRC\nand EVaR-TRC. We also propose exponential value iteration, policy iteration,\nand linear programming algorithms for computing optimal stationary policies for\nERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for\ncomputing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The\nchallenge is that Q-learning ERM Bellman may not be a contraction. Instead, we\nuse the monotonicity of Q-learning ERM Bellman operators to derive a rigorous\nproof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the\noptimal risk-averse value functions. The proposed Q-learning algorithms compute\nthe optimal stationary policy for ERM-TRC and EVaR-TRC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u4e3b\u8981\u8d21\u732e\uff1aCADP\u7b97\u6cd5\u8fde\u63a5\u7b56\u7565\u68af\u5ea6\u4e0e\u52a8\u6001\u89c4\u5212\uff0c\u5efa\u7acb\u4e86ERM Bellman\u7b97\u5b50\u7684\u6536\u7f29\u6761\u4ef6\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u7b97\u6cd5\uff0c\u4ee5\u53ca\u63d0\u51fa\u4e86\u7528\u4e8e\u98ce\u9669\u89c4\u907f\u76ee\u6807\u7684Q\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\u548c\u98ce\u9669\u89c4\u907f\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u578bMDPs\u4e2d\u5bfb\u627e\u6700\u4f18\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1aCADP\u7b97\u6cd5\u8fed\u4ee3\u8c03\u6574\u6a21\u578b\u6743\u91cd\uff1b\u5efa\u7acbERM Bellman\u7b97\u5b50\u7684\u6536\u7f29\u6761\u4ef6\uff1b\u63d0\u51fa\u6307\u6570\u503c\u8fed\u4ee3\u3001\u7b56\u7565\u8fed\u4ee3\u548c\u7ebf\u6027\u89c4\u5212\u7b97\u6cd5\uff1b\u5f00\u53d1\u6a21\u578b\u65e0\u5173\u7684Q\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1aCADP\u80fd\u4fdd\u8bc1\u7b56\u7565\u5355\u8c03\u6539\u8fdb\u5230\u5c40\u90e8\u6700\u4f18\uff1b\u8bc1\u660e\u4e86ERM-TRC\u548cEVaR-TRC\u5b58\u5728\u786e\u5b9a\u6027\u6700\u4f18\u7b56\u7565\uff1bQ\u5b66\u4e60\u7b97\u6cd5\u80fd\u6536\u655b\u5230\u6700\u4f18\u98ce\u9669\u89c4\u907f\u503c\u51fd\u6570\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6210\u529f\u5efa\u7acb\u4e86\u7b56\u7565\u68af\u5ea6\u4e0e\u52a8\u6001\u89c4\u5212\u7684\u65b0\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u8ba1\u7b97\u98ce\u9669\u89c4\u907f\u6700\u4f18\u7b56\u7565\u7684\u6709\u6548\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002"}}
